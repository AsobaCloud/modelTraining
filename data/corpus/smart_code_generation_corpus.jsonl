{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create REST API endpoint with proper request/response handling using Flask/FastAPI decorators\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from fastapi import FastAPI\nfrom fastapi.responses import JSONResponse\nfrom pydantic import BaseModel\n\n\nclass Item(BaseModel):\n    id: str\n    value: str\n\n\nclass Message(BaseModel):\n    message: str\n\n\napp = FastAPI()\n\n\n@app.get(\"/items/{item_id}\", response_model=Item, responses={404: {\"model\": Message}})\nasync def read_item(item_id: str):\n    if item_id == \"foo\":\n        return {\"id\": \"foo\", \"value\": \"there goes my hero\"}\n    return JSONResponse(status_code=404, content={\"message\": \"Item not found\"})\n", "metadata": {"source": "github.com/fastapi/fastapi", "file_path": "docs_src/additional_responses/tutorial001.py", "language": "python", "complexity": "simple", "star_count": 87642, "quality_score": 0.92, "features": {"has_classes": true, "has_functions": true, "has_async": true, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 1, "class_count": 2, "line_count": 23}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create REST API endpoint with proper request/response handling using Flask/FastAPI decorators\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from typing import Union\n\nfrom fastapi import FastAPI\nfrom fastapi.responses import FileResponse\nfrom pydantic import BaseModel\n\n\nclass Item(BaseModel):\n    id: str\n    value: str\n\n\napp = FastAPI()\n\n\n@app.get(\n    \"/items/{item_id}\",\n    response_model=Item,\n    responses={\n        200: {\n            \"content\": {\"image/png\": {}},\n            \"description\": \"Return the JSON item or an image.\",\n        }\n    },\n)\nasync def read_item(item_id: str, img: Union[bool, None] = None):\n    if img:\n        return FileResponse(\"image.png\", media_type=\"image/png\")\n    else:\n        return {\"id\": \"foo\", \"value\": \"there goes my hero\"}\n", "metadata": {"source": "github.com/fastapi/fastapi", "file_path": "docs_src/additional_responses/tutorial002.py", "language": "python", "complexity": "simple", "star_count": 87642, "quality_score": 0.92, "features": {"has_classes": true, "has_functions": true, "has_async": true, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 1, "class_count": 1, "line_count": 31}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create REST API endpoint with proper request/response handling using Flask/FastAPI decorators\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from fastapi import FastAPI\nfrom fastapi.responses import JSONResponse\nfrom pydantic import BaseModel\n\n\nclass Item(BaseModel):\n    id: str\n    value: str\n\n\nclass Message(BaseModel):\n    message: str\n\n\napp = FastAPI()\n\n\n@app.get(\n    \"/items/{item_id}\",\n    response_model=Item,\n    responses={\n        404: {\"model\": Message, \"description\": \"The item was not found\"},\n        200: {\n            \"description\": \"Item requested by ID\",\n            \"content\": {\n                \"application/json\": {\n                    \"example\": {\"id\": \"bar\", \"value\": \"The bar tenders\"}\n                }\n            },\n        },\n    },\n)\nasync def read_item(item_id: str):\n    if item_id == \"foo\":\n        return {\"id\": \"foo\", \"value\": \"there goes my hero\"}\n    else:\n        return JSONResponse(status_code=404, content={\"message\": \"Item not found\"})\n", "metadata": {"source": "github.com/fastapi/fastapi", "file_path": "docs_src/additional_responses/tutorial003.py", "language": "python", "complexity": "simple", "star_count": 87642, "quality_score": 0.92, "features": {"has_classes": true, "has_functions": true, "has_async": true, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 1, "class_count": 2, "line_count": 38}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create REST API endpoint with proper request/response handling using Flask/FastAPI decorators\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from typing import Union\n\nfrom fastapi import FastAPI\nfrom fastapi.responses import FileResponse\nfrom pydantic import BaseModel\n\n\nclass Item(BaseModel):\n    id: str\n    value: str\n\n\nresponses = {\n    404: {\"description\": \"Item not found\"},\n    302: {\"description\": \"The item was moved\"},\n    403: {\"description\": \"Not enough privileges\"},\n}\n\n\napp = FastAPI()\n\n\n@app.get(\n    \"/items/{item_id}\",\n    response_model=Item,\n    responses={**responses, 200: {\"content\": {\"image/png\": {}}}},\n)\nasync def read_item(item_id: str, img: Union[bool, None] = None):\n    if img:\n        return FileResponse(\"image.png\", media_type=\"image/png\")\n    else:\n        return {\"id\": \"foo\", \"value\": \"there goes my hero\"}\n", "metadata": {"source": "github.com/fastapi/fastapi", "file_path": "docs_src/additional_responses/tutorial004.py", "language": "python", "complexity": "simple", "star_count": 87642, "quality_score": 0.92, "features": {"has_classes": true, "has_functions": true, "has_async": true, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 1, "class_count": 1, "line_count": 33}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create REST API endpoint with proper request/response handling with asyncio coroutines\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from typing import Union\n\nfrom fastapi import Body, FastAPI, status\nfrom fastapi.responses import JSONResponse\n\napp = FastAPI()\n\nitems = {\"foo\": {\"name\": \"Fighters\", \"size\": 6}, \"bar\": {\"name\": \"Tenders\", \"size\": 3}}\n\n\n@app.put(\"/items/{item_id}\")\nasync def upsert_item(\n    item_id: str,\n    name: Union[str, None] = Body(default=None),\n    size: Union[int, None] = Body(default=None),\n):\n    if item_id in items:\n        item = items[item_id]\n        item[\"name\"] = name\n        item[\"size\"] = size\n        return item\n    else:\n        item = {\"name\": name, \"size\": size}\n        items[item_id] = item\n        return JSONResponse(status_code=status.HTTP_201_CREATED, content=item)\n", "metadata": {"source": "github.com/fastapi/fastapi", "file_path": "docs_src/additional_status_codes/tutorial001.py", "language": "python", "complexity": "simple", "star_count": 87642, "quality_score": 0.89, "features": {"has_classes": false, "has_functions": true, "has_async": true, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 1, "class_count": 0, "line_count": 26}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create REST API endpoint with proper request/response handling with asyncio coroutines\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from typing import Union\n\nfrom fastapi import Body, FastAPI, status\nfrom fastapi.responses import JSONResponse\nfrom typing_extensions import Annotated\n\napp = FastAPI()\n\nitems = {\"foo\": {\"name\": \"Fighters\", \"size\": 6}, \"bar\": {\"name\": \"Tenders\", \"size\": 3}}\n\n\n@app.put(\"/items/{item_id}\")\nasync def upsert_item(\n    item_id: str,\n    name: Annotated[Union[str, None], Body()] = None,\n    size: Annotated[Union[int, None], Body()] = None,\n):\n    if item_id in items:\n        item = items[item_id]\n        item[\"name\"] = name\n        item[\"size\"] = size\n        return item\n    else:\n        item = {\"name\": name, \"size\": size}\n        items[item_id] = item\n        return JSONResponse(status_code=status.HTTP_201_CREATED, content=item)\n", "metadata": {"source": "github.com/fastapi/fastapi", "file_path": "docs_src/additional_status_codes/tutorial001_an.py", "language": "python", "complexity": "simple", "star_count": 87642, "quality_score": 0.89, "features": {"has_classes": false, "has_functions": true, "has_async": true, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 1, "class_count": 0, "line_count": 27}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create REST API endpoint with proper request/response handling with asyncio coroutines\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from typing import Annotated\n\nfrom fastapi import Body, FastAPI, status\nfrom fastapi.responses import JSONResponse\n\napp = FastAPI()\n\nitems = {\"foo\": {\"name\": \"Fighters\", \"size\": 6}, \"bar\": {\"name\": \"Tenders\", \"size\": 3}}\n\n\n@app.put(\"/items/{item_id}\")\nasync def upsert_item(\n    item_id: str,\n    name: Annotated[str | None, Body()] = None,\n    size: Annotated[int | None, Body()] = None,\n):\n    if item_id in items:\n        item = items[item_id]\n        item[\"name\"] = name\n        item[\"size\"] = size\n        return item\n    else:\n        item = {\"name\": name, \"size\": size}\n        items[item_id] = item\n        return JSONResponse(status_code=status.HTTP_201_CREATED, content=item)\n", "metadata": {"source": "github.com/fastapi/fastapi", "file_path": "docs_src/additional_status_codes/tutorial001_an_py310.py", "language": "python", "complexity": "simple", "star_count": 87642, "quality_score": 0.89, "features": {"has_classes": false, "has_functions": true, "has_async": true, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 1, "class_count": 0, "line_count": 26}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create REST API endpoint with proper request/response handling with asyncio coroutines\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from typing import Annotated, Union\n\nfrom fastapi import Body, FastAPI, status\nfrom fastapi.responses import JSONResponse\n\napp = FastAPI()\n\nitems = {\"foo\": {\"name\": \"Fighters\", \"size\": 6}, \"bar\": {\"name\": \"Tenders\", \"size\": 3}}\n\n\n@app.put(\"/items/{item_id}\")\nasync def upsert_item(\n    item_id: str,\n    name: Annotated[Union[str, None], Body()] = None,\n    size: Annotated[Union[int, None], Body()] = None,\n):\n    if item_id in items:\n        item = items[item_id]\n        item[\"name\"] = name\n        item[\"size\"] = size\n        return item\n    else:\n        item = {\"name\": name, \"size\": size}\n        items[item_id] = item\n        return JSONResponse(status_code=status.HTTP_201_CREATED, content=item)\n", "metadata": {"source": "github.com/fastapi/fastapi", "file_path": "docs_src/additional_status_codes/tutorial001_an_py39.py", "language": "python", "complexity": "simple", "star_count": 87642, "quality_score": 0.89, "features": {"has_classes": false, "has_functions": true, "has_async": true, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 1, "class_count": 0, "line_count": 26}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create REST API endpoint with proper request/response handling with asyncio coroutines\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from fastapi import Body, FastAPI, status\nfrom fastapi.responses import JSONResponse\n\napp = FastAPI()\n\nitems = {\"foo\": {\"name\": \"Fighters\", \"size\": 6}, \"bar\": {\"name\": \"Tenders\", \"size\": 3}}\n\n\n@app.put(\"/items/{item_id}\")\nasync def upsert_item(\n    item_id: str,\n    name: str | None = Body(default=None),\n    size: int | None = Body(default=None),\n):\n    if item_id in items:\n        item = items[item_id]\n        item[\"name\"] = name\n        item[\"size\"] = size\n        return item\n    else:\n        item = {\"name\": name, \"size\": size}\n        items[item_id] = item\n        return JSONResponse(status_code=status.HTTP_201_CREATED, content=item)\n", "metadata": {"source": "github.com/fastapi/fastapi", "file_path": "docs_src/additional_status_codes/tutorial001_py310.py", "language": "python", "complexity": "simple", "star_count": 87642, "quality_score": 0.89, "features": {"has_classes": false, "has_functions": true, "has_async": true, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 1, "class_count": 0, "line_count": 24}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create REST API endpoint with proper request/response handling using Flask/FastAPI decorators\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from fastapi import FastAPI\nfrom fastapi.middleware.httpsredirect import HTTPSRedirectMiddleware\n\napp = FastAPI()\n\napp.add_middleware(HTTPSRedirectMiddleware)\n\n\n@app.get(\"/\")\nasync def main():\n    return {\"message\": \"Hello World\"}\n", "metadata": {"source": "github.com/fastapi/fastapi", "file_path": "docs_src/advanced_middleware/tutorial001.py", "language": "python", "complexity": "simple", "star_count": 87642, "quality_score": 0.89, "features": {"has_classes": false, "has_functions": true, "has_async": true, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 1, "class_count": 0, "line_count": 12}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create REST API endpoint with proper request/response handling using Flask/FastAPI decorators\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from fastapi import FastAPI\nfrom fastapi.middleware.trustedhost import TrustedHostMiddleware\n\napp = FastAPI()\n\napp.add_middleware(\n    TrustedHostMiddleware, allowed_hosts=[\"example.com\", \"*.example.com\"]\n)\n\n\n@app.get(\"/\")\nasync def main():\n    return {\"message\": \"Hello World\"}\n", "metadata": {"source": "github.com/fastapi/fastapi", "file_path": "docs_src/advanced_middleware/tutorial002.py", "language": "python", "complexity": "simple", "star_count": 87642, "quality_score": 0.89, "features": {"has_classes": false, "has_functions": true, "has_async": true, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 1, "class_count": 0, "line_count": 14}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create REST API endpoint with proper request/response handling using Flask/FastAPI decorators\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from fastapi import FastAPI\nfrom fastapi.middleware.gzip import GZipMiddleware\n\napp = FastAPI()\n\napp.add_middleware(GZipMiddleware, minimum_size=1000, compresslevel=5)\n\n\n@app.get(\"/\")\nasync def main():\n    return \"somebigcontent\"\n", "metadata": {"source": "github.com/fastapi/fastapi", "file_path": "docs_src/advanced_middleware/tutorial003.py", "language": "python", "complexity": "simple", "star_count": 87642, "quality_score": 0.89, "features": {"has_classes": false, "has_functions": true, "has_async": true, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": false, "has_testing": false, "has_logging": false, "function_count": 1, "class_count": 0, "line_count": 12}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create REST API endpoint with proper request/response handling using Flask/FastAPI decorators\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from typing import Union\n\nfrom fastapi import FastAPI, Header, HTTPException\nfrom pydantic import BaseModel\n\nfake_secret_token = \"coneofsilence\"\n\nfake_db = {\n    \"foo\": {\"id\": \"foo\", \"title\": \"Foo\", \"description\": \"There goes my hero\"},\n    \"bar\": {\"id\": \"bar\", \"title\": \"Bar\", \"description\": \"The bartenders\"},\n}\n\napp = FastAPI()\n\n\nclass Item(BaseModel):\n    id: str\n    title: str\n    description: Union[str, None] = None\n\n\n@app.get(\"/items/{item_id}\", response_model=Item)\nasync def read_main(item_id: str, x_token: str = Header()):\n    if x_token != fake_secret_token:\n        raise HTTPException(status_code=400, detail=\"Invalid X-Token header\")\n    if item_id not in fake_db:\n        raise HTTPException(status_code=404, detail=\"Item not found\")\n    return fake_db[item_id]\n\n\n@app.post(\"/items/\", response_model=Item)\nasync def create_item(item: Item, x_token: str = Header()):\n    if x_token != fake_secret_token:\n        raise HTTPException(status_code=400, detail=\"Invalid X-Token header\")\n    if item.id in fake_db:\n        raise HTTPException(status_code=409, detail=\"Item already exists\")\n    fake_db[item.id] = item\n    return item\n", "metadata": {"source": "github.com/fastapi/fastapi", "file_path": "docs_src/app_testing/app_b/main.py", "language": "python", "complexity": "simple", "star_count": 87642, "quality_score": 0.92, "features": {"has_classes": true, "has_functions": true, "has_async": true, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 2, "class_count": 1, "line_count": 39}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create REST API endpoint with proper request/response handling with pytest testing framework\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from fastapi.testclient import TestClient\n\nfrom .main import app\n\nclient = TestClient(app)\n\n\ndef test_read_item():\n    response = client.get(\"/items/foo\", headers={\"X-Token\": \"coneofsilence\"})\n    assert response.status_code == 200\n    assert response.json() == {\n        \"id\": \"foo\",\n        \"title\": \"Foo\",\n        \"description\": \"There goes my hero\",\n    }\n\n\ndef test_read_item_bad_token():\n    response = client.get(\"/items/foo\", headers={\"X-Token\": \"hailhydra\"})\n    assert response.status_code == 400\n    assert response.json() == {\"detail\": \"Invalid X-Token header\"}\n\n\ndef test_read_nonexistent_item():\n    response = client.get(\"/items/baz\", headers={\"X-Token\": \"coneofsilence\"})\n    assert response.status_code == 404\n    assert response.json() == {\"detail\": \"Item not found\"}\n\n\ndef test_create_item():\n    response = client.post(\n        \"/items/\",\n        headers={\"X-Token\": \"coneofsilence\"},\n        json={\"id\": \"foobar\", \"title\": \"Foo Bar\", \"description\": \"The Foo Barters\"},\n    )\n    assert response.status_code == 200\n    assert response.json() == {\n        \"id\": \"foobar\",\n        \"title\": \"Foo Bar\",\n        \"description\": \"The Foo Barters\",\n    }\n\n\ndef test_create_item_bad_token():\n    response = client.post(\n        \"/items/\",\n        headers={\"X-Token\": \"hailhydra\"},\n        json={\"id\": \"bazz\", \"title\": \"Bazz\", \"description\": \"Drop the bazz\"},\n    )\n    assert response.status_code == 400\n    assert response.json() == {\"detail\": \"Invalid X-Token header\"}\n\n\ndef test_create_existing_item():\n    response = client.post(\n        \"/items/\",\n        headers={\"X-Token\": \"coneofsilence\"},\n        json={\n            \"id\": \"foo\",\n            \"title\": \"The Foo ID Stealers\",\n            \"description\": \"There goes my stealer\",\n        },\n    )\n    assert response.status_code == 409\n    assert response.json() == {\"detail\": \"Item already exists\"}\n", "metadata": {"source": "github.com/fastapi/fastapi", "file_path": "docs_src/app_testing/app_b/test_main.py", "language": "python", "complexity": "simple", "star_count": 87642, "quality_score": 0.9500000000000001, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": true, "has_logging": false, "function_count": 6, "class_count": 0, "line_count": 66}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create REST API endpoint with proper request/response handling using Flask/FastAPI decorators\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from typing import Union\n\nfrom fastapi import FastAPI, Header, HTTPException\nfrom pydantic import BaseModel\nfrom typing_extensions import Annotated\n\nfake_secret_token = \"coneofsilence\"\n\nfake_db = {\n    \"foo\": {\"id\": \"foo\", \"title\": \"Foo\", \"description\": \"There goes my hero\"},\n    \"bar\": {\"id\": \"bar\", \"title\": \"Bar\", \"description\": \"The bartenders\"},\n}\n\napp = FastAPI()\n\n\nclass Item(BaseModel):\n    id: str\n    title: str\n    description: Union[str, None] = None\n\n\n@app.get(\"/items/{item_id}\", response_model=Item)\nasync def read_main(item_id: str, x_token: Annotated[str, Header()]):\n    if x_token != fake_secret_token:\n        raise HTTPException(status_code=400, detail=\"Invalid X-Token header\")\n    if item_id not in fake_db:\n        raise HTTPException(status_code=404, detail=\"Item not found\")\n    return fake_db[item_id]\n\n\n@app.post(\"/items/\", response_model=Item)\nasync def create_item(item: Item, x_token: Annotated[str, Header()]):\n    if x_token != fake_secret_token:\n        raise HTTPException(status_code=400, detail=\"Invalid X-Token header\")\n    if item.id in fake_db:\n        raise HTTPException(status_code=409, detail=\"Item already exists\")\n    fake_db[item.id] = item\n    return item\n", "metadata": {"source": "github.com/fastapi/fastapi", "file_path": "docs_src/app_testing/app_b_an/main.py", "language": "python", "complexity": "simple", "star_count": 87642, "quality_score": 0.92, "features": {"has_classes": true, "has_functions": true, "has_async": true, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 2, "class_count": 1, "line_count": 40}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate module integration for Create REST API endpoint with proper request/response handling\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Clase en vídeo: https://youtu.be/_y9qQZXE24A?t=20480)\n\n### MongoDB client ###\n\n# Descarga versión community: https://www.mongodb.com/try/download\n# Instalación:https://www.mongodb.com/docs/manual/tutorial\n# Módulo conexión MongoDB: pip install pymongo\n# Ejecución: sudo mongod --dbpath \"/path/a/la/base/de/datos/\"\n# Conexión: mongodb://localhost\n\nfrom pymongo import MongoClient\n\n# Descomentar el db_client local o remoto correspondiente\n\n# Base de datos local MongoDB\ndb_client = MongoClient().local\n\n# Clase en vídeo: https://youtu.be/_y9qQZXE24A?t=25470\n\n# Base de datos remota MongoDB Atlas (https://mongodb.com)\n# db_client = MongoClient(\n#     \"mongodb+srv://<user>:<password>@<url>/?retryWrites=true&w=majority\").test\n\n# Despliegue API en la nube:\n# Deta - https://www.deta.sh/\n# Intrucciones - https://fastapi.tiangolo.com/deployment/deta/\n", "metadata": {"source": "github.com/mouredev/Hello-Python", "file_path": "Backend/FastAPI/db/client.py", "language": "python", "complexity": "simple", "star_count": 31193, "quality_score": 0.8600000000000001, "features": {"has_classes": false, "has_functions": false, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 0, "class_count": 0, "line_count": 27}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create REST API endpoint with proper request/response handling\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Clase en vídeo (22/12/2022): https://www.twitch.tv/videos/1686104006\n\n### User schema ###\n\ndef user_schema(user) -> dict:\n    return {\"id\": str(user[\"_id\"]),\n            \"username\": user[\"username\"],\n            \"email\": user[\"email\"]}\n\n\ndef users_schema(users) -> list:\n    return [user_schema(user) for user in users]\n", "metadata": {"source": "github.com/mouredev/Hello-Python", "file_path": "Backend/FastAPI/db/schemas/user.py", "language": "python", "complexity": "simple", "star_count": 31193, "quality_score": 0.8300000000000001, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": false, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 2, "class_count": 0, "line_count": 13}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create REST API endpoint with proper request/response handling using Flask/FastAPI decorators\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Clase en vídeo: https://youtu.be/_y9qQZXE24A\n\n### Hola Mundo ###\n\n# Documentación oficial: https://fastapi.tiangolo.com/es/\n\n# Instala FastAPI: pip install \"fastapi[all]\"\n\nfrom fastapi import FastAPI\nfrom routers import products, users, basic_auth_users, jwt_auth_users, users_db\nfrom fastapi.staticfiles import StaticFiles\n\napp = FastAPI()\n\n# Clase en vídeo: https://youtu.be/_y9qQZXE24A?t=12475\napp.include_router(products.router)\napp.include_router(users.router)\n\n# Clase en vídeo: https://youtu.be/_y9qQZXE24A?t=14094\napp.include_router(basic_auth_users.router)\n\n# Clase en vídeo: https://youtu.be/_y9qQZXE24A?t=17664\napp.include_router(jwt_auth_users.router)\n\n# Clase en vídeo: https://youtu.be/_y9qQZXE24A?t=20480\napp.include_router(users_db.router)\n\n# Clase en vídeo: https://youtu.be/_y9qQZXE24A?t=13618\napp.mount(\"/static\", StaticFiles(directory=\"static\"), name=\"static\")\n\n\n# Url local: http://127.0.0.1:8000\n\n\n@app.get(\"/\")\nasync def root():\n    return \"Hola FastAPI!\"\n\n# Url local: http://127.0.0.1:8000/url\n\n\n@app.get(\"/url\")\nasync def url():\n    return {\"url\": \"https://mouredev.com/python\"}\n\n# Inicia el server: uvicorn main:app --reload\n# Detener el server: CTRL+C\n\n# Documentación con Swagger: http://127.0.0.1:8000/docs\n# Documentación con Redocly: http://127.0.0.1:8000/redoc\n", "metadata": {"source": "github.com/mouredev/Hello-Python", "file_path": "Backend/FastAPI/main.py", "language": "python", "complexity": "medium", "star_count": 31193, "quality_score": 0.89, "features": {"has_classes": false, "has_functions": true, "has_async": true, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 2, "class_count": 0, "line_count": 51}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create REST API endpoint with proper request/response handling with SQLAlchemy/Django ORM patterns\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Clase en vídeo: https://youtu.be/_y9qQZXE24A?t=14094\n\n### Users API con autorización OAuth2 básica ###\n\nfrom fastapi import APIRouter, Depends, HTTPException, status\nfrom pydantic import BaseModel\nfrom fastapi.security import OAuth2PasswordBearer, OAuth2PasswordRequestForm\n\nrouter = APIRouter(prefix=\"/basicauth\",\n                   tags=[\"basicauth\"],\n                   responses={status.HTTP_404_NOT_FOUND: {\"message\": \"No encontrado\"}})\n\noauth2 = OAuth2PasswordBearer(tokenUrl=\"login\")\n\n\nclass User(BaseModel):\n    username: str\n    full_name: str\n    email: str\n    disabled: bool\n\n\nclass UserDB(User):\n    password: str\n\n\nusers_db = {\n    \"mouredev\": {\n        \"username\": \"mouredev\",\n        \"full_name\": \"Brais Moure\",\n        \"email\": \"braismoure@mourede.com\",\n        \"disabled\": False,\n        \"password\": \"123456\"\n    },\n    \"mouredev2\": {\n        \"username\": \"mouredev2\",\n        \"full_name\": \"Brais Moure 2\",\n        \"email\": \"braismoure2@mourede.com\",\n        \"disabled\": True,\n        \"password\": \"654321\"\n    }\n}\n\n\ndef search_user_db(username: str):\n    if username in users_db:\n        return UserDB(**users_db[username])\n\n\ndef search_user(username: str):\n    if username in users_db:\n        return User(**users_db[username])\n\n\nasync def current_user(token: str = Depends(oauth2)):\n    user = search_user(token)\n    if not user:\n        raise HTTPException(\n            status_code=status.HTTP_401_UNAUTHORIZED,\n            detail=\"Credenciales de autenticación inválidas\",\n            headers={\"WWW-Authenticate\": \"Bearer\"})\n\n    if user.disabled:\n        raise HTTPException(\n            status_code=status.HTTP_400_BAD_REQUEST,\n            detail=\"Usuario inactivo\")\n\n    return user\n\n\n@router.post(\"/login\")\nasync def login(form: OAuth2PasswordRequestForm = Depends()):\n    user_db = users_db.get(form.username)\n    if not user_db:\n        raise HTTPException(\n            status_code=status.HTTP_400_BAD_REQUEST, detail=\"El usuario no es correcto\")\n\n    user = search_user_db(form.username)\n    if not form.password == user.password:\n        raise HTTPException(\n            status_code=status.HTTP_400_BAD_REQUEST, detail=\"La contraseña no es correcta\")\n\n    return {\"access_token\": user.username, \"token_type\": \"bearer\"}\n\n\n@router.get(\"/users/me\")\nasync def me(user: User = Depends(current_user)):\n    return user\n", "metadata": {"source": "github.com/mouredev/Hello-Python", "file_path": "Backend/FastAPI/routers/basic_auth_users.py", "language": "python", "complexity": "medium", "star_count": 31193, "quality_score": 0.92, "features": {"has_classes": true, "has_functions": true, "has_async": true, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": false, "has_logging": true, "function_count": 5, "class_count": 2, "line_count": 89}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create REST API endpoint with proper request/response handling with SQLAlchemy/Django ORM patterns\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Clase en vídeo: https://youtu.be/_y9qQZXE24A?t=17664\n\n### Users API con autorización OAuth2 JWT ###\n\nfrom fastapi import APIRouter, Depends, HTTPException, status\nfrom pydantic import BaseModel\nfrom fastapi.security import OAuth2PasswordBearer, OAuth2PasswordRequestForm\nfrom jose import jwt, JWTError\nfrom passlib.context import CryptContext\nfrom datetime import datetime, timedelta\n\nALGORITHM = \"HS256\"\nACCESS_TOKEN_DURATION = 1\nSECRET = \"201d573bd7d1344d3a3bfce1550b69102fd11be3db6d379508b6cccc58ea230b\"\n\nrouter = APIRouter(prefix=\"/jwtauth\",\n                   tags=[\"jwtauth\"],\n                   responses={status.HTTP_404_NOT_FOUND: {\"message\": \"No encontrado\"}})\n\noauth2 = OAuth2PasswordBearer(tokenUrl=\"login\")\n\ncrypt = CryptContext(schemes=[\"bcrypt\"])\n\n\nclass User(BaseModel):\n    username: str\n    full_name: str\n    email: str\n    disabled: bool\n\n\nclass UserDB(User):\n    password: str\n\n\nusers_db = {\n    \"mouredev\": {\n        \"username\": \"mouredev\",\n        \"full_name\": \"Brais Moure\",\n        \"email\": \"braismoure@mourede.com\",\n        \"disabled\": False,\n        \"password\": \"$2a$12$B2Gq.Dps1WYf2t57eiIKjO4DXC3IUMUXISJF62bSRiFfqMdOI2Xa6\"\n    },\n    \"mouredev2\": {\n        \"username\": \"mouredev2\",\n        \"full_name\": \"Brais Moure 2\",\n        \"email\": \"braismoure2@mourede.com\",\n        \"disabled\": True,\n        \"password\": \"$2a$12$SduE7dE.i3/ygwd0Kol8bOFvEABaoOOlC8JsCSr6wpwB4zl5STU4S\"\n    }\n}\n\n\ndef search_user_db(username: str):\n    if username in users_db:\n        return UserDB(**users_db[username])\n\n\ndef search_user(username: str):\n    if username in users_db:\n        return User(**users_db[username])\n\n\nasync def auth_user(token: str = Depends(oauth2)):\n\n    exception = HTTPException(\n        status_code=status.HTTP_401_UNAUTHORIZED,\n        detail=\"Credenciales de autenticación inválidas\",\n        headers={\"WWW-Authenticate\": \"Bearer\"})\n\n    try:\n        username = jwt.decode(token, SECRET, algorithms=[ALGORITHM]).get(\"sub\")\n        if username is None:\n            raise exception\n\n    except JWTError:\n        raise exception\n\n    return search_user(username)\n\n\nasync def current_user(user: User = Depends(auth_user)):\n    if user.disabled:\n        raise HTTPException(\n            status_code=status.HTTP_400_BAD_REQUEST,\n            detail=\"Usuario inactivo\")\n\n    return user\n\n\n@router.post(\"/login\")\nasync def login(form: OAuth2PasswordRequestForm = Depends()):\n\n    user_db = users_db.get(form.username)\n    if not user_db:\n        raise HTTPException(\n            status_code=status.HTTP_400_BAD_REQUEST, detail=\"El usuario no es correcto\")\n\n    user = search_user_db(form.username)\n\n    if not crypt.verify(form.password, user.password):\n        raise HTTPException(\n            status_code=status.HTTP_400_BAD_REQUEST, detail=\"La contraseña no es correcta\")\n\n    access_token = {\"sub\": user.username,\n                    \"exp\": datetime.utcnow() + timedelta(minutes=ACCESS_TOKEN_DURATION)}\n\n    return {\"access_token\": jwt.encode(access_token, SECRET, algorithm=ALGORITHM), \"token_type\": \"bearer\"}\n\n\n@router.get(\"/users/me\")\nasync def me(user: User = Depends(current_user)):\n    return user\n", "metadata": {"source": "github.com/mouredev/Hello-Python", "file_path": "Backend/FastAPI/routers/jwt_auth_users.py", "language": "python", "complexity": "complex", "star_count": 31193, "quality_score": 0.9500000000000001, "features": {"has_classes": true, "has_functions": true, "has_async": true, "has_decorators": true, "has_imports": true, "has_error_handling": true, "has_documentation": false, "has_type_hints": true, "has_testing": false, "has_logging": true, "function_count": 6, "class_count": 2, "line_count": 114}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create REST API endpoint with proper request/response handling with asyncio coroutines\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Clase en vídeo: https://youtu.be/_y9qQZXE24A?t=12475\n\n### Products API ###\n\nfrom fastapi import APIRouter\n\nrouter = APIRouter(prefix=\"/products\",\n                   tags=[\"products\"],\n                   responses={404: {\"message\": \"No encontrado\"}})\n\nproducts_list = [\"Producto 1\", \"Producto 2\",\n                 \"Producto 3\", \"Producto 4\", \"Producto 5\"]\n\n\n@router.get(\"/\")\nasync def products():\n    return products_list\n\n\n@router.get(\"/{id}\")\nasync def products(id: int):\n    return products_list[id]\n", "metadata": {"source": "github.com/mouredev/Hello-Python", "file_path": "Backend/FastAPI/routers/products.py", "language": "python", "complexity": "simple", "star_count": 31193, "quality_score": 0.89, "features": {"has_classes": false, "has_functions": true, "has_async": true, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 2, "class_count": 0, "line_count": 23}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create REST API endpoint with proper request/response handling with SQLAlchemy/Django ORM patterns\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Clase en vídeo: https://youtu.be/_y9qQZXE24A?t=5382\n\n### Users API ###\n\nfrom fastapi import APIRouter, HTTPException\nfrom pydantic import BaseModel\n\n# Inicia el server: uvicorn users:app --reload\n\nrouter = APIRouter()\n\n\nclass User(BaseModel):\n    id: int\n    name: str\n    surname: str\n    url: str\n    age: int\n\n\nusers_list = [User(id=1, name=\"Brais\", surname=\"Moure\", url=\"https://moure.dev\", age=35),\n              User(id=2, name=\"Moure\", surname=\"Dev\",\n                   url=\"https://mouredev.com\", age=35),\n              User(id=3, name=\"Brais\", surname=\"Dahlberg\", url=\"https://haakon.com\", age=33)]\n\n\n@router.get(\"/usersjson\")\nasync def usersjson():  # Creamos un JSON a mano\n    return [{\"name\": \"Brais\", \"surname\": \"Moure\", \"url\": \"https://moure.dev\", \"age\": 35},\n            {\"name\": \"Moure\", \"surname\": \"Dev\",\n                \"url\": \"https://mouredev.com\", \"age\": 35},\n            {\"name\": \"Haakon\", \"surname\": \"Dahlberg\", \"url\": \"https://haakon.com\", \"age\": 33}]\n\n\n@router.get(\"/users\")\nasync def users():\n    return users_list\n\n\n@router.get(\"/user/{id}\")  # Path\nasync def user(id: int):\n    return search_user(id)\n\n\n@router.get(\"/user/\")  # Query\nasync def user(id: int):\n    return search_user(id)\n\n\n# Clase en vídeo: https://youtu.be/_y9qQZXE24A?t=8529\n\n\n@router.post(\"/user/\", response_model=User, status_code=201)\nasync def user(user: User):\n    if type(search_user(user.id)) == User:\n        raise HTTPException(status_code=404, detail=\"El usuario ya existe\")\n\n    users_list.append(user)\n    return user\n\n\n@router.put(\"/user/\")\nasync def user(user: User):\n\n    found = False\n\n    for index, saved_user in enumerate(users_list):\n        if saved_user.id == user.id:\n            users_list[index] = user\n            found = True\n\n    if not found:\n        return {\"error\": \"No se ha actualizado el usuario\"}\n\n    return user\n\n\n@router.delete(\"/user/{id}\")\nasync def user(id: int):\n\n    found = False\n\n    for index, saved_user in enumerate(users_list):\n        if saved_user.id == id:\n            del users_list[index]\n            found = True\n\n    if not found:\n        return {\"error\": \"No se ha eliminado el usuario\"}\n\n\ndef search_user(id: int):\n    users = filter(lambda user: user.id == id, users_list)\n    try:\n        return list(users)[0]\n    except:\n        return {\"error\": \"No se ha encontrado el usuario\"}\n", "metadata": {"source": "github.com/mouredev/Hello-Python", "file_path": "Backend/FastAPI/routers/users.py", "language": "python", "complexity": "medium", "star_count": 31193, "quality_score": 0.98, "features": {"has_classes": true, "has_functions": true, "has_async": true, "has_decorators": true, "has_imports": true, "has_error_handling": true, "has_documentation": false, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 8, "class_count": 1, "line_count": 98}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create REST API endpoint with proper request/response handling with asyncio coroutines\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Clase en vídeo: https://youtu.be/_y9qQZXE24A?t=20480\n\n### Users DB API ###\n\nfrom fastapi import APIRouter, HTTPException, status\nfrom db.models.user import User\nfrom db.schemas.user import user_schema, users_schema\nfrom db.client import db_client\nfrom bson import ObjectId\n\nrouter = APIRouter(prefix=\"/userdb\",\n                   tags=[\"userdb\"],\n                   responses={status.HTTP_404_NOT_FOUND: {\"message\": \"No encontrado\"}})\n\n\n@router.get(\"/\", response_model=list[User])\nasync def users():\n    return users_schema(db_client.users.find())\n\n\n@router.get(\"/{id}\")  # Path\nasync def user(id: str):\n    return search_user(\"_id\", ObjectId(id))\n\n\n@router.get(\"/\")  # Query\nasync def user(id: str):\n    return search_user(\"_id\", ObjectId(id))\n\n\n@router.post(\"/\", response_model=User, status_code=status.HTTP_201_CREATED)\nasync def user(user: User):\n    if type(search_user(\"email\", user.email)) == User:\n        raise HTTPException(\n            status_code=status.HTTP_404_NOT_FOUND, detail=\"El usuario ya existe\")\n\n    user_dict = dict(user)\n    del user_dict[\"id\"]\n\n    id = db_client.users.insert_one(user_dict).inserted_id\n\n    new_user = user_schema(db_client.users.find_one({\"_id\": id}))\n\n    return User(**new_user)\n\n\n@router.put(\"/\", response_model=User)\nasync def user(user: User):\n\n    user_dict = dict(user)\n    del user_dict[\"id\"]\n\n    try:\n        db_client.users.find_one_and_replace(\n            {\"_id\": ObjectId(user.id)}, user_dict)\n    except:\n        return {\"error\": \"No se ha actualizado el usuario\"}\n\n    return search_user(\"_id\", ObjectId(user.id))\n\n\n@router.delete(\"/{id}\", status_code=status.HTTP_204_NO_CONTENT)\nasync def user(id: str):\n\n    found = db_client.users.find_one_and_delete({\"_id\": ObjectId(id)})\n\n    if not found:\n        return {\"error\": \"No se ha eliminado el usuario\"}\n\n# Helper\n\n\ndef search_user(field: str, key):\n\n    try:\n        user = db_client.users.find_one({field: key})\n        return User(**user_schema(user))\n    except:\n        return {\"error\": \"No se ha encontrado el usuario\"}\n", "metadata": {"source": "github.com/mouredev/Hello-Python", "file_path": "Backend/FastAPI/routers/users_db.py", "language": "python", "complexity": "medium", "star_count": 31193, "quality_score": 0.9500000000000001, "features": {"has_classes": false, "has_functions": true, "has_async": true, "has_decorators": true, "has_imports": true, "has_error_handling": true, "has_documentation": false, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 7, "class_count": 0, "line_count": 80}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate configuration for Create REST API endpoint with proper request/response handling\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Clase en vídeo: https://youtu.be/Kp4Mvapo5kc?t=8643\n\n### Strings ###\n\nmy_string = \"Mi String\"\nmy_other_string = 'Mi otro String'\n\nprint(len(my_string))\nprint(len(my_other_string))\nprint(my_string + \" \" + my_other_string)\n\nmy_new_line_string = \"Este es un String\\ncon salto de línea\"\nprint(my_new_line_string)\n\nmy_tab_string = \"\\tEste es un String con tabulación\"\nprint(my_tab_string)\n\nmy_scape_string = \"\\\\tEste es un String \\\\n escapado\"\nprint(my_scape_string)\n\n# Formateo\n\nname, surname, age = \"Brais\", \"Moure\", 35\nprint(\"Mi nombre es {} {} y mi edad es {}\".format(name, surname, age))\nprint(\"Mi nombre es %s %s y mi edad es %d\" % (name, surname, age))\nprint(\"Mi nombre es \" + name + \" \" + surname + \" y mi edad es \" + str(age))\nprint(f\"Mi nombre es {name} {surname} y mi edad es {age}\")\n\n# Desempaqueado de caracteres\n\nlanguage = \"python\"\na, b, c, d, e, f = language\nprint(a)\nprint(e)\n\n# División\n\nlanguage_slice = language[1:3]\nprint(language_slice)\n\nlanguage_slice = language[1:]\nprint(language_slice)\n\nlanguage_slice = language[-2]\nprint(language_slice)\n\nlanguage_slice = language[0:6:2]\nprint(language_slice)\n\n# Reverse\n\nreversed_language = language[::-1]\nprint(reversed_language)\n\n# Funciones del lenguaje\n\nprint(language.capitalize())\nprint(language.upper())\nprint(language.count(\"t\"))\nprint(language.isnumeric())\nprint(\"1\".isnumeric())\nprint(language.lower())\nprint(language.lower().isupper())\nprint(language.startswith(\"Py\"))\nprint(\"Py\" == \"py\")  # No es lo mismo\n", "metadata": {"source": "github.com/mouredev/Hello-Python", "file_path": "Basic/03_strings.py", "language": "python", "complexity": "simple", "star_count": 31193, "quality_score": 0.8, "features": {"has_classes": false, "has_functions": false, "has_async": false, "has_decorators": false, "has_imports": false, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 0, "class_count": 0, "line_count": 66}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create REST API endpoint with proper request/response handling\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Clase en vídeo: https://youtu.be/Kp4Mvapo5kc?t=26619\n\n### Functions ###\n\n# Definición\n\ndef my_function():\n    print(\"Esto es una función\")\n\n\nmy_function()\nmy_function()\nmy_function()\n\n# Función con parámetros de entrada/argumentos\n\n\ndef sum_two_values(first_value: int, second_value):\n    print(first_value + second_value)\n\n\nsum_two_values(5, 7)\nsum_two_values(54754, 71231)\nsum_two_values(\"5\", \"7\")\nsum_two_values(1.4, 5.2)\n\n# Función con parámetros de entrada/argumentos y retorno\n\n\ndef sum_two_values_with_return(first_value, second_value):\n    my_sum = first_value + second_value\n    return my_sum\n\n\nmy_result = sum_two_values(1.4, 5.2)\nprint(my_result)\n\nmy_result = sum_two_values_with_return(10, 5)\nprint(my_result)\n\n# Función con parámetros de entrada/argumentos por clave\n\n\ndef print_name(name, surname):\n    print(f\"{name} {surname}\")\n\n\nprint_name(surname=\"Moure\", name=\"Brais\")\n\n# Función con parámetros de entrada/argumentos por defecto\n\n\ndef print_name_with_default(name, surname, alias=\"Sin alias\"):\n    print(f\"{name} {surname} {alias}\")\n\n\nprint_name_with_default(\"Brais\", \"Moure\")\nprint_name_with_default(\"Brais\", \"Moure\", \"MoureDev\")\n\n# Función con parámetros de entrada/argumentos arbitrarios\n\n\ndef print_upper_texts(*texts):\n    print(type(texts))\n    for text in texts:\n        print(text.upper())\n\n\nprint_upper_texts(\"Hola\", \"Python\", \"MoureDev\")\nprint_upper_texts(\"Hola\")\n", "metadata": {"source": "github.com/mouredev/Hello-Python", "file_path": "Basic/10_functions.py", "language": "python", "complexity": "simple", "star_count": 31193, "quality_score": 0.8300000000000001, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": false, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 6, "class_count": 0, "line_count": 71}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create REST API endpoint with proper request/response handling\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Clase en vídeo: https://youtu.be/Kp4Mvapo5kc?t=29327\n\n### Classes ###\n\n# Definición\n\nclass MyEmptyPerson:\n    pass  # Para poder dejar la clase vacía\n\n\nprint(MyEmptyPerson)\nprint(MyEmptyPerson())\n\n# Clase con constructor, funciones y propiedades privadas y públicas\n\n\nclass Person:\n    def __init__(self, name, surname, alias=\"Sin alias\"):\n        self.full_name = f\"{name} {surname} ({alias})\"  # Propiedad pública\n        self.__name = name  # Propiedad privada\n\n    def get_name(self):\n        return self.__name\n\n    def walk(self):\n        print(f\"{self.full_name} está caminando\")\n\n\nmy_person = Person(\"Brais\", \"Moure\")\nprint(my_person.full_name)\nprint(my_person.get_name())\nmy_person.walk()\n\nmy_other_person = Person(\"Brais\", \"Moure\", \"MoureDev\")\nprint(my_other_person.full_name)\nmy_other_person.walk()\nmy_other_person.full_name = \"Héctor de León (El loco de los perros)\"\nprint(my_other_person.full_name)\n\nmy_other_person.full_name = 666\nprint(my_other_person.full_name)\n", "metadata": {"source": "github.com/mouredev/Hello-Python", "file_path": "Basic/11_classes.py", "language": "python", "complexity": "simple", "star_count": 31193, "quality_score": 0.8600000000000001, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": false, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 3, "class_count": 2, "line_count": 42}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create REST API endpoint with proper request/response handling\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Clase en vídeo: https://youtu.be/Kp4Mvapo5kc?t=34583\n\n### Módulo para pruebas ###\n\ndef sumValue(numberOne, numberTwo, numberThree):\n    print(numberOne + numberTwo + numberThree)\n\n\ndef printValue(value):\n    print(value)\n", "metadata": {"source": "github.com/mouredev/Hello-Python", "file_path": "Basic/my_module.py", "language": "python", "complexity": "simple", "star_count": 31193, "quality_score": 0.8300000000000001, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": false, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 2, "class_count": 0, "line_count": 11}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create REST API endpoint with proper request/response handling\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Clase en vídeo: https://youtu.be/TbcEqkabAWU\n\n### Dates ###\n\n# Date time\n\nfrom datetime import timedelta\nfrom datetime import date\nfrom datetime import time\nfrom datetime import datetime\n\nnow = datetime.now()\n\n\ndef print_date(date):\n    print(date.year)\n    print(date.month)\n    print(date.day)\n    print(date.hour)\n    print(date.minute)\n    print(date.second)\n    print(date.timestamp())\n\n\nprint_date(now)\n\nyear_2023 = datetime(2023, 1, 1)\n\nprint_date(year_2023)\n\n# Time\n\n\ncurrent_time = time(21, 6, 0)\n\nprint(current_time.hour)\nprint(current_time.minute)\nprint(current_time.second)\n\n# Date\n\n\ncurrent_date = date.today()\n\nprint(current_date.year)\nprint(current_date.month)\nprint(current_date.day)\n\ncurrent_date = date(2022, 10, 6)\n\nprint(current_date.year)\nprint(current_date.month)\nprint(current_date.day)\n\ncurrent_date = date(current_date.year,\n                    current_date.month + 1, current_date.day)\n\nprint(current_date.month)\n\n# Operaciones con fechas\n\ndiff = year_2023 - now\nprint(diff)\n\ndiff = year_2023.date() - current_date\nprint(diff)\n\n# Timedelta\n\n\nstart_timedelta = timedelta(200, 100, 100, weeks=10)\nend_timedelta = timedelta(300, 100, 100, weeks=13)\n\nprint(end_timedelta - start_timedelta)\nprint(end_timedelta + start_timedelta)\n", "metadata": {"source": "github.com/mouredev/Hello-Python", "file_path": "Intermediate/00_dates.py", "language": "python", "complexity": "medium", "star_count": 31193, "quality_score": 0.89, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 1, "class_count": 0, "line_count": 76}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create REST API endpoint with proper request/response handling\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Clase en vídeo: https://youtu.be/TbcEqkabAWU?t=3239\n\n### List Comprehension ###\n\nmy_original_list = [0, 1, 2, 3, 4, 5, 6, 7]\nprint(my_original_list)\n\nmy_range = range(8)\nprint(list(my_range))\n\n# Definición\n\nmy_list = [i + 1 for i in range(8)]\nprint(my_list)\n\nmy_list = [i * 2 for i in range(8)]\nprint(my_list)\n\nmy_list = [i * i for i in range(8)]\nprint(my_list)\n\n\ndef sum_five(number):\n    return number + 5\n\n\nmy_list = [sum_five(i) for i in range(8)]\nprint(my_list)\n", "metadata": {"source": "github.com/mouredev/Hello-Python", "file_path": "Intermediate/01_list_comprehension.py", "language": "python", "complexity": "simple", "star_count": 31193, "quality_score": 0.8300000000000001, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": false, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 1, "class_count": 0, "line_count": 29}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create REST API endpoint with proper request/response handling\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Clase en vídeo: https://youtu.be/TbcEqkabAWU?t=4142\n\n### Challenges ###\n\n\"\"\"\nEL FAMOSO \"FIZZ BUZZ”:\nEscribe un programa que muestre por consola (con un print) los\nnúmeros de 1 a 100 (ambos incluidos y con un salto de línea entre\ncada impresión), sustituyendo los siguientes:\n- Múltiplos de 3 por la palabra \"fizz\".\n- Múltiplos de 5 por la palabra \"buzz\".\n- Múltiplos de 3 y de 5 a la vez por la palabra \"fizzbuzz\".\n\"\"\"\n\n\ndef fizzbuzz():\n    for index in range(1, 101):\n        if index % 3 == 0 and index % 5 == 0:\n            print(\"fizzbuzz\")\n        elif index % 3 == 0:\n            print(\"fizz\")\n        elif index % 5 == 0:\n            print(\"buzz\")\n        else:\n            print(index)\n\n\nfizzbuzz()\n\n\"\"\"\n¿ES UN ANAGRAMA?\nEscribe una función que reciba dos palabras (String) y retorne\nverdadero o falso (Bool) según sean o no anagramas.\n- Un Anagrama consiste en formar una palabra reordenando TODAS\n  las letras de otra palabra inicial.\n- NO hace falta comprobar que ambas palabras existan.\n- Dos palabras exactamente iguales no son anagrama.\n\"\"\"\n\n\ndef is_anagram(word_one, word_two):\n    if word_one.lower() == word_two.lower():\n        return False\n    return sorted(word_one.lower()) == sorted(word_two.lower())\n\n\nprint(is_anagram(\"Amor\", \"Roma\"))\n\n\"\"\"\nLA SUCESIÓN DE FIBONACCI\nEscribe un programa que imprima los 50 primeros números de la sucesión\nde Fibonacci empezando en 0.\n- La serie Fibonacci se compone por una sucesión de números en\n  la que el siguiente siempre es la suma de los dos anteriores.\n  0, 1, 1, 2, 3, 5, 8, 13...\n\"\"\"\n\n\ndef fibonacci():\n\n    prev = 0\n    next = 1\n\n    for index in range(0, 50):\n        print(prev)\n        fib = prev + next\n        prev = next\n        next = fib\n\n\nfibonacci()\n\n\"\"\"\n¿ES UN NÚMERO PRIMO?\nEscribe un programa que se encargue de comprobar si un número es o no primo.\nHecho esto, imprime los números primos entre 1 y 100.\n\"\"\"\n\n\ndef is_prime():\n\n    for number in range(1, 101):\n\n        if number >= 2:\n\n            is_divisible = False\n\n            for index in range(2, number):\n                if number % index == 0:\n                    is_divisible = True\n                    break\n\n            if not is_divisible:\n                print(number)\n\n\nis_prime()\n\n\"\"\"\nINVIRTIENDO CADENAS\nCrea un programa que invierta el orden de una cadena de texto\nsin usar funciones propias del lenguaje que lo hagan de forma automática.\n- Si le pasamos \"Hola mundo\" nos retornaría \"odnum aloH\"\n\"\"\"\n\n\ndef reverse(text):\n    text_len = len(text)\n    reversed_text = \"\"\n    for index in range(0, text_len):\n        reversed_text += text[text_len - index - 1]\n    return reversed_text\n\n\nprint(reverse(\"Hola mundo\"))\n", "metadata": {"source": "github.com/mouredev/Hello-Python", "file_path": "Intermediate/02_challenges.py", "language": "python", "complexity": "medium", "star_count": 31193, "quality_score": 0.8600000000000001, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": false, "has_error_handling": false, "has_documentation": true, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 5, "class_count": 0, "line_count": 116}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create REST API endpoint with proper request/response handling with asyncio coroutines\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import os\nimport re\nimport sys\nfrom os import path\n\n\n# Centralize environment variables\nJINA_VERSION = os.environ.get('JINA_VERSION', None)\n\nsys.path.insert(0, path.abspath('..'))\n\nproject = 'Jina'\nslug = re.sub(r'\\W+', '-', project.lower())\nauthor = 'Jina AI'\ncopyright = 'Jina AI Limited. All rights reserved.'\nsource_suffix = ['.rst', '.md']\nmaster_doc = 'index'\nlanguage = 'en'\nrepo_dir = '../'\n\ntry:\n    if not JINA_VERSION:\n        pkg_name = 'jina'\n        libinfo_py = path.join(repo_dir, pkg_name, '__init__.py')\n        libinfo_content = open(libinfo_py, 'r', encoding='utf-8').readlines()\n        version_line = [\n            l.strip() for l in libinfo_content if l.startswith('__version__')\n        ][0]\n        exec(version_line)\n    else:\n        __version__ = JINA_VERSION\nexcept FileNotFoundError:\n    __version__ = '0.0.0'\n\nversion = __version__\nrelease = __version__\nSPHINX_MULTIVERSION_VERSION = os.environ.get('SPHINX_MULTIVERSION_VERSION', version)\n\ntemplates_path = ['_templates']\nexclude_patterns = [\n    '_build',\n    'Thumbs.db',\n    '.DS_Store',\n    'tests',\n    'page_templates',\n    '.github',\n]\npygments_style = 'rainbow_dash'\nhtml_theme = 'furo'\n\nbase_url = '/'\nhtml_baseurl = 'https://jina.ai/serve'\nsitemap_url_scheme = '{link}'\nsitemap_locales = [None]\nsitemap_filename = \"sitemap.xml\"\nautodoc_default_options = {\n    \"members\": True,\n    \"inherited-members\": True,\n    'class-doc-from': '__init__',\n}\n\nhtml_theme_options = {\n    'light_logo': 'logo-light.svg',\n    'dark_logo': 'logo-dark.svg',\n    \"sidebar_hide_name\": True,\n    \"light_css_variables\": {\n        \"color-brand-primary\": \"#009191\",\n        \"color-brand-content\": \"#009191\",\n    },\n    \"dark_css_variables\": {\n        \"color-brand-primary\": \"#FBCB67\",\n        \"color-brand-content\": \"#FBCB67\",\n    },\n    # PLEASE DO NOT DELETE the empty line between `start-announce` and `end-announce`\n    # PLEASE DO NOT DELETE `start-announce`/ `end-announce` it is used for our dev bot to inject announcement from GH\n    # start-announce\n    # end-announce\n}\n\nhtml_static_path = ['_static']\nhtml_extra_path = ['html_extra']\nhtml_css_files = [\n    'main.css',\n    'docbot.css',\n    'https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.1.1/css/all.min.css',\n]\nhtmlhelp_basename = slug\nhtml_show_sourcelink = False\nhtml_favicon = '_static/favicon.ico'\n\nlatex_documents = [(master_doc, f'{slug}.tex', project, author, 'manual')]\nman_pages = [(master_doc, slug, project, [author], 1)]\ntexinfo_documents = [\n    (master_doc, slug, project, author, slug, project, 'Miscellaneous')\n]\nepub_title = project\nepub_exclude_files = ['search.html']\n\n# -- Extension configuration -------------------------------------------------\n\nextensions = [\n    'sphinx.ext.autodoc',\n    'sphinx.ext.autosummary',\n    'sphinx_autodoc_typehints',\n    'sphinx.ext.viewcode',\n    'sphinx.ext.coverage',\n    'sphinxcontrib.apidoc',\n    'sphinxcontrib.redirects',\n    'sphinxarg.ext',\n    'sphinx_copybutton',\n    'sphinx_sitemap',\n    'sphinx.ext.intersphinx',\n    'sphinxext.opengraph',\n    'notfound.extension',\n    'myst_parser',\n    'sphinx_design',\n    'sphinx_inline_tabs',\n]\n\nintersphinx_mapping = {'docarray': ('https://docs.docarray.org/', None)}\nmyst_enable_extensions = ['colon_fence']\nautosummary_generate = True\nredirects_file = 'redirects.txt'\n# -- Custom 404 page\n\n# sphinx-notfound-page\n# https://github.com/readthedocs/sphinx-notfound-page\nnotfound_context = {\n    'title': 'Page Not Found',\n    'body': '''\n<h1>Page Not Found</h1>\n<p>Oops, we couldn't find that page. </p>\n<p>You can try \"asking our docs\" on the right corner of the page to find answer.</p>\n<p>Otherwise, <a href=\"https://github.com/jina-ai/jina/\">please create a Github issue</a> and one of our team will respond.</p>\n\n''',\n}\nnotfound_no_urls_prefix = True\n\napidoc_module_dir = repo_dir\napidoc_output_dir = 'api'\napidoc_excluded_paths = ['tests', 'legacy', 'hub', 'toy*', 'setup.py']\napidoc_separate_modules = True\napidoc_extra_args = ['-t', 'template/']\nautodoc_member_order = 'bysource'\nautodoc_mock_imports = ['argparse', 'numpy', 'np', 'tensorflow', 'torch', 'scipy']\nautoclass_content = 'both'\nset_type_checking_flag = False\nhtml_last_updated_fmt = ''\nnitpicky = True\nnitpick_ignore = [('py:class', 'type')]\nlinkcheck_ignore = [\n    # Avoid link check on local uri\n    'http://0.0.0.0:*',\n    'pods/encode.yml',\n    'https://github.com/jina-ai/jina/commit/*',\n    '.github/*',\n    'extra-requirements.txt',\n    'fastentrypoints.py' '../../101',\n    '../../102',\n    'http://www.twinsun.com/tz/tz-link.htm',  # Broken link from pytz library\n    'https://urllib3.readthedocs.io/en/latest/contrib.html#google-app-engine',  # Broken link from urllib3 library\n    'https://linuxize.com/post/how-to-add-swap-space-on-ubuntu-20-04/',\n    # This link works but gets 403 error on linkcheck\n]\nlinkcheck_timeout = 20\nlinkcheck_retries = 2\nlinkcheck_anchors = False\n\nogp_site_url = 'https://jina.ai/serve/'\nogp_image = 'https://jina.ai/serve/_static/banner.png'\nogp_use_first_image = True\nogp_description_length = 300\nogp_type = 'website'\nogp_site_name = (\n    f'Jina {SPHINX_MULTIVERSION_VERSION} Documentation'\n)\n\nogp_custom_meta_tags = [\n    '<meta name=\"twitter:card\" content=\"summary_large_image\">',\n    '<meta name=\"twitter:site\" content=\"@JinaAI_\">',\n    '<meta name=\"twitter:creator\" content=\"@JinaAI_\">',\n    '<meta name=\"description\" content=\"Build multimodal AI services via cloud native technologies · Neural Search · Generative AI · Cloud Native · MLOps\">',\n    '<meta property=\"og:description\" content=\"Build multimodal AI services via cloud native technologies · Neural Search · Generative AI · Cloud Native · MLOps\">',\n    '''\n    <!-- Global site tag (gtag.js) - Google Analytics -->\n<script async src=\"https://www.googletagmanager.com/gtag/js?id=G-48ZDWC8GT6\"></script>\n<script>\n  window.dataLayer = window.dataLayer || [];\n  function gtag(){dataLayer.push(arguments);}\n  gtag('js', new Date());\n\n  gtag('config', 'G-48ZDWC8GT6');\n</script>\n\n<script async defer src=\"https://buttons.github.io/buttons.js\"></script>\n    ''',\n]\n\n\ndef setup(app):\n    from sphinx.domains.python import PyField\n    from sphinx.locale import _\n    from sphinx.util.docfields import Field\n\n    app.add_object_type(\n        'confval',\n        'confval',\n        objname='configuration value',\n        indextemplate='pair: %s; configuration value',\n        doc_field_types=[\n            PyField(\n                'type',\n                label=_('Type'),\n                has_arg=False,\n                names=('type',),\n                bodyrolename='class',\n            ),\n            Field(\n                'default',\n                label=_('Default'),\n                has_arg=False,\n                names=('default',),\n            ),\n        ],\n    )\n", "metadata": {"source": "github.com/jina-ai/serve", "file_path": "docs/conf.py", "language": "python", "complexity": "complex", "star_count": 21665, "quality_score": 0.98, "features": {"has_classes": false, "has_functions": true, "has_async": true, "has_decorators": true, "has_imports": true, "has_error_handling": true, "has_documentation": true, "has_type_hints": true, "has_testing": true, "has_logging": true, "function_count": 1, "class_count": 0, "line_count": 227}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create REST API endpoint with proper request/response handling\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# noqa: D300,D400\n# Copyright (c) 2016, Aaron Christianson\n# All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are\n# met:\n#\n# 1. Redistributions of source code must retain the above copyright\n#    notice, this list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS\n# IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED\n# TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A\n# PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\n# HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\n# SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED\n# TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR\n# PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF\n# LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING\n# NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\n# SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n'''\nMonkey patch setuptools to write faster console_scripts with this format:\n\n    import sys\n    from mymodule import entry_function\n    sys.exit(entry_function())\n\nThis is better.\n\n(c) 2016, Aaron Christianson\nhttp://github.com/ninjaaron/fast-entry_points\n'''\nfrom setuptools.command import easy_install\nimport re\n\nTEMPLATE = r'''\n# -*- coding: utf-8 -*-\n# EASY-INSTALL-ENTRY-SCRIPT: '{3}','{4}','{5}'\n__requires__ = '{3}'\nimport re\nimport sys\n\nfrom {0} import {1}\n\nif __name__ == '__main__':\n    sys.argv[0] = re.sub(r'(-script\\.pyw?|\\.exe)?$', '', sys.argv[0])\n    sys.exit({2}())\n'''.lstrip()\n\n\n@classmethod\ndef get_args(cls, dist, header=None):  # noqa: D205,D400\n    \"\"\"\n    Yield write_script() argument tuples for a distribution's\n    console_scripts and gui_scripts entry points.\n    \"\"\"\n    if header is None:\n        # pylint: disable=E1101\n        header = cls.get_header()\n    spec = str(dist.as_requirement())\n    for type_ in 'console', 'gui':\n        group = type_ + '_scripts'\n        for name, ep in dist.get_entry_map(group).items():\n            # ensure_safe_name\n            if re.search(r'[\\\\/]', name):\n                raise ValueError(\"Path separators not allowed in script names\")\n            script_text = TEMPLATE.format(\n                ep.module_name, ep.attrs[0], '.'.join(ep.attrs), spec, group, name\n            )\n            # pylint: disable=E1101\n            args = cls._get_script_args(type_, name, header, script_text)\n            for res in args:\n                yield res\n\n\n# pylint: disable=E1101\neasy_install.ScriptWriter.get_args = get_args\n\n\ndef main():\n    import os\n    import re\n    import shutil\n    import sys\n\n    dests = sys.argv[1:] or ['.']\n    filename = re.sub(r'\\.pyc$', '.py', __file__)\n\n    for dst in dests:\n        shutil.copy(filename, dst)\n        manifest_path = os.path.join(dst, 'MANIFEST.in')\n        setup_path = os.path.join(dst, 'setup.py')\n\n        # Insert the include statement to MANIFEST.in if not present\n        with open(manifest_path, 'a+', encoding='utf-8') as manifest:\n            manifest.seek(0)\n            manifest_content = manifest.read()\n            if 'include fastentrypoints.py' not in manifest_content:\n                manifest.write(\n                    ('\\n' if manifest_content else '') + 'include fastentrypoints.py'\n                )\n\n        # Insert the import statement to setup.py if not present\n        with open(setup_path, 'a+', encoding='utf-8') as setup:\n            setup.seek(0)\n            setup_content = setup.read()\n            if 'import fastentrypoints' not in setup_content:\n                setup.seek(0)\n                setup.truncate()\n                setup.write('import fastentrypoints\\n' + setup_content)\n", "metadata": {"source": "github.com/jina-ai/serve", "file_path": "fastentrypoints.py", "language": "python", "complexity": "medium", "star_count": 21665, "quality_score": 0.9500000000000001, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": true, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 2, "class_count": 0, "line_count": 117}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create REST API endpoint with proper request/response handling\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"\nTop-level module of Jina.\n\nThe primary function of this module is to import all of the public Jina\ninterfaces into a single place. The interfaces themselves are located in\nsub-modules, as described below.\n\n\"\"\"\n\nimport os as _os\nimport platform as _platform\nimport signal as _signal\nimport sys as _sys\nimport warnings as _warnings\n\nimport docarray as _docarray\n\nif _sys.version_info < (3, 7, 0):\n    raise OSError(f'Jina requires Python >= 3.7, but yours is {_sys.version_info}')\n\n\ndef _warning_on_one_line(message, category, filename, lineno, *args, **kwargs):\n    return '\\033[1;33m%s: %s\\033[0m \\033[1;30m(raised from %s:%s)\\033[0m\\n' % (\n        category.__name__,\n        message,\n        filename,\n        lineno,\n    )\n\n\ndef _ignore_google_warnings():\n    import warnings\n\n    warnings.filterwarnings(\n        'ignore',\n        category=DeprecationWarning,\n        message='Deprecated call to `pkg_resources.declare_namespace(\\'google\\')`.',\n        append=True,\n    )\n\n\n_warnings.formatwarning = _warning_on_one_line\n_warnings.simplefilter('always', DeprecationWarning, append=True)\n_ignore_google_warnings()\n\n# fix fork error on MacOS but seems no effect? must do EXPORT manually before jina start\n_os.environ['OBJC_DISABLE_INITIALIZE_FORK_SAFETY'] = 'YES'\n\n# JINA_MP_START_METHOD has higher priority than os-patch\n_start_method = _os.environ.get('JINA_MP_START_METHOD', None)\nif _start_method and _start_method.lower() in {'fork', 'spawn', 'forkserver'}:\n    from multiprocessing import set_start_method as _set_start_method\n\n    try:\n        _set_start_method(_start_method.lower())\n        _warnings.warn(\n            f'multiprocessing start method is set to `{_start_method.lower()}`'\n        )\n    except Exception as e:\n        _warnings.warn(\n            f'failed to set multiprocessing start_method to `{_start_method.lower()}`: {e!r}'\n        )\nelif _sys.version_info >= (3, 8, 0) and _platform.system() == 'Darwin':\n    # DO SOME OS-WISE PATCHES\n\n    # temporary fix for python 3.8 on macos where the default start is set to \"spawn\"\n    # https://docs.python.org/3/library/multiprocessing.html#contexts-and-start-methods\n    from multiprocessing import set_start_method as _set_start_method\n\n    try:\n        _set_start_method('fork')\n        _warnings.warn(f'multiprocessing start method is set to `fork`')\n    except Exception as e:\n        _warnings.warn(f'failed to set multiprocessing start_method to `fork`: {e!r}')\n\n# do not change this line manually this is managed by git tag and updated on every release\n# NOTE: this represents the NEXT release version\n\n__version__ = '3.34.1'\n\n# do not change this line manually\n# this is managed by proto/build-proto.sh and updated on every execution\n__proto_version__ = '0.1.27'\n\ntry:\n    __docarray_version__ = _docarray.__version__\nexcept AttributeError as e:\n    raise RuntimeError(\n        '`docarray` dependency is not installed correctly, please reinstall with `pip install -U --force-reinstall docarray`'\n    )\n\ntry:\n    _signal.signal(_signal.SIGINT, _signal.default_int_handler)\nexcept Exception as exc:\n    _warnings.warn(f'failed to set default signal handler: {exc!r}`')\n\n\ndef _set_nofile(nofile_atleast=4096):\n    \"\"\"\n    Set nofile soft limit to at least 4096, useful for running matlplotlib/seaborn on\n    parallel executing plot generators vs. Ubuntu default ulimit -n 1024 or OS X El Captian 256\n    temporary setting extinguishing with Python session.\n\n    :param nofile_atleast: nofile soft limit\n    :return: nofile soft limit and nofile hard limit\n    \"\"\"\n\n    try:\n        import resource as res\n    except ImportError:  # Windows\n        res = None\n\n    if res is None:\n        return (None,) * 2\n\n    soft, ohard = res.getrlimit(res.RLIMIT_NOFILE)\n    hard = ohard\n\n    if soft < nofile_atleast:\n        soft = nofile_atleast\n        if hard < soft:\n            hard = soft\n\n        try:\n            res.setrlimit(res.RLIMIT_NOFILE, (soft, hard))\n        except (ValueError, res.error):\n            try:\n                hard = soft\n                print(f'trouble with max limit, retrying with soft,hard {soft},{hard}')\n                res.setrlimit(res.RLIMIT_NOFILE, (soft, hard))\n            except Exception:\n                print('failed to set ulimit, giving up')\n                soft, hard = res.getrlimit(res.RLIMIT_NOFILE)\n\n    return soft, hard\n\n\n_set_nofile()\n\n# ONLY FIRST CLASS CITIZENS ARE ALLOWED HERE, namely Document, Executor Flow\n\n# Document\nfrom jina._docarray import Document, DocumentArray\n\n# Client\nfrom jina.clients import Client\n\n# Deployment\nfrom jina.orchestrate.deployments import Deployment\nfrom jina.orchestrate.flow.asyncio import AsyncFlow\n\n# Flow\nfrom jina.orchestrate.flow.base import Flow\n\n# Executor\nfrom jina.serve.executors import BaseExecutor as Executor\nfrom jina.serve.executors.decorators import dynamic_batching, monitor, requests\n\n# Custom Gateway\nfrom jina.serve.runtimes.gateway.gateway import Gateway\n", "metadata": {"source": "github.com/jina-ai/serve", "file_path": "jina/__init__.py", "language": "python", "complexity": "complex", "star_count": 21665, "quality_score": 0.9500000000000001, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": true, "has_documentation": true, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 3, "class_count": 0, "line_count": 161}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create REST API endpoint with proper request/response handling\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from __future__ import annotations\nimport os\nfrom docarray import BaseDoc\nfrom docarray import DocList\n\nfrom pydantic import Field\nfrom typing import Any, Dict, Optional, List, Union\n\nfrom docarray.typing import ID, AnyEmbedding, AnyTensor\ndocarray_v2 = True\n\n\nclass LegacyDocumentJina(BaseDoc):\n    \"\"\"\n    This Document is the LegacyDocumentJina. It follows the same schema as in DocArray <=0.21.\n    It can be useful to start migrating a codebase from v1 to v2.\n\n    Nevertheless, the API is not totally compatible with DocArray <=0.21 `Document`.\n    Indeed, none of the method associated with `Document` are present. Only the schema\n    of the data is similar.\n\n    ```python\n    from docarray import DocList\n    from docarray.documents.legacy import LegacyDocument\n    import numpy as np\n\n    doc = LegacyDocument(text='hello')\n    doc.url = 'http://myimg.png'\n    doc.tensor = np.zeros((3, 224, 224))\n    doc.embedding = np.zeros((100, 1))\n\n    doc.tags['price'] = 10\n\n    doc.chunks = DocList[Document]([Document() for _ in range(10)])\n\n    doc.chunks = DocList[Document]([Document() for _ in range(10)])\n    ```\n\n    \"\"\"\n    id: Optional[ID] = Field(\n        description='The ID of the BaseDoc. This is useful for indexing in vector stores. If not set by user, it will automatically be assigned a random value',\n        default_factory=lambda: ID(os.urandom(16).hex()),\n    )\n\n    tensor: Optional[AnyTensor] = None\n    chunks: Optional[Union[DocList[LegacyDocumentJina], List[LegacyDocumentJina]]] = None\n    matches: Optional[Union[DocList[LegacyDocumentJina], List[LegacyDocumentJina]]] = None\n    blob: Optional[bytes] = None\n    text: Optional[str] = None\n    url: Optional[str] = None\n    embedding: Optional[AnyEmbedding] = None\n    tags: Dict[str, Any] = dict()\n    scores: Optional[Dict[str, Any]] = None\n", "metadata": {"source": "github.com/jina-ai/serve", "file_path": "jina/_docarray_legacy.py", "language": "python", "complexity": "simple", "star_count": 21665, "quality_score": 0.92, "features": {"has_classes": true, "has_functions": false, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": true, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 0, "class_count": 1, "line_count": 54}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create REST API endpoint with proper request/response handling\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import argparse\n\nfrom jina.helper import parse_host_scheme\nfrom jina.logging.predefined import default_logger\n\n\nclass NetworkChecker:\n    \"\"\"Check if a Deployment is running or not.\"\"\"\n\n    def __init__(self, args: 'argparse.Namespace'):\n        \"\"\"\n        Create a new :class:`NetworkChecker`.\n\n        :param args: args provided by the CLI.\n        \"\"\"\n\n        import time\n\n        from jina.clients import Client\n        from jina.logging.profile import TimeContext\n        from jina.serve.runtimes.servers import BaseServer\n\n        try:\n            total_time = 0\n            total_success = 0\n            timeout = args.timeout / 1000 if args.timeout != -1 else None\n            for j in range(args.attempts):\n                with TimeContext(\n                    f'ping {args.target} on {args.host} at {j} round', default_logger\n                ) as tc:\n                    if args.target == 'flow':\n                        r = Client(host=args.host).is_flow_ready(timeout=timeout)\n                    else:\n                        hostname, port, protocol, _ = parse_host_scheme(args.host)\n                        r = BaseServer.is_ready(\n                            ctrl_address=f'{hostname}:{port}',\n                            timeout=timeout,\n                            protocol=protocol,\n                        )\n                    if not r:\n                        default_logger.warning(\n                            'not responding, attempt (%d/%d) in 1s'\n                            % (j + 1, args.attempts)\n                        )\n                    else:\n                        total_success += 1\n                total_time += tc.duration\n                if args.attempts > 0:\n                    time.sleep(1)\n            if total_success < args.attempts:\n                default_logger.debug(\n                    'message lost %.0f%% (%d/%d) '\n                    % (\n                        (1 - total_success / args.attempts) * 100,\n                        args.attempts - total_success,\n                        args.attempts,\n                    )\n                )\n            if total_success > 0:\n                default_logger.debug(\n                    'avg. latency: %.0f ms' % (total_time / total_success * 1000)\n                )\n\n            if total_success >= args.min_successful_attempts:\n                default_logger.debug(\n                    f'readiness check succeeded {total_success} times!!!'\n                )\n                exit(0)\n            else:\n                default_logger.debug(\n                    f'readiness check succeeded {total_success} times, less than {args.min_successful_attempts}'\n                )\n        except KeyboardInterrupt:\n            pass\n\n        # returns 1 (anomaly) when it comes to here\n        exit(1)\n", "metadata": {"source": "github.com/jina-ai/serve", "file_path": "jina/checker.py", "language": "python", "complexity": "medium", "star_count": 21665, "quality_score": 1.0, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": true, "has_documentation": true, "has_type_hints": true, "has_testing": false, "has_logging": true, "function_count": 1, "class_count": 1, "line_count": 78}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create REST API endpoint with proper request/response handling\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"Module wrapping the Client of Jina.\"\"\"\n\nimport argparse\nfrom typing import TYPE_CHECKING, List, Optional, Union, overload\n\nfrom jina.helper import parse_client\n\n__all__ = ['Client']\n\nfrom jina.enums import ProtocolType\n\nif TYPE_CHECKING:  # pragma: no cover\n    from jina.clients.grpc import AsyncGRPCClient, GRPCClient\n    from jina.clients.http import AsyncHTTPClient, HTTPClient\n    from jina.clients.websocket import AsyncWebSocketClient, WebSocketClient\n\n\n# overload_inject_start_client\n@overload\ndef Client(\n    *,\n    asyncio: Optional[bool] = False,\n    grpc_channel_options: Optional[dict] = None,\n    host: Optional[str] = '0.0.0.0',\n    log_config: Optional[str] = None,\n    metrics: Optional[bool] = False,\n    metrics_exporter_host: Optional[str] = None,\n    metrics_exporter_port: Optional[int] = None,\n    port: Optional[int] = None,\n    prefetch: Optional[int] = 1000,\n    protocol: Optional[Union[str, List[str]]] = 'GRPC',\n    proxy: Optional[bool] = False,\n    reuse_session: Optional[bool] = False,\n    suppress_root_logging: Optional[bool] = False,\n    tls: Optional[bool] = False,\n    traces_exporter_host: Optional[str] = None,\n    traces_exporter_port: Optional[int] = None,\n    tracing: Optional[bool] = False,\n    **kwargs\n) -> Union[\n    'AsyncWebSocketClient',\n    'WebSocketClient',\n    'AsyncGRPCClient',\n    'GRPCClient',\n    'HTTPClient',\n    'AsyncHTTPClient',\n]:\n    \"\"\"Create a Client. Client is how user interact with Flow\n\n    :param asyncio: If set, then the input and output of this Client work in an asynchronous manner.\n    :param grpc_channel_options: Dictionary of kwargs arguments that will be passed to the grpc channel as options when creating a channel, example : {'grpc.max_send_message_length': -1}. When max_attempts > 1, the 'grpc.service_config' option will not be applicable.\n    :param host: The host of the Gateway, which the client should connect to, by default it is 0.0.0.0.\n    :param log_config: The config name or the absolute path to the YAML config file of the logger used in this object.\n    :param metrics: If set, the sdk implementation of the OpenTelemetry metrics will be available for default monitoring and custom measurements. Otherwise a no-op implementation will be provided.\n    :param metrics_exporter_host: If tracing is enabled, this hostname will be used to configure the metrics exporter agent.\n    :param metrics_exporter_port: If tracing is enabled, this port will be used to configure the metrics exporter agent.\n    :param port: The port of the Gateway, which the client should connect to.\n    :param prefetch: Number of requests fetched from the client before feeding into the first Executor.\n\n              Used to control the speed of data input into a Flow. 0 disables prefetch (1000 requests is the default)\n    :param protocol: Communication protocol between server and client.\n    :param proxy: If set, respect the http_proxy and https_proxy environment variables. otherwise, it will unset these proxy variables before start. gRPC seems to prefer no proxy\n    :param reuse_session: True if HTTPClient should reuse ClientSession. If true, user will be responsible to close it\n    :param suppress_root_logging: If set, then no root handlers will be suppressed from logging.\n    :param tls: If set, connect to gateway using tls encryption\n    :param traces_exporter_host: If tracing is enabled, this hostname will be used to configure the trace exporter agent.\n    :param traces_exporter_port: If tracing is enabled, this port will be used to configure the trace exporter agent.\n    :param tracing: If set, the sdk implementation of the OpenTelemetry tracer will be available and will be enabled for automatic tracing of requests and customer span creation. Otherwise a no-op implementation will be provided.\n    :return: the new Client object\n\n    .. # noqa: DAR202\n    .. # noqa: DAR101\n    .. # noqa: DAR003\n    \"\"\"\n    # overload_inject_end_client\n\n\ndef Client(args: Optional['argparse.Namespace'] = None, **kwargs) -> Union[\n    'AsyncWebSocketClient',\n    'WebSocketClient',\n    'AsyncGRPCClient',\n    'GRPCClient',\n    'HTTPClient',\n    'AsyncHTTPClient',\n]:\n    # implementation_stub_inject_start_client\n\n    \"\"\"Convenience function that returns client instance for given protocol.\n\n    EXAMPLE USAGE\n\n    .. code-block:: python\n\n        from jina import Client\n        from docarray import Document\n\n        # select protocol from 'grpc', 'http', or 'websocket'; default is 'grpc'\n        # select asyncio True of False; default is False\n        # select host address to connect to\n        c = Client(\n            protocol='grpc', asyncio=False, host='grpc://my.awesome.flow:1234'\n        )  # returns GRPCClient instance\n        c.post(on='/index', inputs=Document(text='hello!'))\n\n    :param asyncio: If set, then the input and output of this Client work in an asynchronous manner.\n    :param grpc_channel_options: Dictionary of kwargs arguments that will be passed to the grpc channel as options when creating a channel, example : {'grpc.max_send_message_length': -1}. When max_attempts > 1, the 'grpc.service_config' option will not be applicable.\n    :param host: The host of the Gateway, which the client should connect to, by default it is 0.0.0.0.\n    :param log_config: The config name or the absolute path to the YAML config file of the logger used in this object.\n    :param metrics: If set, the sdk implementation of the OpenTelemetry metrics will be available for default monitoring and custom measurements. Otherwise a no-op implementation will be provided.\n    :param metrics_exporter_host: If tracing is enabled, this hostname will be used to configure the metrics exporter agent.\n    :param metrics_exporter_port: If tracing is enabled, this port will be used to configure the metrics exporter agent.\n    :param port: The port of the Gateway, which the client should connect to.\n    :param prefetch: Number of requests fetched from the client before feeding into the first Executor.\n\n              Used to control the speed of data input into a Flow. 0 disables prefetch (1000 requests is the default)\n    :param protocol: Communication protocol between server and client.\n    :param proxy: If set, respect the http_proxy and https_proxy environment variables. otherwise, it will unset these proxy variables before start. gRPC seems to prefer no proxy\n    :param reuse_session: True if HTTPClient should reuse ClientSession. If true, user will be responsible to close it\n    :param suppress_root_logging: If set, then no root handlers will be suppressed from logging.\n    :param tls: If set, connect to gateway using tls encryption\n    :param traces_exporter_host: If tracing is enabled, this hostname will be used to configure the trace exporter agent.\n    :param traces_exporter_port: If tracing is enabled, this port will be used to configure the trace exporter agent.\n    :param tracing: If set, the sdk implementation of the OpenTelemetry tracer will be available and will be enabled for automatic tracing of requests and customer span creation. Otherwise a no-op implementation will be provided.\n    :return: the new Client object\n\n    .. # noqa: DAR102\n    .. # noqa: DAR202\n    .. # noqa: DAR101\n    .. # noqa: DAR003\n    \"\"\"\n    # implementation_stub_inject_end_client\n    if not (\n        args and isinstance(args, argparse.Namespace)\n    ):  # we need to parse the kwargs as soon as possible otherwise to get the gateway type\n        args = parse_client(kwargs)\n\n    protocol = args.protocol if args else kwargs.get('protocol', ProtocolType.GRPC)\n    if isinstance(protocol, str):\n        protocol = ProtocolType.from_string(protocol)\n\n    is_async = (args and args.asyncio) or kwargs.get('asyncio', False)\n\n    if protocol == ProtocolType.GRPC:\n        if is_async:\n            from jina.clients.grpc import AsyncGRPCClient\n\n            return AsyncGRPCClient(args, **kwargs)\n        else:\n            from jina.clients.grpc import GRPCClient\n\n            return GRPCClient(args, **kwargs)\n    elif protocol == ProtocolType.WEBSOCKET:\n        if is_async:\n            from jina.clients.websocket import AsyncWebSocketClient\n\n            return AsyncWebSocketClient(args, **kwargs)\n        else:\n            from jina.clients.websocket import WebSocketClient\n\n            return WebSocketClient(args, **kwargs)\n    elif protocol == ProtocolType.HTTP:\n        if is_async:\n            from jina.clients.http import AsyncHTTPClient\n\n            return AsyncHTTPClient(args, **kwargs)\n        else:\n            from jina.clients.http import HTTPClient\n\n            return HTTPClient(args, **kwargs)\n", "metadata": {"source": "github.com/jina-ai/serve", "file_path": "jina/clients/__init__.py", "language": "python", "complexity": "complex", "star_count": 21665, "quality_score": 1.0, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": true, "has_type_hints": true, "has_testing": false, "has_logging": true, "function_count": 2, "class_count": 0, "line_count": 170}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create REST API endpoint with proper request/response handling with asyncio coroutines\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"Module containing the Base Client for Jina.\"\"\"\n\nimport abc\nimport argparse\nimport inspect\nimport os\nfrom abc import ABC\nfrom typing import TYPE_CHECKING, AsyncIterator, Callable, Iterator, Optional, Union, Tuple\n\nfrom jina.excepts import BadClientInput\nfrom jina.helper import T, parse_client, send_telemetry_event, typename\nfrom jina.logging.logger import JinaLogger\nfrom jina.logging.predefined import default_logger\n\nif TYPE_CHECKING:  # pragma: no cover\n    from jina.clients.request import GeneratorSourceType\n    from jina.types.request import Request, Response\n\n    InputType = Union[GeneratorSourceType, Callable[..., GeneratorSourceType]]\n    CallbackFnType = Optional[Callable[[Response], None]]\nfrom jina.serve.instrumentation import InstrumentationMixin\n\n\nclass BaseClient(InstrumentationMixin, ABC):\n    \"\"\"A base client for connecting to the Flow Gateway.\n\n    :param args: the Namespace from argparse\n    :param kwargs: additional parameters that can be accepted by client parser\n    \"\"\"\n\n    def __init__(\n            self,\n            args: Optional['argparse.Namespace'] = None,\n            **kwargs,\n    ):\n        if args and isinstance(args, argparse.Namespace):\n            self.args = args\n        else:\n            self.args = parse_client(kwargs)\n\n        self.logger = JinaLogger(self.__class__.__name__, **vars(self.args))\n\n        if not self.args.proxy and os.name != 'nt':\n            # (Han 2020 12.12): gRPC channel is over HTTP2 and it does not work when we have proxy\n            # as many enterprise users are behind proxy, a quick way to\n            # surpass it is by temporally unset proxy. Please do NOT panic as it will NOT\n            # affect users os-level envs.\n            os.unsetenv('http_proxy')\n            os.unsetenv('https_proxy')\n        self._setup_instrumentation(\n            name=(\n                self.args.name\n                if hasattr(self.args, 'name')\n                else self.__class__.__name__\n            ),\n            tracing=self.args.tracing,\n            traces_exporter_host=self.args.traces_exporter_host,\n            traces_exporter_port=self.args.traces_exporter_port,\n            metrics=self.args.metrics,\n            metrics_exporter_host=self.args.metrics_exporter_host,\n            metrics_exporter_port=self.args.metrics_exporter_port,\n        )\n        send_telemetry_event(event='start', obj_cls_name=self.__class__.__name__)\n\n    async def close(self):\n        \"\"\"Closes the potential resources of the Client.\n        :return: Return whatever a close method may return\n        \"\"\"\n        return self.teardown_instrumentation()\n\n    def teardown_instrumentation(self):\n        \"\"\"Shut down the OpenTelemetry tracer and meter if available. This ensures that the daemon threads for\n        exporting metrics data is properly cleaned up.\n        \"\"\"\n        try:\n            if self.tracing and self.tracer_provider:\n                if hasattr(self.tracer_provider, 'force_flush'):\n                    self.tracer_provider.force_flush()\n                if hasattr(self.tracer_provider, 'shutdown'):\n                    self.tracer_provider.shutdown()\n            if self.metrics and self.meter_provider:\n                if hasattr(self.meter_provider, 'force_flush'):\n                    self.meter_provider.force_flush()\n                if hasattr(self.meter_provider, 'shutdown'):\n                    self.meter_provider.shutdown()\n        except Exception as ex:\n            self.logger.warning(f'Exception during instrumentation teardown, {str(ex)}')\n\n    @staticmethod\n    def check_input(inputs: Optional['InputType'] = None, **kwargs) -> None:\n        \"\"\"Validate the inputs and print the first request if success.\n\n        :param inputs: the inputs\n        :param kwargs: keyword arguments\n        \"\"\"\n\n        if inputs is None:\n            # empty inputs is considered as valid\n            return\n\n        if hasattr(inputs, '__call__'):\n            # it is a function\n            inputs = inputs()\n\n        kwargs['data'] = inputs\n        kwargs['exec_endpoint'] = '/'\n\n        if inspect.isasyncgenfunction(inputs) or inspect.isasyncgen(inputs):\n            raise BadClientInput(\n                'checking the validity of an async generator is not implemented yet'\n            )\n\n        try:\n            from jina.clients.request import request_generator\n\n            r = next(request_generator(**kwargs))\n            from jina.types.request import Request\n\n            if not isinstance(r, Request):\n                raise TypeError(f'{typename(r)} is not a valid Request')\n        except Exception as ex:\n            default_logger.error(f'inputs is not valid!')\n            raise BadClientInput from ex\n\n    def _get_requests(\n            self, inputs, **kwargs\n    ) -> Tuple[Union[Iterator['Request'], AsyncIterator['Request']], Optional[int]]:\n        \"\"\"\n        Get request in generator.\n\n        :param inputs: The inputs argument to get the requests from.\n        :param kwargs: Keyword arguments.\n        :return: Iterator of request and the length of the inputs.\n        \"\"\"\n        _kwargs = vars(self.args)\n        if hasattr(inputs, '__call__'):\n            inputs = inputs()\n\n        _kwargs['data'] = inputs\n        # override by the caller-specific kwargs\n        _kwargs.update(kwargs)\n\n        if hasattr(inputs, '__len__'):\n            total_docs = len(inputs)\n        elif 'total_docs' in _kwargs:\n            total_docs = _kwargs['total_docs']\n        else:\n            total_docs = None\n\n        if total_docs:\n            inputs_length = max(1, total_docs / _kwargs['request_size'])\n        else:\n            inputs_length = None\n\n        if inspect.isasyncgen(inputs):\n            from jina.clients.request.asyncio import request_generator\n\n            return request_generator(**_kwargs), inputs_length\n        else:\n            from jina.clients.request import request_generator\n\n            return request_generator(**_kwargs), inputs_length\n\n    @abc.abstractmethod\n    async def _get_results(\n            self,\n            inputs: 'InputType',\n            on_done: 'CallbackFnType',\n            on_error: Optional['CallbackFnType'] = None,\n            on_always: Optional['CallbackFnType'] = None,\n            **kwargs,\n    ):\n        ...\n\n    @abc.abstractmethod\n    def _is_flow_ready(self, **kwargs) -> bool:\n        \"\"\"Sends a dry run to the Flow to validate if the Flow is ready to receive requests\n\n        :param kwargs: potential kwargs received passed from the public interface\n        \"\"\"\n        ...\n\n    @property\n    def client(self: T) -> T:\n        \"\"\"Return the client object itself\n\n        :return: the Client object\n        \"\"\"\n        return self\n\n    async def _get_streaming_results(self, **kwargs):\n        \"\"\"Get the results from the Flow\n\n        :param kwargs: potential kwargs received passed from the public interface\n        \"\"\"\n        raise NotImplementedError(\n            f'Streaming endpoints are not supported yet for {self.__class__.__name__}'\n        )\n", "metadata": {"source": "github.com/jina-ai/serve", "file_path": "jina/clients/base/__init__.py", "language": "python", "complexity": "complex", "star_count": 21665, "quality_score": 1.0, "features": {"has_classes": true, "has_functions": true, "has_async": true, "has_decorators": true, "has_imports": true, "has_error_handling": true, "has_documentation": true, "has_type_hints": true, "has_testing": false, "has_logging": true, "function_count": 9, "class_count": 1, "line_count": 199}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create REST API endpoint with proper request/response handling with asyncio coroutines\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import asyncio\nimport json\nimport threading\nfrom typing import TYPE_CHECKING, Any, Dict, List, Optional, Tuple, Type\n\nimport grpc\nfrom grpc import RpcError\n\nfrom jina._docarray import Document, docarray_v2\nfrom jina.clients.base import BaseClient\nfrom jina.clients.base.stream_rpc import StreamRpc\nfrom jina.clients.base.unary_rpc import UnaryRpc\nfrom jina.clients.helper import callback_exec\nfrom jina.excepts import BadClientInput, BadServerFlow, InternalNetworkError\nfrom jina.logging.profile import ProgressBar\nfrom jina.proto import jina_pb2, jina_pb2_grpc\nfrom jina.serve.helper import extract_trailing_metadata, get_default_grpc_options\nfrom jina.serve.networking.utils import get_grpc_channel\nfrom jina.types.request.data import SingleDocumentRequest\n\nif TYPE_CHECKING:  # pragma: no cover\n    from jina.clients.base import CallbackFnType, InputType\n\n\nclass GRPCBaseClient(BaseClient):\n    \"\"\"A simple Python client for connecting to the gRPC gateway.\n\n    It manages the asyncio event loop internally, so all interfaces are synchronous from the outside.\n    \"\"\"\n\n    _lock = threading.RLock()\n\n    async def _is_flow_ready(self, **kwargs) -> bool:\n        \"\"\"Sends a dry run to the Flow to validate if the Flow is ready to receive requests\n\n        :param kwargs: potential kwargs received passed from the public interface\n        :return: boolean indicating the health/readiness of the Flow\n        \"\"\"\n        try:\n            async with get_grpc_channel(\n                f'{self.args.host}:{self.args.port}',\n                asyncio=True,\n                tls=self.args.tls,\n            ) as channel:\n                stub = jina_pb2_grpc.JinaGatewayDryRunRPCStub(channel)\n                self.logger.debug(f'connected to {self.args.host}:{self.args.port}')\n                call_result = stub.dry_run(\n                    jina_pb2.google_dot_protobuf_dot_empty__pb2.Empty(),\n                    metadata=kwargs.get('metadata', None),\n                    credentials=kwargs.get('credentials', None),\n                    timeout=kwargs.get('timeout', None),\n                )\n                metadata, response = (\n                    await call_result.trailing_metadata(),\n                    await call_result,\n                )\n                if response.code == jina_pb2.StatusProto.SUCCESS:\n                    return True\n                else:\n                    self.logger.error(\n                        f'Returned code is not expected! Exception: {response.exception}'\n                    )\n        except RpcError as e:\n            self.logger.error(f'RpcError: {e.details()}')\n        except Exception as e:\n            self.logger.error(f'Error while getting response from grpc server {e!r}')\n\n        return False\n\n    async def _get_results(\n        self,\n        inputs: 'InputType',\n        on_done: 'CallbackFnType',\n        on_error: Optional['CallbackFnType'] = None,\n        on_always: Optional['CallbackFnType'] = None,\n        compression: Optional[str] = None,\n        max_attempts: int = 1,\n        initial_backoff: float = 0.5,\n        max_backoff: float = 2,\n        backoff_multiplier: float = 1.5,\n        results_in_order: bool = False,\n        stream: bool = True,\n        prefetch: Optional[int] = None,\n        **kwargs,\n    ):\n        try:\n            self.compression = (\n                getattr(grpc.Compression, compression)\n                if compression\n                else grpc.Compression.NoCompression\n            )\n\n            req_iter, inputs_length = self._get_requests(inputs=inputs, **kwargs)\n            continue_on_error = self.continue_on_error\n            # while loop with retries, check in which state the `iterator` remains after failure\n            options = client_grpc_options(\n                backoff_multiplier,\n                initial_backoff,\n                max_attempts,\n                max_backoff,\n                self.args.grpc_channel_options,\n            )\n\n            metadata = kwargs.pop('metadata', ())\n            if results_in_order:\n                metadata = metadata + (('__results_in_order__', 'true'),)\n\n            if prefetch:\n                metadata = metadata + (('__prefetch__', str(prefetch)),)\n\n            with self._lock:\n                async with get_grpc_channel(\n                    f'{self.args.host}:{self.args.port}',\n                    options=options,\n                    asyncio=True,\n                    tls=self.args.tls,\n                    aio_tracing_client_interceptors=self.aio_tracing_client_interceptors(),\n                ) as channel:\n                    self.logger.debug(f'connected to {self.args.host}:{self.args.port}')\n\n                    with ProgressBar(\n                        total_length=inputs_length, disable=not self.show_progress\n                    ) as p_bar:\n                        try:\n                            if stream:\n                                stream_rpc = StreamRpc(\n                                    channel=channel,\n                                    continue_on_error=continue_on_error,\n                                    metadata=metadata,\n                                    on_always=on_always,\n                                    on_done=on_done,\n                                    on_error=on_error,\n                                    p_bar=p_bar,\n                                    req_iter=req_iter,\n                                    max_attempts=max_attempts,\n                                    backoff_multiplier=backoff_multiplier,\n                                    initial_backoff=initial_backoff,\n                                    max_backoff=max_backoff,\n                                    logger=self.logger,\n                                    show_progress=self.show_progress,\n                                    compression=self.compression,\n                                    **kwargs,\n                                )\n                                async for (\n                                    response\n                                ) in stream_rpc.stream_rpc_with_retry():\n                                    yield response\n                            else:\n                                unary_rpc = UnaryRpc(\n                                    channel=channel,\n                                    continue_on_error=continue_on_error,\n                                    metadata=metadata,\n                                    on_always=on_always,\n                                    on_done=on_done,\n                                    on_error=on_error,\n                                    p_bar=p_bar,\n                                    req_iter=req_iter,\n                                    max_attempts=max_attempts,\n                                    backoff_multiplier=backoff_multiplier,\n                                    initial_backoff=initial_backoff,\n                                    max_backoff=max_backoff,\n                                    logger=self.logger,\n                                    show_progress=self.show_progress,\n                                    compression=self.compression,\n                                    client_args=self.args,\n                                    prefetch=prefetch,\n                                    results_in_order=results_in_order,\n                                    **kwargs,\n                                )\n                                async for response in unary_rpc.unary_rpc_with_retry():\n                                    yield response\n                        except (grpc.aio.AioRpcError, InternalNetworkError) as err:\n                            await self._handle_error_and_metadata(err)\n        except KeyboardInterrupt:\n            self.logger.warning('user cancel the process')\n        except asyncio.CancelledError as ex:\n            self.logger.warning(f'process error: {ex!r}')\n            raise\n        except:\n            # Not sure why, adding this line helps in fixing a hanging test\n            raise\n\n    async def _handle_error_and_metadata(self, err):\n        my_code = err.code()\n        my_details = err.details()\n        trailing_metadata = extract_trailing_metadata(err)\n        msg = f'gRPC error: {my_code} {my_details}\\n{trailing_metadata}'\n        if my_code == grpc.StatusCode.UNAVAILABLE:\n            self.logger.error(\n                f'{msg}\\nThe ongoing request is terminated as the server is not available or closed already.'\n            )\n            raise ConnectionError(msg)\n        if my_code == grpc.StatusCode.NOT_FOUND:\n            self.logger.error(\n                f'{msg}\\nThe ongoing request is terminated as a resource cannot be found.'\n            )\n            raise ConnectionError(msg)\n        elif my_code == grpc.StatusCode.DEADLINE_EXCEEDED:\n            self.logger.error(\n                f'{msg}\\nThe ongoing request is terminated due to a server-side timeout.'\n            )\n            raise ConnectionError(msg)\n        elif my_code == grpc.StatusCode.INTERNAL:\n            self.logger.error(f'{msg}\\ninternal error on the server side')\n            raise err\n        elif (\n            my_code == grpc.StatusCode.UNKNOWN\n            and 'asyncio.exceptions.TimeoutError' in my_details\n        ):\n            raise BadClientInput(\n                f'{msg}\\n'\n                'often the case is that you define/send a bad input iterator to jina, '\n                'please double check your input iterator'\n            ) from err\n        else:\n            raise BadServerFlow(msg) from err\n\n    async def stream_doc_endpoint(\n        self,\n        request: SingleDocumentRequest,\n        timeout: Optional[float] = None,\n    ):\n        \"\"\"\n        Use the stream_doc stub to send one document and stream documents back from the Executor\n\n        :param request: The request to be sent\n        :param timeout: defines timeout for sending request\n\n        :yields: response document\n        \"\"\"\n        async with get_grpc_channel(\n            f'{self.args.host}:{self.args.port}',\n            asyncio=True,\n            tls=self.args.tls,\n            aio_tracing_client_interceptors=self.aio_tracing_client_interceptors(),\n        ) as channel:\n            stub = jina_pb2_grpc.JinaSingleDocumentRequestRPCStub(channel)\n            try:\n                async for response in stub.stream_doc(request, timeout=timeout):\n                    callback_exec(\n                        response=response,\n                        logger=self.logger,\n                    )\n                    yield response\n            except (grpc.aio.AioRpcError, InternalNetworkError) as err:\n                await self._handle_error_and_metadata(err)\n\n    async def _get_streaming_results(\n        self,\n        on: str,\n        inputs: 'Document',\n        parameters: Optional[Dict] = None,\n        return_type: Type[Document] = Document,\n        timeout: Optional[int] = None,\n        **kwargs,\n    ):\n        req = SingleDocumentRequest()\n        req.header.exec_endpoint = on\n        req.document_cls = inputs.__class__\n        req.data.doc = inputs\n        async for response in self.stream_doc_endpoint(request=req, timeout=timeout):\n            yield return_type.from_protobuf(response.document)\n\n\ndef client_grpc_options(\n    backoff_multiplier: float,\n    initial_backoff: float,\n    max_attempts: int,\n    max_backoff: float,\n    args_channel_options: Optional[Dict[str, Any]],\n) -> List[Tuple[str, Any]]:\n    \"\"\"\n    Builds grpc options for the client by taking into account the retry parameters, default gRPC options and additional gRPC options.\n    :param max_attempts: Maximum number of attempts that are allowed.\n    :param backoff_multiplier: Factor that will be raised to the exponent of (attempt - 1) for calculating the backoff wait time.\n    :param initial_backoff: The backoff time on the first error. This will be multiplied by the backoff_multiplier exponent for subsequent wait time calculations.\n    :param max_backoff: The maximum backoff wait time.\n    :param args_channel_options: additional gRPC options that must be merged with the default options\n    :return: List of gRPC options\n    \"\"\"\n    grpc_options = get_default_grpc_options()\n\n    if args_channel_options:\n        grpc_options.extend(list(args_channel_options))\n\n    if max_attempts > 1:\n        options_with_retry_config = remove_grpc_service_config(grpc_options)\n        service_config_json = json.dumps(\n            {\n                \"methodConfig\": [\n                    {\n                        # To apply retry to all methods, put [{}] in the \"name\" field\n                        \"name\": [{}],\n                        \"retryPolicy\": {\n                            \"maxAttempts\": max_attempts,\n                            \"initialBackoff\": f\"{initial_backoff}s\",\n                            \"maxBackoff\": f\"{max_backoff}s\",\n                            \"backoffMultiplier\": backoff_multiplier,\n                            \"retryableStatusCodes\": [\n                                \"UNAVAILABLE\",\n                                \"DEADLINE_EXCEEDED\",\n                                \"INTERNAL\",\n                            ],\n                        },\n                    }\n                ]\n            }\n        )\n        # NOTE: the retry feature will be enabled by default >=v1.40.0\n        options_with_retry_config.append((\"grpc.enable_retries\", 1))\n        options_with_retry_config.append((\"grpc.service_config\", service_config_json))\n        return options_with_retry_config\n\n    return grpc_options\n\n\ndef remove_grpc_service_config(options: List[Tuple[str, Any]]) -> List[Tuple[str, Any]]:\n    \"\"\"\n    Removes the 'grpc.service_config' configuration from the list of options.\n    :param options: List of gRPC options\n    :return: List of gRPC options\n    \"\"\"\n    return [tup for tup in options if tup[0] != 'grpc.service_config']\n", "metadata": {"source": "github.com/jina-ai/serve", "file_path": "jina/clients/base/grpc.py", "language": "python", "complexity": "complex", "star_count": 21665, "quality_score": 1.0, "features": {"has_classes": true, "has_functions": true, "has_async": true, "has_decorators": false, "has_imports": true, "has_error_handling": true, "has_documentation": true, "has_type_hints": true, "has_testing": false, "has_logging": true, "function_count": 7, "class_count": 1, "line_count": 324}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create REST API endpoint with proper request/response handling with asyncio coroutines\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import asyncio\nfrom abc import ABC, abstractmethod\nfrom typing import TYPE_CHECKING, Dict, Optional, Union\n\nimport aiohttp\nfrom aiohttp import WSMsgType\nfrom aiohttp.payload import BytesPayload\nfrom starlette import status\n\nfrom jina._docarray import docarray_v2\nfrom jina.clients.base import retry\nfrom jina.enums import WebsocketSubProtocols\nfrom jina.excepts import BadClient\nfrom jina.importer import ImportExtensions\nfrom jina.types.request import Request\nfrom jina.types.request.data import DataRequest\nfrom jina.types.request.status import StatusMessage\n\nif TYPE_CHECKING:  # pragma: no cover\n    from opentelemetry import trace\n\n    from jina._docarray import Document\n    from jina.logging.logger import JinaLogger\n\nif docarray_v2:\n    from docarray.base_doc.io.json import orjson_dumps\n\n    class JinaJsonPayload(BytesPayload):\n        \"\"\"A JSON payload for Jina Requests\"\"\"\n\n        def __init__(\n            self,\n            value,\n            *args,\n            **kwargs,\n        ) -> None:\n            super().__init__(\n                orjson_dumps(value),\n                content_type=\"application/json\",\n                encoding=\"utf-8\",\n                *args,\n                **kwargs,\n            )\n\n\nclass AioHttpClientlet(ABC):\n    \"\"\"aiohttp session manager\"\"\"\n\n    def __init__(\n        self,\n        logger: 'JinaLogger',\n        max_attempts: int = 1,\n        initial_backoff: float = 0.5,\n        max_backoff: float = 2,\n        backoff_multiplier: float = 1.5,\n        tracer_provider: Optional['trace.TraceProvider'] = None,\n        **kwargs,\n    ) -> None:\n        \"\"\"HTTP Client to be used with the streamer\n\n        :param logger: jina logger\n        :param max_attempts: Number of sending attempts, including the original request.\n        :param initial_backoff: The first retry will happen with a delay of random(0, initial_backoff)\n        :param max_backoff: The maximum accepted backoff after the exponential incremental delay\n        :param backoff_multiplier: The n-th attempt will occur at random(0, min(initialBackoff*backoffMultiplier**(n-1), maxBackoff))\n        :param tracer_provider: Optional tracer_provider that will be used to configure aiohttp tracing.\n        :param kwargs: kwargs  which will be forwarded to the `aiohttp.Session` instance. Used to pass headers to requests\n        \"\"\"\n        self.logger = logger\n        self.msg_recv = 0\n        self.msg_sent = 0\n        if tracer_provider:\n            from opentelemetry.instrumentation.aiohttp_client import create_trace_config\n\n            self._trace_config = [create_trace_config(tracer_provider=tracer_provider)]\n        else:\n            self._trace_config = None\n        self.session = None\n        self._session_kwargs = {}\n        if kwargs.get('headers', None):\n            self._session_kwargs['headers'] = kwargs.get('headers')\n        if kwargs.get('auth', None):\n            self._session_kwargs['auth'] = kwargs.get('auth')\n        if kwargs.get('cookies', None):\n            self._session_kwargs['cookies'] = kwargs.get('cookies')\n        if kwargs.get('timeout', None):\n            timeout = aiohttp.ClientTimeout(total=kwargs.get('timeout'))\n            self._session_kwargs['timeout'] = timeout\n        self.max_attempts = max_attempts\n        self.initial_backoff = initial_backoff\n        self.max_backoff = max_backoff\n        self.backoff_multiplier = backoff_multiplier\n\n    @abstractmethod\n    async def send_message(self, **kwargs):\n        \"\"\"Send message to Gateway\n        :param kwargs: kwargs which will be forwarded to the `aiohttp.Session.post` method. Used to pass headers to requests\n        \"\"\"\n        ...\n\n    @abstractmethod\n    async def send_dry_run(self, **kwargs):\n        \"\"\"Query the dry_run endpoint from Gateway\n        :param kwargs: kwargs which will be forwarded to the `aiohttp.Session.post` method. Used to pass headers to requests\n        \"\"\"\n        ...\n\n    @abstractmethod\n    async def recv_message(self):\n        \"\"\"Receive message from Gateway\"\"\"\n        ...\n\n    async def recv_dry_run(self):\n        \"\"\"Receive dry run response from Gateway\"\"\"\n        pass\n\n    async def __aenter__(self):\n        \"\"\"enter async context\n\n        :return: start self\n        \"\"\"\n        return await self.start()\n\n    async def start(self):\n        \"\"\"Create ClientSession and enter context\n\n        :return: self\n        \"\"\"\n        with ImportExtensions(required=True):\n            import aiohttp\n        self.session = aiohttp.ClientSession(\n            **self._session_kwargs, trace_configs=self._trace_config\n        )\n        await self.session.__aenter__()\n        return self\n\n    async def __aexit__(self, exc_type, exc_val, exc_tb):\n        await self.close(exc_type, exc_val, exc_tb)\n\n    async def close(self, *args, **kwargs):\n        \"\"\"Close ClientSession\n\n        :param args: positional args\n        :param kwargs: keyword args\"\"\"\n        await self.session.__aexit__(*args, **kwargs)\n\n\nclass HTTPClientlet(AioHttpClientlet):\n    \"\"\"HTTP Client to be used with the streamer\"\"\"\n\n    UPDATE_EVENT_PREFIX = 14  # the update event has the following format: \"event: update: {document_json}\"\n\n    async def send_message(self, url, request: 'Request'):\n        \"\"\"Sends a POST request to the server\n\n        :param url: the URL where to send the message\n        :param request: request as dict\n        :return: send post message\n        \"\"\"\n        req_dict = request.to_dict()\n        req_dict['exec_endpoint'] = req_dict['header']['exec_endpoint']\n        if 'target_executor' in req_dict['header']:\n            req_dict['target_executor'] = req_dict['header']['target_executor']\n        for attempt in range(1, self.max_attempts + 1):\n            try:\n                request_kwargs = {'url': url}\n                if not docarray_v2:\n                    request_kwargs['json'] = req_dict\n                else:\n                    from docarray.base_doc.io.json import orjson_dumps\n\n                    request_kwargs['data'] = JinaJsonPayload(value=req_dict)\n\n                async with self.session.post(**request_kwargs) as response:\n                    try:\n                        r_str = await response.json()\n                    except aiohttp.ContentTypeError:\n                        r_str = await response.text()\n                    r_status = response.status\n                    handle_response_status(r_status, r_str, url)\n                return r_status, r_str\n            except (ValueError, ConnectionError, BadClient, aiohttp.ClientError, aiohttp.ClientConnectionError) as err:\n                self.logger.debug(f'Got an error of type {type(err)}: {err} sending POST to {url} in attempt {attempt}/{self.max_attempts}')\n                await retry.wait_or_raise_err(\n                    attempt=attempt,\n                    err=err,\n                    max_attempts=self.max_attempts,\n                    backoff_multiplier=self.backoff_multiplier,\n                    initial_backoff=self.initial_backoff,\n                    max_backoff=self.max_backoff,\n                )\n            except Exception as exc:\n                self.logger.debug(\n                    f'Got a non-retried error of type {type(exc)}: {exc} sending POST to {url}')\n                raise exc\n\n    async def send_streaming_message(self, url, doc: 'Document', on: str):\n        \"\"\"Sends a GET SSE request to the server\n\n        :param url: the URL where to send the message\n        :param doc: Request Document\n        :param on: Request endpoint\n        :yields: responses\n        \"\"\"\n        req_dict = doc.to_dict() if hasattr(doc, \"to_dict\") else doc.dict()\n        request_kwargs = {\n            'url': url,\n            'headers': {'Accept': 'text/event-stream'},\n            'json': req_dict,\n        }\n\n        async with self.session.get(**request_kwargs) as response:\n            async for chunk in response.content.iter_any():\n                events = chunk.split(b'event: ')[1:]\n                for event in events:\n                    if event.startswith(b'update'):\n                        yield event[self.UPDATE_EVENT_PREFIX :].decode()\n                    elif event.startswith(b'end'):\n                        pass\n\n    async def send_dry_run(self, url, **kwargs):\n        \"\"\"Query the dry_run endpoint from Gateway\n        :param url: the URL where to send the message\n        :param kwargs: keyword arguments to make sure compatible API with other clients\n        :return: send get message\n        \"\"\"\n        return await self.session.get(\n            url=url, timeout=kwargs.get('timeout', None)\n        ).__aenter__()\n\n    async def recv_message(self):\n        \"\"\"Receive message for HTTP (sleep)\n\n        :return: await sleep\n        \"\"\"\n        return await asyncio.sleep(1e10)\n\n    async def recv_dry_run(self):\n        \"\"\"Receive dry run response for HTTP (sleep)\n\n        :return: await sleep\n        \"\"\"\n        return await asyncio.sleep(1e10)\n\n\nclass WsResponseIter:\n    \"\"\"\n    Iterates over all the responses that come in over the websocket connection.\n    In contrast to the iterator built into AioHTTP, this also records the message that was sent at closing-time.\n    \"\"\"\n\n    def __init__(self, websocket):\n        self.websocket = websocket\n        self.close_message = None\n\n    def __aiter__(self):\n        return self\n\n    async def __anext__(self):\n        msg = await self.websocket.receive()\n        if msg.type in (WSMsgType.CLOSE, WSMsgType.CLOSING, WSMsgType.CLOSED):\n            self.close_message = msg\n            raise StopAsyncIteration\n        return msg\n\n\nclass WebsocketClientlet(AioHttpClientlet):\n    \"\"\"Websocket Client to be used with the streamer\"\"\"\n\n    def __init__(self, url, *args, **kwargs) -> None:\n        super().__init__(*args, **kwargs)\n        self.url = url\n        self.websocket = None\n        self.response_iter = None\n\n    async def send_message(self, request: 'Request'):\n        \"\"\"Send request in bytes to the server.\n\n        :param request: request object\n        :return: send request as bytes awaitable\n        \"\"\"\n        for attempt in range(1, self.max_attempts + 1):\n            try:\n                return await self.websocket.send_bytes(request.SerializeToString())\n            except aiohttp.ClientError as err:\n                await retry.wait_or_raise_err(\n                    attempt=attempt,\n                    err=err,\n                    max_attempts=self.max_attempts,\n                    backoff_multiplier=self.backoff_multiplier,\n                    initial_backoff=self.initial_backoff,\n                    max_backoff=self.max_backoff,\n                )\n\n    async def send_dry_run(self, **kwargs):\n        \"\"\"Query the dry_run endpoint from Gateway\n        :param kwargs: keyword arguments to make sure compatible API with other clients\n        :return: send dry_run as bytes awaitable\n        \"\"\"\n\n        try:\n            return await self.websocket.send_bytes(b'')\n        except ConnectionResetError:\n            self.logger.critical(f'server connection closed already!')\n\n    async def send_eoi(self):\n        \"\"\"To confirm end of iteration, we send `bytes(True)` to the server.\n\n        :return: send `bytes(True)` awaitable\n        \"\"\"\n        try:\n            return await self.websocket.send_bytes(bytes(True))\n        except ConnectionResetError:\n            # server might be in a `CLOSING` state while sending EOI signal\n            # which raises a `ConnectionResetError`, this can be ignored.\n            pass\n\n    async def recv_message(self) -> 'DataRequest':\n        \"\"\"Receive messages in bytes from server and convert to `DataRequest`\n\n        ..note::\n            aiohttp allows only one task which can `receive` concurrently.\n            we need to make sure we don't create multiple tasks with `recv_message`\n\n        :yield: response objects received from server\n        \"\"\"\n        async for response in self.response_iter:\n            yield DataRequest(response.data)\n\n    async def recv_dry_run(self):\n        \"\"\"Receive dry run response in bytes from server\n\n        ..note::\n            aiohttp allows only one task which can `receive` concurrently.\n            we need to make sure we don't create multiple tasks with `recv_message`\n\n        :yield: response objects received from server\n        \"\"\"\n        async for response in self.response_iter:\n            yield StatusMessage(response.data)\n\n    async def __aenter__(self):\n        for attempt in range(1, self.max_attempts + 1):\n            try:\n                await super().__aenter__()\n                self.websocket = await self.session.ws_connect(\n                    url=self.url,\n                    protocols=(WebsocketSubProtocols.BYTES.value,),\n                    **self._session_kwargs,\n                ).__aenter__()\n                self.response_iter = WsResponseIter(self.websocket)\n                return self\n            except aiohttp.ClientError as err:\n                await retry.wait_or_raise_err(\n                    attempt=attempt,\n                    err=err,\n                    max_attempts=self.max_attempts,\n                    backoff_multiplier=self.backoff_multiplier,\n                    initial_backoff=self.initial_backoff,\n                    max_backoff=self.max_backoff,\n                )\n\n    @property\n    def close_message(self):\n        \"\"\":return: the close message (reason) of the ws closing response\"\"\"\n        return self.response_iter.close_message.extra if self.response_iter else None\n\n    @property\n    def close_code(self):\n        \"\"\":return: the close code of the ws closing response\"\"\"\n        return self.websocket.close_code if self.websocket else None\n\n\ndef handle_response_status(\n    http_status: int, response_content: Union[Dict, str], url: str\n):\n    \"\"\"\n    Raise BadClient exception for HTTP 404 status.\n    Raise ConnectionError for HTTP status codes 504, 504 if header information is available.\n    Raise ValueError for everything other non 200 status code.\n    :param http_status: http status code\n    :param response_content: response content as json dict or string\n    :param url: request url string\n    \"\"\"\n    if http_status == status.HTTP_404_NOT_FOUND:\n        raise BadClient(f'no such endpoint {url}')\n    elif (\n        http_status == status.HTTP_503_SERVICE_UNAVAILABLE\n        or http_status == status.HTTP_504_GATEWAY_TIMEOUT\n    ):\n        if (\n            isinstance(response_content, dict)\n            and 'header' in response_content\n            and 'status' in response_content['header']\n            and 'description' in response_content['header']['status']\n        ):\n            raise ConnectionError(response_content['header']['status']['description'])\n        else:\n            raise ValueError(response_content)\n    elif (\n        http_status < status.HTTP_200_OK\n        or http_status > status.HTTP_300_MULTIPLE_CHOICES\n    ):  # failure codes\n        raise ValueError(response_content)\n", "metadata": {"source": "github.com/jina-ai/serve", "file_path": "jina/clients/base/helper.py", "language": "python", "complexity": "complex", "star_count": 21665, "quality_score": 1.0, "features": {"has_classes": true, "has_functions": true, "has_async": true, "has_decorators": true, "has_imports": true, "has_error_handling": true, "has_documentation": true, "has_type_hints": true, "has_testing": false, "has_logging": true, "function_count": 28, "class_count": 5, "line_count": 405}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create REST API endpoint with proper request/response handling with asyncio coroutines\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import asyncio\nimport json\nfrom contextlib import AsyncExitStack\nfrom typing import TYPE_CHECKING, Dict, Optional, Tuple, Type\n\nfrom jina._docarray import Document, DocumentArray, docarray_v2\nfrom jina.clients.base import BaseClient\nfrom jina.clients.base.helper import HTTPClientlet, handle_response_status\nfrom jina.clients.helper import callback_exec\nfrom jina.importer import ImportExtensions\nfrom jina.logging.profile import ProgressBar\nfrom jina.serve.stream import RequestStreamer\nfrom jina.types.request import Request\nfrom jina.types.request.data import DataRequest\nif docarray_v2:\n    from docarray.utils._internal._typing import safe_issubclass\nelse:\n    safe_issubclass = issubclass\n\nif TYPE_CHECKING:  # pragma: no cover\n    from jina.clients.base import CallbackFnType, InputType\n\n\nclass HTTPBaseClient(BaseClient):\n    \"\"\"A MixIn for HTTP Client.\"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self._endpoints = []\n        self.reuse_session = False\n        self._lock = AsyncExitStack()\n        self.iolet = None\n\n    async def close(self):\n        \"\"\"Closes the potential resources of the Client.\n        :return: Return whatever a close method may return\n        \"\"\"\n        ret = super().close()\n        if self.iolet is not None:\n            await self.iolet.__aexit__(None, None, None)\n        return ret\n\n    async def _get_endpoints_from_openapi(self, **kwargs):\n        def extract_paths_by_method(spec):\n            paths_by_method = {}\n            for path, methods in spec['paths'].items():\n                for method, details in methods.items():\n                    if method not in paths_by_method:\n                        paths_by_method[method] = []\n                    paths_by_method[method].append(path.strip('/'))\n\n            return paths_by_method\n\n        import json\n\n        import aiohttp\n\n        session_kwargs = {}\n        if 'headers' in kwargs:\n            session_kwargs = {'headers': kwargs['headers']}\n\n        proto = 'https' if self.args.tls else 'http'\n        target_url = f'{proto}://{self.args.host}:{self.args.port}/openapi.json'\n        try:\n\n            async with aiohttp.ClientSession(**session_kwargs) as session:\n                async with session.get(target_url) as response:\n                    content = await response.read()\n                    openapi_response = json.loads(content.decode())\n                    self._endpoints = extract_paths_by_method(openapi_response).get(\n                        'post', []\n                    )\n        except:\n            pass\n\n    async def _is_flow_ready(self, **kwargs) -> bool:\n        \"\"\"Sends a dry run to the Flow to validate if the Flow is ready to receive requests\n\n        :param kwargs: kwargs coming from the public interface. Includes arguments to be passed to the `HTTPClientlet`\n        :return: boolean indicating the health/readiness of the Flow\n        \"\"\"\n        from jina.proto import jina_pb2\n\n        async with AsyncExitStack() as stack:\n            try:\n                proto = 'https' if self.args.tls else 'http'\n                url = f'{proto}://{self.args.host}:{self.args.port}/dry_run'\n\n                if not self.reuse_session:\n                    iolet = await stack.enter_async_context(\n                        HTTPClientlet(\n                            logger=self.logger,\n                            tracer_provider=self.tracer_provider,\n                            **kwargs,\n                        )\n                    )\n                else:\n                    async with self._lock:\n                        if self.iolet is None:\n                            self.iolet = HTTPClientlet(\n                                logger=self.logger,\n                                tracer_provider=self.tracer_provider,\n                                **kwargs,\n                            )\n                            await self.iolet.__aenter__()\n                        iolet = self.iolet\n\n                response = await iolet.send_dry_run(url=url, **kwargs)\n                r_status = response.status\n\n                r_str = await response.json()\n                handle_response_status(r_status, r_str, url)\n                if r_str['code'] == jina_pb2.StatusProto.SUCCESS:\n                    return True\n                else:\n                    self.logger.error(\n                        f'Returned code is not expected! Description: {r_str[\"description\"]}'\n                    )\n            except Exception as e:\n                self.logger.error(\n                    f'Error while fetching response from HTTP server {e!r}'\n                )\n        return False\n\n    async def _get_results(\n            self,\n            inputs: 'InputType',\n            on_done: 'CallbackFnType',\n            on_error: Optional['CallbackFnType'] = None,\n            on_always: Optional['CallbackFnType'] = None,\n            max_attempts: int = 1,\n            initial_backoff: float = 0.5,\n            max_backoff: float = 0.1,\n            backoff_multiplier: float = 1.5,\n            results_in_order: bool = False,\n            prefetch: Optional[int] = None,\n            timeout: Optional[int] = None,\n            return_type: Type[DocumentArray] = DocumentArray,\n            **kwargs,\n    ):\n        \"\"\"\n        :param inputs: the callable\n        :param on_done: the callback for on_done\n        :param on_error: the callback for on_error\n        :param on_always: the callback for on_always\n        :param max_attempts: Number of sending attempts, including the original request.\n        :param initial_backoff: The first retry will happen with a delay of random(0, initial_backoff)\n        :param max_backoff: The maximum accepted backoff after the exponential incremental delay\n        :param backoff_multiplier: The n-th attempt will occur at random(0, min(initialBackoff*backoffMultiplier**(n-1), maxBackoff))\n        :param results_in_order: return the results in the same order as the inputs\n        :param prefetch: How many Requests are processed from the Client at the same time.\n        :param timeout: Timeout for the client to remain connected to the server.\n        :param return_type: the DocumentArray type to be returned. By default, it is `DocumentArray`.\n        :param kwargs: kwargs coming from the public interface. Includes arguments to be passed to the `HTTPClientlet`\n        :yields: generator over results\n        \"\"\"\n        with ImportExtensions(required=True):\n            pass\n\n        request_iterator, inputs_length = self._get_requests(inputs=inputs, **kwargs)\n        on = kwargs.get('on', '/post')\n        if len(self._endpoints) == 0:\n            await self._get_endpoints_from_openapi(**kwargs)\n\n        async with AsyncExitStack() as stack:\n            cm1 = ProgressBar(\n                total_length=inputs_length, disable=not self.show_progress\n            )\n            p_bar = stack.enter_context(cm1)\n            proto = 'https' if self.args.tls else 'http'\n            endpoint = on.strip('/')\n            has_default_endpoint = 'default' in self._endpoints\n\n            if endpoint != '' and endpoint in self._endpoints:\n                url = f'{proto}://{self.args.host}:{self.args.port}/{on.strip(\"/\")}'\n            elif has_default_endpoint:\n                url = f'{proto}://{self.args.host}:{self.args.port}/default'\n            else:\n                url = f'{proto}://{self.args.host}:{self.args.port}/post'\n\n            if not self.reuse_session:\n                iolet = await stack.enter_async_context(\n                    HTTPClientlet(\n                        logger=self.logger,\n                        tracer_provider=self.tracer_provider,\n                        timeout=timeout,\n                        max_attempts=max_attempts,\n                        initial_backoff=initial_backoff,\n                        max_backoff=max_backoff,\n                        backoff_multiplier=backoff_multiplier,\n                        **kwargs,\n                    )\n                )\n            else:\n                async with self._lock:\n                    if self.iolet is None:\n                        self.iolet = HTTPClientlet(\n                            logger=self.logger,\n                            tracer_provider=self.tracer_provider,\n                            timeout=timeout,\n                            max_attempts=max_attempts,\n                            initial_backoff=initial_backoff,\n                            max_backoff=max_backoff,\n                            backoff_multiplier=backoff_multiplier,\n                            **kwargs,\n                        )\n                        self.iolet = await self.iolet.__aenter__()\n                iolet = self.iolet\n\n            def _request_handler(\n                    request: 'Request', **kwargs\n            ) -> 'Tuple[asyncio.Future, Optional[asyncio.Future]]':\n                \"\"\"\n                For HTTP Client, for each request in the iterator, we `send_message` using\n                http POST request and add it to the list of tasks which is awaited and yielded.\n                :param request: current request in the iterator\n                :param kwargs: kwargs\n                :return: asyncio Task for sending message\n                \"\"\"\n                return asyncio.ensure_future(iolet.send_message(url=url, request=request)), None\n\n            def _result_handler(result):\n                return result\n\n            streamer_args = vars(self.args)\n            if prefetch:\n                streamer_args['prefetch'] = prefetch\n            streamer = RequestStreamer(\n                request_handler=_request_handler,\n                result_handler=_result_handler,\n                logger=self.logger,\n                **streamer_args,\n            )\n            async for response in streamer.stream(\n                    request_iterator=request_iterator, results_in_order=results_in_order\n            ):\n                r_status, r_str = response\n                handle_response_status(r_status, r_str, url)\n\n                da = None\n                if 'data' in r_str and r_str['data'] is not None:\n                    from jina._docarray import DocumentArray, docarray_v2\n\n                    if not docarray_v2:\n                        da = DocumentArray.from_dict(r_str['data'])\n                    else:\n                        from docarray import DocList\n\n                        if safe_issubclass(return_type, DocList):\n                            da = return_type(\n                                [return_type.doc_type(**v) for v in r_str['data']]\n                            )\n                        else:\n                            da = DocList[return_type](\n                                [return_type(**v) for v in r_str['data']]\n                            )\n                    del r_str['data']\n\n                resp = DataRequest(r_str)\n                if da is not None:\n                    resp.direct_docs = da\n\n                callback_exec(\n                    response=resp,\n                    logger=self.logger,\n                    on_error=on_error,\n                    on_done=on_done,\n                    on_always=on_always,\n                    continue_on_error=self.continue_on_error,\n                )\n                if self.show_progress:\n                    p_bar.update()\n                yield resp\n\n    async def _get_streaming_results(\n            self,\n            on: str,\n            inputs: 'Document',\n            parameters: Optional[Dict] = None,\n            return_type: Type[Document] = Document,\n            timeout: Optional[int] = None,\n            **kwargs,\n    ):\n        proto = 'https' if self.args.tls else 'http'\n        endpoint = on.strip('/')\n        has_default_endpoint = 'default' in self._endpoints\n\n        if (endpoint != '' and endpoint in self._endpoints) or not has_default_endpoint:\n            url = f'{proto}://{self.args.host}:{self.args.port}/{endpoint}'\n        else:\n            url = f'{proto}://{self.args.host}:{self.args.port}/default'\n        async with AsyncExitStack() as stack:\n            if not self.reuse_session:\n                iolet = await stack.enter_async_context(\n                    HTTPClientlet(\n                        logger=self.logger,\n                        tracer_provider=self.tracer_provider,\n                        timeout=timeout,\n                        **kwargs,\n                    )\n                )\n            else:\n                async with self._lock:\n                    if self.iolet is None:\n                        self.iolet = HTTPClientlet(\n                            logger=self.logger,\n                            tracer_provider=self.tracer_provider,\n                            timeout=timeout,\n                            **kwargs,\n                        )\n                        await self.iolet.__aenter__()\n                    iolet = self.iolet\n            async for doc in iolet.send_streaming_message(url=url, doc=inputs, on=on):\n                if not docarray_v2:\n                    yield Document.from_dict(json.loads(doc))\n                else:\n                    yield return_type(**json.loads(doc))\n", "metadata": {"source": "github.com/jina-ai/serve", "file_path": "jina/clients/base/http.py", "language": "python", "complexity": "complex", "star_count": 21665, "quality_score": 1.0, "features": {"has_classes": true, "has_functions": true, "has_async": true, "has_decorators": false, "has_imports": true, "has_error_handling": true, "has_documentation": true, "has_type_hints": true, "has_testing": false, "has_logging": true, "function_count": 9, "class_count": 2, "line_count": 318}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create REST API endpoint with proper request/response handling with asyncio coroutines\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import random\nimport asyncio\nimport time\n\nimport aiohttp\nimport grpc\n\n\ndef _raise_last_attempt(err, attempt):\n    if isinstance(err, asyncio.CancelledError):\n        trailing_metadata = grpc.aio.Metadata()\n        trailing_metadata.add('jina-client-attempts', str(attempt))\n        raise grpc.aio.AioRpcError(\n            code=grpc.StatusCode.CANCELLED,\n            initial_metadata=grpc.aio.Metadata(),\n            trailing_metadata=trailing_metadata,\n        )\n    elif isinstance(err, grpc.aio.AioRpcError):\n        trailing_metadata = err.trailing_metadata() or grpc.aio.Metadata()\n        trailing_metadata.add('jina-client-attempts', str(attempt))\n        raise grpc.aio.AioRpcError(\n            code=err.code(),\n            details=err.details(),\n            initial_metadata=err.initial_metadata(),\n            trailing_metadata=trailing_metadata,\n            debug_error_string=err.debug_error_string(),\n        )\n    elif isinstance(err, aiohttp.ClientConnectorCertificateError):\n        raise err\n    elif isinstance(err, aiohttp.ClientError):\n        raise ConnectionError(str(err))\n    else:\n        raise err\n\n\ndef sync_wait_or_raise_err(\n    attempt: int,\n    err: Exception,\n    max_attempts: float,\n    backoff_multiplier: float,\n    initial_backoff: float,\n    max_backoff: float,\n):\n    \"\"\"\n    Accepts retry parameters and the underlying. The error is raised if the max_attempts has been reached otherwise the\n    method waits based on the backoff calculations.\n    :param attempt: Number of the current attempt.\n    :param err: Underlying error that was raised by the operation.\n    :param max_attempts: Maximum number of attempts that are allowed.\n    :param backoff_multiplier: Factor that will be raised to the exponent of (attempt - 1) for calculating the backoff wait time.\n    :param initial_backoff: The backoff time on the first error. This will be multiplied by the backoff_multiplier exponent for subsequent wait time calculations.\n    :param max_backoff: The maximum backoff wait time.\n    \"\"\"\n    if attempt == max_attempts:\n        _raise_last_attempt(err, attempt)\n    else:\n        time.sleep(\n            _wait_time(attempt, backoff_multiplier, initial_backoff, max_backoff)\n        )\n\n\nasync def wait_or_raise_err(\n    attempt: int,\n    err: Exception,\n    max_attempts: float,\n    backoff_multiplier: float,\n    initial_backoff: float,\n    max_backoff: float,\n):\n    \"\"\"\n    Accepts retry parameters and the underlying. The error is raised if the max_attempts has been reached otherwise the\n    method waits based on the backoff calculations.\n    :param attempt: Number of the current attempt.\n    :param err: Underlying error that was raised by the operation.\n    :param max_attempts: Maximum number of attempts that are allowed.\n    :param backoff_multiplier: Factor that will be raised to the exponent of (attempt - 1) for calculating the backoff wait time.\n    :param initial_backoff: The backoff time on the first error. This will be multiplied by the backoff_multiplier exponent for subsequent wait time calculations.\n    :param max_backoff: The maximum backoff wait time.\n    \"\"\"\n\n    if attempt == max_attempts:\n        _raise_last_attempt(err, attempt)\n    else:\n        await asyncio.sleep(\n            _wait_time(attempt, backoff_multiplier, initial_backoff, max_backoff)\n        )\n\n\ndef _wait_time(attempt, backoff_multiplier, initial_backoff, max_backoff):\n    if attempt == 1:\n        wait_time = initial_backoff\n    else:\n        wait_time = random.uniform(\n            0,\n            min(initial_backoff * backoff_multiplier ** (attempt - 1), max_backoff),\n        )\n    return wait_time\n", "metadata": {"source": "github.com/jina-ai/serve", "file_path": "jina/clients/base/retry.py", "language": "python", "complexity": "medium", "star_count": 21665, "quality_score": 0.89, "features": {"has_classes": false, "has_functions": true, "has_async": true, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": true, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 4, "class_count": 0, "line_count": 98}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create REST API endpoint with proper request/response handling with asyncio coroutines\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from jina.clients.helper import callback_exec\nfrom jina.proto import jina_pb2_grpc\n\n\nclass StreamRpc:\n    \"\"\"Class that encapsulated the methods required to run a stream rpc call from the client. Instantiate a single class\n    for each client request.\n    \"\"\"\n\n    def __init__(\n        self,\n        channel,\n        continue_on_error,\n        metadata,\n        on_always,\n        on_done,\n        on_error,\n        p_bar,\n        req_iter,\n        max_attempts,\n        backoff_multiplier,\n        initial_backoff,\n        max_backoff,\n        logger,\n        show_progress,\n        compression,\n        **kwargs\n    ):\n        self.compression = compression\n        self.show_progress = show_progress\n        self.logger = logger\n        self.max_backoff = max_backoff\n        self.initial_backoff = initial_backoff\n        self.backoff_multiplier = backoff_multiplier\n        self.max_attempts = max_attempts\n        self.req_iter = req_iter\n        self.p_bar = p_bar\n        self.on_error = on_error\n        self.on_done = on_done\n        self.on_always = on_always\n        self.metadata = metadata\n        self.continue_on_error = continue_on_error\n        self.channel = channel\n        self.kwargs = kwargs\n\n    async def stream_rpc_with_retry(self):\n        \"\"\"Wraps the stream rpc logic with retry loop based on the retry params.\n        :yields: Responses received from the target.\n        \"\"\"\n        stub = jina_pb2_grpc.JinaRPCStub(self.channel)\n        async for resp in stub.Call(\n            self.req_iter,\n            compression=self.compression,\n            metadata=self.metadata,\n            credentials=self.kwargs.get('credentials', None),\n            timeout=self.kwargs.get('timeout', None),\n        ):\n            callback_exec(\n                response=resp,\n                logger=self.logger,\n                on_error=self.on_error,\n                on_done=self.on_done,\n                on_always=self.on_always,\n                continue_on_error=self.continue_on_error,\n            )\n            if self.show_progress:\n                self.p_bar.update()\n            yield resp\n", "metadata": {"source": "github.com/jina-ai/serve", "file_path": "jina/clients/base/stream_rpc.py", "language": "python", "complexity": "simple", "star_count": 21665, "quality_score": 0.98, "features": {"has_classes": true, "has_functions": true, "has_async": true, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": true, "has_type_hints": true, "has_testing": false, "has_logging": true, "function_count": 2, "class_count": 1, "line_count": 69}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create REST API endpoint with proper request/response handling with asyncio coroutines\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import asyncio\nfrom typing import TYPE_CHECKING, Optional, Tuple\n\nimport grpc\n\nfrom jina.clients.base.retry import wait_or_raise_err\nfrom jina.clients.helper import callback_exec\nfrom jina.excepts import InternalNetworkError\nfrom jina.proto import jina_pb2_grpc\nfrom jina.serve.stream import RequestStreamer\n\nif TYPE_CHECKING:\n    from jina.types.request import Request\n\n\nclass UnaryRpc:\n    \"\"\"Class that encapsulated the methods required to run unary rpc calls from the client. Instantiate a single class\n    for each client request.\n    \"\"\"\n\n    def __init__(\n        self,\n        channel,\n        continue_on_error,\n        metadata,\n        on_always,\n        on_done,\n        on_error,\n        p_bar,\n        req_iter,\n        max_attempts,\n        backoff_multiplier,\n        initial_backoff,\n        max_backoff,\n        logger,\n        show_progress,\n        compression,\n        client_args,\n        prefetch,\n        results_in_order,\n        **kwargs\n    ):\n        self.results_in_order = results_in_order\n        self.prefetch = prefetch\n        self.client_args = client_args\n        self.compression = compression\n        self.show_progress = show_progress\n        self.logger = logger\n        self.max_backoff = max_backoff\n        self.initial_backoff = initial_backoff\n        self.backoff_multiplier = backoff_multiplier\n        self.max_attempts = max_attempts\n        self.req_iter = req_iter\n        self.p_bar = p_bar\n        self.on_error = on_error\n        self.on_done = on_done\n        self.on_always = on_always\n        self.metadata = metadata\n        self.continue_on_error = continue_on_error\n        self.channel = channel\n        self.kwargs = kwargs\n\n    async def unary_rpc_with_retry(self):\n        \"\"\"Wraps the unary rpc call with retry loop based on the retry params.\n        :yields: Responses received from the target.\n        \"\"\"\n        stub = jina_pb2_grpc.JinaSingleDataRequestRPCStub(self.channel)\n\n        def _request_handler(\n            request: 'Request', **kwargs\n        ) -> 'Tuple[asyncio.Future, Optional[asyncio.Future]]':\n            async def _with_retry(req: 'Request'):\n                for attempt in range(1, self.max_attempts + 1):\n                    try:\n                        return await stub.process_single_data(\n                            req,\n                            compression=self.compression,\n                            metadata=self.metadata,\n                            credentials=self.kwargs.get('credentials', None),\n                            timeout=self.kwargs.get('timeout', None),\n                        )\n                    except (\n                        grpc.aio.AioRpcError,\n                        InternalNetworkError,\n                    ) as err:\n                        await wait_or_raise_err(\n                            attempt=attempt,\n                            err=err,\n                            max_attempts=self.max_attempts,\n                            backoff_multiplier=self.backoff_multiplier,\n                            initial_backoff=self.initial_backoff,\n                            max_backoff=self.max_backoff,\n                        )\n\n            return (\n                asyncio.ensure_future(_with_retry(request)),\n                None,\n            )\n\n        def _result_handler(resp):\n            callback_exec(\n                response=resp,\n                logger=self.logger,\n                on_error=self.on_error,\n                on_done=self.on_done,\n                on_always=self.on_always,\n                continue_on_error=self.continue_on_error,\n            )\n            return resp\n\n        streamer_args = vars(self.client_args)\n        if self.prefetch:\n            streamer_args['prefetch'] = self.prefetch\n        streamer = RequestStreamer(\n            request_handler=_request_handler,\n            result_handler=_result_handler,\n            iterate_sync_in_thread=False,\n            logger=self.logger,\n            **streamer_args,\n        )\n        async for response in streamer.stream(\n            request_iterator=self.req_iter, results_in_order=self.results_in_order\n        ):\n            if self.show_progress:\n                self.p_bar.update()\n            yield response\n", "metadata": {"source": "github.com/jina-ai/serve", "file_path": "jina/clients/base/unary_rpc.py", "language": "python", "complexity": "medium", "star_count": 21665, "quality_score": 1.0, "features": {"has_classes": true, "has_functions": true, "has_async": true, "has_decorators": false, "has_imports": true, "has_error_handling": true, "has_documentation": true, "has_type_hints": true, "has_testing": false, "has_logging": true, "function_count": 5, "class_count": 1, "line_count": 127}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create REST API endpoint with proper request/response handling with asyncio coroutines\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"A module for the websockets-based Client for Jina.\"\"\"\n\nimport asyncio\nfrom contextlib import AsyncExitStack\nfrom typing import TYPE_CHECKING, Dict, Optional, Tuple\n\nfrom starlette import status\n\nfrom jina.clients.base import BaseClient\nfrom jina.clients.base.helper import WebsocketClientlet\nfrom jina.clients.helper import callback_exec\nfrom jina.helper import get_or_reuse_loop\nfrom jina.importer import ImportExtensions\nfrom jina.logging.profile import ProgressBar\nfrom jina.proto import jina_pb2\nfrom jina.serve.stream import RequestStreamer\n\nif TYPE_CHECKING:  # pragma: no cover\n    from jina.clients.base import CallbackFnType, InputType\n    from jina.types.request import Request\n\n\nclass WebSocketBaseClient(BaseClient):\n    \"\"\"A Websocket Client.\"\"\"\n\n    async def _is_flow_ready(self, **kwargs) -> bool:\n        \"\"\"Sends a dry run to the Flow to validate if the Flow is ready to receive requests\n\n        :param kwargs: kwargs coming from the public interface. Includes arguments to be passed to the `WebsocketClientlet`\n        :return: boolean indicating the readiness of the Flow\n        \"\"\"\n        async with AsyncExitStack() as stack:\n            try:\n                proto = 'wss' if self.args.tls else 'ws'\n                url = f'{proto}://{self.args.host}:{self.args.port}/dry_run'\n                iolet = await stack.enter_async_context(\n                    WebsocketClientlet(\n                        url=url,\n                        logger=self.logger,\n                        tracer_provider=self.tracer_provider,\n                        **kwargs,\n                    )\n                )\n\n                async def _receive():\n                    try:\n                        async for response in iolet.recv_dry_run():\n                            return response\n                    except Exception as exc:\n                        self.logger.error(\n                            f'Error while fetching response from Websocket server {exc!r}'\n                        )\n                        raise\n\n                async def _send():\n                    return await iolet.send_dry_run()\n\n                receive_task = asyncio.create_task(_receive())\n\n                if receive_task.done():\n                    raise RuntimeError(\n                        'receive task not running, can not send messages'\n                    )\n                try:\n                    send_task = asyncio.create_task(_send())\n                    _, response_result = await asyncio.gather(send_task, receive_task)\n                    if response_result.proto.code == jina_pb2.StatusProto.SUCCESS:\n                        return True\n                finally:\n                    if iolet.close_code == status.WS_1011_INTERNAL_ERROR:\n                        raise ConnectionError(iolet.close_message)\n                    await receive_task\n\n            except Exception as e:\n                self.logger.error(\n                    f'Error while getting response from websocket server {e!r}'\n                )\n            return False\n\n    async def _get_results(\n        self,\n        inputs: 'InputType',\n        on_done: 'CallbackFnType',\n        on_error: Optional['CallbackFnType'] = None,\n        on_always: Optional['CallbackFnType'] = None,\n        max_attempts: int = 1,\n        initial_backoff: float = 0.5,\n        max_backoff: float = 0.1,\n        backoff_multiplier: float = 1.5,\n        results_in_order: bool = False,\n        prefetch: Optional[int] = None,\n        **kwargs,\n    ):\n        \"\"\"\n        :param inputs: the callable\n        :param on_done: the callback for on_done\n        :param on_error: the callback for on_error\n        :param on_always: the callback for on_always\n        :param max_attempts: Number of sending attempts, including the original request.\n        :param initial_backoff: The first retry will happen with a delay of random(0, initial_backoff)\n        :param max_backoff: The maximum accepted backoff after the exponential incremental delay\n        :param backoff_multiplier: The n-th attempt will occur at random(0, min(initialBackoff*backoffMultiplier**(n-1), maxBackoff))\n        :param results_in_order: return the results in the same order as the inputs\n        :param prefetch: How many Requests are processed from the Client at the same time.\n        :param kwargs: kwargs coming from the public interface. Includes arguments to be passed to the `WebsocketClientlet`\n        :yields: generator over results\n        \"\"\"\n        with ImportExtensions(required=True):\n            pass\n\n        request_iterator, inputs_length = self._get_requests(inputs=inputs, **kwargs)\n\n        async with AsyncExitStack() as stack:\n            cm1 = ProgressBar(\n                total_length=inputs_length, disable=not (self.show_progress)\n            )\n            p_bar = stack.enter_context(cm1)\n\n            proto = 'wss' if self.args.tls else 'ws'\n            url = f'{proto}://{self.args.host}:{self.args.port}/'\n            iolet = await stack.enter_async_context(\n                WebsocketClientlet(\n                    url=url,\n                    logger=self.logger,\n                    tracer_provider=self.tracer_provider,\n                    max_attempts=max_attempts,\n                    initial_backoff=initial_backoff,\n                    max_backoff=max_backoff,\n                    backoff_multiplier=backoff_multiplier,\n                    **kwargs,\n                )\n            )\n\n            request_buffer: Dict[str, asyncio.Future] = (\n                dict()\n            )  # maps request_ids to futures (tasks)\n\n            def _result_handler(result):\n                return result\n\n            async def _receive():\n                def _response_handler(response):\n                    if response.header.request_id in request_buffer:\n                        future = request_buffer.pop(response.header.request_id)\n                        future.set_result(response)\n                    else:\n                        self.logger.warning(\n                            f'discarding unexpected response with request id {response.header.request_id}'\n                        )\n\n                \"\"\"Await messages from WebsocketGateway and process them in the request buffer\"\"\"\n                try:\n                    async for response in iolet.recv_message():\n                        _response_handler(response)\n                finally:\n                    if request_buffer:\n                        self.logger.warning(\n                            f'{self.__class__.__name__} closed, cancelling all outstanding requests'\n                        )\n                        for future in request_buffer.values():\n                            future.cancel()\n                        request_buffer.clear()\n\n            def _handle_end_of_iter():\n                \"\"\"Send End of iteration signal to the Gateway\"\"\"\n                asyncio.create_task(iolet.send_eoi())\n\n            def _request_handler(\n                request: 'Request', **kwargs\n            ) -> 'Tuple[asyncio.Future, Optional[asyncio.Future]]':\n                \"\"\"\n                For each request in the iterator, we send the `Message` using `iolet.send_message()`.\n                For websocket requests from client, for each request in the iterator, we send the request in `bytes`\n                using `iolet.send_message()`.\n                Then add {<request-id>: <an-empty-future>} to the request buffer.\n                This empty future is used to track the `result` of this request during `receive`.\n                :param request: current request in the iterator\n                :param kwargs: kwargs\n                :return: asyncio Future for sending message\n                \"\"\"\n                future = get_or_reuse_loop().create_future()\n                request_buffer[request.header.request_id] = future\n                asyncio.create_task(iolet.send_message(request))\n                return future, None\n\n            streamer_args = vars(self.args)\n            if prefetch:\n                streamer_args['prefetch'] = prefetch\n            streamer = RequestStreamer(\n                request_handler=_request_handler,\n                result_handler=_result_handler,\n                end_of_iter_handler=_handle_end_of_iter,\n                logger=self.logger,\n                **streamer_args,\n            )\n\n            receive_task = asyncio.create_task(_receive())\n\n            exception_raised = None\n\n            if receive_task.done():\n                raise RuntimeError('receive task not running, can not send messages')\n            try:\n                async for response in streamer.stream(\n                    request_iterator=request_iterator,\n                    results_in_order=results_in_order,\n                ):\n                    callback_exec(\n                        response=response,\n                        logger=self.logger,\n                        on_error=on_error,\n                        on_done=on_done,\n                        on_always=on_always,\n                        continue_on_error=self.continue_on_error,\n                    )\n                    if self.show_progress:\n                        p_bar.update()\n                    yield response\n            except Exception as ex:\n                exception_raised = ex\n                try:\n                    receive_task.cancel()\n                except:\n                    raise ex\n            finally:\n                if iolet.close_code == status.WS_1011_INTERNAL_ERROR:\n                    raise ConnectionError(iolet.close_message)\n                try:\n                    await receive_task\n                except asyncio.CancelledError:\n                    if exception_raised is not None:\n                        raise exception_raised\n                    else:\n                        raise\n", "metadata": {"source": "github.com/jina-ai/serve", "file_path": "jina/clients/base/websocket.py", "language": "python", "complexity": "complex", "star_count": 21665, "quality_score": 1.0, "features": {"has_classes": true, "has_functions": true, "has_async": true, "has_decorators": false, "has_imports": true, "has_error_handling": true, "has_documentation": true, "has_type_hints": true, "has_testing": false, "has_logging": true, "function_count": 9, "class_count": 1, "line_count": 235}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create REST API endpoint with proper request/response handling with asyncio coroutines\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from jina.clients.base.grpc import GRPCBaseClient\nfrom jina.clients.mixin import (\n    AsyncHealthCheckMixin,\n    AsyncPostMixin,\n    HealthCheckMixin,\n    PostMixin,\n    ProfileMixin,\n)\n\n\nclass GRPCClient(GRPCBaseClient, PostMixin, HealthCheckMixin, ProfileMixin):\n    \"\"\"A client connecting to a Gateway using gRPC protocol.\n\n    Instantiate this class through the :meth:`jina.Client` convenience method.\n\n    EXAMPLE USAGE\n\n    .. code-block:: python\n\n        from jina import Client\n        from docarray import Document\n\n        # select host address to connect to\n        c = Client(\n            protocol='grpc', asyncio=False, host='grpc://my.awesome.flow:1234'\n        )  # returns GRPCClient instance\n        c.post(on='/index', inputs=Document(text='hello!'))\n\n    \"\"\"\n\n\nclass AsyncGRPCClient(GRPCBaseClient, AsyncPostMixin, AsyncHealthCheckMixin):\n    \"\"\"\n    Asynchronous client connecting to a Gateway using gRPC protocol.\n\n    Instantiate this class through the :meth:`jina.Client` convenience method.\n\n    Unlike :class:`GRPCClient`, here :meth:`post` is a coroutine (i.e. declared with the async/await syntax),\n    simply calling them will not schedule them to be executed.\n\n    To actually run a coroutine, user need to put them in an event loop, e.g. via ``asyncio.run()``,\n    ``asyncio.create_task()``.\n\n    :class:`AsyncGRPCClient` can be very useful in\n    the integration settings, where Jina/Flow/Client is NOT the main logic, but rather served as a part of other program.\n    In this case, users often do **NOT** want to let Jina control the ``asyncio.eventloop``. On contrary, :class:`Client`\n    is controlling and wrapping the event loop internally, making the Client looks synchronous from outside.\n\n    EXAMPLE USAGE\n\n    .. code-block:: python\n\n        from jina import Client\n        from docarray import Document\n\n        # async inputs for the client\n        async def async_inputs():\n            for _ in range(10):\n                yield Document()\n                await asyncio.sleep(0.1)\n\n\n        # select host address to connect to\n        c = Client(\n            protocol='grpc', asyncio=True, host='grpc://my.awesome.flow:1234'\n        )  # returns AsyncGRPCClient instance\n\n        async for resp in client.post(on='/index', async_inputs, request_size=1):\n            print(resp)\n\n    \"\"\"\n", "metadata": {"source": "github.com/jina-ai/serve", "file_path": "jina/clients/grpc.py", "language": "python", "complexity": "medium", "star_count": 21665, "quality_score": 0.98, "features": {"has_classes": true, "has_functions": true, "has_async": true, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": true, "has_type_hints": true, "has_testing": false, "has_logging": true, "function_count": 1, "class_count": 4, "line_count": 72}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate module integration for Create REST API endpoint with proper request/response handling\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import argparse\nimport os\nfrom demo.processor import IDPhotoProcessor\nfrom demo.ui import create_ui\nfrom hivision.creator.choose_handler import HUMAN_MATTING_MODELS\n\nroot_dir = os.path.dirname(os.path.abspath(__file__))\n\n# 获取存在的人像分割模型列表\n# 通过检查 hivision/creator/weights 目录下的 .onnx 和 .mnn 文件\n# 只保留文件名（不包括扩展名）\nHUMAN_MATTING_MODELS_EXIST = [\n    os.path.splitext(file)[0]\n    for file in os.listdir(os.path.join(root_dir, \"hivision/creator/weights\"))\n    if file.endswith(\".onnx\") or file.endswith(\".mnn\")\n]\n# 在HUMAN_MATTING_MODELS中的模型才会被加载到Gradio中显示\nHUMAN_MATTING_MODELS_CHOICE = [\n    model for model in HUMAN_MATTING_MODELS if model in HUMAN_MATTING_MODELS_EXIST\n]\n\nif len(HUMAN_MATTING_MODELS_CHOICE) == 0:\n    raise ValueError(\n        \"未找到任何存在的人像分割模型，请检查 hivision/creator/weights 目录下的文件\"\n        + \"\\n\"\n        + \"No existing portrait segmentation model was found, please check the files in the hivision/creator/weights directory.\"\n    )\n\nFACE_DETECT_MODELS = [\"face++ (联网Online API)\", \"mtcnn\"]\nFACE_DETECT_MODELS_EXPAND = (\n    [\"retinaface-resnet50\"]\n    if os.path.exists(\n        os.path.join(\n            root_dir, \"hivision/creator/retinaface/weights/retinaface-resnet50.onnx\"\n        )\n    )\n    else []\n)\nFACE_DETECT_MODELS_CHOICE = FACE_DETECT_MODELS + FACE_DETECT_MODELS_EXPAND\n\nLANGUAGE = [\"zh\", \"en\", \"ko\", \"ja\"]\n\nif __name__ == \"__main__\":\n    argparser = argparse.ArgumentParser()\n    argparser.add_argument(\n        \"--port\", type=int, default=7860, help=\"The port number of the server\"\n    )\n    argparser.add_argument(\n        \"--host\", type=str, default=\"127.0.0.1\", help=\"The host of the server\"\n    )\n    argparser.add_argument(\n        \"--root_path\",\n        type=str,\n        default=None,\n        help=\"The root path of the server, default is None (='/'), e.g. '/myapp'\",\n    )\n    args = argparser.parse_args()\n\n    processor = IDPhotoProcessor()\n\n    demo = create_ui(\n        processor,\n        root_dir,\n        HUMAN_MATTING_MODELS_CHOICE,\n        FACE_DETECT_MODELS_CHOICE,\n        LANGUAGE,\n    )\n    \n    # 如果RUN_MODE是Beast，打印已开启野兽模式\n    if os.getenv(\"RUN_MODE\") == \"beast\":\n        print(\"[Beast mode activated.] 已开启野兽模式。\")\n\n    demo.launch(\n        server_name=args.host,\n        server_port=args.port,\n        favicon_path=os.path.join(root_dir, \"assets/hivision_logo.png\"),\n        root_path=args.root_path,\n        show_api=False,\n    )\n", "metadata": {"source": "github.com/Zeyi-Lin/HivisionIDPhotos", "file_path": "app.py", "language": "python", "complexity": "medium", "star_count": 18571, "quality_score": 0.8600000000000001, "features": {"has_classes": false, "has_functions": false, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": false, "has_testing": false, "has_logging": true, "function_count": 0, "class_count": 0, "line_count": 80}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create REST API endpoint with proper request/response handling\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import os\nfrom demo.utils import csv_to_size_list, csv_to_color_list\n\n\ndef load_configuration(root_dir):\n    size_list_dict_CN = csv_to_size_list(\n        os.path.join(root_dir, \"assets/size_list_CN.csv\")\n    )\n    size_list_dict_EN = csv_to_size_list(\n        os.path.join(root_dir, \"assets/size_list_EN.csv\")\n    )\n    color_list_dict_CN = csv_to_color_list(\n        os.path.join(root_dir, \"assets/color_list_CN.csv\")\n    )\n    color_list_dict_EN = csv_to_color_list(\n        os.path.join(root_dir, \"assets/color_list_EN.csv\")\n    )\n\n    return size_list_dict_CN, size_list_dict_EN, color_list_dict_CN, color_list_dict_EN\n", "metadata": {"source": "github.com/Zeyi-Lin/HivisionIDPhotos", "file_path": "demo/config.py", "language": "python", "complexity": "simple", "star_count": 18571, "quality_score": 0.92, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": false, "has_testing": false, "has_logging": false, "function_count": 1, "class_count": 0, "line_count": 20}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create REST API endpoint with proper request/response handling\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import numpy as np\nfrom hivision import IDCreator\nfrom hivision.error import FaceError, APIError\nfrom hivision.utils import (\n    add_background,\n    add_background_with_image,\n    resize_image_to_kb,\n    add_watermark,\n    save_image_dpi_to_bytes,\n)\nfrom hivision.creator.layout_calculator import (\n    generate_layout_array,\n    generate_layout_image,\n)\nfrom hivision.creator.choose_handler import choose_handler\nfrom hivision.plugin.template.template_calculator import generte_template_photo\nfrom demo.utils import range_check\nimport gradio as gr\nimport os\nimport cv2\nimport time\nfrom demo.locales import LOCALES\n\n\nbase_path = os.path.dirname(os.path.abspath(__file__))\n\nclass IDPhotoProcessor:\n    def process(\n        self,\n        input_image,\n        mode_option,\n        size_list_option,\n        color_option,\n        render_option,\n        image_kb_options,\n        custom_color_R,\n        custom_color_G,\n        custom_color_B,\n        custom_color_hex_value,\n        custom_size_height,\n        custom_size_width,\n        custom_size_height_mm,\n        custom_size_width_mm,\n        custom_image_kb,\n        language,\n        matting_model_option,\n        watermark_option,\n        watermark_text,\n        watermark_text_color,\n        watermark_text_size,\n        watermark_text_opacity,\n        watermark_text_angle,\n        watermark_text_space,\n        face_detect_option,\n        head_measure_ratio=0.2,\n        top_distance_max=0.12,\n        whitening_strength=0,\n        image_dpi_option=False,\n        custom_image_dpi=None,\n        brightness_strength=0,\n        contrast_strength=0,\n        sharpen_strength=0,\n        saturation_strength=0,\n        plugin_option=[],\n        print_switch=None,\n    ):        \n        # 初始化参数\n        top_distance_min = top_distance_max - 0.02\n        # 得到render_option在LOCALES[\"render_mode\"][language][\"choices\"]中的索引\n        render_option_index = LOCALES[\"render_mode\"][language][\"choices\"].index(\n            render_option\n        )\n        # 读取插件选项\n        # 人脸对齐选项\n        if LOCALES[\"plugin\"][language][\"choices\"][0] in plugin_option:\n            face_alignment_option = True\n        else:\n            face_alignment_option = False\n        # 水平翻转选项\n        if LOCALES[\"plugin\"][language][\"choices\"][1] in plugin_option:\n            horizontal_flip_option = True\n        else:\n            horizontal_flip_option = False\n        # 排版裁剪线选项\n        if LOCALES[\"plugin\"][language][\"choices\"][2] in plugin_option:\n            layout_photo_crop_line_option = True\n        else:\n            layout_photo_crop_line_option = False\n        # JPEG格式选项\n        if LOCALES[\"plugin\"][language][\"choices\"][3] in plugin_option:\n            jpeg_format_option = True\n        else:\n            jpeg_format_option = False\n        \n        idphoto_json = self._initialize_idphoto_json(\n            mode_option, color_option, render_option_index, image_kb_options, layout_photo_crop_line_option, jpeg_format_option, print_switch\n        )\n\n        # 处理尺寸模式\n        size_result = self._process_size_mode(\n            idphoto_json,\n            language,\n            size_list_option,\n            custom_size_height,\n            custom_size_width,\n            custom_size_height_mm,\n            custom_size_width_mm,\n        )\n        if isinstance(size_result, list):\n            return size_result  # 返回错误信息\n\n        # 处理颜色模式\n        self._process_color_mode(\n            idphoto_json,\n            language,\n            color_option,\n            custom_color_R,\n            custom_color_G,\n            custom_color_B,\n            custom_color_hex_value,\n        )\n\n        # 如果设置了自定义KB大小\n        if (\n            idphoto_json[\"image_kb_mode\"]\n            == LOCALES[\"image_kb\"][language][\"choices\"][-1]\n        ):\n            idphoto_json[\"custom_image_kb\"] = custom_image_kb\n\n        # 如果设置了自定义DPI大小\n        if image_dpi_option == LOCALES[\"image_dpi\"][language][\"choices\"][-1]:\n            idphoto_json[\"custom_image_dpi\"] = custom_image_dpi\n\n        # 创建IDCreator实例并设置处理器\n        creator = IDCreator()\n        choose_handler(creator, matting_model_option, face_detect_option)\n\n        # 生成证件照\n        try:\n            result = self._generate_id_photo(\n                creator,\n                input_image,\n                idphoto_json,\n                language,\n                head_measure_ratio,\n                top_distance_max,\n                top_distance_min,\n                whitening_strength,\n                brightness_strength,\n                contrast_strength,\n                sharpen_strength,\n                saturation_strength,\n                face_alignment_option,\n                horizontal_flip_option,\n            )\n        except (FaceError, APIError):\n            return self._handle_photo_generation_error(language)\n\n        # 后处理生成的照片\n        return self._process_generated_photo(\n            result,\n            idphoto_json,\n            language,\n            watermark_option,\n            watermark_text,\n            watermark_text_size,\n            watermark_text_opacity,\n            watermark_text_angle,\n            watermark_text_space,\n            watermark_text_color,\n        )\n\n    # 初始化idphoto_json字典\n    def _initialize_idphoto_json(\n        self,\n        mode_option,\n        color_option,\n        render_option,\n        image_kb_options,\n        layout_photo_crop_line_option,\n        jpeg_format_option,\n        print_switch,\n    ):\n        \"\"\"初始化idphoto_json字典\"\"\"\n        return {\n            \"size_mode\": mode_option,\n            \"color_mode\": color_option,\n            \"render_mode\": render_option,\n            \"image_kb_mode\": image_kb_options,\n            \"custom_image_kb\": None,\n            \"custom_image_dpi\": None,\n            \"layout_photo_crop_line_option\": layout_photo_crop_line_option,\n            \"jpeg_format_option\": jpeg_format_option,\n            \"print_switch\": print_switch,\n        }\n\n    # 处理尺寸模式\n    def _process_size_mode(\n        self,\n        idphoto_json,\n        language,\n        size_list_option,\n        custom_size_height,\n        custom_size_width,\n        custom_size_height_mm,\n        custom_size_width_mm,\n    ):\n        \"\"\"处理尺寸模式\"\"\"\n        # 如果选择了尺寸列表\n        if idphoto_json[\"size_mode\"] == LOCALES[\"size_mode\"][language][\"choices\"][0]:\n            idphoto_json[\"size\"] = LOCALES[\"size_list\"][language][\"develop\"][\n                size_list_option\n            ]\n        # 如果选择了自定义尺寸(px或mm)\n        elif (\n            idphoto_json[\"size_mode\"] == LOCALES[\"size_mode\"][language][\"choices\"][2]\n            or idphoto_json[\"size_mode\"] == LOCALES[\"size_mode\"][language][\"choices\"][3]\n        ):\n            # 如果选择了自定义尺寸(px)\n            if (\n                idphoto_json[\"size_mode\"]\n                == LOCALES[\"size_mode\"][language][\"choices\"][2]\n            ):\n                id_height, id_width = int(custom_size_height), int(custom_size_width)\n            # 如果选择了自定义尺寸(mm)\n            else:\n                # 将mm转换为px\n                id_height = int(custom_size_height_mm / 25.4 * 300)\n                id_width = int(custom_size_width_mm / 25.4 * 300)\n            # 检查尺寸像素是否在100到1800之间\n            if (\n                id_height < id_width\n                or min(id_height, id_width) < 100\n                or max(id_height, id_width) > 1800\n            ):\n                return self._create_error_response(language)\n            idphoto_json[\"size\"] = (id_height, id_width)\n        # 如果选择了只换底\n        else:\n            idphoto_json[\"size\"] = (None, None)\n\n    # 处理颜色模式\n    def _process_color_mode(\n        self,\n        idphoto_json,\n        language,\n        color_option,\n        custom_color_R,\n        custom_color_G,\n        custom_color_B,\n        custom_color_hex_value,\n    ):\n        \"\"\"处理颜色模式\"\"\"\n        # 如果选择了自定义颜色BGR\n        if idphoto_json[\"color_mode\"] == LOCALES[\"bg_color\"][language][\"choices\"][-2]:\n            idphoto_json[\"color_bgr\"] = tuple(\n                map(range_check, [custom_color_R, custom_color_G, custom_color_B])\n            )\n        # 如果选择了自定义颜色HEX\n        elif idphoto_json[\"color_mode\"] == LOCALES[\"bg_color\"][language][\"choices\"][-1]:\n            hex_color = custom_color_hex_value\n            # 将十六进制颜色转换为RGB颜色，如果长度为6，则直接转换，如果长度为7，则去掉#号再转换\n            if len(hex_color) == 6:\n                idphoto_json[\"color_bgr\"] = tuple(\n                    int(hex_color[i : i + 2], 16) for i in (0, 2, 4)\n                )\n            elif len(hex_color) == 7:\n                hex_color = hex_color[1:]\n                idphoto_json[\"color_bgr\"] = tuple(\n                    int(hex_color[i : i + 2], 16) for i in (0, 2, 4)\n                )\n            else:\n                raise ValueError(\n                    \"Invalid hex color. You can only use 6 or 7 characters. For example: #FFFFFF or FFFFFF\"\n                )\n        # 如果选择了美式证件照\n        elif idphoto_json[\"color_mode\"] == LOCALES[\"bg_color\"][language][\"choices\"][-3]:\n            idphoto_json[\"color_bgr\"] = (255, 255, 255)\n        else:\n            hex_color = LOCALES[\"bg_color\"][language][\"develop\"][color_option]\n            idphoto_json[\"color_bgr\"] = tuple(\n                int(hex_color[i : i + 2], 16) for i in (0, 2, 4)\n            )\n\n    # 生成证件照\n    def _generate_id_photo(\n        self,\n        creator: IDCreator,\n        input_image,\n        idphoto_json,\n        language,\n        head_measure_ratio,\n        top_distance_max,\n        top_distance_min,\n        whitening_strength,\n        brightness_strength,\n        contrast_strength,\n        sharpen_strength,\n        saturation_strength,\n        face_alignment_option,\n        horizontal_flip_option,\n    ):\n        \"\"\"生成证件照\"\"\"\n        change_bg_only = (\n            idphoto_json[\"size_mode\"] in LOCALES[\"size_mode\"][language][\"choices\"][1]\n        )\n        return creator(\n            input_image,\n            change_bg_only=change_bg_only,\n            size=idphoto_json[\"size\"],\n            head_measure_ratio=head_measure_ratio,\n            head_top_range=(top_distance_max, top_distance_min),\n            whitening_strength=whitening_strength,\n            brightness_strength=brightness_strength,\n            contrast_strength=contrast_strength,\n            sharpen_strength=sharpen_strength,\n            saturation_strength=saturation_strength,\n            face_alignment=face_alignment_option,\n            horizontal_flip=horizontal_flip_option,\n        )\n\n    # 处理照片生成错误\n    def _handle_photo_generation_error(self, language):\n        \"\"\"处理照片生成错误\"\"\"\n        return [gr.update(value=None) for _ in range(4)] + [\n            gr.update(visible=False),\n            gr.update(value=None),\n            gr.update(value=None),\n            gr.update(\n                value=LOCALES[\"notification\"][language][\"face_error\"], visible=True\n            ),\n        ]\n\n    # 处理生成的照片\n    def _process_generated_photo(\n        self,\n        result,\n        idphoto_json,\n        language,\n        watermark_option,\n        watermark_text,\n        watermark_text_size,\n        watermark_text_opacity,\n        watermark_text_angle,\n        watermark_text_space,\n        watermark_text_color,\n    ):\n        \"\"\"处理生成的照片\"\"\"\n        result_image_standard, result_image_hd, _, _, _, _ = result\n        result_image_standard_png = np.uint8(result_image_standard)\n        result_image_hd_png = np.uint8(result_image_hd)\n\n        # 渲染背景\n        result_image_standard, result_image_hd = self._render_background(\n            result_image_standard, result_image_hd, idphoto_json, language\n        )\n\n        # 添加水印\n        if watermark_option == LOCALES[\"watermark_switch\"][language][\"choices\"][1]:\n            result_image_standard, result_image_hd = self._add_watermark(\n                result_image_standard,\n                result_image_hd,\n                watermark_text,\n                watermark_text_size,\n                watermark_text_opacity,\n                watermark_text_angle,\n                watermark_text_space,\n                watermark_text_color,\n            )\n        \n        # 生成排版照片\n        result_image_layout, result_image_layout_visible = self._generate_image_layout(\n            idphoto_json,\n            result_image_standard,\n            language,\n        )\n        \n        # 生成模板照片\n        result_image_template, result_image_template_visible = self._generate_image_template(\n            idphoto_json,\n            result_image_hd,\n            language,\n        )\n\n        # 调整图片大小\n        output_image_path_dict = self._save_image(\n            result_image_standard,\n            result_image_hd,\n            result_image_layout,\n            idphoto_json,\n            format=\"jpeg\" if idphoto_json[\"jpeg_format_option\"] else \"png\",\n        )\n        \n        # 返回\n        if result_image_layout is not None:\n            result_image_layout = output_image_path_dict[\"layout\"][\"path\"]\n            \n        return self._create_response(\n            output_image_path_dict[\"standard\"][\"path\"],\n            output_image_path_dict[\"hd\"][\"path\"],\n            result_image_standard_png,\n            result_image_hd_png,\n            gr.update(value=result_image_layout, visible=result_image_layout_visible),\n            gr.update(value=result_image_template, visible=result_image_template_visible),\n            gr.update(visible = result_image_template_visible),\n        )\n\n    # 渲染背景\n    def _render_background(self, result_image_standard, result_image_hd, idphoto_json, language):\n        \"\"\"渲染背景\"\"\"\n        render_modes = {0: \"pure_color\", 1: \"updown_gradient\", 2: \"center_gradient\"}\n        render_mode = render_modes[idphoto_json[\"render_mode\"]]\n\n        if idphoto_json[\"color_mode\"] != LOCALES[\"bg_color\"][language][\"choices\"][-3]:\n            result_image_standard = np.uint8(\n                add_background(\n                    result_image_standard, bgr=idphoto_json[\"color_bgr\"], mode=render_mode\n                )\n            )\n            result_image_hd = np.uint8(\n                add_background(\n                    result_image_hd, bgr=idphoto_json[\"color_bgr\"], mode=render_mode\n                )\n            )\n        # 如果选择了美式证件照\n        else:\n            result_image_standard = np.uint8(\n                add_background_with_image(\n                    result_image_standard, \n                    background_image=cv2.imread(os.path.join(base_path, \"assets\", \"american-style.png\"))\n                )\n            )\n            result_image_hd = np.uint8(\n                add_background_with_image(\n                    result_image_hd, \n                    background_image=cv2.imread(os.path.join(base_path, \"assets\", \"american-style.png\"))\n                )\n            )\n        return result_image_standard, result_image_hd\n\n    # 生成排版照片\n    def _generate_image_layout(\n        self,\n        idphoto_json,\n        result_image_standard,\n        language,\n    ):\n        \"\"\"生成排版照片\"\"\"\n        # 如果选择了只换底，则不生成排版照片\n        if idphoto_json[\"size_mode\"] in LOCALES[\"size_mode\"][language][\"choices\"][1]:\n            return None, False\n\n        # 预设排版照尺寸字典\n        PRESET_LAYOUT_SIZE = {\n            choice: shape\n            for choice, shape in zip(\n                LOCALES[\"print_switch\"][language][\"choices\"],\n                LOCALES[\"print_switch\"][\"shape\"]\n            )\n        }\n        \n        choose_layout_size = PRESET_LAYOUT_SIZE[idphoto_json[\"print_switch\"]]\n        \n        typography_arr, typography_rotate = generate_layout_array(\n            input_height=idphoto_json[\"size\"][0],\n            input_width=idphoto_json[\"size\"][1],\n            LAYOUT_HEIGHT= choose_layout_size[0],\n            LAYOUT_WIDTH= choose_layout_size[1],\n        )\n        \n        result_image_layout = generate_layout_image(\n            result_image_standard,\n            typography_arr,\n            typography_rotate,\n            height=idphoto_json[\"size\"][0],\n            width=idphoto_json[\"size\"][1],\n            crop_line=idphoto_json[\"layout_photo_crop_line_option\"],\n            LAYOUT_HEIGHT=choose_layout_size[0],\n            LAYOUT_WIDTH=choose_layout_size[1],\n        )\n\n        return result_image_layout, True\n    \n    # 生成模板照片\n    def _generate_image_template(\n        self,\n        idphoto_json,\n        result_image_hd,\n        language,\n    ):\n        # 如果选择了只换底，则不生成模板照片\n        if idphoto_json[\"size_mode\"] in LOCALES[\"size_mode\"][language][\"choices\"][1]:\n            return None, False\n        \n        TEMPLATE_NAME_LIST = [\"template_1\", \"template_2\"]\n        \"\"\"生成模板照片\"\"\"\n        result_image_template_list = []\n        for template_name in TEMPLATE_NAME_LIST:\n            result_image_template = generte_template_photo(\n                template_name=template_name,\n                input_image=result_image_hd,\n            )\n            result_image_template_list.append(result_image_template)\n        return result_image_template_list, True\n\n    # 添加水印\n    def _add_watermark(\n        self,\n        result_image_standard,\n        result_image_hd,\n        watermark_text,\n        watermark_text_size,\n        watermark_text_opacity,\n        watermark_text_angle,\n        watermark_text_space,\n        watermark_text_color,\n    ):\n        \"\"\"添加水印\"\"\"\n        watermark_params = {\n            \"text\": watermark_text,\n            \"size\": watermark_text_size,\n            \"opacity\": watermark_text_opacity,\n            \"angle\": watermark_text_angle,\n            \"space\": watermark_text_space,\n            \"color\": watermark_text_color,\n        }\n        result_image_standard = add_watermark(\n            image=result_image_standard, **watermark_params\n        )\n        result_image_hd = add_watermark(image=result_image_hd, **watermark_params)\n        return result_image_standard, result_image_hd\n\n    def _save_image(\n        self,\n        result_image_standard,\n        result_image_hd,\n        result_image_layout,\n        idphoto_json,\n        format=\"png\",\n    ):\n        # 设置输出路径（临时目录）\n        import tempfile\n        base_path = tempfile.mkdtemp()\n        timestamp = int(time.time())\n        output_paths = {\n            \"standard\": {\n                \"path\": f\"{base_path}/{timestamp}_standard\",\n                \"processed\": False,\n            },\n            \"hd\": {\"path\": f\"{base_path}/{timestamp}_hd\", \"processed\": False},\n            \"layout\": {\"path\": f\"{base_path}/{timestamp}_layout\", \"processed\": False},\n        }\n\n        # 获取自定义的KB和DPI值\n        custom_kb = idphoto_json.get(\"custom_image_kb\")\n        custom_dpi = idphoto_json.get(\"custom_image_dpi\", 300)\n\n        # 处理同时有自定义KB和DPI的情况\n        if custom_kb and custom_dpi:\n            # 为所有输出路径添加DPI信息\n            for key in output_paths:\n                output_paths[key][\"path\"] += f\"_{custom_dpi}dpi.{format}\"\n            # 为标准图像添加KB信息\n            output_paths[\"standard\"][\"path\"] = output_paths[\"standard\"][\"path\"].replace(\n                f\".{format}\", f\"_{custom_kb}kb.{format}\"\n            )\n\n            # 调整标准图像大小并保存\n            resize_image_to_kb(\n                result_image_standard,\n                output_paths[\"standard\"][\"path\"],\n                custom_kb,\n                dpi=custom_dpi,\n            )\n            # 保存高清图像和排版图像\n            save_image_dpi_to_bytes(\n                result_image_hd, output_paths[\"hd\"][\"path\"], dpi=custom_dpi\n            )\n            if result_image_layout is not None:\n                save_image_dpi_to_bytes(\n                    result_image_layout, output_paths[\"layout\"][\"path\"], dpi=custom_dpi\n                )\n\n            return output_paths\n\n        # 只有自定义DPI的情况\n        elif custom_dpi:\n            for key in output_paths:\n                # 保存所有图像，使用自定义DPI\n                # 如果只换底，则不保存排版图像\n                if key == \"layout\" and result_image_layout is None:\n                    continue\n                output_paths[key][\"path\"] += f\"_{custom_dpi}dpi.{format}\"\n                save_image_dpi_to_bytes(\n                    locals()[f\"result_image_{key}\"],\n                    output_paths[key][\"path\"],\n                    dpi=custom_dpi,\n                )\n\n            return output_paths\n\n        # 只有自定义KB的情况\n        elif custom_kb:\n            output_paths[\"standard\"][\"path\"] += f\"_{custom_kb}kb.{format}\"\n            output_paths[\"hd\"][\"path\"] += f\".{format}\"\n            for key in output_paths:\n                if key == \"layout\" and result_image_layout is None:\n                    continue\n                output_paths[key][\"path\"] += f\".{format}\"\n                \n                # 只调整标准图像大小\n                resize_image_to_kb(\n                    result_image_standard,\n                    output_paths[\"standard\"][\"path\"],\n                    custom_kb,\n                    dpi=300,\n                )\n                \n                # 保存高清图像和排版图像\n                save_image_dpi_to_bytes(\n                    result_image_hd, output_paths[\"hd\"][\"path\"], dpi=300\n                )\n                if result_image_layout is not None:\n                    save_image_dpi_to_bytes(\n                        result_image_layout, output_paths[\"layout\"][\"path\"], dpi=300\n                    )\n\n            return output_paths\n        # 没有自定义设置\n        else: \n            output_paths[\"standard\"][\"path\"] += f\".{format}\"\n            output_paths[\"hd\"][\"path\"] += f\".{format}\"\n            output_paths[\"layout\"][\"path\"] += f\".{format}\"\n            \n            # 保存所有图像\n            save_image_dpi_to_bytes(\n                result_image_standard, output_paths[\"standard\"][\"path\"], dpi=300\n            )\n            save_image_dpi_to_bytes(\n                result_image_hd, output_paths[\"hd\"][\"path\"], dpi=300\n            )\n            if result_image_layout is not None:\n                save_image_dpi_to_bytes(\n                    result_image_layout, output_paths[\"layout\"][\"path\"], dpi=300\n                )\n                \n            return output_paths\n            \n\n    def _create_response(\n        self,\n        result_image_standard,\n        result_image_hd,\n        result_image_standard_png,\n        result_image_hd_png,\n        result_layout_image_gr,\n        result_image_template_gr,\n        result_image_template_accordion_gr,\n    ):    \n        \"\"\"创建响应\"\"\"\n        response = [\n            result_image_standard,\n            result_image_hd,\n            result_image_standard_png,\n            result_image_hd_png,\n            result_layout_image_gr,\n            result_image_template_gr,\n            result_image_template_accordion_gr,\n            gr.update(visible=False),\n        ]\n\n        return response\n\n    def _create_error_response(self, language):\n        \"\"\"创建错误响应\"\"\"\n        return [gr.update(value=None) for _ in range(4)] + [\n            None,\n            gr.update(\n                value=LOCALES[\"size_mode\"][language][\"custom_size_eror\"], visible=True\n            ),\n            None,\n        ]\n", "metadata": {"source": "github.com/Zeyi-Lin/HivisionIDPhotos", "file_path": "demo/processor.py", "language": "python", "complexity": "complex", "star_count": 18571, "quality_score": 0.98, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": true, "has_documentation": true, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 14, "class_count": 1, "line_count": 683}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create REST API endpoint with proper request/response handling\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import gradio as gr\nimport os\nimport pathlib\nfrom demo.locales import LOCALES\nfrom demo.processor import IDPhotoProcessor\n\n\"\"\"\n只裁切模式:\n1. 如果重新上传了照片，然后点击按钮，第一次会调用不裁切的模式，第二次会调用裁切的模式\n\"\"\"\n\n\ndef load_description(fp):\n    \"\"\"\n    加载title.md文件作为Demo的顶部栏\n    \"\"\"\n    with open(fp, \"r\", encoding=\"utf-8\") as f:\n        content = f.read()\n    return content\n\n\ndef create_ui(\n    processor: IDPhotoProcessor,\n    root_dir: str,\n    human_matting_models: list,\n    face_detect_models: list,\n    language: list,\n):\n\n    # 加载环境变量DEFAULT_LANG, 如果有且在language中，则将DEFAULT_LANG设置为环境变量\n    if \"DEFAULT_LANG\" in os.environ and os.environ[\"DEFAULT_LANG\"] in language:\n        DEFAULT_LANG = os.environ[\"DEFAULT_LANG\"]\n    else:\n        DEFAULT_LANG = language[0]\n\n    DEFAULT_HUMAN_MATTING_MODEL = \"modnet_photographic_portrait_matting\"\n    DEFAULT_FACE_DETECT_MODEL = \"retinaface-resnet50\"\n\n    if DEFAULT_HUMAN_MATTING_MODEL in human_matting_models:\n        human_matting_models.remove(DEFAULT_HUMAN_MATTING_MODEL)\n        human_matting_models.insert(0, DEFAULT_HUMAN_MATTING_MODEL)\n\n    if DEFAULT_FACE_DETECT_MODEL not in face_detect_models:\n        DEFAULT_FACE_DETECT_MODEL = \"mtcnn\"\n\n    demo = gr.Blocks(title=\"HivisionIDPhotos\")\n\n    with demo:\n        gr.HTML(load_description(os.path.join(root_dir, \"demo/assets/title.md\")))\n        with gr.Row():\n            # ------------------------ 左半边 UI ------------------------\n            with gr.Column():\n                img_input = gr.Image(height=400)\n\n                with gr.Row():\n                    # 语言选择器\n                    language_options = gr.Dropdown(\n                        choices=language,\n                        label=\"Language\",\n                        value=DEFAULT_LANG,\n                    )\n\n                    face_detect_model_options = gr.Dropdown(\n                        choices=face_detect_models,\n                        label=LOCALES[\"face_model\"][DEFAULT_LANG][\"label\"],\n                        value=DEFAULT_FACE_DETECT_MODEL,\n                    )\n\n                    matting_model_options = gr.Dropdown(\n                        choices=human_matting_models,\n                        label=LOCALES[\"matting_model\"][DEFAULT_LANG][\"label\"],\n                        value=human_matting_models[0],\n                    )\n\n                # TAB1 - 关键参数 ------------------------------------------------\n                with gr.Tab(\n                    LOCALES[\"key_param\"][DEFAULT_LANG][\"label\"]\n                ) as key_parameter_tab:\n                    # 尺寸模式\n                    with gr.Row():\n                        mode_options = gr.Radio(\n                            choices=LOCALES[\"size_mode\"][DEFAULT_LANG][\"choices\"],\n                            label=LOCALES[\"size_mode\"][DEFAULT_LANG][\"label\"],\n                            value=LOCALES[\"size_mode\"][DEFAULT_LANG][\"choices\"][0],\n                            min_width=520,\n                        )\n                        \n                    # 尺寸列表\n                    with gr.Row(visible=True) as size_list_row:\n                        size_list_options = gr.Dropdown(\n                            choices=LOCALES[\"size_list\"][DEFAULT_LANG][\"choices\"],\n                            label=LOCALES[\"size_list\"][DEFAULT_LANG][\"label\"],\n                            value=LOCALES[\"size_list\"][DEFAULT_LANG][\"choices\"][0],\n                            elem_id=\"size_list\",\n                        )\n                    # 自定义尺寸px\n                    with gr.Row(visible=False) as custom_size_px:\n                        custom_size_height_px = gr.Number(\n                            value=413,\n                            label=LOCALES[\"custom_size_px\"][DEFAULT_LANG][\"height\"],\n                            interactive=True,\n                        )\n                        custom_size_width_px = gr.Number(\n                            value=295,\n                            label=LOCALES[\"custom_size_px\"][DEFAULT_LANG][\"width\"],\n                            interactive=True,\n                        )\n                    # 自定义尺寸mm\n                    with gr.Row(visible=False) as custom_size_mm:\n                        custom_size_height_mm = gr.Number(\n                            value=35,\n                            label=LOCALES[\"custom_size_mm\"][DEFAULT_LANG][\"height\"],\n                            interactive=True,\n                        )\n                        custom_size_width_mm = gr.Number(\n                            value=25,\n                            label=LOCALES[\"custom_size_mm\"][DEFAULT_LANG][\"width\"],\n                            interactive=True,\n                        )\n\n                    # 背景颜色\n                    color_options = gr.Radio(\n                        choices=LOCALES[\"bg_color\"][DEFAULT_LANG][\"choices\"],\n                        label=LOCALES[\"bg_color\"][DEFAULT_LANG][\"label\"],\n                        value=LOCALES[\"bg_color\"][DEFAULT_LANG][\"choices\"][0],\n                    )\n                    \n                    # 自定义颜色RGB\n                    with gr.Row(visible=False) as custom_color_rgb:\n                        custom_color_R = gr.Number(value=0, label=\"R\", minimum=0, maximum=255, interactive=True)\n                        custom_color_G = gr.Number(value=0, label=\"G\", minimum=0, maximum=255, interactive=True)\n                        custom_color_B = gr.Number(value=0, label=\"B\", minimum=0, maximum=255, interactive=True)\n                    \n                    # 自定义颜色HEX\n                    with gr.Row(visible=False) as custom_color_hex:\n                        custom_color_hex_value = gr.Text(value=\"000000\", label=\"Hex\", interactive=True)\n\n                    # 渲染模式\n                    render_options = gr.Radio(\n                        choices=LOCALES[\"render_mode\"][DEFAULT_LANG][\"choices\"],\n                        label=LOCALES[\"render_mode\"][DEFAULT_LANG][\"label\"],\n                        value=LOCALES[\"render_mode\"][DEFAULT_LANG][\"choices\"][0],\n                    )\n                    \n                    with gr.Row():\n                        # 插件模式\n                        plugin_options = gr.CheckboxGroup(\n                            label=LOCALES[\"plugin\"][DEFAULT_LANG][\"label\"],\n                            choices=LOCALES[\"plugin\"][DEFAULT_LANG][\"choices\"],\n                            interactive=True,\n                            value=LOCALES[\"plugin\"][DEFAULT_LANG][\"value\"]\n                        )\n\n                # TAB2 - 高级参数 ------------------------------------------------\n                with gr.Tab(\n                    LOCALES[\"advance_param\"][DEFAULT_LANG][\"label\"]\n                ) as advance_parameter_tab:\n                    head_measure_ratio_option = gr.Slider(\n                        minimum=0.1,\n                        maximum=0.5,\n                        value=0.2,\n                        step=0.01,\n                        label=LOCALES[\"head_measure_ratio\"][DEFAULT_LANG][\"label\"],\n                        interactive=True,\n                    )\n                    top_distance_option = gr.Slider(\n                        minimum=0.02,\n                        maximum=0.5,\n                        value=0.12,\n                        step=0.01,\n                        label=LOCALES[\"top_distance\"][DEFAULT_LANG][\"label\"],\n                        interactive=True,\n                    )\n\n                    image_kb_options = gr.Radio(\n                        choices=LOCALES[\"image_kb\"][DEFAULT_LANG][\"choices\"],\n                        label=LOCALES[\"image_kb\"][DEFAULT_LANG][\"label\"],\n                        value=LOCALES[\"image_kb\"][DEFAULT_LANG][\"choices\"][0],\n                    )\n\n                    custom_image_kb_size = gr.Slider(\n                        minimum=10,\n                        maximum=1000,\n                        value=50,\n                        label=LOCALES[\"image_kb_size\"][DEFAULT_LANG][\"label\"],\n                        interactive=True,\n                        visible=False,\n                    )\n\n                    image_dpi_options = gr.Radio(\n                        choices=LOCALES[\"image_dpi\"][DEFAULT_LANG][\"choices\"],\n                        label=LOCALES[\"image_dpi\"][DEFAULT_LANG][\"label\"],\n                        value=LOCALES[\"image_dpi\"][DEFAULT_LANG][\"choices\"][0],\n                    )\n                    custom_image_dpi_size = gr.Slider(\n                        minimum=72,\n                        maximum=600,\n                        value=300,\n                        label=LOCALES[\"image_dpi_size\"][DEFAULT_LANG][\"label\"],\n                        interactive=True,\n                        visible=False,\n                    )\n\n                # TAB3 - 美颜 ------------------------------------------------\n                with gr.Tab(\n                    LOCALES[\"beauty_tab\"][DEFAULT_LANG][\"label\"]\n                ) as beauty_parameter_tab:\n                    # 美白组件\n                    whitening_option = gr.Slider(\n                        label=LOCALES[\"whitening_strength\"][DEFAULT_LANG][\"label\"],\n                        minimum=0,\n                        maximum=15,\n                        value=2,\n                        step=1,\n                        interactive=True,\n                    )\n\n                    with gr.Row():\n                        # 亮度组件\n                        brightness_option = gr.Slider(\n                            label=LOCALES[\"brightness_strength\"][DEFAULT_LANG][\"label\"],\n                            minimum=-5,\n                            maximum=25,\n                            value=0,\n                            step=1,\n                            interactive=True,\n                        )\n                        # 对比度组件\n                        contrast_option = gr.Slider(\n                            label=LOCALES[\"contrast_strength\"][DEFAULT_LANG][\"label\"],\n                            minimum=-10,\n                            maximum=50,\n                            value=0,\n                            step=1,\n                            interactive=True,\n                        )\n                        # 饱和度组件\n                        saturation_option = gr.Slider(\n                            label=LOCALES[\"saturation_strength\"][DEFAULT_LANG][\"label\"],\n                            minimum=-10,\n                            maximum=50,\n                            value=0,\n                            step=1,\n                            interactive=True,\n                        )\n\n                    # 锐化组件\n                    sharpen_option = gr.Slider(\n                        label=LOCALES[\"sharpen_strength\"][DEFAULT_LANG][\"label\"],\n                        minimum=0,\n                        maximum=5,\n                        value=0,\n                        step=1,\n                        interactive=True,\n                    )\n\n                # TAB4 - 水印 ------------------------------------------------\n                with gr.Tab(\n                    LOCALES[\"watermark_tab\"][DEFAULT_LANG][\"label\"]\n                ) as watermark_parameter_tab:\n                    watermark_options = gr.Radio(\n                        choices=LOCALES[\"watermark_switch\"][DEFAULT_LANG][\"choices\"],\n                        label=LOCALES[\"watermark_switch\"][DEFAULT_LANG][\"label\"],\n                        value=LOCALES[\"watermark_switch\"][DEFAULT_LANG][\"choices\"][0],\n                    )\n\n                    with gr.Row():\n                        watermark_text_options = gr.Text(\n                            max_length=20,\n                            label=LOCALES[\"watermark_text\"][DEFAULT_LANG][\"label\"],\n                            value=LOCALES[\"watermark_text\"][DEFAULT_LANG][\"value\"],\n                            placeholder=LOCALES[\"watermark_text\"][DEFAULT_LANG][\n                                \"placeholder\"\n                            ],\n                            interactive=False,\n                        )\n                        watermark_text_color = gr.ColorPicker(\n                            label=LOCALES[\"watermark_color\"][DEFAULT_LANG][\"label\"],\n                            interactive=False,\n                            value=\"#FFFFFF\",\n                        )\n\n                    watermark_text_size = gr.Slider(\n                        minimum=10,\n                        maximum=100,\n                        value=20,\n                        label=LOCALES[\"watermark_size\"][DEFAULT_LANG][\"label\"],\n                        interactive=False,\n                        step=1,\n                    )\n\n                    watermark_text_opacity = gr.Slider(\n                        minimum=0,\n                        maximum=1,\n                        value=0.15,\n                        label=LOCALES[\"watermark_opacity\"][DEFAULT_LANG][\"label\"],\n                        interactive=False,\n                        step=0.01,\n                    )\n\n                    watermark_text_angle = gr.Slider(\n                        minimum=0,\n                        maximum=360,\n                        value=30,\n                        label=LOCALES[\"watermark_angle\"][DEFAULT_LANG][\"label\"],\n                        interactive=False,\n                        step=1,\n                    )\n\n                    watermark_text_space = gr.Slider(\n                        minimum=10,\n                        maximum=200,\n                        value=25,\n                        label=LOCALES[\"watermark_space\"][DEFAULT_LANG][\"label\"],\n                        interactive=False,\n                        step=1,\n                    )\n\n                    def update_watermark_text_visibility(choice, language):\n                        return [\n                            gr.update(\n                                interactive=(\n                                    choice\n                                    == LOCALES[\"watermark_switch\"][language][\"choices\"][\n                                        1\n                                    ]\n                                )\n                            )\n                        ] * 6\n\n                    watermark_options.change(\n                        fn=update_watermark_text_visibility,\n                        inputs=[watermark_options, language_options],\n                        outputs=[\n                            watermark_text_options,\n                            watermark_text_color,\n                            watermark_text_size,\n                            watermark_text_opacity,\n                            watermark_text_angle,\n                            watermark_text_space,\n                        ],\n                    )\n                \n                # TAB5 - 打印 ------------------------------------------------\n                with gr.Tab(\n                    LOCALES[\"print_tab\"][DEFAULT_LANG][\"label\"]\n                ) as print_parameter_tab:\n                    print_options = gr.Radio(\n                        choices=LOCALES[\"print_switch\"][DEFAULT_LANG][\"choices\"],\n                        label=LOCALES[\"print_switch\"][DEFAULT_LANG][\"label\"],\n                        value=LOCALES[\"print_switch\"][DEFAULT_LANG][\"choices\"][0],\n                        interactive=True,\n                    )\n                \n\n                img_but = gr.Button(\n                    LOCALES[\"button\"][DEFAULT_LANG][\"label\"],\n                    elem_id=\"btn\",\n                    variant=\"primary\"\n                )\n\n                example_images = gr.Examples(\n                    inputs=[img_input],\n                    examples=[\n                        [path.as_posix()]\n                        for path in sorted(\n                            pathlib.Path(os.path.join(root_dir, \"demo/images\")).rglob(\n                                \"*.jpg\"\n                            )\n                        )\n                    ],\n                )\n\n            # ---------------- 右半边 UI ----------------\n            with gr.Column():\n                notification = gr.Text(\n                    label=LOCALES[\"notification\"][DEFAULT_LANG][\"label\"], visible=False\n                )\n                with gr.Row():\n                    # 标准照\n                    img_output_standard = gr.Image(\n                        label=LOCALES[\"standard_photo\"][DEFAULT_LANG][\"label\"],\n                        height=350,\n                        format=\"png\",\n                    )\n                    # 高清照\n                    img_output_standard_hd = gr.Image(\n                        label=LOCALES[\"hd_photo\"][DEFAULT_LANG][\"label\"],\n                        height=350,\n                        format=\"png\",\n                    )\n                # 排版照\n                img_output_layout = gr.Image(\n                    label=LOCALES[\"layout_photo\"][DEFAULT_LANG][\"label\"],\n                    height=350,\n                    format=\"png\",\n                )\n                # 模版照片\n                with gr.Accordion(\n                    LOCALES[\"template_photo\"][DEFAULT_LANG][\"label\"], open=False\n                ) as template_image_accordion:      \n                    img_output_template = gr.Gallery(\n                        label=LOCALES[\"template_photo\"][DEFAULT_LANG][\"label\"],\n                        height=350,\n                        format=\"png\",\n                    )\n                # 抠图图像\n                with gr.Accordion(\n                    LOCALES[\"matting_image\"][DEFAULT_LANG][\"label\"], open=False\n                ) as matting_image_accordion:\n                    with gr.Row():\n                        img_output_standard_png = gr.Image(\n                            label=LOCALES[\"standard_photo_png\"][DEFAULT_LANG][\"label\"],\n                            height=350,\n                            format=\"png\",\n                            elem_id=\"standard_photo_png\",\n                        )\n                        img_output_standard_hd_png = gr.Image(\n                            label=LOCALES[\"hd_photo_png\"][DEFAULT_LANG][\"label\"],\n                            height=350,\n                            format=\"png\",\n                            elem_id=\"hd_photo_png\",\n                        )\n\n            # ---------------- 多语言切换函数 ----------------\n            def change_language(language):\n                return {\n                    face_detect_model_options: gr.update(\n                        label=LOCALES[\"face_model\"][language][\"label\"]\n                    ),\n                    matting_model_options: gr.update(\n                        label=LOCALES[\"matting_model\"][language][\"label\"]\n                    ),\n                    size_list_options: gr.update(\n                        label=LOCALES[\"size_list\"][language][\"label\"],\n                        choices=LOCALES[\"size_list\"][language][\"choices\"],\n                        value=LOCALES[\"size_list\"][language][\"choices\"][0],\n                    ),\n                    mode_options: gr.update(\n                        label=LOCALES[\"size_mode\"][language][\"label\"],\n                        choices=LOCALES[\"size_mode\"][language][\"choices\"],\n                        value=LOCALES[\"size_mode\"][language][\"choices\"][0],\n                    ),\n                    color_options: gr.update(\n                        label=LOCALES[\"bg_color\"][language][\"label\"],\n                        choices=LOCALES[\"bg_color\"][language][\"choices\"],\n                        value=LOCALES[\"bg_color\"][language][\"choices\"][0],\n                    ),\n                    img_but: gr.update(value=LOCALES[\"button\"][language][\"label\"]),\n                    render_options: gr.update(\n                        label=LOCALES[\"render_mode\"][language][\"label\"],\n                        choices=LOCALES[\"render_mode\"][language][\"choices\"],\n                        value=LOCALES[\"render_mode\"][language][\"choices\"][0],\n                    ),\n                    image_kb_options: gr.update(\n                        label=LOCALES[\"image_kb_size\"][language][\"label\"],\n                        choices=LOCALES[\"image_kb\"][language][\"choices\"],\n                        value=LOCALES[\"image_kb\"][language][\"choices\"][0],\n                    ),\n                    custom_image_kb_size: gr.update(\n                        label=LOCALES[\"image_kb\"][language][\"label\"]\n                    ),\n                    notification: gr.update(\n                        label=LOCALES[\"notification\"][language][\"label\"]\n                    ),\n                    img_output_standard: gr.update(\n                        label=LOCALES[\"standard_photo\"][language][\"label\"]\n                    ),\n                    img_output_standard_hd: gr.update(\n                        label=LOCALES[\"hd_photo\"][language][\"label\"]\n                    ),\n                    img_output_standard_png: gr.update(\n                        label=LOCALES[\"standard_photo_png\"][language][\"label\"]\n                    ),\n                    img_output_standard_hd_png: gr.update(\n                        label=LOCALES[\"hd_photo_png\"][language][\"label\"]\n                    ),\n                    img_output_layout: gr.update(\n                        label=LOCALES[\"layout_photo\"][language][\"label\"]\n                    ),\n                    head_measure_ratio_option: gr.update(\n                        label=LOCALES[\"head_measure_ratio\"][language][\"label\"]\n                    ),\n                    top_distance_option: gr.update(\n                        label=LOCALES[\"top_distance\"][language][\"label\"]\n                    ),\n                    key_parameter_tab: gr.update(\n                        label=LOCALES[\"key_param\"][language][\"label\"]\n                    ),\n                    advance_parameter_tab: gr.update(\n                        label=LOCALES[\"advance_param\"][language][\"label\"]\n                    ),\n                    watermark_parameter_tab: gr.update(\n                        label=LOCALES[\"watermark_tab\"][language][\"label\"]\n                    ),\n                    watermark_text_options: gr.update(\n                        label=LOCALES[\"watermark_text\"][language][\"label\"],\n                        placeholder=LOCALES[\"watermark_text\"][language][\"placeholder\"],\n                    ),\n                    watermark_text_color: gr.update(\n                        label=LOCALES[\"watermark_color\"][language][\"label\"]\n                    ),\n                    watermark_text_size: gr.update(\n                        label=LOCALES[\"watermark_size\"][language][\"label\"]\n                    ),\n                    watermark_text_opacity: gr.update(\n                        label=LOCALES[\"watermark_opacity\"][language][\"label\"]\n                    ),\n                    watermark_text_angle: gr.update(\n                        label=LOCALES[\"watermark_angle\"][language][\"label\"]\n                    ),\n                    watermark_text_space: gr.update(\n                        label=LOCALES[\"watermark_space\"][language][\"label\"]\n                    ),\n                    watermark_options: gr.update(\n                        label=LOCALES[\"watermark_switch\"][language][\"label\"],\n                        choices=LOCALES[\"watermark_switch\"][language][\"choices\"],\n                        value=LOCALES[\"watermark_switch\"][language][\"choices\"][0],\n                    ),\n                    matting_image_accordion: gr.update(\n                        label=LOCALES[\"matting_image\"][language][\"label\"]\n                    ),\n                    beauty_parameter_tab: gr.update(\n                        label=LOCALES[\"beauty_tab\"][language][\"label\"]\n                    ),\n                    whitening_option: gr.update(\n                        label=LOCALES[\"whitening_strength\"][language][\"label\"]\n                    ),\n                    image_dpi_options: gr.update(\n                        label=LOCALES[\"image_dpi\"][language][\"label\"],\n                        choices=LOCALES[\"image_dpi\"][language][\"choices\"],\n                        value=LOCALES[\"image_dpi\"][language][\"choices\"][0],\n                    ),\n                    custom_image_dpi_size: gr.update(\n                        label=LOCALES[\"image_dpi\"][language][\"label\"]\n                    ),\n                    brightness_option: gr.update(\n                        label=LOCALES[\"brightness_strength\"][language][\"label\"]\n                    ),\n                    contrast_option: gr.update(\n                        label=LOCALES[\"contrast_strength\"][language][\"label\"]\n                    ),\n                    sharpen_option: gr.update(\n                        label=LOCALES[\"sharpen_strength\"][language][\"label\"]\n                    ),\n                    saturation_option: gr.update(\n                        label=LOCALES[\"saturation_strength\"][language][\"label\"]\n                    ),\n                    custom_size_width_px: gr.update(\n                        label=LOCALES[\"custom_size_px\"][language][\"width\"]\n                    ),\n                    custom_size_height_px: gr.update(\n                        label=LOCALES[\"custom_size_px\"][language][\"height\"]\n                    ),\n                    custom_size_width_mm: gr.update(\n                        label=LOCALES[\"custom_size_mm\"][language][\"width\"]\n                    ),\n                    custom_size_height_mm: gr.update(\n                        label=LOCALES[\"custom_size_mm\"][language][\"height\"]\n                    ),\n                    img_output_template: gr.update(\n                        label=LOCALES[\"template_photo\"][language][\"label\"]\n                    ),\n                    template_image_accordion: gr.update(\n                        label=LOCALES[\"template_photo\"][language][\"label\"]\n                    ),\n                    plugin_options: gr.update(\n                        label=LOCALES[\"plugin\"][language][\"label\"],\n                        choices=LOCALES[\"plugin\"][language][\"choices\"],\n                        value=LOCALES[\"plugin\"][language][\"choices\"][0],\n                    ),\n                    print_parameter_tab: gr.update(\n                        label=LOCALES[\"print_tab\"][language][\"label\"]\n                    ),\n                    print_options: gr.update(\n                        label=LOCALES[\"print_switch\"][language][\"label\"],\n                        choices=LOCALES[\"print_switch\"][language][\"choices\"],\n                        value=LOCALES[\"print_switch\"][language][\"choices\"][0],\n                    ),\n                }\n\n            def change_visibility(option, lang, locales_key, custom_component):\n                return {\n                    custom_component: gr.update(\n                        visible=option == LOCALES[locales_key][lang][\"choices\"][-1]\n                    )\n                }\n\n            def change_color(colors, lang):\n                return {\n                    custom_color_rgb: gr.update(visible = colors == LOCALES[\"bg_color\"][lang][\"choices\"][-2]),\n                    custom_color_hex: gr.update(visible = colors == LOCALES[\"bg_color\"][lang][\"choices\"][-1]),\n                }\n                \n\n            def change_size_mode(size_option_item, lang):\n                choices = LOCALES[\"size_mode\"][lang][\"choices\"]\n                # 如果选择自定义尺寸mm\n                if size_option_item == choices[3]:\n                    return {\n                        custom_size_px: gr.update(visible=False),\n                        custom_size_mm: gr.update(visible=True),\n                        size_list_row: gr.update(visible=False),\n                        plugin_options: gr.update(interactive=True),\n                    }\n                # 如果选择自定义尺寸px\n                elif size_option_item == choices[2]:\n                    return {\n                        custom_size_px: gr.update(visible=True),\n                        custom_size_mm: gr.update(visible=False),\n                        size_list_row: gr.update(visible=False),\n                        plugin_options: gr.update(interactive=True),\n                    }\n                # 如果选择只换底，则隐藏所有尺寸组件\n                elif size_option_item == choices[1]:\n                    return {\n                        custom_size_px: gr.update(visible=False),\n                        custom_size_mm: gr.update(visible=False),\n                        size_list_row: gr.update(visible=False),\n                        plugin_options: gr.update(interactive=False),\n                    }\n                # 如果选择预设尺寸，则隐藏自定义尺寸组件\n                else:\n                    return {\n                        custom_size_px: gr.update(visible=False),\n                        custom_size_mm: gr.update(visible=False),\n                        size_list_row: gr.update(visible=True),\n                        plugin_options: gr.update(interactive=True),\n                    }\n\n            def change_image_kb(image_kb_option, lang):\n                return change_visibility(\n                    image_kb_option, lang, \"image_kb\", custom_image_kb_size\n                )\n\n            def change_image_dpi(image_dpi_option, lang):\n                return change_visibility(\n                    image_dpi_option, lang, \"image_dpi\", custom_image_dpi_size\n                )\n\n            # ---------------- 绑定事件 ----------------\n            # 语言切换\n            language_options.input(\n                change_language,\n                inputs=[language_options],\n                outputs=[\n                    size_list_options,\n                    mode_options,\n                    color_options,\n                    img_but,\n                    render_options,\n                    image_kb_options,\n                    matting_model_options,\n                    face_detect_model_options,\n                    custom_image_kb_size,\n                    notification,\n                    img_output_standard,\n                    img_output_standard_hd,\n                    img_output_standard_png,\n                    img_output_standard_hd_png,\n                    img_output_layout,\n                    head_measure_ratio_option,\n                    top_distance_option,\n                    key_parameter_tab,\n                    advance_parameter_tab,\n                    watermark_parameter_tab,\n                    watermark_text_options,\n                    watermark_text_color,\n                    watermark_text_size,\n                    watermark_text_opacity,\n                    watermark_text_angle,\n                    watermark_text_space,\n                    watermark_options,\n                    matting_image_accordion,\n                    beauty_parameter_tab,\n                    whitening_option,\n                    image_dpi_options,\n                    custom_image_dpi_size,\n                    brightness_option,\n                    contrast_option,\n                    sharpen_option,\n                    saturation_option,\n                    plugin_options,\n                    custom_size_width_px,\n                    custom_size_height_px,\n                    custom_size_width_mm,\n                    custom_size_height_mm,\n                    img_output_template,\n                    template_image_accordion,\n                    print_parameter_tab,\n                    print_options,\n                ],\n            )\n\n            # ---------------- 设置隐藏/显示交互效果 ----------------\n            # 尺寸模式\n            mode_options.input(\n                change_size_mode,\n                inputs=[mode_options, language_options],\n                outputs=[\n                    custom_size_px,\n                    custom_size_mm,\n                    size_list_row,\n                    plugin_options,\n                ],\n            )\n\n            # 颜色\n            color_options.input(\n                change_color,\n                inputs=[color_options, language_options],\n                outputs=[custom_color_rgb, custom_color_hex],\n            )\n\n            # 图片kb\n            image_kb_options.input(\n                change_image_kb,\n                inputs=[image_kb_options, language_options],\n                outputs=[custom_image_kb_size],\n            )\n\n            # 图片dpi\n            image_dpi_options.input(\n                change_image_dpi,\n                inputs=[image_dpi_options, language_options],\n                outputs=[custom_image_dpi_size],\n            )\n\n            img_but.click(\n                processor.process,\n                inputs=[\n                    img_input,\n                    mode_options,\n                    size_list_options,\n                    color_options,\n                    render_options,\n                    image_kb_options,\n                    custom_color_R,\n                    custom_color_G,\n                    custom_color_B,\n                    custom_color_hex_value,\n                    custom_size_height_px,\n                    custom_size_width_px,\n                    custom_size_height_mm,\n                    custom_size_width_mm,\n                    custom_image_kb_size,\n                    language_options,\n                    matting_model_options,\n                    watermark_options,\n                    watermark_text_options,\n                    watermark_text_color,\n                    watermark_text_size,\n                    watermark_text_opacity,\n                    watermark_text_angle,\n                    watermark_text_space,\n                    face_detect_model_options,\n                    head_measure_ratio_option,\n                    top_distance_option,\n                    whitening_option,\n                    image_dpi_options,\n                    custom_image_dpi_size,\n                    brightness_option,\n                    contrast_option,\n                    sharpen_option,\n                    saturation_option,\n                    plugin_options,\n                    print_options,\n                ],\n                outputs=[\n                    img_output_standard,\n                    img_output_standard_hd,\n                    img_output_standard_png,\n                    img_output_standard_hd_png,\n                    img_output_layout,\n                    img_output_template,\n                    template_image_accordion,\n                    notification,\n                ],\n            )\n\n    return demo\n", "metadata": {"source": "github.com/Zeyi-Lin/HivisionIDPhotos", "file_path": "demo/ui.py", "language": "python", "complexity": "complex", "star_count": 18571, "quality_score": 0.92, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": true, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 9, "class_count": 0, "line_count": 782}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create REST API endpoint with proper request/response handling\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import csv\n\n\ndef csv_to_size_list(csv_file: str) -> dict:\n    # 初始化一个空字典\n    size_list_dict = {}\n\n    # 打开 CSV 文件并读取数据\n    with open(csv_file, mode=\"r\", encoding=\"utf-8\") as file:\n        reader = csv.reader(file)\n        # 跳过表头\n        next(reader)\n        # 读取数据并填充字典\n        for row in reader:\n            size_name, h, w = row\n            size_name_add_size = \"{}\\t\\t({}, {})\".format(size_name, h, w)\n            size_list_dict[size_name_add_size] = (int(h), int(w))\n\n    return size_list_dict\n\n\ndef csv_to_color_list(csv_file: str) -> dict:\n    # 初始化一个空字典\n    color_list_dict = {}\n\n    # 打开 CSV 文件并读取数据\n    with open(csv_file, mode=\"r\", encoding=\"utf-8\") as file:\n        reader = csv.reader(file)\n        # 跳过表头\n        next(reader)\n        # 读取数据并填充字典\n        for row in reader:\n            color_name, hex_code = row\n            color_list_dict[color_name] = hex_code\n\n    return color_list_dict\n\n\ndef range_check(value, min_value=0, max_value=255):\n    value = int(value)\n    return max(min_value, min(value, max_value))\n", "metadata": {"source": "github.com/Zeyi-Lin/HivisionIDPhotos", "file_path": "demo/utils.py", "language": "python", "complexity": "simple", "star_count": 18571, "quality_score": 0.8600000000000001, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 3, "class_count": 0, "line_count": 42}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create REST API endpoint with proper request/response handling with asyncio coroutines\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from fastapi import FastAPI, UploadFile, Form, File\nfrom hivision import IDCreator\nfrom hivision.error import FaceError\nfrom hivision.creator.layout_calculator import (\n    generate_layout_array,\n    generate_layout_image,\n)\nfrom hivision.creator.choose_handler import choose_handler\nfrom hivision.utils import (\n    add_background,\n    resize_image_to_kb,\n    bytes_2_base64,\n    base64_2_numpy,\n    hex_to_rgb,\n    add_watermark,\n    save_image_dpi_to_bytes,\n)\nimport numpy as np\nimport cv2\nfrom starlette.middleware.cors import CORSMiddleware\nfrom starlette.formparsers import MultiPartParser\n\n# 设置Starlette表单字段大小限制\nMultiPartParser.max_part_size = 10 * 1024 * 1024  # 10MB\n# 设置Starlette文件上传大小限制\nMultiPartParser.max_file_size = 20 * 1024 * 1024   # 20MB\n\napp = FastAPI()\ncreator = IDCreator()\n\n# 添加 CORS 中间件 解决跨域问题\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],  # 允许的请求来源\n    allow_credentials=True,  # 允许携带 Cookie\n    allow_methods=[\n        \"*\"\n    ],  # 允许的请求方法，例如：GET, POST 等，也可以指定 [\"GET\", \"POST\"]\n    allow_headers=[\"*\"],  # 允许的请求头，也可以指定具体的头部\n)\n\n\n# 证件照智能制作接口\n@app.post(\"/idphoto\")\nasync def idphoto_inference(\n    input_image: UploadFile = File(None),\n    input_image_base64: str = Form(None),\n    height: int = Form(413),\n    width: int = Form(295),\n    human_matting_model: str = Form(\"modnet_photographic_portrait_matting\"),\n    face_detect_model: str = Form(\"mtcnn\"),\n    hd: bool = Form(True),\n    dpi: int = Form(300),\n    face_align: bool = Form(False),\n    whitening_strength: int = Form(0),\n    head_measure_ratio: float = Form(0.2),\n    head_height_ratio: float = Form(0.45),\n    top_distance_max: float = Form(0.12),\n    top_distance_min: float = Form(0.10),\n    brightness_strength: float = Form(0),\n    contrast_strength: float = Form(0),\n    sharpen_strength: float = Form(0),\n    saturation_strength: float = Form(0),\n):  \n    # 如果传入了base64，则直接使用base64解码\n    if input_image_base64:\n        img = base64_2_numpy(input_image_base64)\n    # 否则使用上传的图片\n    else:\n        image_bytes = await input_image.read()\n        nparr = np.frombuffer(image_bytes, np.uint8)\n        img = cv2.imdecode(nparr, cv2.IMREAD_COLOR)\n        # 将BGR转换为RGB\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n    # ------------------- 选择抠图与人脸检测模型 -------------------\n    choose_handler(creator, human_matting_model, face_detect_model)\n\n    # 将字符串转为元组\n    size = (int(height), int(width))\n    try:\n        result = creator(\n            img,\n            size=size,\n            head_measure_ratio=head_measure_ratio,\n            head_height_ratio=head_height_ratio,\n            head_top_range=(top_distance_max, top_distance_min),\n            face_alignment=face_align,\n            whitening_strength=whitening_strength,\n            brightness_strength=brightness_strength,\n            contrast_strength=contrast_strength,\n            sharpen_strength=sharpen_strength,\n            saturation_strength=saturation_strength,\n        )\n    except FaceError:\n        result_message = {\"status\": False}\n    # 如果检测到人脸数量等于1, 则返回标准证和高清照结果（png 4通道图像）\n    else:\n        result_image_standard_bytes = save_image_dpi_to_bytes(result.standard, None, dpi)\n        \n        result_message = {\n            \"status\": True,\n            \"image_base64_standard\": bytes_2_base64(result_image_standard_bytes),\n        }\n\n        # 如果hd为True, 则增加高清照结果（png 4通道图像）\n        if hd:\n            result_image_hd_bytes = save_image_dpi_to_bytes(result.hd, None, dpi)\n            result_message[\"image_base64_hd\"] = bytes_2_base64(result_image_hd_bytes)\n\n    return result_message\n\n\n# 人像抠图接口\n@app.post(\"/human_matting\")\nasync def human_matting_inference(\n    input_image: UploadFile = File(None),\n    input_image_base64: str = Form(None),\n    human_matting_model: str = Form(\"hivision_modnet\"),\n    dpi: int = Form(300),\n):\n    if input_image_base64:\n        img = base64_2_numpy(input_image_base64)\n    else:\n        image_bytes = await input_image.read()\n        nparr = np.frombuffer(image_bytes, np.uint8)\n        img = cv2.imdecode(nparr, cv2.IMREAD_COLOR)\n\n    # ------------------- 选择抠图与人脸检测模型 -------------------\n    choose_handler(creator, human_matting_model, None)\n\n    try:\n        result = creator(\n            img,\n            change_bg_only=True,\n        )\n    except FaceError:\n        result_message = {\"status\": False}\n\n    else:\n        result_image_standard_bytes = save_image_dpi_to_bytes(cv2.cvtColor(result.standard, cv2.COLOR_RGBA2BGRA), None, dpi)\n        result_message = {\n            \"status\": True,\n            \"image_base64\": bytes_2_base64(result_image_standard_bytes),\n        }\n    return result_message\n\n\n# 透明图像添加纯色背景接口\n@app.post(\"/add_background\")\nasync def photo_add_background(\n    input_image: UploadFile = File(None),\n    input_image_base64: str = Form(None),\n    color: str = Form(\"000000\"),\n    kb: int = Form(None),\n    dpi: int = Form(300),\n    render: int = Form(0),\n):\n    render_choice = [\"pure_color\", \"updown_gradient\", \"center_gradient\"]\n\n    if input_image_base64:\n        img = base64_2_numpy(input_image_base64)\n    else:\n        image_bytes = await input_image.read()\n        nparr = np.frombuffer(image_bytes, np.uint8)\n        img = cv2.imdecode(nparr, cv2.IMREAD_UNCHANGED)\n\n    color = hex_to_rgb(color)\n    color = (color[2], color[1], color[0])\n\n    result_image = add_background(\n        img,\n        bgr=color,\n        mode=render_choice[render],\n    ).astype(np.uint8)\n\n    result_image = cv2.cvtColor(result_image, cv2.COLOR_RGB2BGR)\n    if kb:\n        result_image_bytes = resize_image_to_kb(result_image, None, int(kb), dpi=dpi)\n    else:\n        result_image_bytes = save_image_dpi_to_bytes(result_image, None, dpi=dpi)\n\n    result_messgae = {\n        \"status\": True,\n        \"image_base64\": bytes_2_base64(result_image_bytes),\n    }\n\n    return result_messgae\n\n\n# 六寸排版照生成接口\n@app.post(\"/generate_layout_photos\")\nasync def generate_layout_photos(\n    input_image: UploadFile = File(None),\n    input_image_base64: str = Form(None),\n    height: int = Form(413),\n    width: int = Form(295),\n    kb: int = Form(None),\n    dpi: int = Form(300),\n):\n    # try:\n    if input_image_base64:\n        img = base64_2_numpy(input_image_base64)\n    else:\n        image_bytes = await input_image.read()\n        nparr = np.frombuffer(image_bytes, np.uint8)\n        img = cv2.imdecode(nparr, cv2.IMREAD_COLOR)\n\n    size = (int(height), int(width))\n\n    typography_arr, typography_rotate = generate_layout_array(\n        input_height=size[0], input_width=size[1]\n    )\n\n    result_layout_image = generate_layout_image(\n        img, typography_arr, typography_rotate, height=size[0], width=size[1]\n    ).astype(np.uint8)\n\n    result_layout_image = cv2.cvtColor(result_layout_image, cv2.COLOR_RGB2BGR)\n    if kb:\n        result_layout_image_bytes = resize_image_to_kb(\n            result_layout_image, None, int(kb), dpi=dpi\n        )\n    else:\n        result_layout_image_bytes = save_image_dpi_to_bytes(result_layout_image, None, dpi=dpi)\n        \n    result_layout_image_base64 = bytes_2_base64(result_layout_image_bytes)\n\n    result_messgae = {\n        \"status\": True,\n        \"image_base64\": result_layout_image_base64,\n    }\n\n    return result_messgae\n\n\n# 透明图像添加水印接口\n@app.post(\"/watermark\")\nasync def watermark(\n    input_image: UploadFile = File(None),\n    input_image_base64: str = Form(None),\n    text: str = Form(\"Hello\"),\n    size: int = 20,\n    opacity: float = 0.5,\n    angle: int = 30,\n    color: str = \"#000000\",\n    space: int = 25,\n    kb: int = Form(None),\n    dpi: int = Form(300),\n):\n    if input_image_base64:\n        img = base64_2_numpy(input_image_base64)\n    else:\n        image_bytes = await input_image.read()\n        nparr = np.frombuffer(image_bytes, np.uint8)\n        img = cv2.imdecode(nparr, cv2.IMREAD_COLOR)\n\n    try:\n        result_image = add_watermark(img, text, size, opacity, angle, color, space)\n\n        result_image = cv2.cvtColor(result_image, cv2.COLOR_RGB2BGR)\n        if kb:\n            result_image_bytes = resize_image_to_kb(result_image, None, int(kb), dpi=dpi)\n        else:\n            result_image_bytes = save_image_dpi_to_bytes(result_image, None, dpi=dpi)\n        result_image_base64 = bytes_2_base64(result_image_bytes)\n\n        result_messgae = {\n            \"status\": True,\n            \"image_base64\": result_image_base64,\n        }\n    except Exception as e:\n        result_messgae = {\n            \"status\": False,\n            \"error\": str(e),\n        }\n\n    return result_messgae\n\n\n# 设置照片KB值接口(RGB图)\n@app.post(\"/set_kb\")\nasync def set_kb(\n    input_image: UploadFile = File(None),\n    input_image_base64: str = Form(None),\n    dpi: int = Form(300),\n    kb: int = Form(50),\n):\n    if input_image_base64:\n        img = base64_2_numpy(input_image_base64)\n    else:\n        image_bytes = await input_image.read()\n        nparr = np.frombuffer(image_bytes, np.uint8)\n        img = cv2.imdecode(nparr, cv2.IMREAD_COLOR)\n\n    try:\n        result_image = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n        result_image_bytes = resize_image_to_kb(result_image, None, int(kb), dpi=dpi)\n        result_image_base64 = bytes_2_base64(result_image_bytes)\n\n        result_messgae = {\n            \"status\": True,\n            \"image_base64\": result_image_base64,\n        }\n    except Exception as e:\n        result_messgae = {\n            \"status\": False,\n            \"error\": e,\n        }\n\n    return result_messgae\n\n\n# 证件照智能裁剪接口\n@app.post(\"/idphoto_crop\")\nasync def idphoto_crop_inference(\n    input_image: UploadFile = File(None),\n    input_image_base64: str = Form(None),\n    height: int = Form(413),\n    width: int = Form(295),\n    face_detect_model: str = Form(\"mtcnn\"),\n    hd: bool = Form(True),\n    dpi: int = Form(300),\n    head_measure_ratio: float = Form(0.2),\n    head_height_ratio: float = Form(0.45),\n    top_distance_max: float = Form(0.12),\n    top_distance_min: float = Form(0.10),\n):\n    if input_image_base64:\n        img = base64_2_numpy(input_image_base64)\n    else:\n        image_bytes = await input_image.read()\n        nparr = np.frombuffer(image_bytes, np.uint8)\n        img = cv2.imdecode(nparr, cv2.IMREAD_UNCHANGED)  # 读取图像(4通道)\n\n    # ------------------- 选择抠图与人脸检测模型 -------------------\n    choose_handler(creator, face_detect_option=face_detect_model)\n\n    # 将字符串转为元组\n    size = (int(height), int(width))\n    try:\n        result = creator(\n            img,\n            size=size,\n            head_measure_ratio=head_measure_ratio,\n            head_height_ratio=head_height_ratio,\n            head_top_range=(top_distance_max, top_distance_min),\n            crop_only=True,\n        )\n    except FaceError:\n        result_message = {\"status\": False}\n    # 如果检测到人脸数量等于1, 则返回标准证和高清照结果（png 4通道图像）\n    else:\n        result_image_standard_bytes = save_image_dpi_to_bytes(cv2.cvtColor(result.standard, cv2.COLOR_RGBA2BGRA), None, dpi)\n        \n        result_message = {\n            \"status\": True,\n            \"image_base64_standard\": bytes_2_base64(result_image_standard_bytes),\n        }\n\n        # 如果hd为True, 则增加高清照结果（png 4通道图像）\n        if hd:\n            result_image_hd_bytes = save_image_dpi_to_bytes(cv2.cvtColor(result.hd, cv2.COLOR_RGBA2BGRA), None, dpi)\n            result_message[\"image_base64_hd\"] = bytes_2_base64(result_image_hd_bytes)\n\n    return result_message\n\n\nif __name__ == \"__main__\":\n    import uvicorn\n\n    # 在8080端口运行推理服务\n    uvicorn.run(app, host=\"0.0.0.0\", port=8080)\n", "metadata": {"source": "github.com/Zeyi-Lin/HivisionIDPhotos", "file_path": "deploy_api.py", "language": "python", "complexity": "complex", "star_count": 18571, "quality_score": 0.92, "features": {"has_classes": false, "has_functions": true, "has_async": true, "has_decorators": true, "has_imports": true, "has_error_handling": true, "has_documentation": false, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 7, "class_count": 0, "line_count": 374}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create REST API endpoint with proper request/response handling\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\nr\"\"\"\n@DATE: 2024/9/5 16:45\n@File: __init__.py\n@IDE: pycharm\n@Description:\n    创建证件照\n\"\"\"\nimport numpy as np\nfrom typing import Tuple\nimport hivision.creator.utils as U\nfrom .context import Context, ContextHandler, Params, Result\nfrom .human_matting import extract_human\nfrom .face_detector import detect_face_mtcnn\nfrom hivision.plugin.beauty.handler import beauty_face\nfrom .photo_adjuster import adjust_photo\nimport cv2\nimport time\n\n\nclass IDCreator:\n    \"\"\"\n    证件照创建类，包含完整的证件照流程\n    \"\"\"\n\n    def __init__(self):\n        # 回调时机\n        self.before_all: ContextHandler = None\n        \"\"\"\n        在所有处理之前，此时图像已经被 resize 到最大边长为 2000\n        \"\"\"\n        self.after_matting: ContextHandler = None\n        \"\"\"\n        在抠图之后，ctx.matting_image 被赋值\n        \"\"\"\n        self.after_detect: ContextHandler = None\n        \"\"\"\n        在人脸检测之后，ctx.face 被赋值，如果为仅换底，则不会执行此回调\n        \"\"\"\n        self.after_all: ContextHandler = None\n        \"\"\"\n        在所有处理之后，此时 ctx.result 被赋值\n        \"\"\"\n        # 处理者\n        self.matting_handler: ContextHandler = extract_human\n        self.detection_handler: ContextHandler = detect_face_mtcnn\n        self.beauty_handler: ContextHandler = beauty_face\n        # 上下文\n        self.ctx = None\n\n    def __call__(\n        self,\n        image: np.ndarray,\n        size: Tuple[int, int] = (413, 295),\n        change_bg_only: bool = False,\n        crop_only: bool = False,\n        head_measure_ratio: float = 0.2,\n        head_height_ratio: float = 0.45,\n        head_top_range: float = (0.12, 0.1),\n        face: Tuple[int, int, int, int] = None,\n        whitening_strength: int = 0,\n        brightness_strength: int = 0,\n        contrast_strength: int = 0,\n        sharpen_strength: int = 0,\n        saturation_strength: int = 0,\n        face_alignment: bool = False,\n        horizontal_flip: bool = False,\n    ) -> Result:\n        \"\"\"\n        证件照处理函数\n        :param image: 输入图像\n        :param change_bg_only: 是否只需要抠图\n        :param crop_only: 是否只需要裁剪\n        :param size: 输出的图像大小（h,w)\n        :param head_measure_ratio: 人脸面积与全图面积的期望比值\n        :param head_height_ratio: 人脸中心处在全图高度的比例期望值\n        :param head_top_range: 头距离顶部的比例（max,min)\n        :param face: 人脸坐标\n        :param whitening_strength: 美白强度\n        :param brightness_strength: 亮度强度\n        :param contrast_strength: 对比度强度\n        :param sharpen_strength: 锐化强度\n        :param face_alignment: 是否需要人脸矫正\n        :param horizontal_flip: 是否需要水平翻转\n\n        :return: 返回处理后的证件照和一系列参数\n        \"\"\"\n        # 0.初始化上下文\n        params = Params(\n            size=size,\n            change_bg_only=change_bg_only,\n            head_measure_ratio=head_measure_ratio,\n            head_height_ratio=head_height_ratio,\n            head_top_range=head_top_range,\n            crop_only=crop_only,\n            face=face,\n            whitening_strength=whitening_strength,\n            brightness_strength=brightness_strength,\n            contrast_strength=contrast_strength,\n            sharpen_strength=sharpen_strength,\n            saturation_strength=saturation_strength,\n            face_alignment=face_alignment,\n            horizontal_flip=horizontal_flip,\n        )\n\n\n        # 总的开始时间\n        total_start_time = time.time()\n        \n        self.ctx = Context(params)\n        ctx = self.ctx\n        ctx.processing_image = image\n        ctx.processing_image = U.resize_image_esp(\n            ctx.processing_image, 2000\n        )  # 将输入图片 resize 到最大边长为 2000\n        ctx.origin_image = ctx.processing_image.copy()\n        self.before_all and self.before_all(ctx)\n\n        # 1. ------------------人像抠图------------------\n        # 如果仅裁剪，则不进行抠图\n        if not ctx.params.crop_only:\n            # 调用抠图工作流\n            print(\"[1]  Start Human Matting...\")\n            start_matting_time = time.time()\n            self.matting_handler(ctx)\n            end_matting_time = time.time()\n            print(f\"[1]  Human Matting Time: {end_matting_time - start_matting_time:.3f}s\")\n            self.after_matting and self.after_matting(ctx)\n        # 如果进行抠图\n        else:\n            ctx.matting_image = ctx.processing_image\n\n\n        # 2. ------------------美颜------------------\n        print(\"[2]  Start Beauty...\")\n        start_beauty_time = time.time()\n        self.beauty_handler(ctx)\n        end_beauty_time = time.time()\n        print(f\"[2]  Beauty Time: {end_beauty_time - start_beauty_time:.3f}s\")\n\n        # 如果仅换底，则直接返回抠图结果\n        if ctx.params.change_bg_only:\n            ctx.result = Result(\n                standard=ctx.matting_image,\n                hd=ctx.matting_image,\n                matting=ctx.matting_image,\n                clothing_params=None,\n                typography_params=None,\n                face=None,\n            )\n            self.after_all and self.after_all(ctx)\n            return ctx.result\n\n        # 3. ------------------人脸检测------------------\n        print(\"[3]  Start Face Detection...\")\n        start_detection_time = time.time()\n        self.detection_handler(ctx)\n        end_detection_time = time.time()\n        print(f\"[3]  Face Detection Time: {end_detection_time - start_detection_time:.3f}s\")\n        self.after_detect and self.after_detect(ctx)\n\n        # 3.1 ------------------人脸对齐------------------\n        if ctx.params.face_alignment and abs(ctx.face[\"roll_angle\"]) > 2:\n            print(\"[3.1]  Start Face Alignment...\")\n            start_alignment_time = time.time()\n            from hivision.creator.rotation_adjust import rotate_bound_4channels\n\n            # 根据角度旋转原图和抠图\n            b, g, r, a = cv2.split(ctx.matting_image)\n            ctx.origin_image, ctx.matting_image, _, _, _, _ = rotate_bound_4channels(\n                cv2.merge((b, g, r)),\n                a,\n                -1 * ctx.face[\"roll_angle\"],\n            )\n\n            # 旋转后再执行一遍人脸检测\n            self.detection_handler(ctx)\n            self.after_detect and self.after_detect(ctx)\n            end_alignment_time = time.time()\n            print(f\"[3.1]  Face Alignment Time: {end_alignment_time - start_alignment_time:.3f}s\")\n\n        # 4. ------------------图像调整------------------\n        print(\"[4]  Start Image Post-Adjustment...\")\n        start_adjust_time = time.time()\n        result_image_hd, result_image_standard, clothing_params, typography_params = (\n            adjust_photo(ctx)\n        )\n        end_adjust_time = time.time()\n        print(f\"[4]  Image Post-Adjustment Time: {end_adjust_time - start_adjust_time:.3f}s\")\n\n        # 5. ------------------返回结果------------------\n        ctx.result = Result(\n            standard=result_image_standard,\n            hd=result_image_hd,\n            matting=ctx.matting_image,\n            clothing_params=clothing_params,\n            typography_params=typography_params,\n            face=ctx.face,\n        )\n        self.after_all and self.after_all(ctx)\n\n        # 总的结束时间\n        total_end_time = time.time()\n        print(f\"[Total]  Total Time: {total_end_time - total_start_time:.3f}s\")\n\n        return ctx.result\n", "metadata": {"source": "github.com/Zeyi-Lin/HivisionIDPhotos", "file_path": "hivision/creator/__init__.py", "language": "python", "complexity": "complex", "star_count": 18571, "quality_score": 0.9500000000000001, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": true, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 2, "class_count": 1, "line_count": 208}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create REST API endpoint with proper request/response handling\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from hivision.creator.human_matting import *\nfrom hivision.creator.face_detector import *\n\n\nHUMAN_MATTING_MODELS = [\n    \"modnet_photographic_portrait_matting\",\n    \"birefnet-v1-lite\",\n    \"hivision_modnet\",\n    \"rmbg-1.4\",\n]\n\nFACE_DETECT_MODELS = [\"face++ (联网Online API)\", \"mtcnn\", \"retinaface-resnet50\"]\n\n\ndef choose_handler(creator, matting_model_option=None, face_detect_option=None):\n    if matting_model_option == \"modnet_photographic_portrait_matting\":\n        creator.matting_handler = extract_human_modnet_photographic_portrait_matting\n    elif matting_model_option == \"mnn_hivision_modnet\":\n        creator.matting_handler = extract_human_mnn_modnet\n    elif matting_model_option == \"rmbg-1.4\":\n        creator.matting_handler = extract_human_rmbg\n    elif matting_model_option == \"birefnet-v1-lite\":\n        creator.matting_handler = extract_human_birefnet_lite\n    else:\n        creator.matting_handler = extract_human\n\n    if (\n        face_detect_option == \"face_plusplus\"\n        or face_detect_option == \"face++ (联网Online API)\"\n    ):\n        creator.detection_handler = detect_face_face_plusplus\n    elif face_detect_option == \"retinaface-resnet50\":\n        creator.detection_handler = detect_face_retinaface\n    else:\n        creator.detection_handler = detect_face_mtcnn\n", "metadata": {"source": "github.com/Zeyi-Lin/HivisionIDPhotos", "file_path": "hivision/creator/choose_handler.py", "language": "python", "complexity": "simple", "star_count": 18571, "quality_score": 0.89, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": false, "has_testing": false, "has_logging": false, "function_count": 1, "class_count": 0, "line_count": 36}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create REST API endpoint with proper request/response handling\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\nr\"\"\"\n@DATE: 2024/9/5 19:20\n@File: context.py\n@IDE: pycharm\n@Description:\n    证件照创建上下文类，用于同步信息\n\"\"\"\nfrom typing import Optional, Callable, Tuple\nimport numpy as np\n\n\nclass Params:\n    def __init__(\n        self,\n        size: Tuple[int, int] = (413, 295),\n        change_bg_only: bool = False,\n        crop_only: bool = False,\n        head_measure_ratio: float = 0.2,\n        head_height_ratio: float = 0.45,\n        head_top_range: float = (0.12, 0.1),\n        face: Tuple[int, int, int, int] = None,\n        whitening_strength: int = 0,\n        brightness_strength: int = 0,\n        contrast_strength: int = 0,\n        sharpen_strength: int = 0,\n        saturation_strength: int = 0,\n        face_alignment: bool = False,\n        horizontal_flip: bool = False,\n    ):\n        self.__size = size\n        self.__change_bg_only = change_bg_only\n        self.__crop_only = crop_only\n        self.__head_measure_ratio = head_measure_ratio\n        self.__head_height_ratio = head_height_ratio\n        self.__head_top_range = head_top_range\n        self.__face = face\n        self.__whitening_strength = whitening_strength\n        self.__brightness_strength = brightness_strength\n        self.__contrast_strength = contrast_strength\n        self.__sharpen_strength = sharpen_strength\n        self.__saturation_strength = saturation_strength\n        self.__face_alignment = face_alignment\n        self.__horizontal_flip = horizontal_flip\n    @property\n    def size(self):\n        return self.__size\n\n    @property\n    def change_bg_only(self):\n        return self.__change_bg_only\n\n    @property\n    def head_measure_ratio(self):\n        return self.__head_measure_ratio\n\n    @property\n    def head_height_ratio(self):\n        return self.__head_height_ratio\n\n    @property\n    def head_top_range(self):\n        return self.__head_top_range\n\n    @property\n    def crop_only(self):\n        return self.__crop_only\n\n    @property\n    def face(self):\n        return self.__face\n\n    @property\n    def whitening_strength(self):\n        return self.__whitening_strength\n\n    @property\n    def brightness_strength(self):\n        return self.__brightness_strength\n\n    @property\n    def contrast_strength(self):\n        return self.__contrast_strength\n\n    @property\n    def sharpen_strength(self):\n        return self.__sharpen_strength\n\n    @property\n    def saturation_strength(self):\n        return self.__saturation_strength\n\n    @property\n    def face_alignment(self):\n        return self.__face_alignment\n\n    @property\n    def horizontal_flip(self):\n        return self.__horizontal_flip\n\n\nclass Result:\n    def __init__(\n        self,\n        standard: np.ndarray,\n        hd: np.ndarray,\n        matting: np.ndarray,\n        clothing_params: Optional[dict],\n        typography_params: Optional[dict],\n        face: Optional[Tuple[int, int, int, int, float]],\n    ):\n        self.standard = standard\n        self.hd = hd\n        self.matting = matting\n        self.clothing_params = clothing_params\n        \"\"\"\n        服装参数，仅换底时为 None\n        \"\"\"\n        self.typography_params = typography_params\n        \"\"\"\n        排版参数，仅换底时为 None\n        \"\"\"\n        self.face = face\n\n    def __iter__(self):\n        return iter(\n            [\n                self.standard,\n                self.hd,\n                self.matting,\n                self.clothing_params,\n                self.typography_params,\n                self.face,\n            ]\n        )\n\n\nclass Context:\n    def __init__(self, params: Params):\n        self.params: Params = params\n        \"\"\"\n        证件照处理参数\n        \"\"\"\n        self.origin_image: Optional[np.ndarray] = None\n        \"\"\"\n        输入的原始图像，处理时会进行resize，长宽不一定等于输入图像\n        \"\"\"\n        self.processing_image: Optional[np.ndarray] = None\n        \"\"\"\n        当前正在处理的图像\n        \"\"\"\n        self.matting_image: Optional[np.ndarray] = None\n        \"\"\"\n        人像抠图结果\n        \"\"\"\n        self.face: dict = dict(rectangle=None, roll_angle=None)\n        \"\"\"\n        人脸检测结果，大于一个人脸时已在上层抛出异常\n        rectangle: 人脸矩形框，包含 x1, y1, width, height 的坐标, x1, y1 为左上角坐标, width, height 为矩形框的宽度和高度\n        roll_angle: 人脸偏转角度，以眼睛为标准，计算的人脸偏转角度，用于人脸矫正\n        \"\"\"\n        self.result: Optional[Result] = None\n        \"\"\"\n        证件照处理结果\n        \"\"\"\n        self.align_info: Optional[dict] = None\n        \"\"\"\n        人脸矫正信息，仅当 align_face 为 True 时存在\n        \"\"\"\n\n\nContextHandler = Optional[Callable[[Context], None]]\n", "metadata": {"source": "github.com/Zeyi-Lin/HivisionIDPhotos", "file_path": "hivision/creator/context.py", "language": "python", "complexity": "complex", "star_count": 18571, "quality_score": 0.9500000000000001, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": true, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 18, "class_count": 3, "line_count": 174}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create REST API endpoint with proper request/response handling\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\nr\"\"\"\n@DATE: 2024/9/5 19:32\n@File: face_detector.py\n@IDE: pycharm\n@Description:\n    人脸检测器\n\"\"\"\ntry:\n    from mtcnnruntime import MTCNN\nexcept ImportError:\n    raise ImportError(\n        \"Please install mtcnn-runtime by running `pip install mtcnn-runtime`\"\n    )\nfrom .context import Context\nfrom hivision.error import FaceError, APIError\nfrom hivision.utils import resize_image_to_kb_base64\nfrom hivision.creator.retinaface import retinaface_detect_faces\nimport requests\nimport cv2\nimport os\nimport numpy as np\n\n\nmtcnn = None\nbase_dir = os.path.dirname(os.path.abspath(__file__))\nRETINAFCE_SESS = None\n\n\ndef detect_face_mtcnn(ctx: Context, scale: int = 2):\n    \"\"\"\n    基于MTCNN模型的人脸检测处理器，只进行人脸数量的检测\n    :param ctx: 上下文，此时已获取到原始图和抠图结果，但是我们只需要原始图\n    :param scale: 最大边长缩放比例，原图:缩放图 = 1:scale\n    :raise FaceError: 人脸检测错误，多个人脸或者没有人脸\n    \"\"\"\n    global mtcnn\n    if mtcnn is None:\n        mtcnn = MTCNN()\n    image = cv2.resize(\n        ctx.origin_image,\n        (ctx.origin_image.shape[1] // scale, ctx.origin_image.shape[0] // scale),\n        interpolation=cv2.INTER_AREA,\n    )\n    # landmarks 是 5 个关键点，分别是左眼、右眼、鼻子、左嘴角、右嘴角，\n    faces, landmarks = mtcnn.detect(image, thresholds=[0.8, 0.8, 0.8])\n\n    # print(len(faces))\n    if len(faces) != 1:\n        # 保险措施，如果检测到多个人脸或者没有人脸，用原图再检测一次\n        faces, landmarks = mtcnn.detect(ctx.origin_image)\n    else:\n        # 如果只有一个人脸，将人脸坐标放大\n        for item, param in enumerate(faces[0]):\n            faces[0][item] = param * 2\n    if len(faces) != 1:\n        raise FaceError(\"Expected 1 face, but got {}\".format(len(faces)), len(faces))\n\n    # 计算人脸坐标\n    left = faces[0][0]\n    top = faces[0][1]\n    width = faces[0][2] - left + 1\n    height = faces[0][3] - top + 1\n    ctx.face[\"rectangle\"] = (left, top, width, height)\n\n    # 根据landmarks计算人脸偏转角度，以眼睛为标准，计算的人脸偏转角度，用于人脸矫正\n    # 示例landmarks [106.37181  150.77415  127.21012  108.369156 144.61522  105.24723 107.45625  133.62355  151.24269  153.34407 ]\n    landmarks = landmarks[0]\n    left_eye = np.array([landmarks[0], landmarks[5]])\n    right_eye = np.array([landmarks[1], landmarks[6]])\n    dy = right_eye[1] - left_eye[1]\n    dx = right_eye[0] - left_eye[0]\n    roll_angle = np.degrees(np.arctan2(dy, dx))\n\n    ctx.face[\"roll_angle\"] = roll_angle\n\n\ndef detect_face_face_plusplus(ctx: Context):\n    \"\"\"\n    基于Face++ API接口的人脸检测处理器，只进行人脸数量的检测\n    :param ctx: 上下文，此时已获取到原始图和抠图结果，但是我们只需要原始图\n    :param scale: 最大边长缩放比例，原图:缩放图 = 1:scale\n    :raise FaceError: 人脸检测错误，多个人脸或者没有人脸\n    \"\"\"\n    url = \"https://api-cn.faceplusplus.com/facepp/v3/detect\"\n    api_key = os.getenv(\"FACE_PLUS_API_KEY\")\n    api_secret = os.getenv(\"FACE_PLUS_API_SECRET\")\n\n    print(\"调用了face++\")\n\n    image = ctx.origin_image\n    # 将图片转为 base64, 且不大于2MB（Face++ API接口限制）\n    image_base64 = resize_image_to_kb_base64(image, 2000, mode=\"max\")\n\n    files = {\n        \"api_key\": (None, api_key),\n        \"api_secret\": (None, api_secret),\n        \"image_base64\": (None, image_base64),\n        \"return_landmark\": (None, \"1\"),\n        \"return_attributes\": (None, \"headpose\"),\n    }\n\n    # 发送 POST 请求\n    response = requests.post(url, files=files)\n\n    # 获取响应状态码\n    status_code = response.status_code\n    response_json = response.json()\n\n    if status_code == 200:\n        face_num = response_json[\"face_num\"]\n        if face_num == 1:\n            face_rectangle = response_json[\"faces\"][0][\"face_rectangle\"]\n\n            # 获取人脸关键点\n            # landmarks = response_json[\"faces\"][0][\"landmark\"]\n            # print(\"face++ landmarks\", landmarks)\n\n            # headpose 是一个字典，包含俯仰角（pitch）、偏航角（yaw）和滚转角（roll）\n            # headpose示例 {'pitch_angle': 6.997899, 'roll_angle': 1.8011835, 'yaw_angle': 5.043002}\n            headpose = response_json[\"faces\"][0][\"attributes\"][\"headpose\"]\n            # 以眼睛为标准，计算的人脸偏转角度，用于人脸矫正\n            roll_angle = headpose[\"roll_angle\"] / 2\n\n            ctx.face[\"rectangle\"] = (\n                face_rectangle[\"left\"],\n                face_rectangle[\"top\"],\n                face_rectangle[\"width\"],\n                face_rectangle[\"height\"],\n            )\n            ctx.face[\"roll_angle\"] = roll_angle\n        else:\n            raise FaceError(\n                \"Expected 1 face, but got {}\".format(face_num), len(face_num)\n            )\n\n    elif status_code == 401:\n        raise APIError(\n            f\"Face++ Status code {status_code} Authentication error: API key and secret do not match.\",\n            status_code,\n        )\n\n    elif status_code == 403:\n        reason = response_json.get(\"error_message\", \"Unknown authorization error.\")\n        raise APIError(\n            f\"Authorization error: {reason}\",\n            status_code,\n        )\n\n    elif status_code == 400:\n        error_message = response_json.get(\"error_message\", \"Bad request.\")\n        raise APIError(\n            f\"Bad request error: {error_message}\",\n            status_code,\n        )\n\n    elif status_code == 413:\n        raise APIError(\n            f\"Face++ Status code {status_code} Request entity too large: The image exceeds the 2MB limit.\",\n            status_code,\n        )\n\n\ndef detect_face_retinaface(ctx: Context):\n    \"\"\"\n    基于RetinaFace模型的人脸检测处理器，只进行人脸数量的检测\n    :param ctx: 上下文，此时已获取到原始图和抠图结果，但是我们只需要原始图\n    :raise FaceError: 人脸检测错误，多个人脸或者没有人脸\n    \"\"\"\n    from time import time\n\n    global RETINAFCE_SESS\n\n    if RETINAFCE_SESS is None:\n        # 计算用时\n        tic = time()\n        faces_dets, sess = retinaface_detect_faces(\n            ctx.origin_image,\n            os.path.join(base_dir, \"retinaface/weights/retinaface-resnet50.onnx\"),\n            sess=None,\n        )\n        RETINAFCE_SESS = sess\n    else:\n        tic = time()\n        faces_dets, _ = retinaface_detect_faces(\n            ctx.origin_image,\n            os.path.join(base_dir, \"retinaface/weights/retinaface-resnet50.onnx\"),\n            sess=RETINAFCE_SESS,\n        )\n\n    faces_num = len(faces_dets)\n    faces_landmarks = []\n    for face_det in faces_dets:\n        faces_landmarks.append(face_det[5:])\n\n    if faces_num != 1:\n        raise FaceError(\"Expected 1 face, but got {}\".format(faces_num), faces_num)\n    face_det = faces_dets[0]\n    ctx.face[\"rectangle\"] = (\n        face_det[0],\n        face_det[1],\n        face_det[2] - face_det[0] + 1,\n        face_det[3] - face_det[1] + 1,\n    )\n\n    # 计算roll_angle\n    face_landmarks = faces_landmarks[0]\n    # print(\"face_landmarks\", face_landmarks)\n    left_eye = np.array([face_landmarks[0], face_landmarks[1]])\n    right_eye = np.array([face_landmarks[2], face_landmarks[3]])\n    dy = right_eye[1] - left_eye[1]\n    dx = right_eye[0] - left_eye[0]\n    roll_angle = np.degrees(np.arctan2(dy, dx))\n    ctx.face[\"roll_angle\"] = roll_angle\n\n    # 如果RUN_MODE不是野兽模式，则释放模型\n    if os.getenv(\"RUN_MODE\") == \"beast\":\n        RETINAFCE_SESS = None", "metadata": {"source": "github.com/Zeyi-Lin/HivisionIDPhotos", "file_path": "hivision/creator/face_detector.py", "language": "python", "complexity": "complex", "star_count": 18571, "quality_score": 0.9500000000000001, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": true, "has_documentation": true, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 3, "class_count": 0, "line_count": 219}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create REST API endpoint with proper request/response handling\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\nr\"\"\"\n@DATE: 2024/9/5 21:21\n@File: human_matting.py\n@IDE: pycharm\n@Description:\n    人像抠图\n\"\"\"\nimport numpy as np\nfrom PIL import Image\nimport onnxruntime\nfrom .tensor2numpy import NNormalize, NTo_Tensor, NUnsqueeze\nfrom .context import Context\nimport cv2\nimport os\nfrom time import time\n\n\nWEIGHTS = {\n    \"hivision_modnet\": os.path.join(\n        os.path.dirname(__file__), \"weights\", \"hivision_modnet.onnx\"\n    ),\n    \"modnet_photographic_portrait_matting\": os.path.join(\n        os.path.dirname(__file__),\n        \"weights\",\n        \"modnet_photographic_portrait_matting.onnx\",\n    ),\n    \"mnn_hivision_modnet\": os.path.join(\n        os.path.dirname(__file__),\n        \"weights\",\n        \"mnn_hivision_modnet.mnn\",\n    ),\n    \"rmbg-1.4\": os.path.join(os.path.dirname(__file__), \"weights\", \"rmbg-1.4.onnx\"),\n    \"birefnet-v1-lite\": os.path.join(\n        os.path.dirname(__file__), \"weights\", \"birefnet-v1-lite.onnx\"\n    ),\n}\n\nONNX_DEVICE = onnxruntime.get_device()\nONNX_PROVIDER = (\n    \"CUDAExecutionProvider\" if ONNX_DEVICE == \"GPU\" else \"CPUExecutionProvider\"\n)\n\nHIVISION_MODNET_SESS = None\nMODNET_PHOTOGRAPHIC_PORTRAIT_MATTING_SESS = None\nRMBG_SESS = None\nBIREFNET_V1_LITE_SESS = None\n\n\ndef load_onnx_model(checkpoint_path, set_cpu=False):\n    providers = (\n        [\"CUDAExecutionProvider\", \"CPUExecutionProvider\"]\n        if ONNX_PROVIDER == \"CUDAExecutionProvider\"\n        else [\"CPUExecutionProvider\"]\n    )\n\n    if set_cpu:\n        sess = onnxruntime.InferenceSession(\n            checkpoint_path, providers=[\"CPUExecutionProvider\"]\n        )\n    else:\n        try:\n            sess = onnxruntime.InferenceSession(checkpoint_path, providers=providers)\n        except Exception as e:\n            if ONNX_DEVICE == \"CUDAExecutionProvider\":\n                print(f\"Failed to load model with CUDAExecutionProvider: {e}\")\n                print(\"Falling back to CPUExecutionProvider\")\n                # 尝试使用CPU加载模型\n                sess = onnxruntime.InferenceSession(\n                    checkpoint_path, providers=[\"CPUExecutionProvider\"]\n                )\n            else:\n                raise e  # 如果是CPU执行失败，重新抛出异常\n\n    return sess\n\n\ndef extract_human(ctx: Context):\n    \"\"\"\n    人像抠图\n    :param ctx: 上下文\n    \"\"\"\n    # 抠图\n    matting_image = get_modnet_matting(ctx.processing_image, WEIGHTS[\"hivision_modnet\"])\n    # 修复抠图\n    ctx.processing_image = hollow_out_fix(matting_image)\n    ctx.matting_image = ctx.processing_image.copy()\n\n\ndef extract_human_modnet_photographic_portrait_matting(ctx: Context):\n    \"\"\"\n    人像抠图\n    :param ctx: 上下文\n    \"\"\"\n    # 抠图\n    matting_image = get_modnet_matting_photographic_portrait_matting(\n        ctx.processing_image, WEIGHTS[\"modnet_photographic_portrait_matting\"]\n    )\n    # 修复抠图\n    ctx.processing_image = matting_image\n    ctx.matting_image = ctx.processing_image.copy()\n\n\ndef extract_human_mnn_modnet(ctx: Context):\n    matting_image = get_mnn_modnet_matting(\n        ctx.processing_image, WEIGHTS[\"mnn_hivision_modnet\"]\n    )\n    ctx.processing_image = hollow_out_fix(matting_image)\n    ctx.matting_image = ctx.processing_image.copy()\n\n\ndef extract_human_rmbg(ctx: Context):\n    matting_image = get_rmbg_matting(ctx.processing_image, WEIGHTS[\"rmbg-1.4\"])\n    ctx.processing_image = matting_image\n    ctx.matting_image = ctx.processing_image.copy()\n\n\n# def extract_human_birefnet_portrait(ctx: Context):\n#     matting_image = get_birefnet_portrait_matting(\n#         ctx.processing_image, WEIGHTS[\"birefnet-portrait\"]\n#     )\n#     ctx.processing_image = matting_image\n#     ctx.matting_image = ctx.processing_image.copy()\n\n\ndef extract_human_birefnet_lite(ctx: Context):\n    matting_image = get_birefnet_portrait_matting(\n        ctx.processing_image, WEIGHTS[\"birefnet-v1-lite\"]\n    )\n    ctx.processing_image = matting_image\n    ctx.matting_image = ctx.processing_image.copy()\n\n\ndef hollow_out_fix(src: np.ndarray) -> np.ndarray:\n    \"\"\"\n    修补抠图区域，作为抠图模型精度不够的补充\n    :param src:\n    :return:\n    \"\"\"\n    b, g, r, a = cv2.split(src)\n    src_bgr = cv2.merge((b, g, r))\n    # -----------padding---------- #\n    add_area = np.zeros((10, a.shape[1]), np.uint8)\n    a = np.vstack((add_area, a, add_area))\n    add_area = np.zeros((a.shape[0], 10), np.uint8)\n    a = np.hstack((add_area, a, add_area))\n    # -------------end------------ #\n    _, a_threshold = cv2.threshold(a, 127, 255, 0)\n    a_erode = cv2.erode(\n        a_threshold,\n        kernel=cv2.getStructuringElement(cv2.MORPH_RECT, (5, 5)),\n        iterations=3,\n    )\n    contours, hierarchy = cv2.findContours(\n        a_erode, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE\n    )\n    contours = [x for x in contours]\n    # contours = np.squeeze(contours)\n    contours.sort(key=lambda c: cv2.contourArea(c), reverse=True)\n    a_contour = cv2.drawContours(np.zeros(a.shape, np.uint8), contours[0], -1, 255, 2)\n    # a_base = a_contour[1:-1, 1:-1]\n    h, w = a.shape[:2]\n    mask = np.zeros(\n        [h + 2, w + 2], np.uint8\n    )  # mask 必须行和列都加 2，且必须为 uint8 单通道阵列\n    cv2.floodFill(a_contour, mask=mask, seedPoint=(0, 0), newVal=255)\n    a = cv2.add(a, 255 - a_contour)\n    return cv2.merge((src_bgr, a[10:-10, 10:-10]))\n\n\ndef image2bgr(input_image):\n    if len(input_image.shape) == 2:\n        input_image = input_image[:, :, None]\n    if input_image.shape[2] == 1:\n        result_image = np.repeat(input_image, 3, axis=2)\n    elif input_image.shape[2] == 4:\n        result_image = input_image[:, :, 0:3]\n    else:\n        result_image = input_image\n\n    return result_image\n\n\ndef read_modnet_image(input_image, ref_size=512):\n    im = Image.fromarray(np.uint8(input_image))\n    width, length = im.size[0], im.size[1]\n    im = np.asarray(im)\n    im = image2bgr(im)\n    im = cv2.resize(im, (ref_size, ref_size), interpolation=cv2.INTER_AREA)\n    im = NNormalize(im, mean=np.array([0.5, 0.5, 0.5]), std=np.array([0.5, 0.5, 0.5]))\n    im = NUnsqueeze(NTo_Tensor(im))\n\n    return im, width, length\n\n\ndef get_modnet_matting(input_image, checkpoint_path, ref_size=512):\n    global HIVISION_MODNET_SESS\n\n    if not os.path.exists(checkpoint_path):\n        print(f\"Checkpoint file not found: {checkpoint_path}\")\n        return None\n\n    # 如果RUN_MODE不是野兽模式，则不加载模型\n    if HIVISION_MODNET_SESS is None:\n        HIVISION_MODNET_SESS = load_onnx_model(checkpoint_path, set_cpu=True)\n\n    input_name = HIVISION_MODNET_SESS.get_inputs()[0].name\n    output_name = HIVISION_MODNET_SESS.get_outputs()[0].name\n\n    im, width, length = read_modnet_image(input_image=input_image, ref_size=ref_size)\n\n    matte = HIVISION_MODNET_SESS.run([output_name], {input_name: im})\n    matte = (matte[0] * 255).astype(\"uint8\")\n    matte = np.squeeze(matte)\n    mask = cv2.resize(matte, (width, length), interpolation=cv2.INTER_AREA)\n    b, g, r = cv2.split(np.uint8(input_image))\n\n    output_image = cv2.merge((b, g, r, mask))\n    \n    # 如果RUN_MODE不是野兽模式，则释放模型\n    if os.getenv(\"RUN_MODE\") != \"beast\":\n        HIVISION_MODNET_SESS = None\n\n    return output_image\n\n\ndef get_modnet_matting_photographic_portrait_matting(\n    input_image, checkpoint_path, ref_size=512\n):\n    global MODNET_PHOTOGRAPHIC_PORTRAIT_MATTING_SESS\n\n    if not os.path.exists(checkpoint_path):\n        print(f\"Checkpoint file not found: {checkpoint_path}\")\n        return None\n\n    # 如果RUN_MODE不是野兽模式，则不加载模型\n    if MODNET_PHOTOGRAPHIC_PORTRAIT_MATTING_SESS is None:\n        MODNET_PHOTOGRAPHIC_PORTRAIT_MATTING_SESS = load_onnx_model(\n            checkpoint_path, set_cpu=True\n        )\n\n    input_name = MODNET_PHOTOGRAPHIC_PORTRAIT_MATTING_SESS.get_inputs()[0].name\n    output_name = MODNET_PHOTOGRAPHIC_PORTRAIT_MATTING_SESS.get_outputs()[0].name\n\n    im, width, length = read_modnet_image(input_image=input_image, ref_size=ref_size)\n\n    matte = MODNET_PHOTOGRAPHIC_PORTRAIT_MATTING_SESS.run(\n        [output_name], {input_name: im}\n    )\n    matte = (matte[0] * 255).astype(\"uint8\")\n    matte = np.squeeze(matte)\n    mask = cv2.resize(matte, (width, length), interpolation=cv2.INTER_AREA)\n    b, g, r = cv2.split(np.uint8(input_image))\n\n    output_image = cv2.merge((b, g, r, mask))\n    \n    # 如果RUN_MODE不是野兽模式，则释放模型\n    if os.getenv(\"RUN_MODE\") != \"beast\":\n        MODNET_PHOTOGRAPHIC_PORTRAIT_MATTING_SESS = None\n\n    return output_image\n\n\ndef get_rmbg_matting(input_image: np.ndarray, checkpoint_path, ref_size=1024):\n    global RMBG_SESS\n\n    if not os.path.exists(checkpoint_path):\n        print(f\"Checkpoint file not found: {checkpoint_path}\")\n        return None\n\n    def resize_rmbg_image(image):\n        image = image.convert(\"RGB\")\n        model_input_size = (ref_size, ref_size)\n        image = image.resize(model_input_size, Image.BILINEAR)\n        return image\n\n    if RMBG_SESS is None:\n        RMBG_SESS = load_onnx_model(checkpoint_path, set_cpu=True)\n\n    orig_image = Image.fromarray(input_image)\n    image = resize_rmbg_image(orig_image)\n    im_np = np.array(image).astype(np.float32)\n    im_np = im_np.transpose(2, 0, 1)  # Change to CxHxW format\n    im_np = np.expand_dims(im_np, axis=0)  # Add batch dimension\n    im_np = im_np / 255.0  # Normalize to [0, 1]\n    im_np = (im_np - 0.5) / 0.5  # Normalize to [-1, 1]\n\n    # Inference\n    result = RMBG_SESS.run(None, {RMBG_SESS.get_inputs()[0].name: im_np})[0]\n\n    # Post process\n    result = np.squeeze(result)\n    ma = np.max(result)\n    mi = np.min(result)\n    result = (result - mi) / (ma - mi)  # Normalize to [0, 1]\n\n    # Convert to PIL image\n    im_array = (result * 255).astype(np.uint8)\n    pil_im = Image.fromarray(\n        im_array, mode=\"L\"\n    )  # Ensure mask is single channel (L mode)\n\n    # Resize the mask to match the original image size\n    pil_im = pil_im.resize(orig_image.size, Image.BILINEAR)\n\n    # Paste the mask on the original image\n    new_im = Image.new(\"RGBA\", orig_image.size, (0, 0, 0, 0))\n    new_im.paste(orig_image, mask=pil_im)\n    \n    # 如果RUN_MODE不是野兽模式，则释放模型\n    if os.getenv(\"RUN_MODE\") != \"beast\":\n        RMBG_SESS = None\n\n    return np.array(new_im)\n\n\ndef get_mnn_modnet_matting(input_image, checkpoint_path, ref_size=512):\n    if not os.path.exists(checkpoint_path):\n        print(f\"Checkpoint file not found: {checkpoint_path}\")\n        return None\n\n    try:\n        import MNN.expr as expr\n        import MNN.nn as nn\n    except ImportError as e:\n        raise ImportError(\n            \"The MNN module is not installed or there was an import error. Please ensure that the MNN library is installed by using the command 'pip install mnn'.\"\n        ) from e\n\n    config = {}\n    config[\"precision\"] = \"low\"  # 当硬件支持（armv8.2）时使用fp16推理\n    config[\"backend\"] = 0  # CPU\n    config[\"numThread\"] = 4  # 线程数\n    im, width, length = read_modnet_image(input_image, ref_size=512)\n    rt = nn.create_runtime_manager((config,))\n    net = nn.load_module_from_file(\n        checkpoint_path, [\"input1\"], [\"output1\"], runtime_manager=rt\n    )\n    input_var = expr.convert(im, expr.NCHW)\n    output_var = net.forward(input_var)\n    matte = expr.convert(output_var, expr.NCHW)\n    matte = matte.read()  # var转换为np\n    matte = (matte * 255).astype(\"uint8\")\n    matte = np.squeeze(matte)\n    mask = cv2.resize(matte, (width, length), interpolation=cv2.INTER_AREA)\n    b, g, r = cv2.split(np.uint8(input_image))\n\n    output_image = cv2.merge((b, g, r, mask))\n\n    return output_image\n\n\ndef get_birefnet_portrait_matting(input_image, checkpoint_path, ref_size=512):\n    global BIREFNET_V1_LITE_SESS\n\n    if not os.path.exists(checkpoint_path):\n        print(f\"Checkpoint file not found: {checkpoint_path}\")\n        return None\n\n    def transform_image(image):\n        image = image.resize((1024, 1024))  # Resize to 1024x1024\n        image = (\n            np.array(image, dtype=np.float32) / 255.0\n        )  # Convert to numpy array and normalize to [0, 1]\n        image = (image - [0.485, 0.456, 0.406]) / [0.229, 0.224, 0.225]  # Normalize\n        image = np.transpose(image, (2, 0, 1))  # Change from (H, W, C) to (C, H, W)\n        image = np.expand_dims(image, axis=0)  # Add batch dimension\n        return image.astype(np.float32)  # Ensure the output is float32\n\n    orig_image = Image.fromarray(input_image)\n    input_images = transform_image(\n        orig_image\n    )  # This will already have the correct shape\n\n    # 记录加载onnx模型的开始时间\n    load_start_time = time()\n\n    # 如果RUN_MODE不是野兽模式，则不加载模型\n    if BIREFNET_V1_LITE_SESS is None:\n        # print(\"首次加载birefnet-v1-lite模型...\")\n        if ONNX_DEVICE == \"GPU\":\n            print(\"onnxruntime-gpu已安装，尝试使用CUDA加载模型\")\n            try:\n                import torch\n            except ImportError:\n                print(\n                    \"torch未安装，尝试直接使用onnxruntime-gpu加载模型，这需要配置好CUDA和cuDNN\"\n                )\n            BIREFNET_V1_LITE_SESS = load_onnx_model(checkpoint_path)\n        else:\n            BIREFNET_V1_LITE_SESS = load_onnx_model(checkpoint_path, set_cpu=True)\n\n    # 记录加载onnx模型的结束时间\n    load_end_time = time()\n\n    # 打印加载onnx模型所花的时间\n    print(f\"Loading ONNX model took {load_end_time - load_start_time:.4f} seconds\")\n\n    input_name = BIREFNET_V1_LITE_SESS.get_inputs()[0].name\n    print(onnxruntime.get_device(), BIREFNET_V1_LITE_SESS.get_providers())\n\n    time_st = time()\n    pred_onnx = BIREFNET_V1_LITE_SESS.run(None, {input_name: input_images})[\n        -1\n    ]  # Use float32 input\n    pred_onnx = np.squeeze(pred_onnx)  # Use numpy to squeeze\n    result = 1 / (1 + np.exp(-pred_onnx))  # Sigmoid function using numpy\n    print(f\"Inference time: {time() - time_st:.4f} seconds\")\n\n    # Convert to PIL image\n    im_array = (result * 255).astype(np.uint8)\n    pil_im = Image.fromarray(\n        im_array, mode=\"L\"\n    )  # Ensure mask is single channel (L mode)\n\n    # Resize the mask to match the original image size\n    pil_im = pil_im.resize(orig_image.size, Image.BILINEAR)\n\n    # Paste the mask on the original image\n    new_im = Image.new(\"RGBA\", orig_image.size, (0, 0, 0, 0))\n    new_im.paste(orig_image, mask=pil_im)\n    \n    # 如果RUN_MODE不是野兽模式，则释放模型\n    if os.getenv(\"RUN_MODE\") != \"beast\":\n        BIREFNET_V1_LITE_SESS = None\n\n    return np.array(new_im)\n", "metadata": {"source": "github.com/Zeyi-Lin/HivisionIDPhotos", "file_path": "hivision/creator/human_matting.py", "language": "python", "complexity": "complex", "star_count": 18571, "quality_score": 0.98, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": true, "has_documentation": true, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 17, "class_count": 0, "line_count": 429}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create REST API endpoint with proper request/response handling\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\nr\"\"\"\n@DATE: 2024/9/5 21:35\n@File: layout_calculator.py\n@IDE: pycharm\n@Description:\n    布局计算器\n\"\"\"\n\nimport cv2.detail\nimport numpy as np\n\n\ndef judge_layout(\n    input_width,\n    input_height,\n    PHOTO_INTERVAL_W,\n    PHOTO_INTERVAL_H,\n    LIMIT_BLOCK_W,\n    LIMIT_BLOCK_H,\n):\n    centerBlockHeight_1, centerBlockWidth_1 = (\n        input_height,\n        input_width,\n    )  # 由证件照们组成的一个中心区块（1 代表不转置排列）\n    centerBlockHeight_2, centerBlockWidth_2 = (\n        input_width,\n        input_height,\n    )  # 由证件照们组成的一个中心区块（2 代表转置排列）\n\n    # 1.不转置排列的情况下：\n    layout_col_no_transpose = 0  # 行\n    layout_row_no_transpose = 0  # 列\n    for i in range(1, 4):\n        centerBlockHeight_temp = input_height * i + PHOTO_INTERVAL_H * (i - 1)\n        if centerBlockHeight_temp < LIMIT_BLOCK_H:\n            centerBlockHeight_1 = centerBlockHeight_temp\n            layout_row_no_transpose = i\n        else:\n            break\n    for j in range(1, 9):\n        centerBlockWidth_temp = input_width * j + PHOTO_INTERVAL_W * (j - 1)\n        if centerBlockWidth_temp < LIMIT_BLOCK_W:\n            centerBlockWidth_1 = centerBlockWidth_temp\n            layout_col_no_transpose = j\n        else:\n            break\n    layout_number_no_transpose = layout_row_no_transpose * layout_col_no_transpose\n\n    # 2.转置排列的情况下：\n    layout_col_transpose = 0  # 行\n    layout_row_transpose = 0  # 列\n    for i in range(1, 4):\n        centerBlockHeight_temp = input_width * i + PHOTO_INTERVAL_H * (i - 1)\n        if centerBlockHeight_temp < LIMIT_BLOCK_H:\n            centerBlockHeight_2 = centerBlockHeight_temp\n            layout_row_transpose = i\n        else:\n            break\n    for j in range(1, 9):\n        centerBlockWidth_temp = input_height * j + PHOTO_INTERVAL_W * (j - 1)\n        if centerBlockWidth_temp < LIMIT_BLOCK_W:\n            centerBlockWidth_2 = centerBlockWidth_temp\n            layout_col_transpose = j\n        else:\n            break\n    layout_number_transpose = layout_row_transpose * layout_col_transpose\n\n    if layout_number_transpose > layout_number_no_transpose:\n        layout_mode = (layout_col_transpose, layout_row_transpose, 2)\n        return layout_mode, centerBlockWidth_2, centerBlockHeight_2\n    else:\n        layout_mode = (layout_col_no_transpose, layout_row_no_transpose, 1)\n        return layout_mode, centerBlockWidth_1, centerBlockHeight_1\n\n\ndef generate_layout_array(input_height, input_width, LAYOUT_WIDTH=1795, LAYOUT_HEIGHT=1205):\n    # 1.基础参数表\n    PHOTO_INTERVAL_H = 30  # 证件照与证件照之间的垂直距离\n    PHOTO_INTERVAL_W = 30  # 证件照与证件照之间的水平距离\n    SIDES_INTERVAL_H = 50  # 证件照与画布边缘的垂直距离\n    SIDES_INTERVAL_W = 70  # 证件照与画布边缘的水平距离\n    LIMIT_BLOCK_W = LAYOUT_WIDTH - 2 * SIDES_INTERVAL_W\n    LIMIT_BLOCK_H = LAYOUT_HEIGHT - 2 * SIDES_INTERVAL_H\n\n    # 2.创建一个 1180x1746 的空白画布\n    white_background = np.zeros([LAYOUT_HEIGHT, LAYOUT_WIDTH, 3], np.uint8)\n    white_background.fill(255)\n\n    # 3.计算照片的 layout（列、行、横竖朝向）,证件照组成的中心区块的分辨率\n    layout_mode, centerBlockWidth, centerBlockHeight = judge_layout(\n        input_width,\n        input_height,\n        PHOTO_INTERVAL_W,\n        PHOTO_INTERVAL_H,\n        LIMIT_BLOCK_W,\n        LIMIT_BLOCK_H,\n    )\n    # 4.开始排列组合\n    x11 = (LAYOUT_WIDTH - centerBlockWidth) // 2\n    y11 = (LAYOUT_HEIGHT - centerBlockHeight) // 2\n    typography_arr = []\n    typography_rotate = False\n    if layout_mode[2] == 2:\n        input_height, input_width = input_width, input_height\n        typography_rotate = True\n\n    for j in range(layout_mode[1]):\n        for i in range(layout_mode[0]):\n            xi = x11 + i * input_width + i * PHOTO_INTERVAL_W\n            yi = y11 + j * input_height + j * PHOTO_INTERVAL_H\n            typography_arr.append([xi, yi])\n\n    return typography_arr, typography_rotate\n\n\ndef generate_layout_image(\n    input_image, typography_arr, typography_rotate, width=295, height=413, \n    crop_line:bool=False,\n    LAYOUT_WIDTH=1795, \n    LAYOUT_HEIGHT=1205,\n):\n  \n    # 创建一个白色背景的空白画布\n    white_background = np.zeros([LAYOUT_HEIGHT, LAYOUT_WIDTH, 3], np.uint8)\n    white_background.fill(255)\n    \n    # 如果输入图像的高度不等于指定高度，则调整图像大小\n    if input_image.shape[0] != height:\n        input_image = cv2.resize(input_image, (width, height))\n    \n    # 如果需要旋转排版，则对图像进行转置和垂直镜像\n    if typography_rotate:\n        input_image = cv2.transpose(input_image)\n        input_image = cv2.flip(input_image, 0)  # 0 表示垂直镜像\n\n        # 交换高度和宽度\n        height, width = width, height\n    \n    # 将图像按照排版数组中的位置放置到白色背景上\n    for arr in typography_arr:\n        locate_x, locate_y = arr[0], arr[1]\n        white_background[locate_y : locate_y + height, locate_x : locate_x + width] = (\n            input_image\n        )\n\n    if crop_line:\n        # 添加裁剪线\n        line_color = (200, 200, 200)  # 浅灰色\n        line_thickness = 1\n\n        # 初始化裁剪线位置列表\n        vertical_lines = []\n        horizontal_lines = []\n\n        # 根据排版数组添加裁剪线\n        for arr in typography_arr:\n            x, y = arr[0], arr[1]\n            if x not in vertical_lines:\n                vertical_lines.append(x)\n            if x + width not in vertical_lines:\n                vertical_lines.append(x + width)\n            if y not in horizontal_lines:\n                horizontal_lines.append(y)\n            if y + height not in horizontal_lines:\n                horizontal_lines.append(y + height)\n\n        # 绘制垂直裁剪线\n        for x in vertical_lines:\n            cv2.line(white_background, (x, 0), (x, LAYOUT_HEIGHT), line_color, line_thickness)\n\n        # 绘制水平裁剪线\n        for y in horizontal_lines:\n            cv2.line(white_background, (0, y), (LAYOUT_WIDTH, y), line_color, line_thickness)\n\n    # 返回排版后的图像\n    return white_background\n", "metadata": {"source": "github.com/Zeyi-Lin/HivisionIDPhotos", "file_path": "hivision/creator/layout_calculator.py", "language": "python", "complexity": "complex", "star_count": 18571, "quality_score": 0.89, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": true, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 3, "class_count": 0, "line_count": 179}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create REST API endpoint with proper request/response handling\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"\n有一些 png 图像下部也会有一些透明的区域，使得图像无法对其底部边框\n本程序实现移动图像，使其下部与 png 图像实际大小相对齐\n\"\"\"\nimport os\nimport cv2\nimport numpy as np\nfrom hivisionai.hycv.utils import get_box_pro\n\npath_pre = os.path.join(os.getcwd(), 'pre')\npath_final = os.path.join(os.getcwd(), 'final')\n\n\ndef merge(boxes):\n    \"\"\"\n    生成的边框可能不止只有一个，需要将边框合并\n    \"\"\"\n    x, y, h, w = boxes[0]\n    # x 和 y 应该是整个 boxes 里面最小的值\n    if len(boxes) > 1:\n        for tmp in boxes:\n            x_tmp, y_tmp, h_tmp, w_tmp = tmp\n            if x > x_tmp:\n                x_max = x_tmp + w_tmp if x_tmp + w_tmp > x + w else x + w\n                x = x_tmp\n                w = x_max - x\n            if y > y_tmp:\n                y_max = y_tmp + h_tmp if y_tmp + h_tmp > y + h else y + h\n                y = y_tmp\n                h = y_max - y\n    return tuple((x, y, h, w))\n\n\ndef get_box(png_img):\n    \"\"\"\n    获取矩形边框最终返回一个元组 (x,y,h,w)，分别对应矩形左上角的坐标和矩形的高和宽\n    \"\"\"\n    r,  g,  b , a = cv2.split(png_img)\n    gray_img = a\n    th, binary = cv2.threshold(gray_img, 127 , 255, cv2.THRESH_BINARY)  # 二值化\n    # cv2.imshow(\"name\", binary)\n    # cv2.waitKey(0)\n    contours, hierarchy = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)  # 得到轮廓列表 contours\n    bounding_boxes = merge([cv2.boundingRect(cnt) for cnt in contours])  # 轮廓合并\n    # print(bounding_boxes)\n    return bounding_boxes\n\n\ndef get_box_2(png_img):\n    \"\"\"\n    不用 opencv 内置算法生成矩形了，改用自己的算法（for 循环）\n    \"\"\"\n    _, _, _, a = cv2.split(png_img)\n    _, a = cv2.threshold(a, 127, 255, cv2.THRESH_BINARY)\n    # 将 r，g，b 通道丢弃，只留下透明度通道\n    # cv2.imshow(\"name\", a)\n    # cv2.waitKey(0)\n    # 在透明度矩阵中，0 代表完全透明\n    height,width=a.shape  # 高和宽\n    f=0\n    tmp1 = 0\n\n    \"\"\"\n    获取上下\n    \"\"\"\n    for tmp1 in range(0,height):\n        tmp_a_high= a[tmp1:tmp1+1,:][0]\n        for tmp2 in range(width):\n            # a = tmp_a_low[tmp2]\n            if tmp_a_high[tmp2]!=0:\n                f=1\n        if f == 1:\n            break\n    delta_y_high = tmp1 + 1\n    f = 0\n    for tmp1 in range(height,-1, -1):\n        tmp_a_low= a[tmp1-1:tmp1+1,:][0]\n        for tmp2 in range(width):\n            # a = tmp_a_low[tmp2]\n            if tmp_a_low[tmp2]!=0:\n                f=1\n        if f == 1:\n            break\n    delta_y_bottom = height - tmp1 + 3\n    \"\"\"\n    获取左右\n    \"\"\"\n    f = 0\n    for tmp1 in range(width):\n        tmp_a_left = a[:, tmp1:tmp1+1]\n        for tmp2 in range(height):\n            if tmp_a_left[tmp2] != 0:\n                f = 1\n        if f==1:\n            break\n    delta_x_left = tmp1 + 1\n    f = 0\n    for tmp1 in range(width, -1, -1):\n        tmp_a_left = a[:, tmp1-1:tmp1]\n        for tmp2 in range(height):\n            if tmp_a_left[tmp2] != 0:\n                f = 1\n        if f==1:\n            break\n    delta_x_right = width - tmp1 + 1\n    return  delta_y_high, delta_y_bottom, delta_x_left, delta_x_right\n\n\ndef move(input_image):\n    \"\"\"\n    裁剪主函数，输入一张 png 图像，该图像周围是透明的\n    \"\"\"\n    png_img = input_image  # 获取图像\n\n    height, width, channels = png_img.shape  # 高 y、宽 x\n    y_low,y_high, _, _ = get_box_pro(png_img, model=2)  # for 循环\n    base = np.zeros((y_high, width, channels),dtype=np.uint8)  # for 循环\n    png_img = png_img[0:height - y_high, :, :]  # for 循环\n    png_img = np.concatenate((base, png_img), axis=0)\n    return png_img, y_high\n\n\ndef main():\n    if not os.path.exists(path_pre):\n        os.makedirs(path_pre)\n    if not os.path.exists(path_final):\n        os.makedirs(path_final)\n    for name in os.listdir(path_pre):\n        pass\n        # move(name)\n\n\nif __name__ == \"__main__\":\n    main()\n", "metadata": {"source": "github.com/Zeyi-Lin/HivisionIDPhotos", "file_path": "hivision/creator/move_image.py", "language": "python", "complexity": "medium", "star_count": 18571, "quality_score": 0.92, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": true, "has_type_hints": false, "has_testing": false, "has_logging": false, "function_count": 5, "class_count": 0, "line_count": 135}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create REST API endpoint with proper request/response handling\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\nr\"\"\"\n@DATE: 2024/9/5 20:02\n@File: photo_adjuster.py\n@IDE: pycharm\n@Description:\n    证件照调整\n\"\"\"\nfrom .context import Context\nfrom .layout_calculator import generate_layout_array\nimport hivision.creator.utils as U\nimport numpy as np\nimport math\nimport cv2\n\n\ndef adjust_photo(ctx: Context):\n    # Step1. 准备人脸参数\n    face_rect = ctx.face[\"rectangle\"]\n    standard_size = ctx.params.size\n    params = ctx.params\n    x, y = face_rect[0], face_rect[1]\n    w, h = face_rect[2], face_rect[3]\n    height, width = ctx.matting_image.shape[:2]\n    width_height_ratio = standard_size[0] / standard_size[1]\n    # Step2. 计算高级参数\n    face_center = (x + w / 2, y + h / 2)  # 面部中心坐标\n    face_measure = w * h  # 面部面积\n    crop_measure = (\n        face_measure / params.head_measure_ratio\n    )  # 裁剪框面积：为面部面积的 5 倍\n    resize_ratio = crop_measure / (standard_size[0] * standard_size[1])  # 裁剪框缩放率\n    resize_ratio_single = math.sqrt(\n        resize_ratio\n    )  # 长和宽的缩放率（resize_ratio 的开方）\n    crop_size = (\n        int(standard_size[0] * resize_ratio_single),\n        int(standard_size[1] * resize_ratio_single),\n    )  # 裁剪框大小\n\n    # 裁剪框的定位信息\n    x1 = int(face_center[0] - crop_size[1] / 2)\n    y1 = int(face_center[1] - crop_size[0] * params.head_height_ratio)\n    y2 = y1 + crop_size[0]\n    x2 = x1 + crop_size[1]\n\n    # Step3, 裁剪框的调整\n    cut_image = IDphotos_cut(x1, y1, x2, y2, ctx.matting_image)\n    cut_image = cv2.resize(cut_image, (crop_size[1], crop_size[0]))\n    y_top, y_bottom, x_left, x_right = U.get_box(\n        cut_image.astype(np.uint8), model=2, correction_factor=0\n    )  # 得到 cut_image 中人像的上下左右距离信息\n\n    # Step5. 判定 cut_image 中的人像是否处于合理的位置，若不合理，则处理数据以便之后调整位置\n    # 检测人像与裁剪框左边或右边是否存在空隙\n    if x_left > 0 or x_right > 0:\n        status_left_right = 1\n        cut_value_top = int(\n            ((x_left + x_right) * width_height_ratio) / 2\n        )  # 减去左右，为了保持比例，上下也要相应减少 cut_value_top\n    else:\n        status_left_right = 0\n        cut_value_top = 0\n\n    \"\"\"\n        检测人头顶与照片的顶部是否在合适的距离内：\n        - status==0: 距离合适，无需移动\n        - status=1: 距离过大，人像应向上移动\n        - status=2: 距离过小，人像应向下移动\n    \"\"\"\n    status_top, move_value = U.detect_distance(\n        y_top - cut_value_top,\n        crop_size[0],\n        max=params.head_top_range[0],\n        min=params.head_top_range[1],\n    )\n\n    # Step6. 对照片的第二轮裁剪\n    if status_left_right == 0 and status_top == 0:\n        result_image = cut_image\n    else:\n        result_image = IDphotos_cut(\n            x1 + x_left,\n            y1 + cut_value_top + status_top * move_value,\n            x2 - x_right,\n            y2 - cut_value_top + status_top * move_value,\n            ctx.matting_image,\n        )\n\n    # 换装参数准备\n    relative_x = x - (x1 + x_left)\n    relative_y = y - (y1 + cut_value_top + status_top * move_value)\n\n    # Step7. 当照片底部存在空隙时，下拉至底部\n    result_image, y_high = move(result_image.astype(np.uint8))\n    relative_y = relative_y + y_high  # 更新换装参数\n    \n    # Step7.1 水平翻转\n    if params.horizontal_flip:\n        result_image = cv2.flip(result_image, 1)\n\n    # Step8. 标准照与高清照转换\n    result_image_standard = standard_photo_resize(result_image, standard_size)\n    result_image_hd, resize_ratio_max = resize_image_by_min(\n        result_image, esp=max(600, standard_size[1])\n    )\n\n    # Step9. 参数准备 - 为换装服务\n    clothing_params = {\n        \"relative_x\": relative_x * resize_ratio_max,\n        \"relative_y\": relative_y * resize_ratio_max,\n        \"w\": w * resize_ratio_max,\n        \"h\": h * resize_ratio_max,\n    }\n\n    # Step7. 排版照参数获取\n    typography_arr, typography_rotate = generate_layout_array(\n        input_height=standard_size[0], input_width=standard_size[1]\n    )\n\n    return (\n        result_image_hd,\n        result_image_standard,\n        clothing_params,\n        {\n            \"arr\": typography_arr,\n            \"rotate\": typography_rotate,\n        },\n    )\n\n\ndef IDphotos_cut(x1, y1, x2, y2, img):\n    \"\"\"\n    在图片上进行滑动裁剪，输入输出为\n    输入：一张图片 img，和裁剪框信息 (x1,x2,y1,y2)\n    输出：裁剪好的图片，然后裁剪框超出了图像范围，那么将用 0 矩阵补位\n    ------------------------------------\n    x:裁剪框左上的横坐标\n    y:裁剪框左上的纵坐标\n    x2:裁剪框右下的横坐标\n    y2:裁剪框右下的纵坐标\n    crop_size:裁剪框大小\n    img:裁剪图像（numpy.array）\n    output_path:裁剪图片的输出路径\n    ------------------------------------\n    \"\"\"\n\n    crop_size = (y2 - y1, x2 - x1)\n    \"\"\"\n    ------------------------------------\n    temp_x_1:裁剪框左边超出图像部分\n    temp_y_1:裁剪框上边超出图像部分\n    temp_x_2:裁剪框右边超出图像部分\n    temp_y_2:裁剪框下边超出图像部分\n    ------------------------------------\n    \"\"\"\n    temp_x_1 = 0\n    temp_y_1 = 0\n    temp_x_2 = 0\n    temp_y_2 = 0\n\n    if y1 < 0:\n        temp_y_1 = abs(y1)\n        y1 = 0\n    if y2 > img.shape[0]:\n        temp_y_2 = y2\n        y2 = img.shape[0]\n        temp_y_2 = temp_y_2 - y2\n\n    if x1 < 0:\n        temp_x_1 = abs(x1)\n        x1 = 0\n    if x2 > img.shape[1]:\n        temp_x_2 = x2\n        x2 = img.shape[1]\n        temp_x_2 = temp_x_2 - x2\n\n    # 生成一张全透明背景\n    background_bgr = np.full((crop_size[0], crop_size[1]), 255, dtype=np.uint8)\n    background_a = np.full((crop_size[0], crop_size[1]), 0, dtype=np.uint8)\n    background = cv2.merge(\n        (background_bgr, background_bgr, background_bgr, background_a)\n    )\n\n    background[\n        temp_y_1 : crop_size[0] - temp_y_2, temp_x_1 : crop_size[1] - temp_x_2\n    ] = img[y1:y2, x1:x2]\n\n    return background\n\n\ndef move(input_image):\n    \"\"\"\n    裁剪主函数，输入一张 png 图像，该图像周围是透明的\n    \"\"\"\n    png_img = input_image  # 获取图像\n\n    height, width, channels = png_img.shape  # 高 y、宽 x\n    y_low, y_high, _, _ = U.get_box(png_img, model=2)  # for 循环\n    base = np.zeros((y_high, width, channels), dtype=np.uint8)  # for 循环\n    png_img = png_img[0 : height - y_high, :, :]  # for 循环\n    png_img = np.concatenate((base, png_img), axis=0)\n    return png_img, y_high\n\n\ndef standard_photo_resize(input_image: np.array, size):\n    \"\"\"\n    input_image: 输入图像，即高清照\n    size: 标准照的尺寸\n    \"\"\"\n    resize_ratio = input_image.shape[0] / size[0]\n    resize_item = int(round(input_image.shape[0] / size[0]))\n    if resize_ratio >= 2:\n        for i in range(resize_item - 1):\n            if i == 0:\n                result_image = cv2.resize(\n                    input_image,\n                    (size[1] * (resize_item - i - 1), size[0] * (resize_item - i - 1)),\n                    interpolation=cv2.INTER_AREA,\n                )\n            else:\n                result_image = cv2.resize(\n                    result_image,\n                    (size[1] * (resize_item - i - 1), size[0] * (resize_item - i - 1)),\n                    interpolation=cv2.INTER_AREA,\n                )\n    else:\n        result_image = cv2.resize(\n            input_image, (size[1], size[0]), interpolation=cv2.INTER_AREA\n        )\n\n    return result_image\n\n\ndef resize_image_by_min(input_image, esp=600):\n    \"\"\"\n    将图像缩放为最短边至少为 esp 的图像。\n    :param input_image: 输入图像（OpenCV 矩阵）\n    :param esp: 缩放后的最短边长\n    :return: 缩放后的图像，缩放倍率\n    \"\"\"\n    height, width = input_image.shape[0], input_image.shape[1]\n    min_border = min(height, width)\n    if min_border < esp:\n        if height >= width:\n            new_width = esp\n            new_height = height * esp // width\n        else:\n            new_height = esp\n            new_width = width * esp // height\n\n        return (\n            cv2.resize(\n                input_image, (new_width, new_height), interpolation=cv2.INTER_AREA\n            ),\n            new_height / height,\n        )\n\n    else:\n        return input_image, 1\n", "metadata": {"source": "github.com/Zeyi-Lin/HivisionIDPhotos", "file_path": "hivision/creator/photo_adjuster.py", "language": "python", "complexity": "complex", "star_count": 18571, "quality_score": 0.92, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": true, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 5, "class_count": 0, "line_count": 262}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create REST API endpoint with proper request/response handling\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import numpy as np\n\n\ndef decode(loc, priors, variances):\n    \"\"\"Decode locations from predictions using priors to undo\n    the encoding we did for offset regression at train time.\n    Args:\n        loc (tensor): location predictions for loc layers,\n            Shape: [num_priors,4]\n        priors (tensor): Prior boxes in center-offset form.\n            Shape: [num_priors,4].\n        variances: (list[float]) Variances of priorboxes\n    Return:\n        decoded bounding box predictions\n    \"\"\"\n\n    boxes = None\n\n    boxes = np.concatenate(\n        (\n            priors[:, :2] + loc[:, :2] * variances[0] * priors[:, 2:],\n            priors[:, 2:] * np.exp(loc[:, 2:] * variances[1]),\n        ),\n        axis=1,\n    )\n\n    boxes[:, :2] -= boxes[:, 2:] / 2\n    boxes[:, 2:] += boxes[:, :2]\n    return boxes\n\n\ndef decode_landm(pre, priors, variances):\n    \"\"\"Decode landm from predictions using priors to undo\n    the encoding we did for offset regression at train time.\n    Args:\n        pre (tensor): landm predictions for loc layers,\n            Shape: [num_priors,10]\n        priors (tensor): Prior boxes in center-offset form.\n            Shape: [num_priors,4].\n        variances: (list[float]) Variances of priorboxes\n    Return:\n        decoded landm predictions\n    \"\"\"\n    landms = None\n\n    landms = np.concatenate(\n        (\n            priors[:, :2] + pre[:, :2] * variances[0] * priors[:, 2:],\n            priors[:, :2] + pre[:, 2:4] * variances[0] * priors[:, 2:],\n            priors[:, :2] + pre[:, 4:6] * variances[0] * priors[:, 2:],\n            priors[:, :2] + pre[:, 6:8] * variances[0] * priors[:, 2:],\n            priors[:, :2] + pre[:, 8:10] * variances[0] * priors[:, 2:],\n        ),\n        axis=1,\n    )\n\n    return landms\n", "metadata": {"source": "github.com/Zeyi-Lin/HivisionIDPhotos", "file_path": "hivision/creator/retinaface/box_utils.py", "language": "python", "complexity": "simple", "star_count": 18571, "quality_score": 0.92, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": true, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 2, "class_count": 0, "line_count": 58}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create REST API endpoint with proper request/response handling with SQLAlchemy/Django ORM patterns\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from decimal import Decimal\nfrom typing import Optional\n\nfrom sqlmodel import Field, Session, SQLModel, create_engine, select\n\n\nclass Hero(SQLModel, table=True):\n    id: Optional[int] = Field(default=None, primary_key=True)\n    name: str = Field(index=True)\n    secret_name: str\n    age: Optional[int] = Field(default=None, index=True)\n    money: Decimal = Field(default=0, max_digits=5, decimal_places=3)\n\n\nsqlite_file_name = \"database.db\"\nsqlite_url = f\"sqlite:///{sqlite_file_name}\"\n\nengine = create_engine(sqlite_url, echo=True)\n\n\ndef create_db_and_tables():\n    SQLModel.metadata.create_all(engine)\n\n\ndef create_heroes():\n    hero_1 = Hero(name=\"Deadpond\", secret_name=\"Dive Wilson\", money=1.1)\n    hero_2 = Hero(name=\"Spider-Boy\", secret_name=\"Pedro Parqueador\", money=0.001)\n    hero_3 = Hero(name=\"Rusty-Man\", secret_name=\"Tommy Sharp\", age=48, money=2.2)\n\n    with Session(engine) as session:\n        session.add(hero_1)\n        session.add(hero_2)\n        session.add(hero_3)\n\n        session.commit()\n\n\ndef select_heroes():\n    with Session(engine) as session:\n        statement = select(Hero).where(Hero.name == \"Deadpond\")\n        results = session.exec(statement)\n        hero_1 = results.one()\n        print(\"Hero 1:\", hero_1)\n\n        statement = select(Hero).where(Hero.name == \"Rusty-Man\")\n        results = session.exec(statement)\n        hero_2 = results.one()\n        print(\"Hero 2:\", hero_2)\n\n        total_money = hero_1.money + hero_2.money\n        print(f\"Total money: {total_money}\")\n\n\ndef main():\n    create_db_and_tables()\n    create_heroes()\n    select_heroes()\n\n\nif __name__ == \"__main__\":\n    main()\n", "metadata": {"source": "github.com/fastapi/sqlmodel", "file_path": "docs_src/advanced/decimal/tutorial001.py", "language": "python", "complexity": "simple", "star_count": 16410, "quality_score": 0.92, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 4, "class_count": 1, "line_count": 62}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create REST API endpoint with proper request/response handling with SQLAlchemy/Django ORM patterns\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from decimal import Decimal\n\nfrom sqlmodel import Field, Session, SQLModel, create_engine, select\n\n\nclass Hero(SQLModel, table=True):\n    id: int | None = Field(default=None, primary_key=True)\n    name: str = Field(index=True)\n    secret_name: str\n    age: int | None = Field(default=None, index=True)\n    money: Decimal = Field(default=0, max_digits=5, decimal_places=3)\n\n\nsqlite_file_name = \"database.db\"\nsqlite_url = f\"sqlite:///{sqlite_file_name}\"\n\nengine = create_engine(sqlite_url, echo=True)\n\n\ndef create_db_and_tables():\n    SQLModel.metadata.create_all(engine)\n\n\ndef create_heroes():\n    hero_1 = Hero(name=\"Deadpond\", secret_name=\"Dive Wilson\", money=1.1)\n    hero_2 = Hero(name=\"Spider-Boy\", secret_name=\"Pedro Parqueador\", money=0.001)\n    hero_3 = Hero(name=\"Rusty-Man\", secret_name=\"Tommy Sharp\", age=48, money=2.2)\n\n    with Session(engine) as session:\n        session.add(hero_1)\n        session.add(hero_2)\n        session.add(hero_3)\n\n        session.commit()\n\n\ndef select_heroes():\n    with Session(engine) as session:\n        statement = select(Hero).where(Hero.name == \"Deadpond\")\n        results = session.exec(statement)\n        hero_1 = results.one()\n        print(\"Hero 1:\", hero_1)\n\n        statement = select(Hero).where(Hero.name == \"Rusty-Man\")\n        results = session.exec(statement)\n        hero_2 = results.one()\n        print(\"Hero 2:\", hero_2)\n\n        total_money = hero_1.money + hero_2.money\n        print(f\"Total money: {total_money}\")\n\n\ndef main():\n    create_db_and_tables()\n    create_heroes()\n    select_heroes()\n\n\nif __name__ == \"__main__\":\n    main()\n", "metadata": {"source": "github.com/fastapi/sqlmodel", "file_path": "docs_src/advanced/decimal/tutorial001_py310.py", "language": "python", "complexity": "simple", "star_count": 16410, "quality_score": 0.92, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 4, "class_count": 1, "line_count": 61}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create REST API endpoint with proper request/response handling with SQLAlchemy/Django ORM patterns\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import uuid\nfrom typing import Union\n\nfrom sqlmodel import Field, Session, SQLModel, create_engine, select\n\n\nclass Hero(SQLModel, table=True):\n    id: uuid.UUID = Field(default_factory=uuid.uuid4, primary_key=True)\n    name: str = Field(index=True)\n    secret_name: str\n    age: Union[int, None] = Field(default=None, index=True)\n\n\nsqlite_file_name = \"database.db\"\nsqlite_url = f\"sqlite:///{sqlite_file_name}\"\n\nengine = create_engine(sqlite_url, echo=True)\n\n\ndef create_db_and_tables():\n    SQLModel.metadata.create_all(engine)\n\n\ndef create_hero():\n    with Session(engine) as session:\n        hero = Hero(name=\"Deadpond\", secret_name=\"Dive Wilson\")\n        print(\"The hero before saving in the DB\")\n        print(hero)\n        print(\"The hero ID was already set\")\n        print(hero.id)\n        session.add(hero)\n        session.commit()\n        session.refresh(hero)\n        print(\"After saving in the DB\")\n        print(hero)\n\n\ndef select_hero():\n    with Session(engine) as session:\n        hero_2 = Hero(name=\"Spider-Boy\", secret_name=\"Pedro Parqueador\")\n        session.add(hero_2)\n        session.commit()\n        session.refresh(hero_2)\n        hero_id = hero_2.id\n        print(\"Created hero:\")\n        print(hero_2)\n        print(\"Created hero ID:\")\n        print(hero_id)\n\n        statement = select(Hero).where(Hero.id == hero_id)\n        selected_hero = session.exec(statement).one()\n        print(\"Selected hero:\")\n        print(selected_hero)\n        print(\"Selected hero ID:\")\n        print(selected_hero.id)\n\n\ndef main() -> None:\n    create_db_and_tables()\n    create_hero()\n    select_hero()\n\n\nif __name__ == \"__main__\":\n    main()\n", "metadata": {"source": "github.com/fastapi/sqlmodel", "file_path": "docs_src/advanced/uuid/tutorial001.py", "language": "python", "complexity": "simple", "star_count": 16410, "quality_score": 0.92, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 4, "class_count": 1, "line_count": 66}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create REST API endpoint with proper request/response handling with SQLAlchemy/Django ORM patterns\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import uuid\n\nfrom sqlmodel import Field, Session, SQLModel, create_engine, select\n\n\nclass Hero(SQLModel, table=True):\n    id: uuid.UUID = Field(default_factory=uuid.uuid4, primary_key=True)\n    name: str = Field(index=True)\n    secret_name: str\n    age: int | None = Field(default=None, index=True)\n\n\nsqlite_file_name = \"database.db\"\nsqlite_url = f\"sqlite:///{sqlite_file_name}\"\n\nengine = create_engine(sqlite_url, echo=True)\n\n\ndef create_db_and_tables():\n    SQLModel.metadata.create_all(engine)\n\n\ndef create_hero():\n    with Session(engine) as session:\n        hero = Hero(name=\"Deadpond\", secret_name=\"Dive Wilson\")\n        print(\"The hero before saving in the DB\")\n        print(hero)\n        print(\"The hero ID was already set\")\n        print(hero.id)\n        session.add(hero)\n        session.commit()\n        session.refresh(hero)\n        print(\"After saving in the DB\")\n        print(hero)\n\n\ndef select_hero():\n    with Session(engine) as session:\n        hero_2 = Hero(name=\"Spider-Boy\", secret_name=\"Pedro Parqueador\")\n        session.add(hero_2)\n        session.commit()\n        session.refresh(hero_2)\n        hero_id = hero_2.id\n        print(\"Created hero:\")\n        print(hero_2)\n        print(\"Created hero ID:\")\n        print(hero_id)\n\n        statement = select(Hero).where(Hero.id == hero_id)\n        selected_hero = session.exec(statement).one()\n        print(\"Selected hero:\")\n        print(selected_hero)\n        print(\"Selected hero ID:\")\n        print(selected_hero.id)\n\n\ndef main() -> None:\n    create_db_and_tables()\n    create_hero()\n    select_hero()\n\n\nif __name__ == \"__main__\":\n    main()\n", "metadata": {"source": "github.com/fastapi/sqlmodel", "file_path": "docs_src/advanced/uuid/tutorial001_py310.py", "language": "python", "complexity": "simple", "star_count": 16410, "quality_score": 0.92, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 4, "class_count": 1, "line_count": 65}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create REST API endpoint with proper request/response handling with SQLAlchemy/Django ORM patterns\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import uuid\nfrom typing import Union\n\nfrom sqlmodel import Field, Session, SQLModel, create_engine\n\n\nclass Hero(SQLModel, table=True):\n    id: uuid.UUID = Field(default_factory=uuid.uuid4, primary_key=True)\n    name: str = Field(index=True)\n    secret_name: str\n    age: Union[int, None] = Field(default=None, index=True)\n\n\nsqlite_file_name = \"database.db\"\nsqlite_url = f\"sqlite:///{sqlite_file_name}\"\n\nengine = create_engine(sqlite_url, echo=True)\n\n\ndef create_db_and_tables():\n    SQLModel.metadata.create_all(engine)\n\n\ndef create_hero():\n    with Session(engine) as session:\n        hero_1 = Hero(name=\"Deadpond\", secret_name=\"Dive Wilson\")\n        print(\"The hero before saving in the DB\")\n        print(hero_1)\n        print(\"The hero ID was already set\")\n        print(hero_1.id)\n        session.add(hero_1)\n        session.commit()\n        session.refresh(hero_1)\n        print(\"After saving in the DB\")\n        print(hero_1)\n\n\ndef select_hero():\n    with Session(engine) as session:\n        hero_2 = Hero(name=\"Spider-Boy\", secret_name=\"Pedro Parqueador\")\n        session.add(hero_2)\n        session.commit()\n        session.refresh(hero_2)\n        hero_id = hero_2.id\n        print(\"Created hero:\")\n        print(hero_2)\n        print(\"Created hero ID:\")\n        print(hero_id)\n\n        selected_hero = session.get(Hero, hero_id)\n        print(\"Selected hero:\")\n        print(selected_hero)\n        print(\"Selected hero ID:\")\n        print(selected_hero.id)\n\n\ndef main() -> None:\n    create_db_and_tables()\n    create_hero()\n    select_hero()\n\n\nif __name__ == \"__main__\":\n    main()\n", "metadata": {"source": "github.com/fastapi/sqlmodel", "file_path": "docs_src/advanced/uuid/tutorial002.py", "language": "python", "complexity": "simple", "star_count": 16410, "quality_score": 0.92, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 4, "class_count": 1, "line_count": 65}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create REST API endpoint with proper request/response handling with SQLAlchemy/Django ORM patterns\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import uuid\n\nfrom sqlmodel import Field, Session, SQLModel, create_engine\n\n\nclass Hero(SQLModel, table=True):\n    id: uuid.UUID = Field(default_factory=uuid.uuid4, primary_key=True)\n    name: str = Field(index=True)\n    secret_name: str\n    age: int | None = Field(default=None, index=True)\n\n\nsqlite_file_name = \"database.db\"\nsqlite_url = f\"sqlite:///{sqlite_file_name}\"\n\nengine = create_engine(sqlite_url, echo=True)\n\n\ndef create_db_and_tables():\n    SQLModel.metadata.create_all(engine)\n\n\ndef create_hero():\n    with Session(engine) as session:\n        hero = Hero(name=\"Deadpond\", secret_name=\"Dive Wilson\")\n        print(\"The hero before saving in the DB\")\n        print(hero)\n        print(\"The hero ID was already set\")\n        print(hero.id)\n        session.add(hero)\n        session.commit()\n        session.refresh(hero)\n        print(\"After saving in the DB\")\n        print(hero)\n\n\ndef select_hero():\n    with Session(engine) as session:\n        hero_2 = Hero(name=\"Spider-Boy\", secret_name=\"Pedro Parqueador\")\n        session.add(hero_2)\n        session.commit()\n        session.refresh(hero_2)\n        hero_id = hero_2.id\n        print(\"Created hero:\")\n        print(hero_2)\n        print(\"Created hero ID:\")\n        print(hero_id)\n\n        selected_hero = session.get(Hero, hero_id)\n        print(\"Selected hero:\")\n        print(selected_hero)\n        print(\"Selected hero ID:\")\n        print(selected_hero.id)\n\n\ndef main() -> None:\n    create_db_and_tables()\n    create_hero()\n    select_hero()\n\n\nif __name__ == \"__main__\":\n    main()\n", "metadata": {"source": "github.com/fastapi/sqlmodel", "file_path": "docs_src/advanced/uuid/tutorial002_py310.py", "language": "python", "complexity": "simple", "star_count": 16410, "quality_score": 0.92, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 4, "class_count": 1, "line_count": 64}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create REST API endpoint with proper request/response handling with SQLAlchemy/Django ORM patterns\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from typing import Optional\n\nfrom sqlmodel import Field, Session, SQLModel, create_engine\n\n\nclass Hero(SQLModel, table=True):\n    id: Optional[int] = Field(default=None, primary_key=True)\n    name: str\n    secret_name: str\n    age: Optional[int] = None\n\n\nsqlite_file_name = \"database.db\"\nsqlite_url = f\"sqlite:///{sqlite_file_name}\"\n\nengine = create_engine(sqlite_url, echo=True)\n\n\ndef create_db_and_tables():\n    SQLModel.metadata.create_all(engine)\n\n\ndef create_heroes():\n    hero_1 = Hero(name=\"Deadpond\", secret_name=\"Dive Wilson\")\n    hero_2 = Hero(name=\"Spider-Boy\", secret_name=\"Pedro Parqueador\")\n    hero_3 = Hero(name=\"Rusty-Man\", secret_name=\"Tommy Sharp\", age=48)\n\n    print(\"Before interacting with the database\")\n    print(\"Hero 1:\", hero_1)\n    print(\"Hero 2:\", hero_2)\n    print(\"Hero 3:\", hero_3)\n\n    with Session(engine) as session:\n        session.add(hero_1)\n        session.add(hero_2)\n        session.add(hero_3)\n\n        print(\"After adding to the session\")\n        print(\"Hero 1:\", hero_1)\n        print(\"Hero 2:\", hero_2)\n        print(\"Hero 3:\", hero_3)\n\n        session.commit()\n\n        print(\"After committing the session\")\n        print(\"Hero 1:\", hero_1)\n        print(\"Hero 2:\", hero_2)\n        print(\"Hero 3:\", hero_3)\n\n        print(\"After committing the session, show IDs\")\n        print(\"Hero 1 ID:\", hero_1.id)\n        print(\"Hero 2 ID:\", hero_2.id)\n        print(\"Hero 3 ID:\", hero_3.id)\n\n        print(\"After committing the session, show names\")\n        print(\"Hero 1 name:\", hero_1.name)\n        print(\"Hero 2 name:\", hero_2.name)\n        print(\"Hero 3 name:\", hero_3.name)\n\n        session.refresh(hero_1)\n        session.refresh(hero_2)\n        session.refresh(hero_3)\n\n        print(\"After refreshing the heroes\")\n        print(\"Hero 1:\", hero_1)\n        print(\"Hero 2:\", hero_2)\n        print(\"Hero 3:\", hero_3)\n\n    print(\"After the session closes\")\n    print(\"Hero 1:\", hero_1)\n    print(\"Hero 2:\", hero_2)\n    print(\"Hero 3:\", hero_3)\n\n\ndef main():\n    create_db_and_tables()\n    create_heroes()\n\n\nif __name__ == \"__main__\":\n    main()\n", "metadata": {"source": "github.com/fastapi/sqlmodel", "file_path": "docs_src/tutorial/automatic_id_none_refresh/tutorial001.py", "language": "python", "complexity": "medium", "star_count": 16410, "quality_score": 0.92, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 3, "class_count": 1, "line_count": 82}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create REST API endpoint with proper request/response handling with SQLAlchemy/Django ORM patterns\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from sqlmodel import Field, Session, SQLModel, create_engine\n\n\nclass Hero(SQLModel, table=True):\n    id: int | None = Field(default=None, primary_key=True)\n    name: str\n    secret_name: str\n    age: int | None = None\n\n\nsqlite_file_name = \"database.db\"\nsqlite_url = f\"sqlite:///{sqlite_file_name}\"\n\nengine = create_engine(sqlite_url, echo=True)\n\n\ndef create_db_and_tables():\n    SQLModel.metadata.create_all(engine)\n\n\ndef create_heroes():\n    hero_1 = Hero(name=\"Deadpond\", secret_name=\"Dive Wilson\")\n    hero_2 = Hero(name=\"Spider-Boy\", secret_name=\"Pedro Parqueador\")\n    hero_3 = Hero(name=\"Rusty-Man\", secret_name=\"Tommy Sharp\", age=48)\n\n    print(\"Before interacting with the database\")\n    print(\"Hero 1:\", hero_1)\n    print(\"Hero 2:\", hero_2)\n    print(\"Hero 3:\", hero_3)\n\n    with Session(engine) as session:\n        session.add(hero_1)\n        session.add(hero_2)\n        session.add(hero_3)\n\n        print(\"After adding to the session\")\n        print(\"Hero 1:\", hero_1)\n        print(\"Hero 2:\", hero_2)\n        print(\"Hero 3:\", hero_3)\n\n        session.commit()\n\n        print(\"After committing the session\")\n        print(\"Hero 1:\", hero_1)\n        print(\"Hero 2:\", hero_2)\n        print(\"Hero 3:\", hero_3)\n\n        print(\"After committing the session, show IDs\")\n        print(\"Hero 1 ID:\", hero_1.id)\n        print(\"Hero 2 ID:\", hero_2.id)\n        print(\"Hero 3 ID:\", hero_3.id)\n\n        print(\"After committing the session, show names\")\n        print(\"Hero 1 name:\", hero_1.name)\n        print(\"Hero 2 name:\", hero_2.name)\n        print(\"Hero 3 name:\", hero_3.name)\n\n        session.refresh(hero_1)\n        session.refresh(hero_2)\n        session.refresh(hero_3)\n\n        print(\"After refreshing the heroes\")\n        print(\"Hero 1:\", hero_1)\n        print(\"Hero 2:\", hero_2)\n        print(\"Hero 3:\", hero_3)\n\n    print(\"After the session closes\")\n    print(\"Hero 1:\", hero_1)\n    print(\"Hero 2:\", hero_2)\n    print(\"Hero 3:\", hero_3)\n\n\ndef main():\n    create_db_and_tables()\n    create_heroes()\n\n\nif __name__ == \"__main__\":\n    main()\n", "metadata": {"source": "github.com/fastapi/sqlmodel", "file_path": "docs_src/tutorial/automatic_id_none_refresh/tutorial001_py310.py", "language": "python", "complexity": "medium", "star_count": 16410, "quality_score": 0.92, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 3, "class_count": 1, "line_count": 80}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create REST API endpoint with proper request/response handling with SQLAlchemy/Django ORM patterns\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from typing import Optional\n\nfrom sqlmodel import Field, Session, SQLModel, create_engine\n\n\nclass Hero(SQLModel, table=True):\n    id: Optional[int] = Field(default=None, primary_key=True)\n    name: str\n    secret_name: str\n    age: Optional[int] = None\n\n\nsqlite_file_name = \"database.db\"\nsqlite_url = f\"sqlite:///{sqlite_file_name}\"\n\nengine = create_engine(sqlite_url, echo=True)\n\n\ndef create_db_and_tables():\n    SQLModel.metadata.create_all(engine)\n\n\ndef create_heroes():\n    hero_1 = Hero(name=\"Deadpond\", secret_name=\"Dive Wilson\")  # (1)!\n    hero_2 = Hero(name=\"Spider-Boy\", secret_name=\"Pedro Parqueador\")  # (2)!\n    hero_3 = Hero(name=\"Rusty-Man\", secret_name=\"Tommy Sharp\", age=48)  # (3)!\n\n    print(\"Before interacting with the database\")  # (4)!\n    print(\"Hero 1:\", hero_1)  # (5)!\n    print(\"Hero 2:\", hero_2)  # (6)!\n    print(\"Hero 3:\", hero_3)  # (7)!\n\n    with Session(engine) as session:  # (8)!\n        session.add(hero_1)  # (9)!\n        session.add(hero_2)  # (10)!\n        session.add(hero_3)  # (11)!\n\n        print(\"After adding to the session\")  # (12)!\n        print(\"Hero 1:\", hero_1)  # (13)!\n        print(\"Hero 2:\", hero_2)  # (14)!\n        print(\"Hero 3:\", hero_3)  # (15)!\n\n        session.commit()  # (16)!\n\n        print(\"After committing the session\")  # (17)!\n        print(\"Hero 1:\", hero_1)  # (18)!\n        print(\"Hero 2:\", hero_2)  # (19)!\n        print(\"Hero 3:\", hero_3)  # (20)!\n\n        print(\"After committing the session, show IDs\")  # (21)!\n        print(\"Hero 1 ID:\", hero_1.id)  # (22)!\n        print(\"Hero 2 ID:\", hero_2.id)  # (23)!\n        print(\"Hero 3 ID:\", hero_3.id)  # (24)!\n\n        print(\"After committing the session, show names\")  # (25)!\n        print(\"Hero 1 name:\", hero_1.name)  # (26)!\n        print(\"Hero 2 name:\", hero_2.name)  # (27)!\n        print(\"Hero 3 name:\", hero_3.name)  # (28)!\n\n        session.refresh(hero_1)  # (29)!\n        session.refresh(hero_2)  # (30)!\n        session.refresh(hero_3)  # (31)!\n\n        print(\"After refreshing the heroes\")  # (32)!\n        print(\"Hero 1:\", hero_1)  # (33)!\n        print(\"Hero 2:\", hero_2)  # (34)!\n        print(\"Hero 3:\", hero_3)  # (35)!\n    # (36)!\n\n    print(\"After the session closes\")  # (37)!\n    print(\"Hero 1:\", hero_1)  # (38)!\n    print(\"Hero 2:\", hero_2)  # (39)!\n    print(\"Hero 3:\", hero_3)  # (40)!\n\n\ndef main():\n    create_db_and_tables()\n    create_heroes()\n\n\nif __name__ == \"__main__\":\n    main()\n", "metadata": {"source": "github.com/fastapi/sqlmodel", "file_path": "docs_src/tutorial/automatic_id_none_refresh/tutorial002.py", "language": "python", "complexity": "medium", "star_count": 16410, "quality_score": 0.92, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 3, "class_count": 1, "line_count": 83}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create REST API endpoint with proper request/response handling with SQLAlchemy/Django ORM patterns\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from sqlmodel import Field, Session, SQLModel, create_engine\n\n\nclass Hero(SQLModel, table=True):\n    id: int | None = Field(default=None, primary_key=True)\n    name: str\n    secret_name: str\n    age: int | None = None\n\n\nsqlite_file_name = \"database.db\"\nsqlite_url = f\"sqlite:///{sqlite_file_name}\"\n\nengine = create_engine(sqlite_url, echo=True)\n\n\ndef create_db_and_tables():\n    SQLModel.metadata.create_all(engine)\n\n\ndef create_heroes():\n    hero_1 = Hero(name=\"Deadpond\", secret_name=\"Dive Wilson\")  # (1)!\n    hero_2 = Hero(name=\"Spider-Boy\", secret_name=\"Pedro Parqueador\")  # (2)!\n    hero_3 = Hero(name=\"Rusty-Man\", secret_name=\"Tommy Sharp\", age=48)  # (3)!\n\n    print(\"Before interacting with the database\")  # (4)!\n    print(\"Hero 1:\", hero_1)  # (5)!\n    print(\"Hero 2:\", hero_2)  # (6)!\n    print(\"Hero 3:\", hero_3)  # (7)!\n\n    with Session(engine) as session:  # (8)!\n        session.add(hero_1)  # (9)!\n        session.add(hero_2)  # (10)!\n        session.add(hero_3)  # (11)!\n\n        print(\"After adding to the session\")  # (12)!\n        print(\"Hero 1:\", hero_1)  # (13)!\n        print(\"Hero 2:\", hero_2)  # (14)!\n        print(\"Hero 3:\", hero_3)  # (15)!\n\n        session.commit()  # (16)!\n\n        print(\"After committing the session\")  # (17)!\n        print(\"Hero 1:\", hero_1)  # (18)!\n        print(\"Hero 2:\", hero_2)  # (19)!\n        print(\"Hero 3:\", hero_3)  # (20)!\n\n        print(\"After committing the session, show IDs\")  # (21)!\n        print(\"Hero 1 ID:\", hero_1.id)  # (22)!\n        print(\"Hero 2 ID:\", hero_2.id)  # (23)!\n        print(\"Hero 3 ID:\", hero_3.id)  # (24)!\n\n        print(\"After committing the session, show names\")  # (25)!\n        print(\"Hero 1 name:\", hero_1.name)  # (26)!\n        print(\"Hero 2 name:\", hero_2.name)  # (27)!\n        print(\"Hero 3 name:\", hero_3.name)  # (28)!\n\n        session.refresh(hero_1)  # (29)!\n        session.refresh(hero_2)  # (30)!\n        session.refresh(hero_3)  # (31)!\n\n        print(\"After refreshing the heroes\")  # (32)!\n        print(\"Hero 1:\", hero_1)  # (33)!\n        print(\"Hero 2:\", hero_2)  # (34)!\n        print(\"Hero 3:\", hero_3)  # (35)!\n    # (36)!\n\n    print(\"After the session closes\")  # (37)!\n    print(\"Hero 1:\", hero_1)  # (38)!\n    print(\"Hero 2:\", hero_2)  # (39)!\n    print(\"Hero 3:\", hero_3)  # (40)!\n\n\ndef main():\n    create_db_and_tables()\n    create_heroes()\n\n\nif __name__ == \"__main__\":\n    main()\n", "metadata": {"source": "github.com/fastapi/sqlmodel", "file_path": "docs_src/tutorial/automatic_id_none_refresh/tutorial002_py310.py", "language": "python", "complexity": "medium", "star_count": 16410, "quality_score": 0.92, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 3, "class_count": 1, "line_count": 81}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create REST API endpoint with proper request/response handling\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from sqlmodel import Session\n\nfrom .database import create_db_and_tables, engine\nfrom .models import Hero, Team\n\n\ndef create_heroes():\n    with Session(engine) as session:\n        team_z_force = Team(name=\"Z-Force\", headquarters=\"Sister Margaret's Bar\")\n\n        hero_deadpond = Hero(\n            name=\"Deadpond\", secret_name=\"Dive Wilson\", team=team_z_force\n        )\n        session.add(hero_deadpond)\n        session.commit()\n\n        session.refresh(hero_deadpond)\n\n        print(\"Created hero:\", hero_deadpond)\n        print(\"Hero's team:\", hero_deadpond.team)\n\n\ndef main():\n    create_db_and_tables()\n    create_heroes()\n\n\nif __name__ == \"__main__\":\n    main()\n", "metadata": {"source": "github.com/fastapi/sqlmodel", "file_path": "docs_src/tutorial/code_structure/tutorial001/app.py", "language": "python", "complexity": "simple", "star_count": 16410, "quality_score": 0.89, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": false, "has_testing": false, "has_logging": false, "function_count": 2, "class_count": 0, "line_count": 30}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create REST API endpoint with proper request/response handling\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from sqlmodel import SQLModel, create_engine\n\nsqlite_file_name = \"database.db\"\nsqlite_url = f\"sqlite:///{sqlite_file_name}\"\n\nengine = create_engine(sqlite_url)\n\n\ndef create_db_and_tables():\n    SQLModel.metadata.create_all(engine)\n", "metadata": {"source": "github.com/fastapi/sqlmodel", "file_path": "docs_src/tutorial/code_structure/tutorial001/database.py", "language": "python", "complexity": "simple", "star_count": 16410, "quality_score": 0.89, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": false, "has_testing": false, "has_logging": false, "function_count": 1, "class_count": 0, "line_count": 11}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create REST API endpoint with proper request/response handling\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from sqlmodel import Session\n\nfrom .database import create_db_and_tables, engine\nfrom .models import Hero, Team\n\n\ndef create_heroes():\n    with Session(engine) as session:\n        team_z_force = Team(name=\"Z-Force\", headquarters=\"Sister Margaret's Bar\")\n\n        hero_deadpond = Hero(\n            name=\"Deadpond\", secret_name=\"Dive Wilson\", team=team_z_force\n        )\n        session.add(hero_deadpond)\n        session.commit()\n\n        session.refresh(hero_deadpond)\n\n        print(\"Created hero:\", hero_deadpond)\n        print(\"Hero's team:\", hero_deadpond.team)\n\n\ndef main():\n    create_db_and_tables()\n    create_heroes()\n\n\nif __name__ == \"__main__\":\n    main()\n", "metadata": {"source": "github.com/fastapi/sqlmodel", "file_path": "docs_src/tutorial/code_structure/tutorial001_py310/app.py", "language": "python", "complexity": "simple", "star_count": 16410, "quality_score": 0.89, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": false, "has_testing": false, "has_logging": false, "function_count": 2, "class_count": 0, "line_count": 30}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create REST API endpoint with proper request/response handling\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from sqlmodel import SQLModel, create_engine\n\nsqlite_file_name = \"database.db\"\nsqlite_url = f\"sqlite:///{sqlite_file_name}\"\n\nengine = create_engine(sqlite_url)\n\n\ndef create_db_and_tables():\n    SQLModel.metadata.create_all(engine)\n", "metadata": {"source": "github.com/fastapi/sqlmodel", "file_path": "docs_src/tutorial/code_structure/tutorial001_py310/database.py", "language": "python", "complexity": "simple", "star_count": 16410, "quality_score": 0.89, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": false, "has_testing": false, "has_logging": false, "function_count": 1, "class_count": 0, "line_count": 11}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create REST API endpoint with proper request/response handling\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from sqlmodel import Session\n\nfrom .database import create_db_and_tables, engine\nfrom .models import Hero, Team\n\n\ndef create_heroes():\n    with Session(engine) as session:\n        team_z_force = Team(name=\"Z-Force\", headquarters=\"Sister Margaret's Bar\")\n\n        hero_deadpond = Hero(\n            name=\"Deadpond\", secret_name=\"Dive Wilson\", team=team_z_force\n        )\n        session.add(hero_deadpond)\n        session.commit()\n\n        session.refresh(hero_deadpond)\n\n        print(\"Created hero:\", hero_deadpond)\n        print(\"Hero's team:\", hero_deadpond.team)\n\n\ndef main():\n    create_db_and_tables()\n    create_heroes()\n\n\nif __name__ == \"__main__\":\n    main()\n", "metadata": {"source": "github.com/fastapi/sqlmodel", "file_path": "docs_src/tutorial/code_structure/tutorial001_py39/app.py", "language": "python", "complexity": "simple", "star_count": 16410, "quality_score": 0.89, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": false, "has_testing": false, "has_logging": false, "function_count": 2, "class_count": 0, "line_count": 30}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create REST API endpoint with proper request/response handling with asyncio coroutines\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from fastapi import APIRouter, Body, Query, Request, HTTPException  # 导入FastAPI组件\r\nfrom app.api.models.APIResponseModel import ResponseModel, ErrorResponseModel  # 导入响应模型\r\n\r\nfrom crawlers.bilibili.web.web_crawler import BilibiliWebCrawler  # 导入哔哩哔哩web爬虫\r\n\r\n\r\nrouter = APIRouter()\r\nBilibiliWebCrawler = BilibiliWebCrawler()\r\n\r\n\r\n# 获取单个视频详情信息\r\n@router.get(\"/fetch_one_video\", response_model=ResponseModel, summary=\"获取单个视频详情信息/Get single video data\")\r\nasync def fetch_one_video(request: Request,\r\n                          bv_id: str = Query(example=\"BV1M1421t7hT\", description=\"作品id/Video id\")):\r\n    \"\"\"\r\n    # [中文]\r\n    ### 用途:\r\n    - 获取单个视频详情信息\r\n    ### 参数:\r\n    - bv_id: 作品id\r\n    ### 返回:\r\n    - 视频详情信息\r\n\r\n    # [English]\r\n    ### Purpose:\r\n    - Get single video data\r\n    ### Parameters:\r\n    - bv_id: Video id\r\n    ### Return:\r\n    - Video data\r\n\r\n    # [示例/Example]\r\n    bv_id = \"BV1M1421t7hT\"\r\n    \"\"\"\r\n    try:\r\n        data = await BilibiliWebCrawler.fetch_one_video(bv_id)\r\n        return ResponseModel(code=200,\r\n                             router=request.url.path,\r\n                             data=data)\r\n    except Exception as e:\r\n        status_code = 400\r\n        detail = ErrorResponseModel(code=status_code,\r\n                                    router=request.url.path,\r\n                                    params=dict(request.query_params),\r\n                                    )\r\n        raise HTTPException(status_code=status_code, detail=detail.dict())\r\n\r\n\r\n# 获取视频流地址\r\n@router.get(\"/fetch_video_playurl\", response_model=ResponseModel, summary=\"获取视频流地址/Get video playurl\")\r\nasync def fetch_one_video(request: Request,\r\n                          bv_id: str = Query(example=\"BV1y7411Q7Eq\", description=\"作品id/Video id\"),\r\n                          cid:str = Query(example=\"171776208\", description=\"作品cid/Video cid\")):\r\n    \"\"\"\r\n    # [中文]\r\n    ### 用途:\r\n    - 获取视频流地址\r\n    ### 参数:\r\n    - bv_id: 作品id\r\n    - cid: 作品cid\r\n    ### 返回:\r\n    - 视频流地址\r\n\r\n    # [English]\r\n    ### Purpose:\r\n    - Get video playurl\r\n    ### Parameters:\r\n    - bv_id: Video id\r\n    - cid: Video cid\r\n    ### Return:\r\n    - Video playurl\r\n\r\n    # [示例/Example]\r\n    bv_id = \"BV1y7411Q7Eq\"\r\n    cid = \"171776208\"\r\n    \"\"\"\r\n    try:\r\n        data = await BilibiliWebCrawler.fetch_video_playurl(bv_id, cid)\r\n        return ResponseModel(code=200,\r\n                             router=request.url.path,\r\n                             data=data)\r\n    except Exception as e:\r\n        status_code = 400\r\n        detail = ErrorResponseModel(code=status_code,\r\n                                    router=request.url.path,\r\n                                    params=dict(request.query_params),\r\n                                    )\r\n        raise HTTPException(status_code=status_code, detail=detail.dict())\r\n\r\n\r\n# 获取用户发布视频作品数据\r\n@router.get(\"/fetch_user_post_videos\", response_model=ResponseModel,\r\n            summary=\"获取用户主页作品数据/Get user homepage video data\")\r\nasync def fetch_user_post_videos(request: Request,\r\n                                 uid: str = Query(example=\"178360345\", description=\"用户UID\"),\r\n                                 pn: int = Query(default=1, description=\"页码/Page number\"),):\r\n    \"\"\"\r\n    # [中文]\r\n    ### 用途:\r\n    - 获取用户发布的视频数据\r\n    ### 参数:\r\n    - uid: 用户UID\r\n    - pn: 页码\r\n    ### 返回:\r\n    - 用户发布的视频数据\r\n\r\n    # [English]\r\n    ### Purpose:\r\n    - Get user post video data\r\n    ### Parameters:\r\n    - uid: User UID\r\n    - pn: Page number\r\n    ### Return:\r\n    - User posted video data\r\n\r\n    # [示例/Example]\r\n    uid = \"178360345\"\r\n    pn = 1\r\n    \"\"\"\r\n    try:\r\n        data = await BilibiliWebCrawler.fetch_user_post_videos(uid, pn)\r\n        return ResponseModel(code=200,\r\n                             router=request.url.path,\r\n                             data=data)\r\n    except Exception as e:\r\n        status_code = 400\r\n        detail = ErrorResponseModel(code=status_code,\r\n                                    router=request.url.path,\r\n                                    params=dict(request.query_params),\r\n                                    )\r\n        raise HTTPException(status_code=status_code, detail=detail.dict())\r\n\r\n\r\n# 获取用户所有收藏夹信息\r\n@router.get(\"/fetch_collect_folders\", response_model=ResponseModel,\r\n            summary=\"获取用户所有收藏夹信息/Get user collection folders\")\r\nasync def fetch_collect_folders(request: Request,\r\n                                uid: str = Query(example=\"178360345\", description=\"用户UID\")):\r\n    \"\"\"\r\n    # [中文]\r\n    ### 用途:\r\n    - 获取用户收藏作品数据\r\n    ### 参数:\r\n    - uid: 用户UID\r\n    ### 返回:\r\n    - 用户收藏夹信息\r\n\r\n    # [English]\r\n    ### Purpose:\r\n    - Get user collection folders\r\n    ### Parameters:\r\n    - uid: User UID\r\n    ### Return:\r\n    - user collection folders\r\n\r\n    # [示例/Example]\r\n    uid = \"178360345\"\r\n    \"\"\"\r\n    try:\r\n        data = await BilibiliWebCrawler.fetch_collect_folders(uid)\r\n        return ResponseModel(code=200,\r\n                             router=request.url.path,\r\n                             data=data)\r\n    except Exception as e:\r\n        status_code = 400\r\n        detail = ErrorResponseModel(code=status_code,\r\n                                    router=request.url.path,\r\n                                    params=dict(request.query_params),\r\n                                    )\r\n        raise HTTPException(status_code=status_code, detail=detail.dict())\r\n\r\n\r\n# 获取指定收藏夹内视频数据\r\n@router.get(\"/fetch_user_collection_videos\", response_model=ResponseModel,\r\n            summary=\"获取指定收藏夹内视频数据/Gets video data from a collection folder\")\r\nasync def fetch_user_collection_videos(request: Request,\r\n                                       folder_id: str = Query(example=\"1756059545\",\r\n                                                              description=\"收藏夹id/collection folder id\"),\r\n                                       pn: int = Query(default=1, description=\"页码/Page number\")\r\n                                       ):\r\n    \"\"\"\r\n    # [中文]\r\n    ### 用途:\r\n    - 获取指定收藏夹内视频数据\r\n    ### 参数:\r\n    - folder_id: 用户UID\r\n    - pn: 页码\r\n    ### 返回:\r\n    - 指定收藏夹内视频数据\r\n\r\n    # [English]\r\n    ### Purpose:\r\n    - Gets video data from a collection folder\r\n    ### Parameters:\r\n    - folder_id: collection folder id\r\n    - pn: Page number\r\n    ### Return:\r\n    - video data from collection folder\r\n\r\n    # [示例/Example]\r\n    folder_id = \"1756059545\"\r\n    pn = 1\r\n    \"\"\"\r\n    try:\r\n        data = await BilibiliWebCrawler.fetch_folder_videos(folder_id, pn)\r\n        return ResponseModel(code=200,\r\n                             router=request.url.path,\r\n                             data=data)\r\n    except Exception as e:\r\n        status_code = 400\r\n        detail = ErrorResponseModel(code=status_code,\r\n                                    router=request.url.path,\r\n                                    params=dict(request.query_params),\r\n                                    )\r\n        raise HTTPException(status_code=status_code, detail=detail.dict())\r\n\r\n\r\n# 获取指定用户的信息\r\n@router.get(\"/fetch_user_profile\", response_model=ResponseModel,\r\n            summary=\"获取指定用户的信息/Get information of specified user\")\r\nasync def fetch_collect_folders(request: Request,\r\n                                uid: str = Query(example=\"178360345\", description=\"用户UID\")):\r\n    \"\"\"\r\n    # [中文]\r\n    ### 用途:\r\n    - 获取指定用户的信息\r\n    ### 参数:\r\n    - uid: 用户UID\r\n    ### 返回:\r\n    - 指定用户的个人信息\r\n\r\n    # [English]\r\n    ### Purpose:\r\n    - Get information of specified user\r\n    ### Parameters:\r\n    - uid: User UID\r\n    ### Return:\r\n    - information of specified user\r\n\r\n    # [示例/Example]\r\n    uid = \"178360345\"\r\n    \"\"\"\r\n    try:\r\n        data = await BilibiliWebCrawler.fetch_user_profile(uid)\r\n        return ResponseModel(code=200,\r\n                             router=request.url.path,\r\n                             data=data)\r\n    except Exception as e:\r\n        status_code = 400\r\n        detail = ErrorResponseModel(code=status_code,\r\n                                    router=request.url.path,\r\n                                    params=dict(request.query_params),\r\n                                    )\r\n        raise HTTPException(status_code=status_code, detail=detail.dict())\r\n\r\n\r\n# 获取综合热门视频信息\r\n@router.get(\"/fetch_com_popular\", response_model=ResponseModel,\r\n            summary=\"获取综合热门视频信息/Get comprehensive popular video information\")\r\nasync def fetch_collect_folders(request: Request,\r\n                                pn: int = Query(default=1, description=\"页码/Page number\")):\r\n    \"\"\"\r\n    # [中文]\r\n    ### 用途:\r\n    - 获取综合热门视频信息\r\n    ### 参数:\r\n    - pn: 页码\r\n    ### 返回:\r\n    - 综合热门视频信息\r\n\r\n    # [English]\r\n    ### Purpose:\r\n    - Get comprehensive popular video information\r\n    ### Parameters:\r\n    - pn: Page number\r\n    ### Return:\r\n    - comprehensive popular video information\r\n\r\n    # [示例/Example]\r\n    pn = 1\r\n    \"\"\"\r\n    try:\r\n        data = await BilibiliWebCrawler.fetch_com_popular(pn)\r\n        return ResponseModel(code=200,\r\n                             router=request.url.path,\r\n                             data=data)\r\n    except Exception as e:\r\n        status_code = 400\r\n        detail = ErrorResponseModel(code=status_code,\r\n                                    router=request.url.path,\r\n                                    params=dict(request.query_params),\r\n                                    )\r\n        raise HTTPException(status_code=status_code, detail=detail.dict())\r\n\r\n\r\n# 获取指定视频的评论\r\n@router.get(\"/fetch_video_comments\", response_model=ResponseModel,\r\n            summary=\"获取指定视频的评论/Get comments on the specified video\")\r\nasync def fetch_collect_folders(request: Request,\r\n                                bv_id: str = Query(example=\"BV1M1421t7hT\", description=\"作品id/Video id\"),\r\n                                pn: int = Query(default=1, description=\"页码/Page number\")):\r\n    \"\"\"\r\n    # [中文]\r\n    ### 用途:\r\n    - 获取指定视频的评论\r\n    ### 参数:\r\n    - bv_id: 作品id\r\n    - pn: 页码\r\n    ### 返回:\r\n    - 指定视频的评论数据\r\n\r\n    # [English]\r\n    ### Purpose:\r\n    - Get comments on the specified video\r\n    ### Parameters:\r\n    - bv_id: Video id\r\n    - pn: Page number\r\n    ### Return:\r\n    - comments of the specified video\r\n\r\n    # [示例/Example]\r\n    bv_id = \"BV1M1421t7hT\"\r\n    pn = 1\r\n    \"\"\"\r\n    try:\r\n        data = await BilibiliWebCrawler.fetch_video_comments(bv_id, pn)\r\n        return ResponseModel(code=200,\r\n                             router=request.url.path,\r\n                             data=data)\r\n    except Exception as e:\r\n        status_code = 400\r\n        detail = ErrorResponseModel(code=status_code,\r\n                                    router=request.url.path,\r\n                                    params=dict(request.query_params),\r\n                                    )\r\n        raise HTTPException(status_code=status_code, detail=detail.dict())\r\n\r\n\r\n# 获取视频下指定评论的回复\r\n@router.get(\"/fetch_comment_reply\", response_model=ResponseModel,\r\n            summary=\"获取视频下指定评论的回复/Get reply to the specified comment\")\r\nasync def fetch_collect_folders(request: Request,\r\n                                bv_id: str = Query(example=\"BV1M1421t7hT\", description=\"作品id/Video id\"),\r\n                                pn: int = Query(default=1, description=\"页码/Page number\"),\r\n                                rpid: str = Query(example=\"237109455120\", description=\"回复id/Reply id\")):\r\n    \"\"\"\r\n    # [中文]\r\n    ### 用途:\r\n    - 获取视频下指定评论的回复\r\n    ### 参数:\r\n    - bv_id: 作品id\r\n    - pn: 页码\r\n    - rpid: 回复id\r\n    ### 返回:\r\n    - 指定评论的回复数据\r\n\r\n    # [English]\r\n    ### Purpose:\r\n    - Get reply to the specified comment\r\n    ### Parameters:\r\n    - bv_id: Video id\r\n    - pn: Page number\r\n    - rpid: Reply id\r\n    ### Return:\r\n    - Reply of the specified comment\r\n\r\n    # [示例/Example]\r\n    bv_id = \"BV1M1421t7hT\"\r\n    pn = 1\r\n    rpid = \"237109455120\"\r\n    \"\"\"\r\n    try:\r\n        data = await BilibiliWebCrawler.fetch_comment_reply(bv_id, pn, rpid)\r\n        return ResponseModel(code=200,\r\n                             router=request.url.path,\r\n                             data=data)\r\n    except Exception as e:\r\n        status_code = 400\r\n        detail = ErrorResponseModel(code=status_code,\r\n                                    router=request.url.path,\r\n                                    params=dict(request.query_params),\r\n                                    )\r\n        raise HTTPException(status_code=status_code, detail=detail.dict())\r\n\r\n\r\n# 获取指定用户动态\r\n@router.get(\"/fetch_user_dynamic\", response_model=ResponseModel,\r\n            summary=\"获取指定用户动态/Get dynamic information of specified user\")\r\nasync def fetch_collect_folders(request: Request,\r\n                                uid: str = Query(example=\"16015678\", description=\"用户UID\"),\r\n                                offset: str = Query(default=\"\", example=\"953154282154098691\",\r\n                                                    description=\"开始索引/offset\")):\r\n    \"\"\"\r\n    # [中文]\r\n    ### 用途:\r\n    - 获取指定用户动态\r\n    ### 参数:\r\n    - uid: 用户UID\r\n    - offset: 开始索引\r\n    ### 返回:\r\n    - 指定用户动态数据\r\n\r\n    # [English]\r\n    ### Purpose:\r\n    - Get dynamic information of specified user\r\n    ### Parameters:\r\n    - uid: User UID\r\n    - offset: offset\r\n    ### Return:\r\n    - dynamic information of specified user\r\n\r\n    # [示例/Example]\r\n    uid = \"178360345\"\r\n    offset = \"953154282154098691\"\r\n    \"\"\"\r\n    try:\r\n        data = await BilibiliWebCrawler.fetch_user_dynamic(uid, offset)\r\n        return ResponseModel(code=200,\r\n                             router=request.url.path,\r\n                             data=data)\r\n    except Exception as e:\r\n        status_code = 400\r\n        detail = ErrorResponseModel(code=status_code,\r\n                                    router=request.url.path,\r\n                                    params=dict(request.query_params),\r\n                                    )\r\n        raise HTTPException(status_code=status_code, detail=detail.dict())\r\n\r\n\r\n# 获取视频实时弹幕\r\n@router.get(\"/fetch_video_danmaku\", response_model=ResponseModel, summary=\"获取视频实时弹幕/Get Video Danmaku\")\r\nasync def fetch_one_video(request: Request,\r\n                          cid: str = Query(example=\"1639235405\", description=\"作品cid/Video cid\")):\r\n    \"\"\"\r\n    # [中文]\r\n    ### 用途:\r\n    - 获取视频实时弹幕\r\n    ### 参数:\r\n    - cid: 作品cid\r\n    ### 返回:\r\n    - 视频实时弹幕\r\n\r\n    # [English]\r\n    ### Purpose:\r\n    - Get Video Danmaku\r\n    ### Parameters:\r\n    - cid: Video cid\r\n    ### Return:\r\n    - Video Danmaku\r\n\r\n    # [示例/Example]\r\n    cid = \"1639235405\"\r\n    \"\"\"\r\n    try:\r\n        data = await BilibiliWebCrawler.fetch_video_danmaku(cid)\r\n        return ResponseModel(code=200,\r\n                             router=request.url.path,\r\n                             data=data)\r\n    except Exception as e:\r\n        status_code = 400\r\n        detail = ErrorResponseModel(code=status_code,\r\n                                    router=request.url.path,\r\n                                    params=dict(request.query_params),\r\n                                    )\r\n        raise HTTPException(status_code=status_code, detail=detail.dict())\r\n\r\n\r\n# 获取指定直播间信息\r\n@router.get(\"/fetch_live_room_detail\", response_model=ResponseModel,\r\n            summary=\"获取指定直播间信息/Get information of specified live room\")\r\nasync def fetch_collect_folders(request: Request,\r\n                                room_id: str = Query(example=\"22816111\", description=\"直播间ID/Live room ID\")):\r\n    \"\"\"\r\n    # [中文]\r\n    ### 用途:\r\n    - 获取指定直播间信息\r\n    ### 参数:\r\n    - room_id: 直播间ID\r\n    ### 返回:\r\n    - 指定直播间信息\r\n\r\n    # [English]\r\n    ### Purpose:\r\n    - Get information of specified live room\r\n    ### Parameters:\r\n    - room_id: Live room ID\r\n    ### Return:\r\n    - information of specified live room\r\n\r\n    # [示例/Example]\r\n    room_id = \"22816111\"\r\n    \"\"\"\r\n    try:\r\n        data = await BilibiliWebCrawler.fetch_live_room_detail(room_id)\r\n        return ResponseModel(code=200,\r\n                             router=request.url.path,\r\n                             data=data)\r\n    except Exception as e:\r\n        status_code = 400\r\n        detail = ErrorResponseModel(code=status_code,\r\n                                    router=request.url.path,\r\n                                    params=dict(request.query_params),\r\n                                    )\r\n        raise HTTPException(status_code=status_code, detail=detail.dict())\r\n\r\n\r\n# 获取指定直播间视频流\r\n@router.get(\"/fetch_live_videos\", response_model=ResponseModel,\r\n            summary=\"获取直播间视频流/Get live video data of specified room\")\r\nasync def fetch_collect_folders(request: Request,\r\n                                room_id: str = Query(example=\"1815229528\", description=\"直播间ID/Live room ID\")):\r\n    \"\"\"\r\n    # [中文]\r\n    ### 用途:\r\n    - 获取指定直播间视频流\r\n    ### 参数:\r\n    - room_id: 直播间ID\r\n    ### 返回:\r\n    - 指定直播间视频流\r\n\r\n    # [English]\r\n    ### Purpose:\r\n    - Get live video data of specified room\r\n    ### Parameters:\r\n    - room_id: Live room ID\r\n    ### Return:\r\n    - live video data of specified room\r\n\r\n    # [示例/Example]\r\n    room_id = \"1815229528\"\r\n    \"\"\"\r\n    try:\r\n        data = await BilibiliWebCrawler.fetch_live_videos(room_id)\r\n        return ResponseModel(code=200,\r\n                             router=request.url.path,\r\n                             data=data)\r\n    except Exception as e:\r\n        status_code = 400\r\n        detail = ErrorResponseModel(code=status_code,\r\n                                    router=request.url.path,\r\n                                    params=dict(request.query_params),\r\n                                    )\r\n        raise HTTPException(status_code=status_code, detail=detail.dict())\r\n\r\n\r\n# 获取指定分区正在直播的主播\r\n@router.get(\"/fetch_live_streamers\", response_model=ResponseModel,\r\n            summary=\"获取指定分区正在直播的主播/Get live streamers of specified live area\")\r\nasync def fetch_collect_folders(request: Request,\r\n                                area_id: str = Query(example=\"9\", description=\"直播分区id/Live area ID\"),\r\n                                pn: int = Query(default=1, description=\"页码/Page number\")):\r\n    \"\"\"\r\n    # [中文]\r\n    ### 用途:\r\n    - 获取指定分区正在直播的主播\r\n    ### 参数:\r\n    - area_id: 直播分区id\r\n    - pn: 页码\r\n    ### 返回:\r\n    - 指定分区正在直播的主播\r\n\r\n    # [English]\r\n    ### Purpose:\r\n    - Get live streamers of specified live area\r\n    ### Parameters:\r\n    - area_id: Live area ID\r\n    - pn: Page number\r\n    ### Return:\r\n    - live streamers of specified live area\r\n\r\n    # [示例/Example]\r\n    area_id = \"9\"\r\n    pn = 1\r\n    \"\"\"\r\n    try:\r\n        data = await BilibiliWebCrawler.fetch_live_streamers(area_id, pn)\r\n        return ResponseModel(code=200,\r\n                             router=request.url.path,\r\n                             data=data)\r\n    except Exception as e:\r\n        status_code = 400\r\n        detail = ErrorResponseModel(code=status_code,\r\n                                    router=request.url.path,\r\n                                    params=dict(request.query_params),\r\n                                    )\r\n        raise HTTPException(status_code=status_code, detail=detail.dict())\r\n\r\n\r\n# 获取所有直播分区列表\r\n@router.get(\"/fetch_all_live_areas\", response_model=ResponseModel,\r\n            summary=\"获取所有直播分区列表/Get a list of all live areas\")\r\nasync def fetch_collect_folders(request: Request,):\r\n    \"\"\"\r\n    # [中文]\r\n    ### 用途:\r\n    - 获取所有直播分区列表\r\n    ### 参数:\r\n    ### 返回:\r\n    - 所有直播分区列表\r\n\r\n    # [English]\r\n    ### Purpose:\r\n    - Get a list of all live areas\r\n    ### Parameters:\r\n    ### Return:\r\n    - list of all live areas\r\n\r\n    # [示例/Example]\r\n    \"\"\"\r\n    try:\r\n        data = await BilibiliWebCrawler.fetch_all_live_areas()\r\n        return ResponseModel(code=200,\r\n                             router=request.url.path,\r\n                             data=data)\r\n    except Exception as e:\r\n        status_code = 400\r\n        detail = ErrorResponseModel(code=status_code,\r\n                                    router=request.url.path,\r\n                                    params=dict(request.query_params),\r\n                                    )\r\n        raise HTTPException(status_code=status_code, detail=detail.dict())\r\n\r\n\r\n# 通过bv号获得视频aid号\r\n@router.get(\"/bv_to_aid\", response_model=ResponseModel, summary=\"通过bv号获得视频aid号/Generate aid by bvid\")\r\nasync def fetch_one_video(request: Request,\r\n                          bv_id: str = Query(example=\"BV1M1421t7hT\", description=\"作品id/Video id\")):\r\n    \"\"\"\r\n    # [中文]\r\n    ### 用途:\r\n    - 通过bv号获得视频aid号\r\n    ### 参数:\r\n    - bv_id: 作品id\r\n    ### 返回:\r\n    - 视频aid号\r\n\r\n    # [English]\r\n    ### Purpose:\r\n    - Generate aid by bvid\r\n    ### Parameters:\r\n    - bv_id: Video id\r\n    ### Return:\r\n    - Video aid\r\n\r\n    # [示例/Example]\r\n    bv_id = \"BV1M1421t7hT\"\r\n    \"\"\"\r\n    try:\r\n        data = await BilibiliWebCrawler.bv_to_aid(bv_id)\r\n        return ResponseModel(code=200,\r\n                             router=request.url.path,\r\n                             data=data)\r\n    except Exception as e:\r\n        status_code = 400\r\n        detail = ErrorResponseModel(code=status_code,\r\n                                    router=request.url.path,\r\n                                    params=dict(request.query_params),\r\n                                    )\r\n        raise HTTPException(status_code=status_code, detail=detail.dict())\r\n\r\n\r\n# 通过bv号获得视频分p信息\r\n@router.get(\"/fetch_video_parts\", response_model=ResponseModel, summary=\"通过bv号获得视频分p信息/Get Video Parts By bvid\")\r\nasync def fetch_one_video(request: Request,\r\n                          bv_id: str = Query(example=\"BV1vf421i7hV\", description=\"作品id/Video id\")):\r\n    \"\"\"\r\n    # [中文]\r\n    ### 用途:\r\n    - 通过bv号获得视频分p信息\r\n    ### 参数:\r\n    - bv_id: 作品id\r\n    ### 返回:\r\n    - 视频分p信息\r\n\r\n    # [English]\r\n    ### Purpose:\r\n    - Get Video Parts By bvid\r\n    ### Parameters:\r\n    - bv_id: Video id\r\n    ### Return:\r\n    - Video Parts\r\n\r\n    # [示例/Example]\r\n    bv_id = \"BV1vf421i7hV\"\r\n    \"\"\"\r\n    try:\r\n        data = await BilibiliWebCrawler.fetch_video_parts(bv_id)\r\n        return ResponseModel(code=200,\r\n                             router=request.url.path,\r\n                             data=data)\r\n    except Exception as e:\r\n        status_code = 400\r\n        detail = ErrorResponseModel(code=status_code,\r\n                                    router=request.url.path,\r\n                                    params=dict(request.query_params),\r\n                                    )\r\n        raise HTTPException(status_code=status_code, detail=detail.dict())\r\n", "metadata": {"source": "github.com/Evil0ctal/Douyin_TikTok_Download_API", "file_path": "app/api/endpoints/bilibili_web.py", "language": "python", "complexity": "complex", "star_count": 13591, "quality_score": 0.9500000000000001, "features": {"has_classes": false, "has_functions": true, "has_async": true, "has_decorators": true, "has_imports": true, "has_error_handling": true, "has_documentation": true, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 17, "class_count": 0, "line_count": 698}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create REST API endpoint with proper request/response handling with asyncio coroutines\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from typing import List\n\nfrom fastapi import APIRouter, Body, Query, Request, HTTPException  # 导入FastAPI组件\nfrom app.api.models.APIResponseModel import ResponseModel, ErrorResponseModel  # 导入响应模型\n\nfrom crawlers.douyin.web.web_crawler import DouyinWebCrawler  # 导入抖音Web爬虫\n\n\nrouter = APIRouter()\nDouyinWebCrawler = DouyinWebCrawler()\n\n\n# 获取单个作品数据\n@router.get(\"/fetch_one_video\", response_model=ResponseModel, summary=\"获取单个作品数据/Get single video data\")\nasync def fetch_one_video(request: Request,\n                          aweme_id: str = Query(example=\"7372484719365098803\", description=\"作品id/Video id\")):\n    \"\"\"\n    # [中文]\n    ### 用途:\n    - 获取单个作品数据\n    ### 参数:\n    - aweme_id: 作品id\n    ### 返回:\n    - 作品数据\n\n    # [English]\n    ### Purpose:\n    - Get single video data\n    ### Parameters:\n    - aweme_id: Video id\n    ### Return:\n    - Video data\n\n    # [示例/Example]\n    aweme_id = \"7372484719365098803\"\n    \"\"\"\n    try:\n        data = await DouyinWebCrawler.fetch_one_video(aweme_id)\n        return ResponseModel(code=200,\n                             router=request.url.path,\n                             data=data)\n    except Exception as e:\n        status_code = 400\n        detail = ErrorResponseModel(code=status_code,\n                                    router=request.url.path,\n                                    params=dict(request.query_params),\n                                    )\n        raise HTTPException(status_code=status_code, detail=detail.dict())\n\n\n# 获取用户作品集合数据\n@router.get(\"/fetch_user_post_videos\", response_model=ResponseModel,\n            summary=\"获取用户主页作品数据/Get user homepage video data\")\nasync def fetch_user_post_videos(request: Request,\n                                 sec_user_id: str = Query(\n                                     example=\"MS4wLjABAAAANXSltcLCzDGmdNFI2Q_QixVTr67NiYzjKOIP5s03CAE\",\n                                     description=\"用户sec_user_id/User sec_user_id\"),\n                                 max_cursor: int = Query(default=0, description=\"最大游标/Maximum cursor\"),\n                                 count: int = Query(default=20, description=\"每页数量/Number per page\")):\n    \"\"\"\n    # [中文]\n    ### 用途:\n    - 获取用户主页作品数据\n    ### 参数:\n    - sec_user_id: 用户sec_user_id\n    - max_cursor: 最大游标\n    - count: 最大数量\n    ### 返回:\n    - 用户作品数据\n\n    # [English]\n    ### Purpose:\n    - Get user homepage video data\n    ### Parameters:\n    - sec_user_id: User sec_user_id\n    - max_cursor: Maximum cursor\n    - count: Maximum count number\n    ### Return:\n    - User video data\n\n    # [示例/Example]\n    sec_user_id = \"MS4wLjABAAAANXSltcLCzDGmdNFI2Q_QixVTr67NiYzjKOIP5s03CAE\"\n    max_cursor = 0\n    counts = 20\n    \"\"\"\n    try:\n        data = await DouyinWebCrawler.fetch_user_post_videos(sec_user_id, max_cursor, count)\n        return ResponseModel(code=200,\n                             router=request.url.path,\n                             data=data)\n    except Exception as e:\n        status_code = 400\n        detail = ErrorResponseModel(code=status_code,\n                                    router=request.url.path,\n                                    params=dict(request.query_params),\n                                    )\n        raise HTTPException(status_code=status_code, detail=detail.dict())\n\n\n# 获取用户喜欢作品数据\n@router.get(\"/fetch_user_like_videos\", response_model=ResponseModel,\n            summary=\"获取用户喜欢作品数据/Get user like video data\")\nasync def fetch_user_like_videos(request: Request,\n                                 sec_user_id: str = Query(\n                                     example=\"MS4wLjABAAAAW9FWcqS7RdQAWPd2AA5fL_ilmqsIFUCQ_Iym6Yh9_cUa6ZRqVLjVQSUjlHrfXY1Y\",\n                                     description=\"用户sec_user_id/User sec_user_id\"),\n                                 max_cursor: int = Query(default=0, description=\"最大游标/Maximum cursor\"),\n                                 counts: int = Query(default=20, description=\"每页数量/Number per page\")):\n    \"\"\"\n    # [中文]\n    ### 用途:\n    - 获取用户喜欢作品数据\n    ### 参数:\n    - sec_user_id: 用户sec_user_id\n    - max_cursor: 最大游标\n    - count: 最大数量\n    ### 返回:\n    - 用户作品数据\n\n    # [English]\n    ### Purpose:\n    - Get user like video data\n    ### Parameters:\n    - sec_user_id: User sec_user_id\n    - max_cursor: Maximum cursor\n    - count: Maximum count number\n    ### Return:\n    - User video data\n\n    # [示例/Example]\n    sec_user_id = \"MS4wLjABAAAAW9FWcqS7RdQAWPd2AA5fL_ilmqsIFUCQ_Iym6Yh9_cUa6ZRqVLjVQSUjlHrfXY1Y\"\n    max_cursor = 0\n    counts = 20\n    \"\"\"\n    try:\n        data = await DouyinWebCrawler.fetch_user_like_videos(sec_user_id, max_cursor, counts)\n        return ResponseModel(code=200,\n                             router=request.url.path,\n                             data=data)\n    except Exception as e:\n        status_code = 400\n        detail = ErrorResponseModel(code=status_code,\n                                    router=request.url.path,\n                                    params=dict(request.query_params),\n                                    )\n        raise HTTPException(status_code=status_code, detail=detail.dict())\n\n\n# 获取用户收藏作品数据（用户提供自己的Cookie）\n@router.get(\"/fetch_user_collection_videos\", response_model=ResponseModel,\n            summary=\"获取用户收藏作品数据/Get user collection video data\")\nasync def fetch_user_collection_videos(request: Request,\n                                       cookie: str = Query(example=\"YOUR_COOKIE\",\n                                                           description=\"用户网页版抖音Cookie/Your web version of Douyin Cookie\"),\n                                       max_cursor: int = Query(default=0, description=\"最大游标/Maximum cursor\"),\n                                       counts: int = Query(default=20, description=\"每页数量/Number per page\")):\n    \"\"\"\n    # [中文]\n    ### 用途:\n    - 获取用户收藏作品数据\n    ### 参数:\n    - cookie: 用户网页版抖音Cookie(此接口需要用户提供自己的Cookie)\n    - max_cursor: 最大游标\n    - count: 最大数量\n    ### 返回:\n    - 用户作品数据\n\n    # [English]\n    ### Purpose:\n    - Get user collection video data\n    ### Parameters:\n    - cookie: User's web version of Douyin Cookie (This interface requires users to provide their own Cookie)\n    - max_cursor: Maximum cursor\n    - count: Maximum number\n    ### Return:\n    - User video data\n\n    # [示例/Example]\n    cookie = \"YOUR_COOKIE\"\n    max_cursor = 0\n    counts = 20\n    \"\"\"\n    try:\n        data = await DouyinWebCrawler.fetch_user_collection_videos(cookie, max_cursor, counts)\n        return ResponseModel(code=200,\n                             router=request.url.path,\n                             data=data)\n    except Exception as e:\n        status_code = 400\n        detail = ErrorResponseModel(code=status_code,\n                                    router=request.url.path,\n                                    params=dict(request.query_params),\n                                    )\n        raise HTTPException(status_code=status_code, detail=detail.dict())\n\n\n# 获取用户合辑作品数据\n@router.get(\"/fetch_user_mix_videos\", response_model=ResponseModel,\n            summary=\"获取用户合辑作品数据/Get user mix video data\")\nasync def fetch_user_mix_videos(request: Request,\n                                mix_id: str = Query(example=\"7348687990509553679\", description=\"合辑id/Mix id\"),\n                                max_cursor: int = Query(default=0, description=\"最大游标/Maximum cursor\"),\n                                counts: int = Query(default=20, description=\"每页数量/Number per page\")):\n    \"\"\"\n    # [中文]\n    ### 用途:\n    - 获取用户合辑作品数据\n    ### 参数:\n    - mix_id: 合辑id\n    - max_cursor: 最大游标\n    - count: 最大数量\n    ### 返回:\n    - 用户作品数据\n\n    # [English]\n    ### Purpose:\n    - Get user mix video data\n    ### Parameters:\n    - mix_id: Mix id\n    - max_cursor: Maximum cursor\n    - count: Maximum number\n    ### Return:\n    - User video data\n\n    # [示例/Example]\n    url = https://www.douyin.com/collection/7348687990509553679\n    mix_id = \"7348687990509553679\"\n    max_cursor = 0\n    counts = 20\n    \"\"\"\n    try:\n        data = await DouyinWebCrawler.fetch_user_mix_videos(mix_id, max_cursor, counts)\n        return ResponseModel(code=200,\n                             router=request.url.path,\n                             data=data)\n    except Exception as e:\n        status_code = 400\n        detail = ErrorResponseModel(code=status_code,\n                                    router=request.url.path,\n                                    params=dict(request.query_params),\n                                    )\n        raise HTTPException(status_code=status_code, detail=detail.dict())\n\n\n# 获取用户直播流数据\n@router.get(\"/fetch_user_live_videos\", response_model=ResponseModel,\n            summary=\"获取用户直播流数据/Get user live video data\")\nasync def fetch_user_live_videos(request: Request,\n                                 webcast_id: str = Query(example=\"285520721194\",\n                                                         description=\"直播间webcast_id/Room webcast_id\")):\n    \"\"\"\n    # [中文]\n    ### 用途:\n    - 获取用户直播流数据\n    ### 参数:\n    - webcast_id: 直播间webcast_id\n    ### 返回:\n    - 直播流数据\n\n    # [English]\n    ### Purpose:\n    - Get user live video data\n    ### Parameters:\n    - webcast_id: Room webcast_id\n    ### Return:\n    - Live stream data\n\n    # [示例/Example]\n    webcast_id = \"285520721194\"\n    \"\"\"\n    try:\n        data = await DouyinWebCrawler.fetch_user_live_videos(webcast_id)\n        return ResponseModel(code=200,\n                             router=request.url.path,\n                             data=data)\n    except Exception as e:\n        status_code = 400\n        detail = ErrorResponseModel(code=status_code,\n                                    router=request.url.path,\n                                    params=dict(request.query_params),\n                                    )\n        raise HTTPException(status_code=status_code, detail=detail.dict())\n\n\n# 获取指定用户的直播流数据\n@router.get(\"/fetch_user_live_videos_by_room_id\",\n            response_model=ResponseModel,\n            summary=\"获取指定用户的直播流数据/Get live video data of specified user\")\nasync def fetch_user_live_videos_by_room_id(request: Request,\n                                            room_id: str = Query(example=\"7318296342189919011\",\n                                                                 description=\"直播间room_id/Room room_id\")):\n    \"\"\"\n    # [中文]\n    ### 用途:\n    - 获取指定用户的直播流数据\n    ### 参数:\n    - room_id: 直播间room_id\n    ### 返回:\n    - 直播流数据\n\n    # [English]\n    ### Purpose:\n    - Get live video data of specified user\n    ### Parameters:\n    - room_id: Room room_id\n    ### Return:\n    - Live stream data\n\n    # [示例/Example]\n    room_id = \"7318296342189919011\"\n    \"\"\"\n    try:\n        data = await DouyinWebCrawler.fetch_user_live_videos_by_room_id(room_id)\n        return ResponseModel(code=200,\n                             router=request.url.path,\n                             data=data)\n    except Exception as e:\n        status_code = 400\n        detail = ErrorResponseModel(code=status_code,\n                                    router=request.url.path,\n                                    params=dict(request.query_params),\n                                    )\n        raise HTTPException(status_code=status_code, detail=detail.dict())\n\n\n# 获取直播间送礼用户排行榜\n@router.get(\"/fetch_live_gift_ranking\",\n            response_model=ResponseModel,\n            summary=\"获取直播间送礼用户排行榜/Get live room gift user ranking\")\nasync def fetch_live_gift_ranking(request: Request,\n                                  room_id: str = Query(example=\"7356585666190461731\",\n                                                       description=\"直播间room_id/Room room_id\"),\n                                  rank_type: int = Query(default=30, description=\"排行类型/Leaderboard type\")):\n    \"\"\"\n    # [中文]\n    ### 用途:\n    - 获取直播间送礼用户排行榜\n    ### 参数:\n    - room_id: 直播间room_id\n    - rank_type: 排行类型，默认为30不用修改。\n    ### 返回:\n    - 排行榜数据\n\n    # [English]\n    ### Purpose:\n    - Get live room gift user ranking\n    ### Parameters:\n    - room_id: Room room_id\n    - rank_type: Leaderboard type, default is 30, no need to modify.\n    ### Return:\n    - Leaderboard data\n\n    # [示例/Example]\n    room_id = \"7356585666190461731\"\n    rank_type = 30\n    \"\"\"\n    try:\n        data = await DouyinWebCrawler.fetch_live_gift_ranking(room_id, rank_type)\n        return ResponseModel(code=200,\n                             router=request.url.path,\n                             data=data)\n    except Exception as e:\n        status_code = 400\n        detail = ErrorResponseModel(code=status_code,\n                                    router=request.url.path,\n                                    params=dict(request.query_params),\n                                    )\n        raise HTTPException(status_code=status_code, detail=detail.dict())\n\n\n# 抖音直播间商品信息\n@router.get(\"/fetch_live_room_product_result\",\n            response_model=ResponseModel,\n            summary=\"抖音直播间商品信息/Douyin live room product information\")\nasync def fetch_live_room_product_result(request: Request,\n                                         cookie: str = Query(example=\"YOUR_COOKIE\",\n                                                             description=\"用户网页版抖音Cookie/Your web version of Douyin Cookie\"),\n                                         room_id: str = Query(example=\"7356742011975715619\",\n                                                              description=\"直播间room_id/Room room_id\"),\n                                         author_id: str = Query(example=\"2207432981615527\",\n                                                                description=\"作者id/Author id\"),\n                                         limit: int = Query(default=20, description=\"数量/Number\")):\n    \"\"\"\n    # [中文]\n    ### 用途:\n    - 抖音直播间商品信息\n    ### 参数:\n    - cookie: 用户网页版抖音Cookie(此接口需要用户提供自己的Cookie，如获取失败请手动过一次验证码)\n    - room_id: 直播间room_id\n    - author_id: 作者id\n    - limit: 数量\n    ### 返回:\n    - 商品信息\n\n    # [English]\n    ### Purpose:\n    - Douyin live room product information\n    ### Parameters:\n    - cookie: User's web version of Douyin Cookie (This interface requires users to provide their own Cookie, if the acquisition fails, please manually pass the captcha code once)\n    - room_id: Room room_id\n    - author_id: Author id\n    - limit: Number\n    ### Return:\n    - Product information\n\n    # [示例/Example]\n    cookie = \"YOUR_COOKIE\"\n    room_id = \"7356742011975715619\"\n    author_id = \"2207432981615527\"\n    limit = 20\n    \"\"\"\n    try:\n        data = await DouyinWebCrawler.fetch_live_room_product_result(cookie, room_id, author_id, limit)\n        return ResponseModel(code=200,\n                             router=request.url.path,\n                             data=data)\n    except Exception as e:\n        status_code = 400\n        detail = ErrorResponseModel(code=status_code,\n                                    router=request.url.path,\n                                    params=dict(request.query_params),\n                                    )\n        raise HTTPException(status_code=status_code, detail=detail.dict())\n\n\n# 获取指定用户的信息\n@router.get(\"/handler_user_profile\",\n            response_model=ResponseModel,\n            summary=\"获取指定用户的信息/Get information of specified user\")\nasync def handler_user_profile(request: Request,\n                               sec_user_id: str = Query(\n                                   example=\"MS4wLjABAAAAW9FWcqS7RdQAWPd2AA5fL_ilmqsIFUCQ_Iym6Yh9_cUa6ZRqVLjVQSUjlHrfXY1Y\",\n                                   description=\"用户sec_user_id/User sec_user_id\")):\n    \"\"\"\n    # [中文]\n    ### 用途:\n    - 获取指定用户的信息\n    ### 参数:\n    - sec_user_id: 用户sec_user_id\n    ### 返回:\n    - 用户信息\n\n    # [English]\n    ### Purpose:\n    - Get information of specified user\n    ### Parameters:\n    - sec_user_id: User sec_user_id\n    ### Return:\n    - User information\n\n    # [示例/Example]\n    sec_user_id = \"MS4wLjABAAAAW9FWcqS7RdQAWPd2AA5fL_ilmqsIFUCQ_Iym6Yh9_cUa6ZRqVLjVQSUjlHrfXY1Y\"\n    \"\"\"\n    try:\n        data = await DouyinWebCrawler.handler_user_profile(sec_user_id)\n        return ResponseModel(code=200,\n                             router=request.url.path,\n                             data=data)\n    except Exception as e:\n        status_code = 400\n        detail = ErrorResponseModel(code=status_code,\n                                    router=request.url.path,\n                                    params=dict(request.query_params),\n                                    )\n        raise HTTPException(status_code=status_code, detail=detail.dict())\n\n\n# 获取单个视频评论数据\n@router.get(\"/fetch_video_comments\",\n            response_model=ResponseModel,\n            summary=\"获取单个视频评论数据/Get single video comments data\")\nasync def fetch_video_comments(request: Request,\n                               aweme_id: str = Query(example=\"7372484719365098803\", description=\"作品id/Video id\"),\n                               cursor: int = Query(default=0, description=\"游标/Cursor\"),\n                               count: int = Query(default=20, description=\"数量/Number\")):\n    \"\"\"\n    # [中文]\n    ### 用途:\n    - 获取单个视频评论数据\n    ### 参数:\n    - aweme_id: 作品id\n    - cursor: 游标\n    - count: 数量\n    ### 返回:\n    - 评论数据\n\n    # [English]\n    ### Purpose:\n    - Get single video comments data\n    ### Parameters:\n    - aweme_id: Video id\n    - cursor: Cursor\n    - count: Number\n    ### Return:\n    - Comments data\n\n    # [示例/Example]\n    aweme_id = \"7372484719365098803\"\n    cursor = 0\n    count = 20\n    \"\"\"\n    try:\n        data = await DouyinWebCrawler.fetch_video_comments(aweme_id, cursor, count)\n        return ResponseModel(code=200,\n                             router=request.url.path,\n                             data=data)\n    except Exception as e:\n        status_code = 400\n        detail = ErrorResponseModel(code=status_code,\n                                    router=request.url.path,\n                                    params=dict(request.query_params),\n                                    )\n        raise HTTPException(status_code=status_code, detail=detail.dict())\n\n\n# 获取指定视频的评论回复数据\n@router.get(\"/fetch_video_comment_replies\",\n            response_model=ResponseModel,\n            summary=\"获取指定视频的评论回复数据/Get comment replies data of specified video\")\nasync def fetch_video_comments_reply(request: Request,\n                                     item_id: str = Query(example=\"7354666303006723354\", description=\"作品id/Video id\"),\n                                     comment_id: str = Query(example=\"7354669356632638218\",\n                                                             description=\"评论id/Comment id\"),\n                                     cursor: int = Query(default=0, description=\"游标/Cursor\"),\n                                     count: int = Query(default=20, description=\"数量/Number\")):\n    \"\"\"\n    # [中文]\n    ### 用途:\n    - 获取指定视频的评论回复数据\n    ### 参数:\n    - item_id: 作品id\n    - comment_id: 评论id\n    - cursor: 游标\n    - count: 数量\n    ### 返回:\n    - 评论回复数据\n\n    # [English]\n    ### Purpose:\n    - Get comment replies data of specified video\n    ### Parameters:\n    - item_id: Video id\n    - comment_id: Comment id\n    - cursor: Cursor\n    - count: Number\n    ### Return:\n    - Comment replies data\n\n    # [示例/Example]\n    aweme_id = \"7354666303006723354\"\n    comment_id = \"7354669356632638218\"\n    cursor = 0\n    count = 20\n    \"\"\"\n    try:\n        data = await DouyinWebCrawler.fetch_video_comments_reply(item_id, comment_id, cursor, count)\n        return ResponseModel(code=200,\n                             router=request.url.path,\n                             data=data)\n    except Exception as e:\n        status_code = 400\n        detail = ErrorResponseModel(code=status_code,\n                                    router=request.url.path,\n                                    params=dict(request.query_params),\n                                    )\n        raise HTTPException(status_code=status_code, detail=detail.dict())\n\n\n# 生成真实msToken\n@router.get(\"/generate_real_msToken\",\n            response_model=ResponseModel,\n            summary=\"生成真实msToken/Generate real msToken\")\nasync def generate_real_msToken(request: Request):\n    \"\"\"\n    # [中文]\n    ### 用途:\n    - 生成真实msToken\n    ### 返回:\n    - msToken\n\n    # [English]\n    ### Purpose:\n    - Generate real msToken\n    ### Return:\n    - msToken\n    \"\"\"\n    try:\n        data = await DouyinWebCrawler.gen_real_msToken()\n        return ResponseModel(code=200,\n                             router=request.url.path,\n                             data=data)\n    except Exception as e:\n        status_code = 400\n        detail = ErrorResponseModel(code=status_code,\n                                    router=request.url.path,\n                                    params=dict(request.query_params),\n                                    )\n        raise HTTPException(status_code=status_code, detail=detail.dict())\n\n\n# 生成ttwid\n@router.get(\"/generate_ttwid\",\n            response_model=ResponseModel,\n            summary=\"生成ttwid/Generate ttwid\")\nasync def generate_ttwid(request: Request):\n    \"\"\"\n    # [中文]\n    ### 用途:\n    - 生成ttwid\n    ### 返回:\n    - ttwid\n\n    # [English]\n    ### Purpose:\n    - Generate ttwid\n    ### Return:\n    - ttwid\n    \"\"\"\n    try:\n        data = await DouyinWebCrawler.gen_ttwid()\n        return ResponseModel(code=200,\n                             router=request.url.path,\n                             data=data)\n    except Exception as e:\n        status_code = 400\n        detail = ErrorResponseModel(code=status_code,\n                                    router=request.url.path,\n                                    params=dict(request.query_params),\n                                    )\n        raise HTTPException(status_code=status_code, detail=detail.dict())\n\n\n# 生成verify_fp\n@router.get(\"/generate_verify_fp\",\n            response_model=ResponseModel,\n            summary=\"生成verify_fp/Generate verify_fp\")\nasync def generate_verify_fp(request: Request):\n    \"\"\"\n    # [中文]\n    ### 用途:\n    - 生成verify_fp\n    ### 返回:\n    - verify_fp\n\n    # [English]\n    ### Purpose:\n    - Generate verify_fp\n    ### Return:\n    - verify_fp\n    \"\"\"\n    try:\n        data = await DouyinWebCrawler.gen_verify_fp()\n        return ResponseModel(code=200,\n                             router=request.url.path,\n                             data=data)\n    except Exception as e:\n        status_code = 400\n        detail = ErrorResponseModel(code=status_code,\n                                    router=request.url.path,\n                                    params=dict(request.query_params),\n                                    )\n        raise HTTPException(status_code=status_code, detail=detail.dict())\n\n\n# 生成s_v_web_id\n@router.get(\"/generate_s_v_web_id\",\n            response_model=ResponseModel,\n            summary=\"生成s_v_web_id/Generate s_v_web_id\")\nasync def generate_s_v_web_id(request: Request):\n    \"\"\"\n    # [中文]\n    ### 用途:\n    - 生成s_v_web_id\n    ### 返回:\n    - s_v_web_id\n\n    # [English]\n    ### Purpose:\n    - Generate s_v_web_id\n    ### Return:\n    - s_v_web_id\n    \"\"\"\n    try:\n        data = await DouyinWebCrawler.gen_s_v_web_id()\n        return ResponseModel(code=200,\n                             router=request.url.path,\n                             data=data)\n    except Exception as e:\n        status_code = 400\n        detail = ErrorResponseModel(code=status_code,\n                                    router=request.url.path,\n                                    params=dict(request.query_params),\n                                    )\n        raise HTTPException(status_code=status_code, detail=detail.dict())\n\n\n# 使用接口地址生成Xbogus参数\n@router.get(\"/generate_x_bogus\",\n            response_model=ResponseModel,\n            summary=\"使用接口网址生成X-Bogus参数/Generate X-Bogus parameter using API URL\")\nasync def generate_x_bogus(request: Request,\n                           url: str = Query(\n                               example=\"https://www.douyin.com/aweme/v1/web/aweme/detail/?aweme_id=7148736076176215311&device_platform=webapp&aid=6383&channel=channel_pc_web&pc_client_type=1&version_code=170400&version_name=17.4.0&cookie_enabled=true&screen_width=1920&screen_height=1080&browser_language=zh-CN&browser_platform=Win32&browser_name=Edge&browser_version=117.0.2045.47&browser_online=true&engine_name=Blink&engine_version=\"),\n                           user_agent: str = Query(\n                               example=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.212 Safari/537.36\")):\n    \"\"\"\n    # [中文]\n    ### 用途:\n    - 使用接口网址生成X-Bogus参数\n    ### 参数:\n    - url: 接口网址\n\n    # [English]\n    ### Purpose:\n    - Generate X-Bogus parameter using API URL\n    ### Parameters:\n    - url: API URL\n\n    # [示例/Example]\n    url = \"https://www.douyin.com/aweme/v1/web/aweme/detail/?aweme_id=7148736076176215311&device_platform=webapp&aid=6383&channel=channel_pc_web&pc_client_type=1&version_code=170400&version_name=17.4.0&cookie_enabled=true&screen_width=1920&screen_height=1080&browser_language=zh-CN&browser_platform=Win32&browser_name=Edge&browser_version=117.0.2045.47&browser_online=true&engine_name=Blink&engine_version=117.0.0.0&os_name=Windows&os_version=10&cpu_core_num=128&device_memory=10240&platform=PC&downlink=10&effective_type=4g&round_trip_time=100\"\n    user_agent = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.212 Safari/537.36\"\n    \"\"\"\n    try:\n        x_bogus = await DouyinWebCrawler.get_x_bogus(url, user_agent)\n        return ResponseModel(code=200,\n                             router=request.url.path,\n                             data=x_bogus)\n    except Exception as e:\n        status_code = 400\n        detail = ErrorResponseModel(code=status_code,\n                                    router=request.url.path,\n                                    params=dict(request.query_params),\n                                    )\n        raise HTTPException(status_code=status_code, detail=detail.dict())\n\n\n# 使用接口地址生成Abogus参数\n@router.get(\"/generate_a_bogus\",\n            response_model=ResponseModel,\n            summary=\"使用接口网址生成A-Bogus参数/Generate A-Bogus parameter using API URL\")\nasync def generate_a_bogus(request: Request,\n                           url: str = Query(\n                               example=\"https://www.douyin.com/aweme/v1/web/aweme/detail/?device_platform=webapp&aid=6383&channel=channel_pc_web&pc_client_type=1&version_code=190500&version_name=19.5.0&cookie_enabled=true&browser_language=zh-CN&browser_platform=Win32&browser_name=Firefox&browser_online=true&engine_name=Gecko&os_name=Windows&os_version=10&platform=PC&screen_width=1920&screen_height=1080&browser_version=124.0&engine_version=122.0.0.0&cpu_core_num=12&device_memory=8&aweme_id=7372484719365098803\"),\n                           user_agent: str = Query(\n                               example=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.212 Safari/537.36\")):\n    \"\"\"\n    # [中文]\n    ### 用途:\n    - 使用接口网址生成A-Bogus参数\n    ### 参数:\n    - url: 接口网址\n    - user_agent: 用户代理，暂时不支持自定义，直接使用默认值即可。\n\n    # [English]\n    ### Purpose:\n    - Generate A-Bogus parameter using API URL\n    ### Parameters:\n    - url: API URL\n    - user_agent: User agent, temporarily does not support customization, just use the default value.\n\n    # [示例/Example]\n    url = \"https://www.douyin.com/aweme/v1/web/aweme/detail/?device_platform=webapp&aid=6383&channel=channel_pc_web&pc_client_type=1&version_code=190500&version_name=19.5.0&cookie_enabled=true&browser_language=zh-CN&browser_platform=Win32&browser_name=Firefox&browser_online=true&engine_name=Gecko&os_name=Windows&os_version=10&platform=PC&screen_width=1920&screen_height=1080&browser_version=124.0&engine_version=122.0.0.0&cpu_core_num=12&device_memory=8&aweme_id=7372484719365098803\"\n    user_agent = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.212 Safari/537.36\"\n    \"\"\"\n    try:\n        a_bogus = await DouyinWebCrawler.get_a_bogus(url, user_agent)\n        return ResponseModel(code=200,\n                             router=request.url.path,\n                             data=a_bogus)\n    except Exception as e:\n        status_code = 400\n        detail = ErrorResponseModel(code=status_code,\n                                    router=request.url.path,\n                                    params=dict(request.query_params),\n                                    )\n        raise HTTPException(status_code=status_code, detail=detail.dict())\n\n\n# 提取单个用户id\n@router.get(\"/get_sec_user_id\",\n            response_model=ResponseModel,\n            summary=\"提取单个用户id/Extract single user id\")\nasync def get_sec_user_id(request: Request,\n                          url: str = Query(\n                              example=\"https://www.douyin.com/user/MS4wLjABAAAANXSltcLCzDGmdNFI2Q_QixVTr67NiYzjKOIP5s03CAE\")):\n    \"\"\"\n    # [中文]\n    ### 用途:\n    - 提取单个用户id\n    ### 参数:\n    - url: 用户主页链接\n    ### 返回:\n    - 用户sec_user_id\n\n    # [English]\n    ### Purpose:\n    - Extract single user id\n    ### Parameters:\n    - url: User homepage link\n    ### Return:\n    - User sec_user_id\n\n    # [示例/Example]\n    url = \"https://www.douyin.com/user/MS4wLjABAAAANXSltcLCzDGmdNFI2Q_QixVTr67NiYzjKOIP5s03CAE\"\n    \"\"\"\n    try:\n        data = await DouyinWebCrawler.get_sec_user_id(url)\n        return ResponseModel(code=200,\n                             router=request.url.path,\n                             data=data)\n    except Exception as e:\n        status_code = 400\n        detail = ErrorResponseModel(code=status_code,\n                                    router=request.url.path,\n                                    params=dict(request.query_params),\n                                    )\n        raise HTTPException(status_code=status_code, detail=detail.dict())\n\n\n# 提取列表用户id\n@router.post(\"/get_all_sec_user_id\",\n             response_model=ResponseModel,\n             summary=\"提取列表用户id/Extract list user id\")\nasync def get_all_sec_user_id(request: Request,\n                              url: List[str] = Body(\n                                  example=[\n                                      \"https://www.douyin.com/user/MS4wLjABAAAANXSltcLCzDGmdNFI2Q_QixVTr67NiYzjKOIP5s03CAE?vid=7285950278132616463\",\n                                      \"https://www.douyin.com/user/MS4wLjABAAAAVsneOf144eGDFf8Xp9QNb1VW6ovXnNT5SqJBhJfe8KQBKWKDTWK5Hh-_i9mJzb8C\",\n                                      \"长按复制此条消息，打开抖音搜索，查看TA的更多作品。 https://v.douyin.com/idFqvUms/\",\n                                      \"https://v.douyin.com/idFqvUms/\",\n                                  ],\n                                  description=\"用户主页链接列表/User homepage link list\"\n                              )):\n    \"\"\"\n    # [中文]\n    ### 用途:\n    - 提取列表用户id\n    ### 参数:\n    - url: 用户主页链接列表\n    ### 返回:\n    - 用户sec_user_id列表\n\n    # [English]\n    ### Purpose:\n    - Extract list user id\n    ### Parameters:\n    - url: User homepage link list\n    ### Return:\n    - User sec_user_id list\n\n    # [示例/Example]\n    ```json\n    {\n   \"urls\":[\n      \"https://www.douyin.com/user/MS4wLjABAAAANXSltcLCzDGmdNFI2Q_QixVTr67NiYzjKOIP5s03CAE?vid=7285950278132616463\",\n      \"https://www.douyin.com/user/MS4wLjABAAAAVsneOf144eGDFf8Xp9QNb1VW6ovXnNT5SqJBhJfe8KQBKWKDTWK5Hh-_i9mJzb8C\",\n      \"长按复制此条消息，打开抖音搜索，查看TA的更多作品。 https://v.douyin.com/idFqvUms/\",\n      \"https://v.douyin.com/idFqvUms/\"\n       ]\n    }\n    ```\n    \"\"\"\n    try:\n        data = await DouyinWebCrawler.get_all_sec_user_id(url)\n        return ResponseModel(code=200,\n                             router=request.url.path,\n                             data=data)\n    except Exception as e:\n        status_code = 400\n        detail = ErrorResponseModel(code=status_code,\n                                    router=request.url.path,\n                                    params=dict(request.query_params),\n                                    )\n        raise HTTPException(status_code=status_code, detail=detail.dict())\n\n\n# 提取单个作品id\n@router.get(\"/get_aweme_id\",\n            response_model=ResponseModel,\n            summary=\"提取单个作品id/Extract single video id\")\nasync def get_aweme_id(request: Request,\n                       url: str = Query(example=\"https://www.douyin.com/video/7298145681699622182\")):\n    \"\"\"\n    # [中文]\n    ### 用途:\n    - 提取单个作品id\n    ### 参数:\n    - url: 作品链接\n    ### 返回:\n    - 作品id\n\n    # [English]\n    ### Purpose:\n    - Extract single video id\n    ### Parameters:\n    - url: Video link\n    ### Return:\n    - Video id\n\n    # [示例/Example]\n    url = \"https://www.douyin.com/video/7298145681699622182\"\n    \"\"\"\n    try:\n        data = await DouyinWebCrawler.get_aweme_id(url)\n        return ResponseModel(code=200,\n                             router=request.url.path,\n                             data=data)\n    except Exception as e:\n        status_code = 400\n        detail = ErrorResponseModel(code=status_code,\n                                    router=request.url.path,\n                                    params=dict(request.query_params),\n                                    )\n        raise HTTPException(status_code=status_code, detail=detail.dict())\n\n\n# 提取列表作品id\n@router.post(\"/get_all_aweme_id\",\n             response_model=ResponseModel,\n             summary=\"提取列表作品id/Extract list video id\")\nasync def get_all_aweme_id(request: Request,\n                           url: List[str] = Body(\n                               example=[\n                                   \"0.53 02/26 I@v.sE Fus:/ 你别太帅了郑润泽# 现场版live # 音乐节 # 郑润泽  https://v.douyin.com/iRNBho6u/ 复制此链接，打开Dou音搜索，直接观看视频!\",\n                                   \"https://v.douyin.com/iRNBho6u/\",\n                                   \"https://www.iesdouyin.com/share/video/7298145681699622182/?region=CN&mid=7298145762238565171&u_code=l1j9bkbd&did=MS4wLjABAAAAtqpCx0hpOERbdSzQdjRZw-wFPxaqdbAzsKDmbJMUI3KWlMGQHC-n6dXAqa-dM2EP&iid=MS4wLjABAAAANwkJuWIRFOzg5uCpDRpMj4OX-QryoDgn-yYlXQnRwQQ&with_sec_did=1&titleType=title&share_sign=05kGlqGmR4_IwCX.ZGk6xuL0osNA..5ur7b0jbOx6cc-&share_version=170400&ts=1699262937&from_aid=6383&from_ssr=1&from=web_code_link\",\n                                   \"https://www.douyin.com/video/7298145681699622182?previous_page=web_code_link\",\n                                   \"https://www.douyin.com/video/7298145681699622182\",\n                               ],\n                               description=\"作品链接列表/Video link list\")):\n    \"\"\"\n    # [中文]\n    ### 用途:\n    - 提取列表作品id\n    ### 参数:\n    - url: 作品链接列表\n    ### 返回:\n    - 作品id列表\n\n    # [English]\n    ### Purpose:\n    - Extract list video id\n    ### Parameters:\n    - url: Video link list\n    ### Return:\n    - Video id list\n\n    # [示例/Example]\n    ```json\n    {\n   \"urls\":[\n       \"0.53 02/26 I@v.sE Fus:/ 你别太帅了郑润泽# 现场版live # 音乐节 # 郑润泽  https://v.douyin.com/iRNBho6u/ 复制此链接，打开Dou音搜索，直接观看视频!\",\n       \"https://v.douyin.com/iRNBho6u/\",\n       \"https://www.iesdouyin.com/share/video/7298145681699622182/?region=CN&mid=7298145762238565171&u_code=l1j9bkbd&did=MS4wLjABAAAAtqpCx0hpOERbdSzQdjRZw-wFPxaqdbAzsKDmbJMUI3KWlMGQHC-n6dXAqa-dM2EP&iid=MS4wLjABAAAANwkJuWIRFOzg5uCpDRpMj4OX-QryoDgn-yYlXQnRwQQ&with_sec_did=1&titleType=title&share_sign=05kGlqGmR4_IwCX.ZGk6xuL0osNA..5ur7b0jbOx6cc-&share_version=170400&ts=1699262937&from_aid=6383&from_ssr=1&from=web_code_link\",\n       \"https://www.douyin.com/video/7298145681699622182?previous_page=web_code_link\",\n       \"https://www.douyin.com/video/7298145681699622182\",\n    ]\n    }\n    ```\n    \"\"\"\n    try:\n        data = await DouyinWebCrawler.get_all_aweme_id(url)\n        return ResponseModel(code=200,\n                             router=request.url.path,\n                             data=data)\n    except Exception as e:\n        status_code = 400\n        detail = ErrorResponseModel(code=status_code,\n                                    router=request.url.path,\n                                    params=dict(request.query_params),\n                                    )\n        raise HTTPException(status_code=status_code, detail=detail.dict())\n\n\n# 提取列表直播间号\n@router.get(\"/get_webcast_id\",\n            response_model=ResponseModel,\n            summary=\"提取列表直播间号/Extract list webcast id\")\nasync def get_webcast_id(request: Request,\n                         url: str = Query(example=\"https://live.douyin.com/775841227732\")):\n    \"\"\"\n    # [中文]\n    ### 用途:\n    - 提取列表直播间号\n    ### 参数:\n    - url: 直播间链接\n    ### 返回:\n    - 直播间号\n\n    # [English]\n    ### Purpose:\n    - Extract list webcast id\n    ### Parameters:\n    - url: Room link\n    ### Return:\n    - Room id\n\n    # [示例/Example]\n    url = \"https://live.douyin.com/775841227732\"\n    \"\"\"\n    try:\n        data = await DouyinWebCrawler.get_webcast_id(url)\n        return ResponseModel(code=200,\n                             router=request.url.path,\n                             data=data)\n    except Exception as e:\n        status_code = 400\n        detail = ErrorResponseModel(code=status_code,\n                                    router=request.url.path,\n                                    params=dict(request.query_params),\n                                    )\n        raise HTTPException(status_code=status_code, detail=detail.dict())\n\n\n# 提取列表直播间号\n@router.post(\"/get_all_webcast_id\",\n             response_model=ResponseModel,\n             summary=\"提取列表直播间号/Extract list webcast id\")\nasync def get_all_webcast_id(request: Request,\n                             url: List[str] = Body(\n                                 example=[\n                                     \"https://live.douyin.com/775841227732\",\n                                     \"https://live.douyin.com/775841227732?room_id=7318296342189919011&enter_from_merge=web_share_link&enter_method=web_share_link&previous_page=app_code_link\",\n                                     'https://webcast.amemv.com/douyin/webcast/reflow/7318296342189919011?u_code=l1j9bkbd&did=MS4wLjABAAAAEs86TBQPNwAo-RGrcxWyCdwKhI66AK3Pqf3ieo6HaxI&iid=MS4wLjABAAAA0ptpM-zzoliLEeyvWOCUt-_dQza4uSjlIvbtIazXnCY&with_sec_did=1&use_link_command=1&ecom_share_track_params=&extra_params={\"from_request_id\":\"20231230162057EC005772A8EAA0199906\",\"im_channel_invite_id\":\"0\"}&user_id=3644207898042206&liveId=7318296342189919011&from=share&style=share&enter_method=click_share&roomId=7318296342189919011&activity_info={}',\n                                     \"6i- Q@x.Sl 03/23 【醒子8ke的直播间】  点击打开👉https://v.douyin.com/i8tBR7hX/  或长按复制此条消息，打开抖音，看TA直播\",\n                                     \"https://v.douyin.com/i8tBR7hX/\",\n                                 ],\n                                 description=\"直播间链接列表/Room link list\")):\n    \"\"\"\n    # [中文]\n    ### 用途:\n    - 提取列表直播间号\n    ### 参数:\n    - url: 直播间链接列表\n    ### 返回:\n    - 直播间号列表\n\n    # [English]\n    ### Purpose:\n    - Extract list webcast id\n    ### Parameters:\n    - url: Room link list\n    ### Return:\n    - Room id list\n\n    # [示例/Example]\n    ```json\n    {\n      \"urls\": [\n            \"https://live.douyin.com/775841227732\",\n            \"https://live.douyin.com/775841227732?room_id=7318296342189919011&enter_from_merge=web_share_link&enter_method=web_share_link&previous_page=app_code_link\",\n            'https://webcast.amemv.com/douyin/webcast/reflow/7318296342189919011?u_code=l1j9bkbd&did=MS4wLjABAAAAEs86TBQPNwAo-RGrcxWyCdwKhI66AK3Pqf3ieo6HaxI&iid=MS4wLjABAAAA0ptpM-zzoliLEeyvWOCUt-_dQza4uSjlIvbtIazXnCY&with_sec_did=1&use_link_command=1&ecom_share_track_params=&extra_params={\"from_request_id\":\"20231230162057EC005772A8EAA0199906\",\"im_channel_invite_id\":\"0\"}&user_id=3644207898042206&liveId=7318296342189919011&from=share&style=share&enter_method=click_share&roomId=7318296342189919011&activity_info={}',\n            \"6i- Q@x.Sl 03/23 【醒子8ke的直播间】  点击打开👉https://v.douyin.com/i8tBR7hX/  或长按复制此条消息，打开抖音，看TA直播\",\n            \"https://v.douyin.com/i8tBR7hX/\",\n            ]\n    }\n    ```\n    \"\"\"\n    try:\n        data = await DouyinWebCrawler.get_all_webcast_id(url)\n        return ResponseModel(code=200,\n                             router=request.url.path,\n                             data=data)\n    except Exception as e:\n        status_code = 400\n        detail = ErrorResponseModel(code=status_code,\n                                    router=request.url.path,\n                                    params=dict(request.query_params),\n                                    )\n        raise HTTPException(status_code=status_code, detail=detail.dict())\n", "metadata": {"source": "github.com/Evil0ctal/Douyin_TikTok_Download_API", "file_path": "app/api/endpoints/douyin_web.py", "language": "python", "complexity": "complex", "star_count": 13591, "quality_score": 0.8500000000000001, "features": {"has_classes": false, "has_functions": true, "has_async": true, "has_decorators": true, "has_imports": true, "has_error_handling": true, "has_documentation": true, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 24, "class_count": 0, "line_count": 1071}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create REST API endpoint with proper request/response handling with asyncio coroutines\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import os\nimport zipfile\n\nimport aiofiles\nimport httpx\nimport yaml\nfrom fastapi import APIRouter, Request, Query, HTTPException  # 导入FastAPI组件\nfrom starlette.responses import FileResponse\n\nfrom app.api.models.APIResponseModel import ErrorResponseModel  # 导入响应模型\nfrom crawlers.hybrid.hybrid_crawler import HybridCrawler  # 导入混合数据爬虫\n\nrouter = APIRouter()\nHybridCrawler = HybridCrawler()\n\n# 读取上级再上级目录的配置文件\nconfig_path = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(__file__)))), 'config.yaml')\nwith open(config_path, 'r', encoding='utf-8') as file:\n    config = yaml.safe_load(file)\n\nasync def fetch_data(url: str, headers: dict = None):\n    headers = {\n        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n    } if headers is None else headers.get('headers')\n    async with httpx.AsyncClient() as client:\n        response = await client.get(url, headers=headers)\n        response.raise_for_status()  # 确保响应是成功的\n        return response\n\n# 下载视频专用\nasync def fetch_data_stream(url: str, request:Request , headers: dict = None, file_path: str = None):\n    headers = {\n        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n    } if headers is None else headers.get('headers')\n    async with httpx.AsyncClient() as client:\n        # 启用流式请求\n        async with client.stream(\"GET\", url, headers=headers) as response:\n            response.raise_for_status()\n\n            # 流式保存文件\n            async with aiofiles.open(file_path, 'wb') as out_file:\n                async for chunk in response.aiter_bytes():\n                    if await request.is_disconnected():\n                        print(\"客户端断开连接，清理未完成的文件\")\n                        await out_file.close()\n                        os.remove(file_path)\n                        return False\n                    await out_file.write(chunk)\n            return True\n\n@router.get(\"/download\", summary=\"在线下载抖音|TikTok视频/图片/Online download Douyin|TikTok video/image\")\nasync def download_file_hybrid(request: Request,\n                               url: str = Query(\n                                   example=\"https://www.douyin.com/video/7372484719365098803\",\n                                   description=\"视频或图片的URL地址，也支持抖音|TikTok的分享链接，例如：https://v.douyin.com/e4J8Q7A/\"),\n                               prefix: bool = True,\n                               with_watermark: bool = False):\n    \"\"\"\n    # [中文]\n    ### 用途:\n    - 在线下载抖音|TikTok 无水印或有水印的视频/图片\n    - 通过传入的视频URL参数，获取对应的视频或图片数据，然后下载到本地。\n    - 如果你在尝试直接访问TikTok单一视频接口的JSON数据中的视频播放地址时遇到HTTP403错误，那么你可以使用此接口来下载视频。\n    - 这个接口会占用一定的服务器资源，所以在Demo站点是默认关闭的，你可以在本地部署后调用此接口。\n    ### 参数:\n    - url: 视频或图片的URL地址，也支持抖音|TikTok的分享链接，例如：https://v.douyin.com/e4J8Q7A/。\n    - prefix: 下载文件的前缀，默认为True，可以在配置文件中修改。\n    - with_watermark: 是否下载带水印的视频或图片，默认为False。\n    ### 返回:\n    - 返回下载的视频或图片文件响应。\n\n    # [English]\n    ### Purpose:\n    - Download Douyin|TikTok video/image with or without watermark online.\n    - By passing the video URL parameter, get the corresponding video or image data, and then download it to the local.\n    - If you encounter an HTTP403 error when trying to access the video playback address in the JSON data of the TikTok single video interface directly, you can use this interface to download the video.\n    - This interface will occupy a certain amount of server resources, so it is disabled by default on the Demo site, you can call this interface after deploying it locally.\n    ### Parameters:\n    - url: The URL address of the video or image, also supports Douyin|TikTok sharing links, for example: https://v.douyin.com/e4J8Q7A/.\n    - prefix: The prefix of the downloaded file, the default is True, and can be modified in the configuration file.\n    - with_watermark: Whether to download videos or images with watermarks, the default is False.\n    ### Returns:\n    - Return the response of the downloaded video or image file.\n\n    # [示例/Example]\n    url: https://www.douyin.com/video/7372484719365098803\n    \"\"\"\n    # 是否开启此端点/Whether to enable this endpoint\n    if not config[\"API\"][\"Download_Switch\"]:\n        code = 400\n        message = \"Download endpoint is disabled in the configuration file. | 配置文件中已禁用下载端点。\"\n        return ErrorResponseModel(code=code, message=message, router=request.url.path,\n                                  params=dict(request.query_params))\n\n    # 开始解析数据/Start parsing data\n    try:\n        data = await HybridCrawler.hybrid_parsing_single_video(url, minimal=True)\n    except Exception as e:\n        code = 400\n        return ErrorResponseModel(code=code, message=str(e), router=request.url.path, params=dict(request.query_params))\n\n    # 开始下载文件/Start downloading files\n    try:\n        data_type = data.get('type')\n        platform = data.get('platform')\n        aweme_id = data.get('aweme_id')\n        file_prefix = config.get(\"API\").get(\"Download_File_Prefix\") if prefix else ''\n        download_path = os.path.join(config.get(\"API\").get(\"Download_Path\"), f\"{platform}_{data_type}\")\n\n        # 确保目录存在/Ensure the directory exists\n        os.makedirs(download_path, exist_ok=True)\n\n        # 下载视频文件/Download video file\n        if data_type == 'video':\n            file_name = f\"{file_prefix}{platform}_{aweme_id}.mp4\" if not with_watermark else f\"{file_prefix}{platform}_{aweme_id}_watermark.mp4\"\n            url = data.get('video_data').get('nwm_video_url_HQ') if not with_watermark else data.get('video_data').get(\n                'wm_video_url_HQ')\n            file_path = os.path.join(download_path, file_name)\n\n            # 判断文件是否存在，存在就直接返回\n            if os.path.exists(file_path):\n                return FileResponse(path=file_path, media_type='video/mp4', filename=file_name)\n\n            # 获取视频文件\n            __headers = await HybridCrawler.TikTokWebCrawler.get_tiktok_headers() if platform == 'tiktok' else await HybridCrawler.DouyinWebCrawler.get_douyin_headers()\n            # response = await fetch_data(url, headers=__headers)\n\n            success = await fetch_data_stream(url, request, headers=__headers, file_path=file_path)\n            if not success:\n                raise HTTPException(\n                    status_code=500,\n                    detail=\"An error occurred while fetching data\"\n                )\n\n            # # 保存文件\n            # async with aiofiles.open(file_path, 'wb') as out_file:\n            #     await out_file.write(response.content)\n\n            # 返回文件内容\n            return FileResponse(path=file_path, filename=file_name, media_type=\"video/mp4\")\n\n        # 下载图片文件/Download image file\n        elif data_type == 'image':\n            # 压缩文件属性/Compress file properties\n            zip_file_name = f\"{file_prefix}{platform}_{aweme_id}_images.zip\" if not with_watermark else f\"{file_prefix}{platform}_{aweme_id}_images_watermark.zip\"\n            zip_file_path = os.path.join(download_path, zip_file_name)\n\n            # 判断文件是否存在，存在就直接返回、\n            if os.path.exists(zip_file_path):\n                return FileResponse(path=zip_file_path, filename=zip_file_name, media_type=\"application/zip\")\n\n            # 获取图片文件/Get image file\n            urls = data.get('image_data').get('no_watermark_image_list') if not with_watermark else data.get(\n                'image_data').get('watermark_image_list')\n            image_file_list = []\n            for url in urls:\n                # 请求图片文件/Request image file\n                response = await fetch_data(url)\n                index = int(urls.index(url))\n                content_type = response.headers.get('content-type')\n                file_format = content_type.split('/')[1]\n                file_name = f\"{file_prefix}{platform}_{aweme_id}_{index + 1}.{file_format}\" if not with_watermark else f\"{file_prefix}{platform}_{aweme_id}_{index + 1}_watermark.{file_format}\"\n                file_path = os.path.join(download_path, file_name)\n                image_file_list.append(file_path)\n\n                # 保存文件/Save file\n                async with aiofiles.open(file_path, 'wb') as out_file:\n                    await out_file.write(response.content)\n\n            # 压缩文件/Compress file\n            with zipfile.ZipFile(zip_file_path, 'w') as zip_file:\n                for image_file in image_file_list:\n                    zip_file.write(image_file, os.path.basename(image_file))\n\n            # 返回压缩文件/Return compressed file\n            return FileResponse(path=zip_file_path, filename=zip_file_name, media_type=\"application/zip\")\n\n    # 异常处理/Exception handling\n    except Exception as e:\n        print(e)\n        code = 400\n        return ErrorResponseModel(code=code, message=str(e), router=request.url.path, params=dict(request.query_params))\n", "metadata": {"source": "github.com/Evil0ctal/Douyin_TikTok_Download_API", "file_path": "app/api/endpoints/download.py", "language": "python", "complexity": "complex", "star_count": 13591, "quality_score": 0.98, "features": {"has_classes": false, "has_functions": true, "has_async": true, "has_decorators": true, "has_imports": true, "has_error_handling": true, "has_documentation": true, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 3, "class_count": 0, "line_count": 183}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create REST API endpoint with proper request/response handling with asyncio coroutines\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import asyncio\n\nfrom fastapi import APIRouter, Body, Query, Request, HTTPException  # 导入FastAPI组件\n\nfrom app.api.models.APIResponseModel import ResponseModel, ErrorResponseModel  # 导入响应模型\n\n# 爬虫/Crawler\nfrom crawlers.hybrid.hybrid_crawler import HybridCrawler  # 导入混合爬虫\n\nHybridCrawler = HybridCrawler()  # 实例化混合爬虫\n\nrouter = APIRouter()\n\n\n@router.get(\"/video_data\", response_model=ResponseModel, tags=[\"Hybrid-API\"],\n            summary=\"混合解析单一视频接口/Hybrid parsing single video endpoint\")\nasync def hybrid_parsing_single_video(request: Request,\n                                      url: str = Query(example=\"https://v.douyin.com/L4FJNR3/\"),\n                                      minimal: bool = Query(default=False)):\n    \"\"\"\n    # [中文]\n    ### 用途:\n    - 该接口用于解析抖音/TikTok单一视频的数据。\n    ### 参数:\n    - `url`: 视频链接、分享链接、分享文本。\n    ### 返回:\n    - `data`: 视频数据。\n\n    # [English]\n    ### Purpose:\n    - This endpoint is used to parse data of a single Douyin/TikTok video.\n    ### Parameters:\n    - `url`: Video link, share link, or share text.\n    ### Returns:\n    - `data`: Video data.\n\n    # [Example]\n    url = \"https://v.douyin.com/L4FJNR3/\"\n    \"\"\"\n    try:\n        # 解析视频/Parse video\n        data = await HybridCrawler.hybrid_parsing_single_video(url=url, minimal=minimal)\n        # 返回数据/Return data\n        return ResponseModel(code=200,\n                             router=request.url.path,\n                             data=data)\n    except Exception as e:\n        status_code = 400\n        detail = ErrorResponseModel(code=status_code,\n                                    router=request.url.path,\n                                    params=dict(request.query_params),\n                                    )\n        raise HTTPException(status_code=status_code, detail=detail.dict())\n", "metadata": {"source": "github.com/Evil0ctal/Douyin_TikTok_Download_API", "file_path": "app/api/endpoints/hybrid_parsing.py", "language": "python", "complexity": "simple", "star_count": 13591, "quality_score": 0.9500000000000001, "features": {"has_classes": false, "has_functions": true, "has_async": true, "has_decorators": true, "has_imports": true, "has_error_handling": true, "has_documentation": true, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 1, "class_count": 0, "line_count": 54}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create REST API endpoint with proper request/response handling with asyncio coroutines\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import os\nimport yaml\nfrom fastapi import APIRouter\nfrom app.api.models.APIResponseModel import iOS_Shortcut\n\n\n# 读取上级再上级目录的配置文件\nconfig_path = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(__file__)))), 'config.yaml')\nwith open(config_path, 'r', encoding='utf-8') as file:\n    config = yaml.safe_load(file)\n\nrouter = APIRouter()\n\n\n@router.get(\"/shortcut\", response_model=iOS_Shortcut, summary=\"用于iOS快捷指令的版本更新信息/Version update information for iOS shortcuts\")\nasync def get_shortcut():\n    shortcut_config = config[\"iOS_Shortcut\"]\n    version = shortcut_config[\"iOS_Shortcut_Version\"]\n    update = shortcut_config['iOS_Shortcut_Update_Time']\n    link = shortcut_config['iOS_Shortcut_Link']\n    link_en = shortcut_config['iOS_Shortcut_Link_EN']\n    note = shortcut_config['iOS_Shortcut_Update_Note']\n    note_en = shortcut_config['iOS_Shortcut_Update_Note_EN']\n    return iOS_Shortcut(version=str(version), update=update, link=link, link_en=link_en, note=note, note_en=note_en)", "metadata": {"source": "github.com/Evil0ctal/Douyin_TikTok_Download_API", "file_path": "app/api/endpoints/ios_shortcut.py", "language": "python", "complexity": "simple", "star_count": 13591, "quality_score": 0.92, "features": {"has_classes": false, "has_functions": true, "has_async": true, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": false, "has_testing": false, "has_logging": false, "function_count": 1, "class_count": 0, "line_count": 24}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create REST API endpoint with proper request/response handling with asyncio coroutines\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from fastapi import APIRouter, Query, Request, HTTPException  # 导入FastAPI组件\nfrom app.api.models.APIResponseModel import ResponseModel, ErrorResponseModel  # 导入响应模型\n\nfrom crawlers.tiktok.app.app_crawler import TikTokAPPCrawler  # 导入APP爬虫\n\nrouter = APIRouter()\nTikTokAPPCrawler = TikTokAPPCrawler()\n\n\n# 获取单个作品数据\n@router.get(\"/fetch_one_video\",\n            response_model=ResponseModel,\n            summary=\"获取单个作品数据/Get single video data\"\n            )\nasync def fetch_one_video(request: Request,\n                          aweme_id: str = Query(example=\"7350810998023949599\", description=\"作品id/Video id\")):\n    \"\"\"\n    # [中文]\n    ### 用途:\n    - 获取单个作品数据\n    ### 参数:\n    - aweme_id: 作品id\n    ### 返回:\n    - 作品数据\n\n    # [English]\n    ### Purpose:\n    - Get single video data\n    ### Parameters:\n    - aweme_id: Video id\n    ### Return:\n    - Video data\n\n    # [示例/Example]\n    aweme_id = \"7350810998023949599\"\n    \"\"\"\n    try:\n        data = await TikTokAPPCrawler.fetch_one_video(aweme_id)\n        return ResponseModel(code=200,\n                             router=request.url.path,\n                             data=data)\n    except Exception as e:\n        status_code = 400\n        detail = ErrorResponseModel(code=status_code,\n                                    router=request.url.path,\n                                    params=dict(request.query_params),\n                                    )\n        raise HTTPException(status_code=status_code, detail=detail.dict())\n    ", "metadata": {"source": "github.com/Evil0ctal/Douyin_TikTok_Download_API", "file_path": "app/api/endpoints/tiktok_app.py", "language": "python", "complexity": "simple", "star_count": 13591, "quality_score": 0.9500000000000001, "features": {"has_classes": false, "has_functions": true, "has_async": true, "has_decorators": true, "has_imports": true, "has_error_handling": true, "has_documentation": true, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 1, "class_count": 0, "line_count": 49}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create REST API endpoint with proper request/response handling with asyncio coroutines\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from typing import List\n\nfrom fastapi import APIRouter, Query, Body, Request, HTTPException  # 导入FastAPI组件\n\nfrom app.api.models.APIResponseModel import ResponseModel, ErrorResponseModel  # 导入响应模型\n\nfrom crawlers.tiktok.web.web_crawler import TikTokWebCrawler  # 导入TikTokWebCrawler类\n\nrouter = APIRouter()\nTikTokWebCrawler = TikTokWebCrawler()\n\n\n# 获取单个作品数据\n@router.get(\"/fetch_one_video\",\n            response_model=ResponseModel,\n            summary=\"获取单个作品数据/Get single video data\")\nasync def fetch_one_video(request: Request,\n                            itemId: str = Query(example=\"7339393672959757570\", description=\"作品id/Video id\")):\n    \"\"\"\n    # [中文]\n    ### 用途:\n    - 获取单个作品数据\n    ### 参数:\n    - itemId: 作品id\n    ### 返回:\n    - 作品数据\n\n    # [English]\n    ### Purpose:\n    - Get single video data\n    ### Parameters:\n    - itemId: Video id\n    ### Return:\n    - Video data\n\n    # [示例/Example]\n    itemId = \"7339393672959757570\"\n    \"\"\"\n    try:\n        data = await TikTokWebCrawler.fetch_one_video(itemId)\n        return ResponseModel(code=200,\n                             router=request.url.path,\n                             data=data)\n    except Exception as e:\n        status_code = 400\n        detail = ErrorResponseModel(code=status_code,\n                                    router=request.url.path,\n                                    params=dict(request.query_params),\n                                    )\n        raise HTTPException(status_code=status_code, detail=detail.dict())\n\n\n# 获取用户的个人信息\n@router.get(\"/fetch_user_profile\",\n            response_model=ResponseModel,\n            summary=\"获取用户的个人信息/Get user profile\")\nasync def fetch_user_profile(request: Request,\n                             uniqueId: str = Query(default=\"tiktok\", description=\"用户uniqueId/User uniqueId\"),\n                             secUid: str = Query(default=\"\", description=\"用户secUid/User secUid\"),):\n    \"\"\"\n    # [中文]\n    ### 用途:\n    - 获取用户的个人信息\n    ### 参数:\n    - secUid: 用户secUid\n    - uniqueId: 用户uniqueId\n    - secUid和uniqueId至少提供一个, 优先使用uniqueId, 也就是用户主页的链接中的用户名。\n    ### 返回:\n    - 用户的个人信息\n\n    # [English]\n    ### Purpose:\n    - Get user profile\n    ### Parameters:\n    - secUid: User secUid\n    - uniqueId: User uniqueId\n    - At least one of secUid and uniqueId is provided, and uniqueId is preferred, that is, the username in the user's homepage link.\n    ### Return:\n    - User profile\n\n    # [示例/Example]\n    secUid = \"MS4wLjABAAAAv7iSuuXDJGDvJkmH_vz1qkDZYo1apxgzaxdBSeIuPiM\"\n    uniqueId = \"tiktok\"\n    \"\"\"\n    try:\n        data = await TikTokWebCrawler.fetch_user_profile(secUid, uniqueId)\n        return ResponseModel(code=200,\n                             router=request.url.path,\n                             data=data)\n    except Exception as e:\n        status_code = 400\n        detail = ErrorResponseModel(code=status_code,\n                                    router=request.url.path,\n                                    params=dict(request.query_params),\n                                    )\n        raise HTTPException(status_code=status_code, detail=detail.dict())\n\n\n# 获取用户的作品列表\n@router.get(\"/fetch_user_post\",\n            response_model=ResponseModel,\n            summary=\"获取用户的作品列表/Get user posts\")\nasync def fetch_user_post(request: Request,\n                          secUid: str = Query(example=\"MS4wLjABAAAAv7iSuuXDJGDvJkmH_vz1qkDZYo1apxgzaxdBSeIuPiM\",\n                                              description=\"用户secUid/User secUid\"),\n                          cursor: int = Query(default=0, description=\"翻页游标/Page cursor\"),\n                          count: int = Query(default=35, description=\"每页数量/Number per page\"),\n                          coverFormat: int = Query(default=2, description=\"封面格式/Cover format\")):\n    \"\"\"\n    # [中文]\n    ### 用途:\n    - 获取用户的作品列表\n    ### 参数:\n    - secUid: 用户secUid\n    - cursor: 翻页游标\n    - count: 每页数量\n    - coverFormat: 封面格式\n    ### 返回:\n    - 用户的作品列表\n\n    # [English]\n    ### Purpose:\n    - Get user posts\n    ### Parameters:\n    - secUid: User secUid\n    - cursor: Page cursor\n    - count: Number per page\n    - coverFormat: Cover format\n    ### Return:\n    - User posts\n\n    # [示例/Example]\n    secUid = \"MS4wLjABAAAAv7iSuuXDJGDvJkmH_vz1qkDZYo1apxgzaxdBSeIuPiM\"\n    cursor = 0\n    count = 35\n    coverFormat = 2\n    \"\"\"\n    try:\n        data = await TikTokWebCrawler.fetch_user_post(secUid, cursor, count, coverFormat)\n        return ResponseModel(code=200,\n                             router=request.url.path,\n                             data=data)\n    except Exception as e:\n        status_code = 400\n        detail = ErrorResponseModel(code=status_code,\n                                    router=request.url.path,\n                                    params=dict(request.query_params),\n                                    )\n        raise HTTPException(status_code=status_code, detail=detail.dict())\n\n\n# 获取用户的点赞列表\n@router.get(\"/fetch_user_like\",\n            response_model=ResponseModel,\n            summary=\"获取用户的点赞列表/Get user likes\")\nasync def fetch_user_like(request: Request,\n                          secUid: str = Query(\n                              example=\"MS4wLjABAAAAq1iRXNduFZpY301UkVpJ1eQT60_NiWS9QQSeNqmNQEDJp0pOF8cpleNEdiJx5_IU\",\n                              description=\"用户secUid/User secUid\"),\n                          cursor: int = Query(default=0, description=\"翻页游标/Page cursor\"),\n                          count: int = Query(default=35, description=\"每页数量/Number per page\"),\n                          coverFormat: int = Query(default=2, description=\"封面格式/Cover format\")):\n    \"\"\"\n    # [中文]\n    ### 用途:\n    - 获取用户的点赞列表\n    - 注意: 该接口需要用户点赞列表为公开状态\n    ### 参数:\n    - secUid: 用户secUid\n    - cursor: 翻页游标\n    - count: 每页数量\n    - coverFormat: 封面格式\n    ### 返回:\n    - 用户的点赞列表\n\n    # [English]\n    ### Purpose:\n    - Get user likes\n    - Note: This interface requires that the user's like list be public\n    ### Parameters:\n    - secUid: User secUid\n    - cursor: Page cursor\n    - count: Number per page\n    - coverFormat: Cover format\n    ### Return:\n    - User likes\n\n    # [示例/Example]\n    secUid = \"MS4wLjABAAAAq1iRXNduFZpY301UkVpJ1eQT60_NiWS9QQSeNqmNQEDJp0pOF8cpleNEdiJx5_IU\"\n    cursor = 0\n    count = 35\n    coverFormat = 2\n    \"\"\"\n    try:\n        data = await TikTokWebCrawler.fetch_user_like(secUid, cursor, count, coverFormat)\n        return ResponseModel(code=200,\n                             router=request.url.path,\n                             data=data)\n    except Exception as e:\n        status_code = 400\n        detail = ErrorResponseModel(code=status_code,\n                                    router=request.url.path,\n                                    params=dict(request.query_params),\n                                    )\n        raise HTTPException(status_code=status_code, detail=detail.dict())\n\n\n# 获取用户的收藏列表\n@router.get(\"/fetch_user_collect\",\n            response_model=ResponseModel,\n            summary=\"获取用户的收藏列表/Get user favorites\")\nasync def fetch_user_collect(request: Request,\n                             cookie: str = Query(example=\"Your_Cookie\", description=\"用户cookie/User cookie\"),\n                             secUid: str = Query(example=\"Your_SecUid\", description=\"用户secUid/User secUid\"),\n                             cursor: int = Query(default=0, description=\"翻页游标/Page cursor\"),\n                             count: int = Query(default=30, description=\"每页数量/Number per page\"),\n                             coverFormat: int = Query(default=2, description=\"封面格式/Cover format\")):\n    \"\"\"\n    # [中文]\n    ### 用途:\n    - 获取用户的收藏列表\n    - 注意: 该接口目前只能获取自己的收藏列表，需要提供自己账号的cookie。\n    ### 参数:\n    - cookie: 用户cookie\n    - secUid: 用户secUid\n    - cursor: 翻页游标\n    - count: 每页数量\n    - coverFormat: 封面格式\n    ### 返回:\n    - 用户的收藏列表\n\n    # [English]\n    ### Purpose:\n    - Get user favorites\n    - Note: This interface can currently only get your own favorites list, you need to provide your account cookie.\n    ### Parameters:\n    - cookie: User cookie\n    - secUid: User secUid\n    - cursor: Page cursor\n    - count: Number per page\n    - coverFormat: Cover format\n    ### Return:\n    - User favorites\n\n    # [示例/Example]\n    cookie = \"Your_Cookie\"\n    secUid = \"Your_SecUid\"\n    cursor = 0\n    count = 30\n    coverFormat = 2\n    \"\"\"\n    try:\n        data = await TikTokWebCrawler.fetch_user_collect(cookie, secUid, cursor, count, coverFormat)\n        return ResponseModel(code=200,\n                             router=request.url.path,\n                             data=data)\n    except Exception as e:\n        status_code = 400\n        detail = ErrorResponseModel(code=status_code,\n                                    router=request.url.path,\n                                    params=dict(request.query_params),\n                                    )\n        raise HTTPException(status_code=status_code, detail=detail.dict())\n\n\n# 获取用户的播放列表\n@router.get(\"/fetch_user_play_list\",\n            response_model=ResponseModel,\n            summary=\"获取用户的播放列表/Get user play list\")\nasync def fetch_user_play_list(request: Request,\n                               secUid: str = Query(example=\"MS4wLjABAAAAv7iSuuXDJGDvJkmH_vz1qkDZYo1apxgzaxdBSeIuPiM\",\n                                                   description=\"用户secUid/User secUid\"),\n                               cursor: int = Query(default=0, description=\"翻页游标/Page cursor\"),\n                               count: int = Query(default=30, description=\"每页数量/Number per page\")):\n    \"\"\"\n    # [中文]\n    ### 用途:\n    - 获取用户的播放列表\n    ### 参数:\n    - secUid: 用户secUid\n    - cursor: 翻页游标\n    - count: 每页数量\n    ### 返回:\n    - 用户的播放列表\n\n    # [English]\n    ### Purpose:\n    - Get user play list\n    ### Parameters:\n    - secUid: User secUid\n    - cursor: Page cursor\n    - count: Number per page\n    ### Return:\n    - User play list\n\n    # [示例/Eample]\n    secUid = \"MS4wLjABAAAAv7iSuuXDJGDvJkmH_vz1qkDZYo1apxgzaxdBSeIuPiM\"\n    cursor = 0\n    count = 30\n    \"\"\"\n    try:\n        data = await TikTokWebCrawler.fetch_user_play_list(secUid, cursor, count)\n        return ResponseModel(code=200,\n                             router=request.url.path,\n                             data=data)\n    except Exception as e:\n        status_code = 400\n        detail = ErrorResponseModel(code=status_code,\n                                    router=request.url.path,\n                                    params=dict(request.query_params),\n                                    )\n        raise HTTPException(status_code=status_code, detail=detail.dict())\n\n\n# 获取用户的合辑列表\n@router.get(\"/fetch_user_mix\",\n            response_model=ResponseModel,\n            summary=\"获取用户的合辑列表/Get user mix list\")\nasync def fetch_user_mix(request: Request,\n                         mixId: str = Query(example=\"7101538765474106158\",\n                                            description=\"合辑id/Mix id\"),\n                         cursor: int = Query(default=0, description=\"翻页游标/Page cursor\"),\n                         count: int = Query(default=30, description=\"每页数量/Number per page\")):\n    \"\"\"\n    # [中文]\n    ### 用途:\n    - 获取用户的合辑列表\n    ### 参数:\n    - mixId: 合辑id\n    - cursor: 翻页游标\n    - count: 每页数量\n    ### 返回:\n    - 用户的合辑列表\n\n    # [English]\n    ### Purpose:\n    - Get user mix list\n    ### Parameters:\n    - mixId: Mix id\n    - cursor: Page cursor\n    - count: Number per page\n    ### Return:\n    - User mix list\n\n    # [示例/Eample]\n    mixId = \"7101538765474106158\"\n    cursor = 0\n    count = 30\n    \"\"\"\n    try:\n        data = await TikTokWebCrawler.fetch_user_mix(mixId, cursor, count)\n        return ResponseModel(code=200,\n                             router=request.url.path,\n                             data=data)\n    except Exception as e:\n        status_code = 400\n        detail = ErrorResponseModel(code=status_code,\n                                    router=request.url.path,\n                                    params=dict(request.query_params),\n                                    )\n        raise HTTPException(status_code=status_code, detail=detail.dict())\n\n\n# 获取作品的评论列表\n@router.get(\"/fetch_post_comment\",\n            response_model=ResponseModel,\n            summary=\"获取作品的评论列表/Get video comments\")\nasync def fetch_post_comment(request: Request,\n                             aweme_id: str = Query(example=\"7304809083817774382\", description=\"作品id/Video id\"),\n                             cursor: int = Query(default=0, description=\"翻页游标/Page cursor\"),\n                             count: int = Query(default=20, description=\"每页数量/Number per page\"),\n                             current_region: str = Query(default=\"\", description=\"当前地区/Current region\")):\n    \"\"\"\n    # [中文]\n    ### 用途:\n    - 获取作品的评论列表\n    ### 参数:\n    - aweme_id: 作品id\n    - cursor: 翻页游标\n    - count: 每页数量\n    - current_region: 当前地区，默认为空。\n    ### 返回:\n    - 作品的评论列表\n\n    # [English]\n    ### Purpose:\n    - Get video comments\n    ### Parameters:\n    - aweme_id: Video id\n    - cursor: Page cursor\n    - count: Number per page\n    - current_region: Current region, default is empty.\n    ### Return:\n    - Video comments\n\n    # [示例/Eample]\n    aweme_id = \"7304809083817774382\"\n    cursor = 0\n    count = 20\n    current_region = \"\"\n    \"\"\"\n    try:\n        data = await TikTokWebCrawler.fetch_post_comment(aweme_id, cursor, count, current_region)\n        return ResponseModel(code=200,\n                             router=request.url.path,\n                             data=data)\n    except Exception as e:\n        status_code = 400\n        detail = ErrorResponseModel(code=status_code,\n                                    router=request.url.path,\n                                    params=dict(request.query_params),\n                                    )\n        raise HTTPException(status_code=status_code, detail=detail.dict())\n\n\n# 获取作品的评论回复列表\n@router.get(\"/fetch_post_comment_reply\",\n            response_model=ResponseModel,\n            summary=\"获取作品的评论回复列表/Get video comment replies\")\nasync def fetch_post_comment_reply(request: Request,\n                                   item_id: str = Query(example=\"7304809083817774382\", description=\"作品id/Video id\"),\n                                   comment_id: str = Query(example=\"7304877760886588191\",\n                                                           description=\"评论id/Comment id\"),\n                                   cursor: int = Query(default=0, description=\"翻页游标/Page cursor\"),\n                                   count: int = Query(default=20, description=\"每页数量/Number per page\"),\n                                   current_region: str = Query(default=\"\", description=\"当前地区/Current region\")):\n    \"\"\"\n    # [中文]\n    ### 用途:\n    - 获取作品的评论回复列表\n    ### 参数:\n    - item_id: 作品id\n    - comment_id: 评论id\n    - cursor: 翻页游标\n    - count: 每页数量\n    - current_region: 当前地区，默认为空。\n    ### 返回:\n    - 作品的评论回复列表\n\n    # [English]\n    ### Purpose:\n    - Get video comment replies\n    ### Parameters:\n    - item_id: Video id\n    - comment_id: Comment id\n    - cursor: Page cursor\n    - count: Number per page\n    - current_region: Current region, default is empty.\n    ### Return:\n    - Video comment replies\n\n    # [示例/Eample]\n    item_id = \"7304809083817774382\"\n    comment_id = \"7304877760886588191\"\n    cursor = 0\n    count = 20\n    current_region = \"\"\n    \"\"\"\n    try:\n        data = await TikTokWebCrawler.fetch_post_comment_reply(item_id, comment_id, cursor, count, current_region)\n        return ResponseModel(code=200,\n                             router=request.url.path,\n                             data=data)\n    except Exception as e:\n        status_code = 400\n        detail = ErrorResponseModel(code=status_code,\n                                    router=request.url.path,\n                                    params=dict(request.query_params),\n                                    )\n        raise HTTPException(status_code=status_code, detail=detail.dict())\n\n\n# 获取用户的粉丝列表\n@router.get(\"/fetch_user_fans\",\n            response_model=ResponseModel,\n            summary=\"获取用户的粉丝列表/Get user followers\")\nasync def fetch_user_fans(request: Request,\n                          secUid: str = Query(example=\"MS4wLjABAAAAv7iSuuXDJGDvJkmH_vz1qkDZYo1apxgzaxdBSeIuPiM\",\n                                              description=\"用户secUid/User secUid\"),\n                          count: int = Query(default=30, description=\"每页数量/Number per page\"),\n                          maxCursor: int = Query(default=0, description=\"最大游标/Max cursor\"),\n                          minCursor: int = Query(default=0, description=\"最小游标/Min cursor\")):\n    \"\"\"\n    # [中文]\n    ### 用途:\n    - 获取用户的粉丝列表\n    ### 参数:\n    - secUid: 用户secUid\n    - count: 每页数量\n    - maxCursor: 最大游标\n    - minCursor: 最小游标\n    ### 返回:\n    - 用户的粉丝列表\n\n    # [English]\n    ### Purpose:\n    - Get user followers\n    ### Parameters:\n    - secUid: User secUid\n    - count: Number per page\n    - maxCursor: Max cursor\n    - minCursor: Min cursor\n    ### Return:\n    - User followers\n\n    # [示例/Example]\n    secUid = \"MS4wLjABAAAAv7iSuuXDJGDvJkmH_vz1qkDZYo1apxgzaxdBSeIuPiM\"\n    count = 30\n    maxCursor = 0\n    minCursor = 0\n    \"\"\"\n    try:\n        data = await TikTokWebCrawler.fetch_user_fans(secUid, count, maxCursor, minCursor)\n        return ResponseModel(code=200,\n                             router=request.url.path,\n                             data=data)\n    except Exception as e:\n        status_code = 400\n        detail = ErrorResponseModel(code=status_code,\n                                    router=request.url.path,\n                                    params=dict(request.query_params),\n                                    )\n        raise HTTPException(status_code=status_code, detail=detail.dict())\n\n\n# 获取用户的关注列表\n@router.get(\"/fetch_user_follow\",\n            response_model=ResponseModel,\n            summary=\"获取用户的关注列表/Get user followings\")\nasync def fetch_user_follow(request: Request,\n                            secUid: str = Query(example=\"MS4wLjABAAAAv7iSuuXDJGDvJkmH_vz1qkDZYo1apxgzaxdBSeIuPiM\",\n                                                description=\"用户secUid/User secUid\"),\n                            count: int = Query(default=30, description=\"每页数量/Number per page\"),\n                            maxCursor: int = Query(default=0, description=\"最大游标/Max cursor\"),\n                            minCursor: int = Query(default=0, description=\"最小游标/Min cursor\")):\n    \"\"\"\n    # [中文]\n    ### 用途:\n    - 获取用户的关注列表\n    ### 参数:\n    - secUid: 用户secUid\n    - count: 每页数量\n    - maxCursor: 最大游标\n    - minCursor: 最小游标\n    ### 返回:\n    - 用户的关注列表\n\n    # [English]\n    ### Purpose:\n    - Get user followings\n    ### Parameters:\n    - secUid: User secUid\n    - count: Number per page\n    - maxCursor: Max cursor\n    - minCursor: Min cursor\n    ### Return:\n    - User followings\n\n    # [示例/Example]\n    secUid = \"MS4wLjABAAAAv7iSuuXDJGDvJkmH_vz1qkDZYo1apxgzaxdBSeIuPiM\"\n    count = 30\n    maxCursor = 0\n    minCursor = 0\n    \"\"\"\n    try:\n        data = await TikTokWebCrawler.fetch_user_follow(secUid, count, maxCursor, minCursor)\n        return ResponseModel(code=200,\n                             router=request.url.path,\n                             data=data)\n    except Exception as e:\n        status_code = 400\n        detail = ErrorResponseModel(code=status_code,\n                                    router=request.url.path,\n                                    params=dict(request.query_params),\n                                    )\n        raise HTTPException(status_code=status_code, detail=detail.dict())\n\n\n\"\"\"-------------------------------------------------------utils接口列表-------------------------------------------------------\"\"\"\n\n\n# 生成真实msToken\n@router.get(\"/generate_real_msToken\",\n            response_model=ResponseModel,\n            summary=\"生成真实msToken/Generate real msToken\")\nasync def generate_real_msToken(request: Request):\n    \"\"\"\n    # [中文]\n    ### 用途:\n    - 生成真实msToken\n    ### 返回:\n    - 真实msToken\n\n    # [English]\n    ### Purpose:\n    - Generate real msToken\n    ### Return:\n    - Real msToken\n    \"\"\"\n    try:\n        data = await TikTokWebCrawler.fetch_real_msToken()\n        return ResponseModel(code=200,\n                             router=request.url.path,\n                             data=data)\n    except Exception as e:\n        status_code = 400\n        detail = ErrorResponseModel(code=status_code,\n                                    router=request.url.path,\n                                    params=dict(request.query_params),\n                                    )\n        raise HTTPException(status_code=status_code, detail=detail.dict())\n\n\n# 生成ttwid\n@router.get(\"/generate_ttwid\",\n            response_model=ResponseModel,\n            summary=\"生成ttwid/Generate ttwid\")\nasync def generate_ttwid(request: Request,\n                         cookie: str = Query(example=\"Your_Cookie\", description=\"用户cookie/User cookie\")):\n    \"\"\"\n    # [中文]\n    ### 用途:\n    - 生成ttwid\n    ### 参数:\n    - cookie: 用户cookie\n    ### 返回:\n    - ttwid\n\n    # [English]\n    ### Purpose:\n    - Generate ttwid\n    ### Parameters:\n    - cookie: User cookie\n    ### Return:\n    - ttwid\n\n    # [示例/Example]\n    cookie = \"Your_Cookie\"\n    \"\"\"\n    try:\n        data = await TikTokWebCrawler.fetch_ttwid(cookie)\n        return ResponseModel(code=200,\n                             router=request.url.path,\n                             data=data)\n    except Exception as e:\n        status_code = 400\n        detail = ErrorResponseModel(code=status_code,\n                                    router=request.url.path,\n                                    params=dict(request.query_params),\n                                    )\n        raise HTTPException(status_code=status_code, detail=detail.dict())\n\n\n# 生成xbogus\n@router.get(\"/generate_xbogus\",\n            response_model=ResponseModel,\n            summary=\"生成xbogus/Generate xbogus\")\nasync def generate_xbogus(request: Request,\n                          url: str = Query(\n                              example=\"https://www.tiktok.com/api/item/detail/?WebIdLastTime=1712665533&aid=1988&app_language=en&app_name=tiktok_web&browser_language=en-US&browser_name=Mozilla&browser_online=true&browser_platform=Win32&browser_version=5.0%20%28Windows%29&channel=tiktok_web&cookie_enabled=true&device_id=7349090360347690538&device_platform=web_pc&focus_state=true&from_page=user&history_len=4&is_fullscreen=false&is_page_visible=true&language=en&os=windows&priority_region=US&referer=&region=US&root_referer=https%3A%2F%2Fwww.tiktok.com%2F&screen_height=1080&screen_width=1920&webcast_language=en&tz_name=America%2FTijuana&msToken=AYFCEapCLbMrS8uTLBoYdUMeeVLbCdFQ_QF_-OcjzJw1CPr4JQhWUtagy0k4a9IITAqi5Qxr2Vdh9mgCbyGxTnvWLa4ZVY6IiSf6lcST-tr0IXfl-r_ZTpzvWDoQfqOVsWCTlSNkhAwB-tap5g==&itemId=7339393672959757570\",\n                              description=\"未签名的API URL/Unsigned API URL\"),\n                          user_agent: str = Query(\n                              example=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\",\n                              description=\"用户浏览器User-Agent/User browser User-Agent\")):\n    \"\"\"\n    # [中文]\n    ### 用途:\n    - 生成xbogus\n    ### 参数:\n    - url: 未签名的API URL\n    - user_agent: 用户浏览器User-Agent\n    ### 返回:\n    - xbogus\n\n    # [English]\n    ### Purpose:\n    - Generate xbogus\n    ### Parameters:\n    - url: Unsigned API URL\n    - user_agent: User browser User-Agent\n    ### Return:\n    - xbogus\n\n    # [示例/Example]\n    url = \"https://www.tiktok.com/api/item/detail/?WebIdLastTime=1712665533&aid=1988&app_language=en&app_name=tiktok_web&browser_language=en-US&browser_name=Mozilla&browser_online=true&browser_platform=Win32&browser_version=5.0%20%28Windows%29&channel=tiktok_web&cookie_enabled=true&device_id=7349090360347690538&device_platform=web_pc&focus_state=true&from_page=user&history_len=4&is_fullscreen=false&is_page_visible=true&language=en&os=windows&priority_region=US&referer=&region=US&root_referer=https%3A%2F%2Fwww.tiktok.com%2F&screen_height=1080&screen_width=1920&webcast_language=en&tz_name=America%2FTijuana&msToken=AYFCEapCLbMrS8uTLBoYdUMeeVLbCdFQ_QF_-OcjzJw1CPr4JQhWUtagy0k4a9IITAqi5Qxr2Vdh9mgCbyGxTnvWLa4ZVY6IiSf6lcST-tr0IXfl-r_ZTpzvWDoQfqOVsWCTlSNkhAwB-tap5g==&itemId=7339393672959757570\"\n    user_agent = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"\n    \"\"\"\n    try:\n        data = await TikTokWebCrawler.gen_xbogus(url, user_agent)\n        return ResponseModel(code=200,\n                             router=request.url.path,\n                             data=data)\n    except Exception as e:\n        status_code = 400\n        detail = ErrorResponseModel(code=status_code,\n                                    router=request.url.path,\n                                    params=dict(request.query_params),\n                                    )\n        raise HTTPException(status_code=status_code, detail=detail.dict())\n\n\n# 提取列表用户id\n@router.get(\"/get_sec_user_id\",\n            response_model=ResponseModel,\n            summary=\"提取列表用户id/Extract list user id\")\nasync def get_sec_user_id(request: Request,\n                          url: str = Query(\n                              example=\"https://www.tiktok.com/@tiktok\",\n                              description=\"用户主页链接/User homepage link\")):\n    \"\"\"\n    # [中文]\n    ### 用途:\n    - 提取列表用户id\n    ### 参数:\n    - url: 用户主页链接\n    ### 返回:\n    - 用户id\n\n    # [English]\n    ### Purpose:\n    - Extract list user id\n    ### Parameters:\n    - url: User homepage link\n    ### Return:\n    - User id\n\n    # [示例/Example]\n    url = \"https://www.tiktok.com/@tiktok\"\n    \"\"\"\n    try:\n        data = await TikTokWebCrawler.get_sec_user_id(url)\n        return ResponseModel(code=200,\n                             router=request.url.path,\n                             data=data)\n    except Exception as e:\n        status_code = 400\n        detail = ErrorResponseModel(code=status_code,\n                                    router=request.url.path,\n                                    params=dict(request.query_params),\n                                    )\n        raise HTTPException(status_code=status_code, detail=detail.dict())\n\n\n# 提取列表用户id\n@router.post(\"/get_all_sec_user_id\",\n             response_model=ResponseModel,\n             summary=\"提取列表用户id/Extract list user id\")\nasync def get_all_sec_user_id(request: Request,\n                              url: List[str] = Body(\n                                  example=[\"https://www.tiktok.com/@tiktok\"],\n                                  description=\"用户主页链接/User homepage link\")):\n    \"\"\"\n    # [中文]\n    ### 用途:\n    - 提取列表用户id\n    ### 参数:\n    - url: 用户主页链接\n    ### 返回:\n    - 用户id\n\n    # [English]\n    ### Purpose:\n    - Extract list user id\n    ### Parameters:\n    - url: User homepage link\n    ### Return:\n    - User id\n\n    # [示例/Example]\n    url = [\"https://www.tiktok.com/@tiktok\"]\n    \"\"\"\n    try:\n        data = await TikTokWebCrawler.get_all_sec_user_id(url)\n        return ResponseModel(code=200,\n                             router=request.url.path,\n                             data=data)\n    except Exception as e:\n        status_code = 400\n        detail = ErrorResponseModel(code=status_code,\n                                    router=request.url.path,\n                                    params=dict(request.query_params),\n                                    )\n        raise HTTPException(status_code=status_code, detail=detail.dict())\n\n\n# 提取单个作品id\n@router.get(\"/get_aweme_id\",\n            response_model=ResponseModel,\n            summary=\"提取单个作品id/Extract single video id\")\nasync def get_aweme_id(request: Request,\n                       url: str = Query(\n                           example=\"https://www.tiktok.com/@owlcitymusic/video/7218694761253735723\",\n                           description=\"作品链接/Video link\")):\n    \"\"\"\n    # [中文]\n    ### 用途:\n    - 提取单个作品id\n    ### 参数:\n    - url: 作品链接\n    ### 返回:\n    - 作品id\n\n    # [English]\n    ### Purpose:\n    - Extract single video id\n    ### Parameters:\n    - url: Video link\n    ### Return:\n    - Video id\n\n    # [示例/Example]\n    url = \"https://www.tiktok.com/@owlcitymusic/video/7218694761253735723\"\n    \"\"\"\n    try:\n        data = await TikTokWebCrawler.get_aweme_id(url)\n        return ResponseModel(code=200,\n                             router=request.url.path,\n                             data=data)\n    except Exception as e:\n        status_code = 400\n        detail = ErrorResponseModel(code=status_code,\n                                    router=request.url.path,\n                                    params=dict(request.query_params),\n                                    )\n        raise HTTPException(status_code=status_code, detail=detail.dict())\n\n\n# 提取列表作品id\n@router.post(\"/get_all_aweme_id\",\n             response_model=ResponseModel,\n             summary=\"提取列表作品id/Extract list video id\")\nasync def get_all_aweme_id(request: Request,\n                           url: List[str] = Body(\n                               example=[\"https://www.tiktok.com/@owlcitymusic/video/7218694761253735723\"],\n                               description=\"作品链接/Video link\")):\n    \"\"\"\n    # [中文]\n    ### 用途:\n    - 提取列表作品id\n    ### 参数:\n    - url: 作品链接\n    ### 返回:\n    - 作品id\n\n    # [English]\n    ### Purpose:\n    - Extract list video id\n    ### Parameters:\n    - url: Video link\n    ### Return:\n    - Video id\n\n    # [示例/Example]\n    url = [\"https://www.tiktok.com/@owlcitymusic/video/7218694761253735723\"]\n    \"\"\"\n    try:\n        data = await TikTokWebCrawler.get_all_aweme_id(url)\n        return ResponseModel(code=200,\n                             router=request.url.path,\n                             data=data)\n    except Exception as e:\n        status_code = 400\n        detail = ErrorResponseModel(code=status_code,\n                                    router=request.url.path,\n                                    params=dict(request.query_params),\n                                    )\n        raise HTTPException(status_code=status_code, detail=detail.dict())\n\n\n# 获取用户unique_id\n@router.get(\"/get_unique_id\",\n            response_model=ResponseModel,\n            summary=\"获取用户unique_id/Get user unique_id\")\nasync def get_unique_id(request: Request,\n                        url: str = Query(\n                            example=\"https://www.tiktok.com/@tiktok\",\n                            description=\"用户主页链接/User homepage link\")):\n    \"\"\"\n    # [中文]\n    ### 用途:\n    - 获取用户unique_id\n    ### 参数:\n    - url: 用户主页链接\n    ### 返回:\n    - unique_id\n\n    # [English]\n    ### Purpose:\n    - Get user unique_id\n    ### Parameters:\n    - url: User homepage link\n    ### Return:\n    - unique_id\n\n    # [示例/Example]\n    url = \"https://www.tiktok.com/@tiktok\"\n    \"\"\"\n    try:\n        data = await TikTokWebCrawler.get_unique_id(url)\n        return ResponseModel(code=200,\n                             router=request.url.path,\n                             data=data)\n    except Exception as e:\n        status_code = 400\n        detail = ErrorResponseModel(code=status_code,\n                                    router=request.url.path,\n                                    params=dict(request.query_params),\n                                    )\n        raise HTTPException(status_code=status_code, detail=detail.dict())\n\n\n# 获取列表unique_id列表\n@router.post(\"/get_all_unique_id\",\n             response_model=ResponseModel,\n             summary=\"获取列表unique_id/Get list unique_id\")\nasync def get_all_unique_id(request: Request,\n                            url: List[str] = Body(\n                                example=[\"https://www.tiktok.com/@tiktok\"],\n                                description=\"用户主页链接/User homepage link\")):\n    \"\"\"\n    # [中文]\n    ### 用途:\n    - 获取列表unique_id\n    ### 参数:\n    - url: 用户主页链接\n    ### 返回:\n    - unique_id\n\n    # [English]\n    ### Purpose:\n    - Get list unique_id\n    ### Parameters:\n    - url: User homepage link\n    ### Return:\n    - unique_id\n\n    # [示例/Example]\n    url = [\"https://www.tiktok.com/@tiktok\"]\n    \"\"\"\n    try:\n        data = await TikTokWebCrawler.get_all_unique_id(url)\n        return ResponseModel(code=200,\n                             router=request.url.path,\n                             data=data)\n    except Exception as e:\n        status_code = 400\n        detail = ErrorResponseModel(code=status_code,\n                                    router=request.url.path,\n                                    params=dict(request.query_params),\n                                    )\n        raise HTTPException(status_code=status_code, detail=detail.dict())\n", "metadata": {"source": "github.com/Evil0ctal/Douyin_TikTok_Download_API", "file_path": "app/api/endpoints/tiktok_web.py", "language": "python", "complexity": "complex", "star_count": 13591, "quality_score": 0.9500000000000001, "features": {"has_classes": false, "has_functions": true, "has_async": true, "has_decorators": true, "has_imports": true, "has_error_handling": true, "has_documentation": true, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 20, "class_count": 0, "line_count": 952}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create REST API endpoint with proper request/response handling with SQLAlchemy/Django ORM patterns\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from fastapi import Body, FastAPI, Query, Request, HTTPException\nfrom pydantic import BaseModel\nfrom typing import Any, Callable, Type, Optional, Dict\nfrom functools import wraps\nimport datetime\n\napp = FastAPI()\n\n\n# 定义响应模型\nclass ResponseModel(BaseModel):\n    code: int = 200\n    router: str = \"Endpoint path\"\n    data: Optional[Any] = {}\n\n\n# 定义错误响应模型\nclass ErrorResponseModel(BaseModel):\n    code: int = 400\n    message: str = \"An error occurred.\"\n    support: str = \"Please contact us on Github: https://github.com/Evil0ctal/Douyin_TikTok_Download_API\"\n    time: str = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n    router: str\n    params: dict = {}\n\n\n# 混合解析响应模型\nclass HybridResponseModel(BaseModel):\n    code: int = 200\n    router: str = \"Hybrid parsing single video endpoint\"\n    data: Optional[Any] = {}\n\n\n# iOS_Shortcut响应模型\nclass iOS_Shortcut(BaseModel):\n    version: str\n    update: str\n    link: str\n    link_en: str\n    note: str\n    note_en: str\n", "metadata": {"source": "github.com/Evil0ctal/Douyin_TikTok_Download_API", "file_path": "app/api/models/APIResponseModel.py", "language": "python", "complexity": "medium", "star_count": 13591, "quality_score": 0.89, "features": {"has_classes": true, "has_functions": false, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 0, "class_count": 4, "line_count": 42}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate module integration for Create REST API endpoint with proper request/response handling\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from fastapi import APIRouter\nfrom app.api.endpoints import (\n    tiktok_web,\n    tiktok_app,\n    douyin_web,\n    bilibili_web,\n    hybrid_parsing, ios_shortcut, download,\n)\n\nrouter = APIRouter()\n\n# TikTok routers\nrouter.include_router(tiktok_web.router, prefix=\"/tiktok/web\", tags=[\"TikTok-Web-API\"])\nrouter.include_router(tiktok_app.router, prefix=\"/tiktok/app\", tags=[\"TikTok-App-API\"])\n\n# Douyin routers\nrouter.include_router(douyin_web.router, prefix=\"/douyin/web\", tags=[\"Douyin-Web-API\"])\n\n# Bilibili routers\nrouter.include_router(bilibili_web.router, prefix=\"/bilibili/web\", tags=[\"Bilibili-Web-API\"])\n\n# Hybrid routers\nrouter.include_router(hybrid_parsing.router, prefix=\"/hybrid\", tags=[\"Hybrid-API\"])\n\n# iOS_Shortcut routers\nrouter.include_router(ios_shortcut.router, prefix=\"/ios\", tags=[\"iOS-Shortcut\"])\n\n# Download routers\nrouter.include_router(download.router, tags=[\"Download\"])\n", "metadata": {"source": "github.com/Evil0ctal/Douyin_TikTok_Download_API", "file_path": "app/api/router.py", "language": "python", "complexity": "simple", "star_count": 13591, "quality_score": 0.8600000000000001, "features": {"has_classes": false, "has_functions": false, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": false, "has_testing": false, "has_logging": false, "function_count": 0, "class_count": 0, "line_count": 30}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate module integration for Create REST API endpoint with proper request/response handling\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# ==============================================================================\n# Copyright (C) 2021 Evil0ctal\n#\n# This file is part of the Douyin_TikTok_Download_API project.\n#\n# This project is licensed under the Apache License 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at:\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n# 　　　　 　　  ＿＿\n# 　　　 　　 ／＞　　フ\n# 　　　 　　| 　_　 _ l\n# 　 　　 　／` ミ＿xノ\n# 　　 　 /　　　 　 |       Feed me Stars ⭐ ️\n# 　　　 /　 ヽ　　 ﾉ\n# 　 　 │　　|　|　|\n# 　／￣|　　 |　|　|\n# 　| (￣ヽ＿_ヽ_)__)\n# 　＼二つ\n# ==============================================================================\n#\n# Contributor Link:\n# - https://github.com/Evil0ctal\n# - https://github.com/Johnserf-Seed\n#\n# ==============================================================================\n\n\n# FastAPI APP\nimport uvicorn\nfrom fastapi import FastAPI\nfrom app.api.router import router as api_router\n\n# PyWebIO APP\nfrom app.web.app import MainView\nfrom pywebio.platform.fastapi import asgi_app\n\n# OS\nimport os\n\n# YAML\nimport yaml\n\n# Load Config\n\n# 读取上级再上级目录的配置文件\nconfig_path = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'config.yaml')\nwith open(config_path, 'r', encoding='utf-8') as file:\n    config = yaml.safe_load(file)\n\n\nHost_IP = config['API']['Host_IP']\nHost_Port = config['API']['Host_Port']\n\n# API Tags\ntags_metadata = [\n    {\n        \"name\": \"Hybrid-API\",\n        \"description\": \"**(混合数据接口/Hybrid-API data endpoints)**\",\n    },\n    {\n        \"name\": \"Douyin-Web-API\",\n        \"description\": \"**(抖音Web数据接口/Douyin-Web-API data endpoints)**\",\n    },\n    {\n        \"name\": \"TikTok-Web-API\",\n        \"description\": \"**(TikTok-Web-API数据接口/TikTok-Web-API data endpoints)**\",\n    },\n    {\n        \"name\": \"TikTok-App-API\",\n        \"description\": \"**(TikTok-App-API数据接口/TikTok-App-API data endpoints)**\",\n    },\n    {\n        \"name\": \"Bilibili-Web-API\",\n        \"description\": \"**(Bilibili-Web-API数据接口/Bilibili-Web-API data endpoints)**\",\n    },\n    {\n        \"name\": \"iOS-Shortcut\",\n        \"description\": \"**(iOS快捷指令数据接口/iOS-Shortcut data endpoints)**\",\n    },\n    {\n        \"name\": \"Download\",\n        \"description\": \"**(下载数据接口/Download data endpoints)**\",\n    },\n]\n\nversion = config['API']['Version']\nupdate_time = config['API']['Update_Time']\nenvironment = config['API']['Environment']\n\ndescription = f\"\"\"\n### [中文]\n\n#### 关于\n- **Github**: [Douyin_TikTok_Download_API](https://github.com/Evil0ctal/Douyin_TikTok_Download_API)\n- **版本**: `{version}`\n- **更新时间**: `{update_time}`\n- **环境**: `{environment}`\n- **文档**: [API Documentation](https://douyin.wtf/docs)\n#### 备注\n- 本项目仅供学习交流使用，不得用于违法用途，否则后果自负。\n- 如果你不想自己部署，可以直接使用我们的在线API服务：[Douyin_TikTok_Download_API](https://douyin.wtf/docs)\n- 如果你需要更稳定以及更多功能的API服务，可以使用付费API服务：[TikHub API](https://api.tikhub.io/)\n\n### [English]\n\n#### About\n- **Github**: [Douyin_TikTok_Download_API](https://github.com/Evil0ctal/Douyin_TikTok_Download_API)\n- **Version**: `{version}`\n- **Last Updated**: `{update_time}`\n- **Environment**: `{environment}`\n- **Documentation**: [API Documentation](https://douyin.wtf/docs)\n#### Note\n- This project is for learning and communication only, and shall not be used for illegal purposes, otherwise the consequences shall be borne by yourself.\n- If you do not want to deploy it yourself, you can directly use our online API service: [Douyin_TikTok_Download_API](https://douyin.wtf/docs)\n- If you need a more stable and feature-rich API service, you can use the paid API service: [TikHub API](https://api.tikhub.io)\n\"\"\"\n\ndocs_url = config['API']['Docs_URL']\nredoc_url = config['API']['Redoc_URL']\n\napp = FastAPI(\n    title=\"Douyin TikTok Download API\",\n    description=description,\n    version=version,\n    openapi_tags=tags_metadata,\n    docs_url=docs_url,  # 文档路径\n    redoc_url=redoc_url,  # redoc文档路径\n)\n\n# API router\napp.include_router(api_router, prefix=\"/api\")\n\n# PyWebIO APP\nif config['Web']['PyWebIO_Enable']:\n    webapp = asgi_app(lambda: MainView().main_view())\n    app.mount(\"/\", webapp)\n\nif __name__ == '__main__':\n    uvicorn.run(app, host=Host_IP, port=Host_Port)\n", "metadata": {"source": "github.com/Evil0ctal/Douyin_TikTok_Download_API", "file_path": "app/main.py", "language": "python", "complexity": "medium", "star_count": 13591, "quality_score": 0.92, "features": {"has_classes": false, "has_functions": false, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": true, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 0, "class_count": 0, "line_count": 148}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create REST API endpoint with proper request/response handling\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# PyWebIO组件/PyWebIO components\nimport os\n\nimport yaml\nfrom pywebio import session, config as pywebio_config\nfrom pywebio.input import *\nfrom pywebio.output import *\n\nfrom app.web.views.About import about_pop_window\nfrom app.web.views.Document import api_document_pop_window\nfrom app.web.views.Downloader import downloader_pop_window\nfrom app.web.views.EasterEgg import a\nfrom app.web.views.ParseVideo import parse_video\nfrom app.web.views.Shortcuts import ios_pop_window\n# PyWebIO的各个视图/Views of PyWebIO\nfrom app.web.views.ViewsUtils import ViewsUtils\n\n# 读取上级再上级目录的配置文件\nconfig_path = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), 'config.yaml')\nwith open(config_path, 'r', encoding='utf-8') as file:\n    _config = yaml.safe_load(file)\n\npywebio_config(theme=_config['Web']['PyWebIO_Theme'],\n               title=_config['Web']['Tab_Title'],\n               description=_config['Web']['Description'],\n               js_file=[\n                   # 整一个看板娘，二次元浓度++\n                   _config['Web']['Live2D_JS'] if _config['Web']['Live2D_Enable'] else None,\n               ])\n\n\nclass MainView:\n    def __init__(self):\n        self.utils = ViewsUtils()\n\n    # 主界面/Main view\n    def main_view(self):\n        # 左侧导航栏/Left navbar\n        with use_scope('main'):\n            # 设置favicon/Set favicon\n            favicon_url = _config['Web']['Favicon']\n            session.run_js(f\"\"\"\n                            $('head').append('<link rel=\"icon\" type=\"image/png\" href=\"{favicon_url}\">')\n                            \"\"\")\n            # 修改footer/Remove footer\n            session.run_js(\"\"\"$('footer').remove()\"\"\")\n            # 设置不允许referrer/Set no referrer\n            session.run_js(\"\"\"$('head').append('<meta name=referrer content=no-referrer>');\"\"\")\n            # 设置标题/Set title\n            title = self.utils.t(\"TikTok/抖音无水印在线解析下载\",\n                                 \"Douyin/TikTok online parsing and download without watermark\")\n            put_html(f\"\"\"\n                    <div align=\"center\">\n                    <a href=\"/\" alt=\"logo\" ><img src=\"{favicon_url}\" width=\"100\"/></a>\n                    <h1 align=\"center\">{title}</h1>\n                    </div>\n                    \"\"\")\n            # 设置导航栏/Navbar\n            put_row(\n                [\n                    put_button(self.utils.t(\"快捷指令\", 'iOS Shortcut'),\n                               onclick=lambda: ios_pop_window(), link_style=True, small=True),\n                    put_button(self.utils.t(\"开放接口\", 'Open API'),\n                               onclick=lambda: api_document_pop_window(), link_style=True, small=True),\n                    put_button(self.utils.t(\"下载器\", \"Downloader\"),\n                               onclick=lambda: downloader_pop_window(), link_style=True, small=True),\n                    put_button(self.utils.t(\"关于\", 'About'),\n                               onclick=lambda: about_pop_window(), link_style=True, small=True),\n                ])\n\n            # 设置功能选择/Function selection\n            options = [\n                # Index: 0\n                self.utils.t('🔍批量解析视频', '🔍Batch Parse Video'),\n                # Index: 1\n                self.utils.t('🔍解析用户主页视频', '🔍Parse User Homepage Video'),\n                # Index: 2\n                self.utils.t('🥚小彩蛋', '🥚Easter Egg'),\n            ]\n            select_options = select(\n                self.utils.t('请在这里选择一个你想要的功能吧 ~', 'Please select a function you want here ~'),\n                required=True,\n                options=options,\n                help_text=self.utils.t('📎选上面的选项然后点击提交', '📎Select the options above and click Submit')\n            )\n            # 根据输入运行不同的函数\n            if select_options == options[0]:\n                parse_video()\n            elif select_options == options[1]:\n                put_markdown(self.utils.t('暂未开放，敬请期待~', 'Not yet open, please look forward to it~'))\n            elif select_options == options[2]:\n                a() if _config['Web']['Easter_Egg'] else put_markdown(self.utils.t('没有小彩蛋哦~', 'No Easter Egg~'))\n", "metadata": {"source": "github.com/Evil0ctal/Douyin_TikTok_Download_API", "file_path": "app/web/app.py", "language": "python", "complexity": "medium", "star_count": 13591, "quality_score": 0.98, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": true, "has_type_hints": true, "has_testing": false, "has_logging": true, "function_count": 2, "class_count": 1, "line_count": 93}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create REST API endpoint with proper request/response handling\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from pywebio.output import popup, put_markdown, put_html, put_text, put_link, put_image\nfrom app.web.views.ViewsUtils import ViewsUtils\n\nt = ViewsUtils().t\n\n\n# 关于弹窗/About pop-up\ndef about_pop_window():\n    with popup(t('更多信息', 'More Information')):\n        put_html('<h3>👀{}</h3>'.format(t('访问记录', 'Visit Record')))\n        put_image('https://views.whatilearened.today/views/github/evil0ctal/TikTokDownload_PyWebIO.svg',\n                  title='访问记录')\n        put_html('<hr>')\n        put_html('<h3>⭐Github</h3>')\n        put_markdown('[Douyin_TikTok_Download_API](https://github.com/Evil0ctal/Douyin_TikTok_Download_API)')\n        put_html('<hr>')\n        put_html('<h3>🎯{}</h3>'.format(t('反馈', 'Feedback')))\n        put_markdown('{}：[issues](https://github.com/Evil0ctal/Douyin_TikTok_Download_API/issues)'.format(\n            t('Bug反馈', 'Bug Feedback')))\n        put_html('<hr>')\n        put_html('<h3>💖WeChat</h3>')\n        put_markdown('WeChat：[Evil0ctal](https://mycyberpunk.com/)')\n        put_html('<hr>')\n", "metadata": {"source": "github.com/Evil0ctal/Douyin_TikTok_Download_API", "file_path": "app/web/views/About.py", "language": "python", "complexity": "simple", "star_count": 13591, "quality_score": 0.89, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": false, "has_testing": false, "has_logging": false, "function_count": 1, "class_count": 0, "line_count": 24}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create REST API endpoint with proper request/response handling\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from pywebio.output import popup, put_markdown, put_html, put_text, put_link\nfrom app.web.views.ViewsUtils import ViewsUtils\n\nt = ViewsUtils().t\n\n\n# API文档弹窗/API documentation pop-up\ndef api_document_pop_window():\n    with popup(t(\"📑API文档\", \"📑API Document\")):\n        put_markdown(t(\"> 介绍\",\n                       \"> Introduction\"))\n        put_markdown(t(\"你可以利用本项目提供的API接口来获取抖音/TikTok的数据，具体接口文档请参考下方链接。\",\n                       \"You can use the API provided by this project to obtain Douyin/TikTok data. For specific API documentation, please refer to the link below.\"))\n        put_markdown(t(\"如果API不可用，请尝试自己部署本项目，然后再配置文件中修改cookie的值。\",\n                       \"If the API is not available, please try to deploy this project by yourself, and then modify the value of the cookie in the configuration file.\"))\n        put_link('[API Docs]', '/docs', new_window=True)\n        put_markdown(\"----\")\n        put_markdown(t(\"> 更多接口\",\n                       \"> More APIs\"))\n        put_markdown(\n            t(\"[TikHub.io](https://beta-web.tikhub.io/en-us/users/signin)是一个API平台，提供包括Douyin、TikTok在内的各种公开数据接口，如果您想支持 [Douyin_TikTok_Download_API](https://github.com/Evil0ctal/Douyin_TikTok_Download_API) 项目的开发，我们强烈建议您选择[TikHub.io](https://beta-web.tikhub.io/en-us/users/signin)。\",\n              \"[TikHub.io](https://beta-web.tikhub.io/en-us/users/signin) is an API platform that provides various public data interfaces including Douyin and TikTok. If you want to support the development of the [Douyin_TikTok_Download_API](https://github.com/Evil0ctal/Douyin_TikTok_Download_API) project, we strongly recommend that you choose [TikHub.io](https://beta-web.tikhub.io/en-us/users/signin).\"))\n        put_markdown(\n            t(\"#### 特点：\",\n              \"#### Features:\"))\n        put_markdown(\n            t(\"> 📦 开箱即用\",\n              \"> 📦 Ready to use\"))\n        put_markdown(\n            t(\"简化使用流程，利用封装好的SDK迅速开展开发工作。所有API接口均依据RESTful架构设计，并使用OpenAPI规范进行描述和文档化，附带示例参数，确保调用更加简便。\",\n                \"Simplify the use process and quickly carry out development work using the encapsulated SDK. All API interfaces are designed based on the RESTful architecture and described and documented using the OpenAPI specification, with example parameters attached to ensure easier calls.\"))\n        put_markdown(\n            t(\"> 💰 成本优势\",\n              \"> 💰 Cost advantage\"))\n        put_markdown(\n            t(\"不预设套餐限制，没有月度使用门槛，所有消费按实际使用量即时计费，并且根据用户每日的请求量进行阶梯式计费，同时可以通过每日签到在用户后台进行签到获取免费的额度，并且这些免费额度不会过期。\",\n              \"There is no preset package limit, no monthly usage threshold, all consumption is billed in real time according to the actual usage, and billed in a step-by-step manner according to the user's daily request volume. At the same time, you can sign in daily in the user background to get free quotas, and these free quotas will not expire.\"))\n        put_markdown(\n            t(\"> ⚡️ 快速支持\",\n              \"> ⚡️ Quick support\"))\n        put_markdown(\n            t(\"我们有一个庞大的Discord社区服务器，管理员和其他用户会在服务器中快速的回复你，帮助你快速解决当前的问题。\",\n              \"We have a huge Discord community server, where administrators and other users will quickly reply to you in the server and help you quickly solve the current problem.\"))\n        put_markdown(\n            t(\"> 🎉 拥抱开源\",\n              \"> 🎉 Embrace open source\"))\n        put_markdown(\n            t(\"TikHub的部分源代码会开源在Github上，并且会赞助一些开源项目的作者。\",\n              \"Some of TikHub's source code will be open sourced on Github, and will sponsor some open source project authors.\"))\n        put_markdown(\n            t(\"#### 链接：\",\n              \"#### Links:\"))\n        put_markdown(\n            t(\"- Github: [TikHub Github](https://github.com/TikHubIO)\",\n                \"- Github: [TikHub Github](https://github.com/TikHubIO)\"))\n        put_markdown(\n            t(\"- Discord: [TikHub Discord](https://discord.com/invite/aMEAS8Xsvz)\",\n              \"- Discord: [TikHub Discord](https://discord.com/invite/aMEAS8Xsvz)\"))\n        put_markdown(\n            t(\"- Register: [TikHub signup](https://beta-web.tikhub.io/en-us/users/signup)\",\n              \"- Register: [TikHub signup](https://beta-web.tikhub.io/en-us/users/signup)\"))\n        put_markdown(\n            t(\"- API Docs: [TikHub API Docs](https://api.tikhub.io/)\",\n              \"- API Docs: [TikHub API Docs](https://api.tikhub.io/)\"))\n        put_markdown(\"----\")\n", "metadata": {"source": "github.com/Evil0ctal/Douyin_TikTok_Download_API", "file_path": "app/web/views/Document.py", "language": "python", "complexity": "simple", "star_count": 13591, "quality_score": 0.92, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 1, "class_count": 0, "line_count": 66}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create REST API endpoint with proper request/response handling\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from pywebio.output import popup, put_markdown, put_html, put_text, put_link\nfrom app.web.views.ViewsUtils import ViewsUtils\n\nt = ViewsUtils().t\n\n\n# 下载器弹窗/Downloader pop-up\ndef downloader_pop_window():\n    with popup(t(\"💾 下载器\", \"💾 Downloader\")):\n        put_markdown(t(\"> 桌面端下载器\", \"> Desktop Downloader\"))\n        put_markdown(t(\"你可以使用下面的开源项目在桌面端下载视频：\",\n                       \"You can use the following open source projects to download videos on the desktop:\"))\n        put_markdown(\"1. [TikTokDownload](https://github.com/Johnserf-Seed/TikTokDownload)\")\n        put_markdown(t(\"> 备注\", \"> Note\"))\n        put_markdown(t(\"1. 请注意下载器的使用规范，不要用于违法用途。\",\n                       \"1. Please pay attention to the use specifications of the downloader and do not use it for illegal purposes.\"))\n        put_markdown(t(\"2. 下载器相关问题请咨询对应项目的开发者。\",\n                       \"2. For issues related to the downloader, please consult the developer of the corresponding project.\"))\n", "metadata": {"source": "github.com/Evil0ctal/Douyin_TikTok_Download_API", "file_path": "app/web/views/Downloader.py", "language": "python", "complexity": "simple", "star_count": 13591, "quality_score": 0.89, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": false, "has_testing": false, "has_logging": false, "function_count": 1, "class_count": 0, "line_count": 19}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create REST API endpoint with proper request/response handling\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import numpy as np\nimport time\n\nimport pyfiglet\nfrom pywebio import start_server\nfrom pywebio.output import put_text, clear, put_html\n\n\ndef a():\n    H, W = 60, 80\n    g = np.random.choice([0, 1], size=(H, W))\n\n    def u():\n        n = g.copy()\n        for i in range(H):\n            for j in range(W):\n                t = sum([g[i, (j - 1) % W], g[i, (j + 1) % W], g[(i - 1) % H, j], g[(i + 1) % H, j],\n                         g[(i - 1) % H, (j - 1) % W], g[(i - 1) % H, (j + 1) % W], g[(i + 1) % H, (j - 1) % W],\n                         g[(i + 1) % H, (j + 1) % W]])\n                n[i, j] = 1 if g[i, j] == 0 and t == 3 else 0 if g[i, j] == 1 and (t < 2 or t > 3) else g[i, j]\n        return n\n\n    def m(s):\n        put_text(pyfiglet.figlet_format(s, font=\"slant\"))\n\n    def c():\n        m(''.join([chr(int(c, 2)) for c in\n                   ['01000101', '01110110', '01101001', '01101100', '01001111', '01100011', '01110100', '01100001',\n                    '01101100', '00001010', '01000111', '01000001', '01001101', '01000101', '00001010', '01001111',\n                    '01000110', '00001010', '01001100', '01001001', '01000110', '01000101', '00001010', '00110010',\n                    '00110000', '00110010', '00110100']]));\n        time.sleep(3)\n        for i in range(3, 0, -1): clear(); m(str(i)); time.sleep(1)\n        clear()\n\n    def h(g):\n        return '<table id=\"life-grid\" style=\"table-layout: fixed; border-spacing:0;\">' + ''.join('<tr>' + ''.join(\n            f'<td style=\"width:10px; height:10px; background:{\"black\" if c else \"white\"};\"></td>' for c in r) + '</tr>'\n                                                                                                 for r in\n                                                                                                 g) + '</table>'\n\n    c();\n    put_html(h(g))\n\n    def r(g):\n        return f\"<script>\" + ''.join(\n            f'document.getElementById(\"life-grid\").rows[{i}].cells[{j}].style.background = \"{\"black\" if g[i, j] else \"white\"}\";'\n            for i in range(H) for j in range(W)) + \"</script>\"\n\n    e = time.time() + 120\n    while time.time() < e:\n        time.sleep(0.1);\n        g = u();\n        put_html(r(g))\n\n\nif __name__ == '__main__':\n    # A boring code is ready to run!\n    # 原神，启动！\n    start_server(a, port=80)\n", "metadata": {"source": "github.com/Evil0ctal/Douyin_TikTok_Download_API", "file_path": "app/web/views/EasterEgg.py", "language": "python", "complexity": "simple", "star_count": 13591, "quality_score": 0.89, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 6, "class_count": 0, "line_count": 61}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create REST API endpoint with proper request/response handling\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import packaging.version\nfrom pallets_sphinx_themes import get_version\nfrom pallets_sphinx_themes import ProjectLink\n\n# Project --------------------------------------------------------------\n\nproject = \"Flask\"\ncopyright = \"2010 Pallets\"\nauthor = \"Pallets\"\nrelease, version = get_version(\"Flask\")\n\n# General --------------------------------------------------------------\n\ndefault_role = \"code\"\nextensions = [\n    \"sphinx.ext.autodoc\",\n    \"sphinx.ext.extlinks\",\n    \"sphinx.ext.intersphinx\",\n    \"sphinxcontrib.log_cabinet\",\n    \"sphinx_tabs.tabs\",\n    \"pallets_sphinx_themes\",\n]\nautodoc_member_order = \"bysource\"\nautodoc_typehints = \"description\"\nautodoc_preserve_defaults = True\nextlinks = {\n    \"issue\": (\"https://github.com/pallets/flask/issues/%s\", \"#%s\"),\n    \"pr\": (\"https://github.com/pallets/flask/pull/%s\", \"#%s\"),\n    \"ghsa\": (\"https://github.com/pallets/flask/security/advisories/GHSA-%s\", \"GHSA-%s\"),\n}\nintersphinx_mapping = {\n    \"python\": (\"https://docs.python.org/3/\", None),\n    \"werkzeug\": (\"https://werkzeug.palletsprojects.com/\", None),\n    \"click\": (\"https://click.palletsprojects.com/\", None),\n    \"jinja\": (\"https://jinja.palletsprojects.com/\", None),\n    \"itsdangerous\": (\"https://itsdangerous.palletsprojects.com/\", None),\n    \"sqlalchemy\": (\"https://docs.sqlalchemy.org/\", None),\n    \"wtforms\": (\"https://wtforms.readthedocs.io/\", None),\n    \"blinker\": (\"https://blinker.readthedocs.io/\", None),\n}\n\n# HTML -----------------------------------------------------------------\n\nhtml_theme = \"flask\"\nhtml_theme_options = {\"index_sidebar_logo\": False}\nhtml_context = {\n    \"project_links\": [\n        ProjectLink(\"Donate\", \"https://palletsprojects.com/donate\"),\n        ProjectLink(\"PyPI Releases\", \"https://pypi.org/project/Flask/\"),\n        ProjectLink(\"Source Code\", \"https://github.com/pallets/flask/\"),\n        ProjectLink(\"Issue Tracker\", \"https://github.com/pallets/flask/issues/\"),\n        ProjectLink(\"Chat\", \"https://discord.gg/pallets\"),\n    ]\n}\nhtml_sidebars = {\n    \"index\": [\"project.html\", \"localtoc.html\", \"searchbox.html\", \"ethicalads.html\"],\n    \"**\": [\"localtoc.html\", \"relations.html\", \"searchbox.html\", \"ethicalads.html\"],\n}\nsinglehtml_sidebars = {\"index\": [\"project.html\", \"localtoc.html\", \"ethicalads.html\"]}\nhtml_static_path = [\"_static\"]\nhtml_favicon = \"_static/flask-icon.svg\"\nhtml_logo = \"_static/flask-logo.svg\"\nhtml_title = f\"Flask Documentation ({version})\"\nhtml_show_sourcelink = False\n\ngettext_uuid = True\ngettext_compact = False\n\n# Local Extensions -----------------------------------------------------\n\n\ndef github_link(name, rawtext, text, lineno, inliner, options=None, content=None):\n    app = inliner.document.settings.env.app\n    release = app.config.release\n    base_url = \"https://github.com/pallets/flask/tree/\"\n\n    if text.endswith(\">\"):\n        words, text = text[:-1].rsplit(\"<\", 1)\n        words = words.strip()\n    else:\n        words = None\n\n    if packaging.version.parse(release).is_devrelease:\n        url = f\"{base_url}main/{text}\"\n    else:\n        url = f\"{base_url}{release}/{text}\"\n\n    if words is None:\n        words = url\n\n    from docutils.nodes import reference\n    from docutils.parsers.rst.roles import set_classes\n\n    options = options or {}\n    set_classes(options)\n    node = reference(rawtext, words, refuri=url, **options)\n    return [node], []\n\n\ndef setup(app):\n    app.add_role(\"gh\", github_link)\n", "metadata": {"source": "github.com/pallets/flask", "file_path": "docs/conf.py", "language": "python", "complexity": "medium", "star_count": 70038, "quality_score": 0.9500000000000001, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": false, "has_logging": true, "function_count": 2, "class_count": 0, "line_count": 102}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create REST API endpoint with proper request/response handling using Flask/FastAPI decorators\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from celery import Celery\nfrom celery import Task\nfrom flask import Flask\nfrom flask import render_template\n\n\ndef create_app() -> Flask:\n    app = Flask(__name__)\n    app.config.from_mapping(\n        CELERY=dict(\n            broker_url=\"redis://localhost\",\n            result_backend=\"redis://localhost\",\n            task_ignore_result=True,\n        ),\n    )\n    app.config.from_prefixed_env()\n    celery_init_app(app)\n\n    @app.route(\"/\")\n    def index() -> str:\n        return render_template(\"index.html\")\n\n    from . import views\n\n    app.register_blueprint(views.bp)\n    return app\n\n\ndef celery_init_app(app: Flask) -> Celery:\n    class FlaskTask(Task):\n        def __call__(self, *args: object, **kwargs: object) -> object:\n            with app.app_context():\n                return self.run(*args, **kwargs)\n\n    celery_app = Celery(app.name, task_cls=FlaskTask)\n    celery_app.config_from_object(app.config[\"CELERY\"])\n    celery_app.set_default()\n    app.extensions[\"celery\"] = celery_app\n    return celery_app\n", "metadata": {"source": "github.com/pallets/flask", "file_path": "examples/celery/src/task_app/__init__.py", "language": "python", "complexity": "medium", "star_count": 70038, "quality_score": 0.9500000000000001, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 4, "class_count": 1, "line_count": 40}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create REST API endpoint with proper request/response handling\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import time\n\nfrom celery import shared_task\nfrom celery import Task\n\n\n@shared_task(ignore_result=False)\ndef add(a: int, b: int) -> int:\n    return a + b\n\n\n@shared_task()\ndef block() -> None:\n    time.sleep(5)\n\n\n@shared_task(bind=True, ignore_result=False)\ndef process(self: Task, total: int) -> object:\n    for i in range(total):\n        self.update_state(state=\"PROGRESS\", meta={\"current\": i + 1, \"total\": total})\n        time.sleep(1)\n\n    return {\"current\": total, \"total\": total}\n", "metadata": {"source": "github.com/pallets/flask", "file_path": "examples/celery/src/task_app/tasks.py", "language": "python", "complexity": "simple", "star_count": 70038, "quality_score": 0.89, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 3, "class_count": 0, "line_count": 24}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create REST API endpoint with proper request/response handling\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from celery.result import AsyncResult\nfrom flask import Blueprint\nfrom flask import request\n\nfrom . import tasks\n\nbp = Blueprint(\"tasks\", __name__, url_prefix=\"/tasks\")\n\n\n@bp.get(\"/result/<id>\")\ndef result(id: str) -> dict[str, object]:\n    result = AsyncResult(id)\n    ready = result.ready()\n    return {\n        \"ready\": ready,\n        \"successful\": result.successful() if ready else None,\n        \"value\": result.get() if ready else result.result,\n    }\n\n\n@bp.post(\"/add\")\ndef add() -> dict[str, object]:\n    a = request.form.get(\"a\", type=int)\n    b = request.form.get(\"b\", type=int)\n    result = tasks.add.delay(a, b)\n    return {\"result_id\": result.id}\n\n\n@bp.post(\"/block\")\ndef block() -> dict[str, object]:\n    result = tasks.block.delay()\n    return {\"result_id\": result.id}\n\n\n@bp.post(\"/process\")\ndef process() -> dict[str, object]:\n    result = tasks.process.delay(total=request.form.get(\"total\", type=int))\n    return {\"result_id\": result.id}\n", "metadata": {"source": "github.com/pallets/flask", "file_path": "examples/celery/src/task_app/views.py", "language": "python", "complexity": "simple", "star_count": 70038, "quality_score": 0.89, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 4, "class_count": 0, "line_count": 39}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate module integration for Create REST API endpoint with proper request/response handling\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from flask import Flask\n\napp = Flask(__name__)\n\nfrom js_example import views  # noqa: E402, F401\n", "metadata": {"source": "github.com/pallets/flask", "file_path": "examples/javascript/js_example/__init__.py", "language": "python", "complexity": "simple", "star_count": 70038, "quality_score": 0.6600000000000001, "features": {"has_classes": false, "has_functions": false, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 0, "class_count": 0, "line_count": 6}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create REST API endpoint with proper request/response handling using Flask/FastAPI decorators\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from flask import jsonify\nfrom flask import render_template\nfrom flask import request\n\nfrom . import app\n\n\n@app.route(\"/\", defaults={\"js\": \"fetch\"})\n@app.route(\"/<any(xhr, jquery, fetch):js>\")\ndef index(js):\n    return render_template(f\"{js}.html\", js=js)\n\n\n@app.route(\"/add\", methods=[\"POST\"])\ndef add():\n    a = request.form.get(\"a\", 0, type=float)\n    b = request.form.get(\"b\", 0, type=float)\n    return jsonify(result=a + b)\n", "metadata": {"source": "github.com/pallets/flask", "file_path": "examples/javascript/js_example/views.py", "language": "python", "complexity": "simple", "star_count": 70038, "quality_score": 0.89, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 2, "class_count": 0, "line_count": 19}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create REST API endpoint with proper request/response handling\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import pytest\n\nfrom js_example import app\n\n\n@pytest.fixture(name=\"app\")\ndef fixture_app():\n    app.testing = True\n    yield app\n    app.testing = False\n\n\n@pytest.fixture\ndef client(app):\n    return app.test_client()\n", "metadata": {"source": "github.com/pallets/flask", "file_path": "examples/javascript/tests/conftest.py", "language": "python", "complexity": "simple", "star_count": 70038, "quality_score": 0.92, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": false, "has_testing": true, "has_logging": false, "function_count": 2, "class_count": 0, "line_count": 16}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create REST API endpoint with proper request/response handling with pytest testing framework\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import pytest\nfrom flask import template_rendered\n\n\n@pytest.mark.parametrize(\n    (\"path\", \"template_name\"),\n    (\n        (\"/\", \"fetch.html\"),\n        (\"/plain\", \"xhr.html\"),\n        (\"/fetch\", \"fetch.html\"),\n        (\"/jquery\", \"jquery.html\"),\n    ),\n)\ndef test_index(app, client, path, template_name):\n    def check(sender, template, context):\n        assert template.name == template_name\n\n    with template_rendered.connected_to(check, app):\n        client.get(path)\n\n\n@pytest.mark.parametrize(\n    (\"a\", \"b\", \"result\"), ((2, 3, 5), (2.5, 3, 5.5), (2, None, 2), (2, \"b\", 2))\n)\ndef test_add(client, a, b, result):\n    response = client.post(\"/add\", data={\"a\": a, \"b\": b})\n    assert response.get_json()[\"result\"] == result\n", "metadata": {"source": "github.com/pallets/flask", "file_path": "examples/javascript/tests/test_js_example.py", "language": "python", "complexity": "simple", "star_count": 70038, "quality_score": 0.9500000000000001, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": true, "has_logging": false, "function_count": 3, "class_count": 0, "line_count": 28}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create REST API endpoint with proper request/response handling using Flask/FastAPI decorators\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import os\n\nfrom flask import Flask\n\n\ndef create_app(test_config=None):\n    \"\"\"Create and configure an instance of the Flask application.\"\"\"\n    app = Flask(__name__, instance_relative_config=True)\n    app.config.from_mapping(\n        # a default secret that should be overridden by instance config\n        SECRET_KEY=\"dev\",\n        # store the database in the instance folder\n        DATABASE=os.path.join(app.instance_path, \"flaskr.sqlite\"),\n    )\n\n    if test_config is None:\n        # load the instance config, if it exists, when not testing\n        app.config.from_pyfile(\"config.py\", silent=True)\n    else:\n        # load the test config if passed in\n        app.config.update(test_config)\n\n    # ensure the instance folder exists\n    try:\n        os.makedirs(app.instance_path)\n    except OSError:\n        pass\n\n    @app.route(\"/hello\")\n    def hello():\n        return \"Hello, World!\"\n\n    # register the database commands\n    from . import db\n\n    db.init_app(app)\n\n    # apply the blueprints to the app\n    from . import auth\n    from . import blog\n\n    app.register_blueprint(auth.bp)\n    app.register_blueprint(blog.bp)\n\n    # make url_for('index') == url_for('blog.index')\n    # in another app, you might define a separate main index here with\n    # app.route, while giving the blog blueprint a url_prefix, but for\n    # the tutorial the blog will be the main index\n    app.add_url_rule(\"/\", endpoint=\"index\")\n\n    return app\n", "metadata": {"source": "github.com/pallets/flask", "file_path": "examples/tutorial/flaskr/__init__.py", "language": "python", "complexity": "simple", "star_count": 70038, "quality_score": 1.0, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": true, "has_documentation": true, "has_type_hints": false, "has_testing": true, "has_logging": true, "function_count": 2, "class_count": 0, "line_count": 52}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create REST API endpoint with proper request/response handling\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import functools\n\nfrom flask import Blueprint\nfrom flask import flash\nfrom flask import g\nfrom flask import redirect\nfrom flask import render_template\nfrom flask import request\nfrom flask import session\nfrom flask import url_for\nfrom werkzeug.security import check_password_hash\nfrom werkzeug.security import generate_password_hash\n\nfrom .db import get_db\n\nbp = Blueprint(\"auth\", __name__, url_prefix=\"/auth\")\n\n\ndef login_required(view):\n    \"\"\"View decorator that redirects anonymous users to the login page.\"\"\"\n\n    @functools.wraps(view)\n    def wrapped_view(**kwargs):\n        if g.user is None:\n            return redirect(url_for(\"auth.login\"))\n\n        return view(**kwargs)\n\n    return wrapped_view\n\n\n@bp.before_app_request\ndef load_logged_in_user():\n    \"\"\"If a user id is stored in the session, load the user object from\n    the database into ``g.user``.\"\"\"\n    user_id = session.get(\"user_id\")\n\n    if user_id is None:\n        g.user = None\n    else:\n        g.user = (\n            get_db().execute(\"SELECT * FROM user WHERE id = ?\", (user_id,)).fetchone()\n        )\n\n\n@bp.route(\"/register\", methods=(\"GET\", \"POST\"))\ndef register():\n    \"\"\"Register a new user.\n\n    Validates that the username is not already taken. Hashes the\n    password for security.\n    \"\"\"\n    if request.method == \"POST\":\n        username = request.form[\"username\"]\n        password = request.form[\"password\"]\n        db = get_db()\n        error = None\n\n        if not username:\n            error = \"Username is required.\"\n        elif not password:\n            error = \"Password is required.\"\n\n        if error is None:\n            try:\n                db.execute(\n                    \"INSERT INTO user (username, password) VALUES (?, ?)\",\n                    (username, generate_password_hash(password)),\n                )\n                db.commit()\n            except db.IntegrityError:\n                # The username was already taken, which caused the\n                # commit to fail. Show a validation error.\n                error = f\"User {username} is already registered.\"\n            else:\n                # Success, go to the login page.\n                return redirect(url_for(\"auth.login\"))\n\n        flash(error)\n\n    return render_template(\"auth/register.html\")\n\n\n@bp.route(\"/login\", methods=(\"GET\", \"POST\"))\ndef login():\n    \"\"\"Log in a registered user by adding the user id to the session.\"\"\"\n    if request.method == \"POST\":\n        username = request.form[\"username\"]\n        password = request.form[\"password\"]\n        db = get_db()\n        error = None\n        user = db.execute(\n            \"SELECT * FROM user WHERE username = ?\", (username,)\n        ).fetchone()\n\n        if user is None:\n            error = \"Incorrect username.\"\n        elif not check_password_hash(user[\"password\"], password):\n            error = \"Incorrect password.\"\n\n        if error is None:\n            # store the user id in a new session and return to the index\n            session.clear()\n            session[\"user_id\"] = user[\"id\"]\n            return redirect(url_for(\"index\"))\n\n        flash(error)\n\n    return render_template(\"auth/login.html\")\n\n\n@bp.route(\"/logout\")\ndef logout():\n    \"\"\"Clear the current session, including the stored user id.\"\"\"\n    session.clear()\n    return redirect(url_for(\"index\"))\n", "metadata": {"source": "github.com/pallets/flask", "file_path": "examples/tutorial/flaskr/auth.py", "language": "python", "complexity": "medium", "star_count": 70038, "quality_score": 0.9500000000000001, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": true, "has_documentation": true, "has_type_hints": false, "has_testing": false, "has_logging": true, "function_count": 6, "class_count": 0, "line_count": 117}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create REST API endpoint with proper request/response handling\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from flask import Blueprint\nfrom flask import flash\nfrom flask import g\nfrom flask import redirect\nfrom flask import render_template\nfrom flask import request\nfrom flask import url_for\nfrom werkzeug.exceptions import abort\n\nfrom .auth import login_required\nfrom .db import get_db\n\nbp = Blueprint(\"blog\", __name__)\n\n\n@bp.route(\"/\")\ndef index():\n    \"\"\"Show all the posts, most recent first.\"\"\"\n    db = get_db()\n    posts = db.execute(\n        \"SELECT p.id, title, body, created, author_id, username\"\n        \" FROM post p JOIN user u ON p.author_id = u.id\"\n        \" ORDER BY created DESC\"\n    ).fetchall()\n    return render_template(\"blog/index.html\", posts=posts)\n\n\ndef get_post(id, check_author=True):\n    \"\"\"Get a post and its author by id.\n\n    Checks that the id exists and optionally that the current user is\n    the author.\n\n    :param id: id of post to get\n    :param check_author: require the current user to be the author\n    :return: the post with author information\n    :raise 404: if a post with the given id doesn't exist\n    :raise 403: if the current user isn't the author\n    \"\"\"\n    post = (\n        get_db()\n        .execute(\n            \"SELECT p.id, title, body, created, author_id, username\"\n            \" FROM post p JOIN user u ON p.author_id = u.id\"\n            \" WHERE p.id = ?\",\n            (id,),\n        )\n        .fetchone()\n    )\n\n    if post is None:\n        abort(404, f\"Post id {id} doesn't exist.\")\n\n    if check_author and post[\"author_id\"] != g.user[\"id\"]:\n        abort(403)\n\n    return post\n\n\n@bp.route(\"/create\", methods=(\"GET\", \"POST\"))\n@login_required\ndef create():\n    \"\"\"Create a new post for the current user.\"\"\"\n    if request.method == \"POST\":\n        title = request.form[\"title\"]\n        body = request.form[\"body\"]\n        error = None\n\n        if not title:\n            error = \"Title is required.\"\n\n        if error is not None:\n            flash(error)\n        else:\n            db = get_db()\n            db.execute(\n                \"INSERT INTO post (title, body, author_id) VALUES (?, ?, ?)\",\n                (title, body, g.user[\"id\"]),\n            )\n            db.commit()\n            return redirect(url_for(\"blog.index\"))\n\n    return render_template(\"blog/create.html\")\n\n\n@bp.route(\"/<int:id>/update\", methods=(\"GET\", \"POST\"))\n@login_required\ndef update(id):\n    \"\"\"Update a post if the current user is the author.\"\"\"\n    post = get_post(id)\n\n    if request.method == \"POST\":\n        title = request.form[\"title\"]\n        body = request.form[\"body\"]\n        error = None\n\n        if not title:\n            error = \"Title is required.\"\n\n        if error is not None:\n            flash(error)\n        else:\n            db = get_db()\n            db.execute(\n                \"UPDATE post SET title = ?, body = ? WHERE id = ?\", (title, body, id)\n            )\n            db.commit()\n            return redirect(url_for(\"blog.index\"))\n\n    return render_template(\"blog/update.html\", post=post)\n\n\n@bp.route(\"/<int:id>/delete\", methods=(\"POST\",))\n@login_required\ndef delete(id):\n    \"\"\"Delete a post.\n\n    Ensures that the post exists and that the logged in user is the\n    author of the post.\n    \"\"\"\n    get_post(id)\n    db = get_db()\n    db.execute(\"DELETE FROM post WHERE id = ?\", (id,))\n    db.commit()\n    return redirect(url_for(\"blog.index\"))\n", "metadata": {"source": "github.com/pallets/flask", "file_path": "examples/tutorial/flaskr/blog.py", "language": "python", "complexity": "medium", "star_count": 70038, "quality_score": 0.92, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": true, "has_type_hints": true, "has_testing": false, "has_logging": true, "function_count": 5, "class_count": 0, "line_count": 126}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create REST API endpoint with proper request/response handling\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import sqlite3\nfrom datetime import datetime\n\nimport click\nfrom flask import current_app\nfrom flask import g\n\n\ndef get_db():\n    \"\"\"Connect to the application's configured database. The connection\n    is unique for each request and will be reused if this is called\n    again.\n    \"\"\"\n    if \"db\" not in g:\n        g.db = sqlite3.connect(\n            current_app.config[\"DATABASE\"], detect_types=sqlite3.PARSE_DECLTYPES\n        )\n        g.db.row_factory = sqlite3.Row\n\n    return g.db\n\n\ndef close_db(e=None):\n    \"\"\"If this request connected to the database, close the\n    connection.\n    \"\"\"\n    db = g.pop(\"db\", None)\n\n    if db is not None:\n        db.close()\n\n\ndef init_db():\n    \"\"\"Clear existing data and create new tables.\"\"\"\n    db = get_db()\n\n    with current_app.open_resource(\"schema.sql\") as f:\n        db.executescript(f.read().decode(\"utf8\"))\n\n\n@click.command(\"init-db\")\ndef init_db_command():\n    \"\"\"Clear existing data and create new tables.\"\"\"\n    init_db()\n    click.echo(\"Initialized the database.\")\n\n\nsqlite3.register_converter(\"timestamp\", lambda v: datetime.fromisoformat(v.decode()))\n\n\ndef init_app(app):\n    \"\"\"Register database functions with the Flask app. This is called by\n    the application factory.\n    \"\"\"\n    app.teardown_appcontext(close_db)\n    app.cli.add_command(init_db_command)\n", "metadata": {"source": "github.com/pallets/flask", "file_path": "examples/tutorial/flaskr/db.py", "language": "python", "complexity": "simple", "star_count": 70038, "quality_score": 0.9500000000000001, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": true, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 5, "class_count": 0, "line_count": 57}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create REST API endpoint with proper request/response handling\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import os\nimport tempfile\n\nimport pytest\n\nfrom flaskr import create_app\nfrom flaskr.db import get_db\nfrom flaskr.db import init_db\n\n# read in SQL for populating test data\nwith open(os.path.join(os.path.dirname(__file__), \"data.sql\"), \"rb\") as f:\n    _data_sql = f.read().decode(\"utf8\")\n\n\n@pytest.fixture\ndef app():\n    \"\"\"Create and configure a new app instance for each test.\"\"\"\n    # create a temporary file to isolate the database for each test\n    db_fd, db_path = tempfile.mkstemp()\n    # create the app with common test config\n    app = create_app({\"TESTING\": True, \"DATABASE\": db_path})\n\n    # create the database and load test data\n    with app.app_context():\n        init_db()\n        get_db().executescript(_data_sql)\n\n    yield app\n\n    # close and remove the temporary database\n    os.close(db_fd)\n    os.unlink(db_path)\n\n\n@pytest.fixture\ndef client(app):\n    \"\"\"A test client for the app.\"\"\"\n    return app.test_client()\n\n\n@pytest.fixture\ndef runner(app):\n    \"\"\"A test runner for the app's Click commands.\"\"\"\n    return app.test_cli_runner()\n\n\nclass AuthActions:\n    def __init__(self, client):\n        self._client = client\n\n    def login(self, username=\"test\", password=\"test\"):\n        return self._client.post(\n            \"/auth/login\", data={\"username\": username, \"password\": password}\n        )\n\n    def logout(self):\n        return self._client.get(\"/auth/logout\")\n\n\n@pytest.fixture\ndef auth(client):\n    return AuthActions(client)\n", "metadata": {"source": "github.com/pallets/flask", "file_path": "examples/tutorial/tests/conftest.py", "language": "python", "complexity": "medium", "star_count": 70038, "quality_score": 1.0, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": true, "has_type_hints": true, "has_testing": true, "has_logging": true, "function_count": 7, "class_count": 1, "line_count": 63}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create REST API endpoint with proper request/response handling with pytest testing framework\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import pytest\nfrom flask import g\nfrom flask import session\n\nfrom flaskr.db import get_db\n\n\ndef test_register(client, app):\n    # test that viewing the page renders without template errors\n    assert client.get(\"/auth/register\").status_code == 200\n\n    # test that successful registration redirects to the login page\n    response = client.post(\"/auth/register\", data={\"username\": \"a\", \"password\": \"a\"})\n    assert response.headers[\"Location\"] == \"/auth/login\"\n\n    # test that the user was inserted into the database\n    with app.app_context():\n        assert (\n            get_db().execute(\"SELECT * FROM user WHERE username = 'a'\").fetchone()\n            is not None\n        )\n\n\n@pytest.mark.parametrize(\n    (\"username\", \"password\", \"message\"),\n    (\n        (\"\", \"\", b\"Username is required.\"),\n        (\"a\", \"\", b\"Password is required.\"),\n        (\"test\", \"test\", b\"already registered\"),\n    ),\n)\ndef test_register_validate_input(client, username, password, message):\n    response = client.post(\n        \"/auth/register\", data={\"username\": username, \"password\": password}\n    )\n    assert message in response.data\n\n\ndef test_login(client, auth):\n    # test that viewing the page renders without template errors\n    assert client.get(\"/auth/login\").status_code == 200\n\n    # test that successful login redirects to the index page\n    response = auth.login()\n    assert response.headers[\"Location\"] == \"/\"\n\n    # login request set the user_id in the session\n    # check that the user is loaded from the session\n    with client:\n        client.get(\"/\")\n        assert session[\"user_id\"] == 1\n        assert g.user[\"username\"] == \"test\"\n\n\n@pytest.mark.parametrize(\n    (\"username\", \"password\", \"message\"),\n    ((\"a\", \"test\", b\"Incorrect username.\"), (\"test\", \"a\", b\"Incorrect password.\")),\n)\ndef test_login_validate_input(auth, username, password, message):\n    response = auth.login(username, password)\n    assert message in response.data\n\n\ndef test_logout(client, auth):\n    auth.login()\n\n    with client:\n        auth.logout()\n        assert \"user_id\" not in session\n", "metadata": {"source": "github.com/pallets/flask", "file_path": "examples/tutorial/tests/test_auth.py", "language": "python", "complexity": "simple", "star_count": 70038, "quality_score": 0.9500000000000001, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": true, "has_logging": true, "function_count": 5, "class_count": 0, "line_count": 70}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create REST API endpoint with proper request/response handling with pytest testing framework\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import pytest\n\nfrom flaskr.db import get_db\n\n\ndef test_index(client, auth):\n    response = client.get(\"/\")\n    assert b\"Log In\" in response.data\n    assert b\"Register\" in response.data\n\n    auth.login()\n    response = client.get(\"/\")\n    assert b\"test title\" in response.data\n    assert b\"by test on 2018-01-01\" in response.data\n    assert b\"test\\nbody\" in response.data\n    assert b'href=\"/1/update\"' in response.data\n\n\n@pytest.mark.parametrize(\"path\", (\"/create\", \"/1/update\", \"/1/delete\"))\ndef test_login_required(client, path):\n    response = client.post(path)\n    assert response.headers[\"Location\"] == \"/auth/login\"\n\n\ndef test_author_required(app, client, auth):\n    # change the post author to another user\n    with app.app_context():\n        db = get_db()\n        db.execute(\"UPDATE post SET author_id = 2 WHERE id = 1\")\n        db.commit()\n\n    auth.login()\n    # current user can't modify other user's post\n    assert client.post(\"/1/update\").status_code == 403\n    assert client.post(\"/1/delete\").status_code == 403\n    # current user doesn't see edit link\n    assert b'href=\"/1/update\"' not in client.get(\"/\").data\n\n\n@pytest.mark.parametrize(\"path\", (\"/2/update\", \"/2/delete\"))\ndef test_exists_required(client, auth, path):\n    auth.login()\n    assert client.post(path).status_code == 404\n\n\ndef test_create(client, auth, app):\n    auth.login()\n    assert client.get(\"/create\").status_code == 200\n    client.post(\"/create\", data={\"title\": \"created\", \"body\": \"\"})\n\n    with app.app_context():\n        db = get_db()\n        count = db.execute(\"SELECT COUNT(id) FROM post\").fetchone()[0]\n        assert count == 2\n\n\ndef test_update(client, auth, app):\n    auth.login()\n    assert client.get(\"/1/update\").status_code == 200\n    client.post(\"/1/update\", data={\"title\": \"updated\", \"body\": \"\"})\n\n    with app.app_context():\n        db = get_db()\n        post = db.execute(\"SELECT * FROM post WHERE id = 1\").fetchone()\n        assert post[\"title\"] == \"updated\"\n\n\n@pytest.mark.parametrize(\"path\", (\"/create\", \"/1/update\"))\ndef test_create_update_validate(client, auth, path):\n    auth.login()\n    response = client.post(path, data={\"title\": \"\", \"body\": \"\"})\n    assert b\"Title is required.\" in response.data\n\n\ndef test_delete(client, auth, app):\n    auth.login()\n    response = client.post(\"/1/delete\")\n    assert response.headers[\"Location\"] == \"/\"\n\n    with app.app_context():\n        db = get_db()\n        post = db.execute(\"SELECT * FROM post WHERE id = 1\").fetchone()\n        assert post is None\n", "metadata": {"source": "github.com/pallets/flask", "file_path": "examples/tutorial/tests/test_blog.py", "language": "python", "complexity": "medium", "star_count": 70038, "quality_score": 0.9500000000000001, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": true, "has_logging": true, "function_count": 8, "class_count": 0, "line_count": 84}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate configuration for Create REST API endpoint with proper request/response handling\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\n# Single line comment\nletter = 'P'                # A string could be a single character or a bunch of texts\nprint(letter)               # P\nprint(len(letter))          # 1\ngreeting = 'Hello, World!'  # String could be  a single or double quote,\"Hello, World!\"\nprint(greeting)             # Hello, World!\nprint(len(greeting))        # 13\nsentence = \"I hope you are enjoying 30 days of python challenge\"\nprint(sentence)\n\n# Multiline String\nmultiline_string = '''I am a teacher and enjoy teaching.\nI didn't find anything as rewarding as empowering people.\nThat is why I created 30 days of python.'''\nprint(multiline_string)\n# Another way of doing the same thing\nmultiline_string = \"\"\"I am a teacher and enjoy teaching.\nI didn't find anything as rewarding as empowering people.\nThat is why I created 30 days of python.\"\"\"\nprint(multiline_string)\n\n# String Concatenation\nfirst_name = 'Asabeneh'\nlast_name = 'Yetayeh'\nspace = ' '\nfull_name = first_name  +  space + last_name\nprint(full_name) # Asabeneh Yetayeh\n# Checking length of a string using len() builtin function\nprint(len(first_name))  # 8\nprint(len(last_name))   # 7\nprint(len(first_name) > len(last_name)) # True\nprint(len(full_name)) # 15\n\n#### Unpacking characters \nlanguage = 'Python'\na,b,c,d,e,f = language # unpacking sequence characters into variables\nprint(a) # P\nprint(b) # y\nprint(c) # t \nprint(d) # h\nprint(e) # o\nprint(f) # n\n\n# Accessing characters in strings by index\nlanguage = 'Python'\nfirst_letter = language[0]\nprint(first_letter) # P\nsecond_letter = language[1]\nprint(second_letter) # y\nlast_index = len(language) - 1\nlast_letter = language[last_index]\nprint(last_letter) # n\n\n# If we want to start from right end we can use negative indexing. -1 is the last index\nlanguage = 'Python'\nlast_letter = language[-1]\nprint(last_letter) # n\nsecond_last = language[-2]\nprint(second_last) # o\n\n# Slicing\n\nlanguage = 'Python'\nfirst_three = language[0:3] # starts at zero index and up to 3 but not include 3\nlast_three = language[3:6]\nprint(last_three) # hon\n# Another way\nlast_three = language[-3:]\nprint(last_three)   # hon\nlast_three = language[3:]\nprint(last_three)   # hon\n\n# Skipping character while splitting Python strings\nlanguage = 'Python'\npto = language[0:6:2] # \nprint(pto) # pto\n\n# Escape sequence\nprint('I hope every one enjoying the python challenge.\\nDo you ?') # line break\nprint('Days\\tTopics\\tExercises')\nprint('Day 1\\t3\\t5')\nprint('Day 2\\t3\\t5')\nprint('Day 3\\t3\\t5')\nprint('Day 4\\t3\\t5')\nprint('This is a back slash  symbol (\\\\)') # To write a back slash\nprint('In every programming language it starts with \\\"Hello, World!\\\"')\n\n## String Methods\n# capitalize(): Converts the first character the string to Capital Letter\n\nchallenge = 'thirty days of python'\nprint(challenge.capitalize()) # 'Thirty days of python'\n\n# count(): returns occurrences of substring in string, count(substring, start=.., end=..)\n\nchallenge = 'thirty days of python'\nprint(challenge.count('y')) # 3\nprint(challenge.count('y', 7, 14)) # 1\nprint(challenge.count('th')) # 2`\n\n# endswith(): Checks if a string ends with a specified ending\n\nchallenge = 'thirty days of python'\nprint(challenge.endswith('on'))   # True\nprint(challenge.endswith('tion')) # False\n\n# expandtabs(): Replaces tab character with spaces, default tab size is 8. It takes tab size argument\n\nchallenge = 'thirty\\tdays\\tof\\tpython'\nprint(challenge.expandtabs())   # 'thirty  days    of      python'\nprint(challenge.expandtabs(10)) # 'thirty    days      of        python'\n\n# find(): Returns the index of first occurrence of substring\n\nchallenge = 'thirty days of python'\nprint(challenge.find('y'))  # 5\nprint(challenge.find('th')) # 0\n\n# format()\tformats string into nicer output    \nfirst_name = 'Asabeneh'\nlast_name = 'Yetayeh'\njob = 'teacher'\ncountry = 'Finland'\nsentence = 'I am {} {}. I am a {}. I live in {}.'.format(first_name, last_name, job, country)\nprint(sentence) # I am Asabeneh Yetayeh. I am a teacher. I live in Finland.\n\nradius = 10\npi = 3.14\narea = pi # radius ## 2\nresult = 'The area of circle with {} is {}'.format(str(radius), str(area))\nprint(result) # The area of circle with 10 is 314.0\n\n# index(): Returns the index of substring\nchallenge = 'thirty days of python'\nprint(challenge.find('y'))  # 5\nprint(challenge.find('th')) # 0\n\n# isalnum(): Checks alphanumeric character\n\nchallenge = 'ThirtyDaysPython'\nprint(challenge.isalnum()) # True\n\nchallenge = '30DaysPython'\nprint(challenge.isalnum()) # True\n\nchallenge = 'thirty days of python'\nprint(challenge.isalnum()) # False\n\nchallenge = 'thirty days of python 2019'\nprint(challenge.isalnum()) # False\n\n# isalpha(): Checks if all characters are alphabets\n\nchallenge = 'thirty days of python'\nprint(challenge.isalpha()) # True\nnum = '123'\nprint(num.isalpha())      # False\n\n# isdecimal(): Checks Decimal Characters\n\nchallenge = 'thirty days of python'\nprint(challenge.find('y'))  # 5\nprint(challenge.find('th')) # 0\n\n# isdigit(): Checks Digit Characters\n\nchallenge = 'Thirty'\nprint(challenge.isdigit()) # False\nchallenge = '30'\nprint(challenge.digit())   # True\n\n# isdecimal():Checks decimal characters\n\nnum = '10'\nprint(num.isdecimal()) # True\nnum = '10.5'\nprint(num.isdecimal()) # False\n\n\n# isidentifier():Checks for valid identifier means it check if a string is a valid variable name\n\nchallenge = '30DaysOfPython'\nprint(challenge.isidentifier()) # False, because it starts with a number\nchallenge = 'thirty_days_of_python'\nprint(challenge.isidentifier()) # True\n\n\n# islower():Checks if all alphabets in a string are lowercase\n\nchallenge = 'thirty days of python'\nprint(challenge.islower()) # True\nchallenge = 'Thirty days of python'\nprint(challenge.islower()) # False\n\n# isupper(): returns if all characters are uppercase characters\n\nchallenge = 'thirty days of python'\nprint(challenge.isupper()) #  False\nchallenge = 'THIRTY DAYS OF PYTHON'\nprint(challenge.isupper()) # True\n\n\n# isnumeric():Checks numeric characters\n\nnum = '10'\nprint(num.isnumeric())      # True\nprint('ten'.isnumeric())    # False\n\n# join(): Returns a concatenated string\n\nweb_tech = ['HTML', 'CSS', 'JavaScript', 'React']\nresult = '#, '.join(web_tech)\nprint(result) # 'HTML# CSS# JavaScript# React'\n\n# strip(): Removes both leading and trailing characters\n\nchallenge = ' thirty days of python '\nprint(challenge.strip('y')) # 5\n\n# replace(): Replaces substring inside\n\nchallenge = 'thirty days of python'\nprint(challenge.replace('python', 'coding')) # 'thirty days of coding'\n\n# split():Splits String from Left\n\nchallenge = 'thirty days of python'\nprint(challenge.split()) # ['thirty', 'days', 'of', 'python']\n\n# title(): Returns a Title Cased String\n\nchallenge = 'thirty days of python'\nprint(challenge.title()) # Thirty Days Of Python\n\n# swapcase(): Checks if String Starts with the Specified String\n  \nchallenge = 'thirty days of python'\nprint(challenge.swapcase())   # THIRTY DAYS OF PYTHON\nchallenge = 'Thirty Days Of Python'\nprint(challenge.swapcase())  # tHIRTY dAYS oF pYTHON\n\n# startswith(): Checks if String Starts with the Specified String\n\nchallenge = 'thirty days of python'\nprint(challenge.startswith('thirty')) # True\nchallenge = '30 days of python'\nprint(challenge.startswith('thirty')) # False", "metadata": {"source": "github.com/Asabeneh/30-Days-Of-Python", "file_path": "04_Day_Strings/day_4.py", "language": "python", "complexity": "complex", "star_count": 47914, "quality_score": 0.89, "features": {"has_classes": false, "has_functions": false, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": true, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 0, "class_count": 0, "line_count": 248}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create REST API endpoint with proper request/response handling\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "def generate_full_name(firstname, lastname):\n      space = ' '\n      fullname = firstname + space + lastname\n      return fullname\n\ndef sum_two_nums (num1, num2):\n    return num1 + num2\ngravity = 9.81\nperson = {\n    \"firstname\": \"Asabeneh\",\n    \"age\": 250,\n    \"country\": \"Finland\",\n    \"city\":'Helsinki'\n}\n\n\n", "metadata": {"source": "github.com/Asabeneh/30-Days-Of-Python", "file_path": "12_Day_Modules/mymodule.py", "language": "python", "complexity": "simple", "star_count": 47914, "quality_score": 0.8300000000000001, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": false, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 2, "class_count": 0, "line_count": 17}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create REST API endpoint with proper request/response handling\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "def add_numbers(*args):\n    total = 0\n    for num in args:\n        total += num\n    return total\n\n\ndef subtract(a, b):\n    return (a - b)\n\n\ndef multiple(a, b):\n    return a * b\n\n\ndef division(a, b):\n    return a / b\n\n\ndef remainder(a, b):\n    return a % b\n\n\ndef power(a, b):\n    return a ** b", "metadata": {"source": "github.com/Asabeneh/30-Days-Of-Python", "file_path": "20_Day_Python_package_manager/arithmetic.py", "language": "python", "complexity": "simple", "star_count": 47914, "quality_score": 0.8300000000000001, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": false, "has_error_handling": false, "has_documentation": false, "has_type_hints": false, "has_testing": false, "has_logging": false, "function_count": 6, "class_count": 0, "line_count": 25}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create REST API endpoint with proper request/response handling\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "def greet_person(firstname, lastname):\n    return f'{firstname} {lastname}, welcome to 30DaysOfPython Challenge!'", "metadata": {"source": "github.com/Asabeneh/30-Days-Of-Python", "file_path": "20_Day_Python_package_manager/greet.py", "language": "python", "complexity": "simple", "star_count": 47914, "quality_score": 0.6300000000000001, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": false, "has_error_handling": false, "has_documentation": false, "has_type_hints": false, "has_testing": false, "has_logging": false, "function_count": 1, "class_count": 0, "line_count": 2}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create REST API endpoint with proper request/response handling\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "def generate_full_name(firstname, lastname):\n      space = ' '\n      fullname = firstname + space + lastname\n      return fullname\n\ndef sum_two_nums (num1, num2):\n    return num1 + num2\ngravity = 9.81\nperson = {\n    \"firstname\": \"Asabeneh\",\n    \"age\": 250,\n    \"country\": \"Finland\",\n    \"city\":'Helsinki'\n}\n\n\n", "metadata": {"source": "github.com/Asabeneh/30-Days-Of-Python", "file_path": "mymodule.py", "language": "python", "complexity": "simple", "star_count": 47914, "quality_score": 0.8300000000000001, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": false, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 2, "class_count": 0, "line_count": 17}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create REST API endpoint with proper request/response handling\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "def add_numbers(*args):\n    total = 0\n    for num in args:\n        total += num\n    return total\n\n\ndef subtract(a, b):\n    return (a - b)\n\n\ndef multiple(a, b):\n    return a * b\n\n\ndef division(a, b):\n    return a / b\n\n\ndef remainder(a, b):\n    return a % b\n\n\ndef power(a, b):\n    return a ** b\n\n\n\n", "metadata": {"source": "github.com/Asabeneh/30-Days-Of-Python", "file_path": "mypackage/arithmetics.py", "language": "python", "complexity": "simple", "star_count": 47914, "quality_score": 0.8300000000000001, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": false, "has_error_handling": false, "has_documentation": false, "has_type_hints": false, "has_testing": false, "has_logging": false, "function_count": 6, "class_count": 0, "line_count": 29}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create REST API endpoint with proper request/response handling\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "def greet_person(firstname, lastname):\n    return f'{firstname} {lastname}, welcome to 30DaysOfPython Challenge!'\n", "metadata": {"source": "github.com/Asabeneh/30-Days-Of-Python", "file_path": "mypackage/greet.py", "language": "python", "complexity": "simple", "star_count": 47914, "quality_score": 0.6300000000000001, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": false, "has_error_handling": false, "has_documentation": false, "has_type_hints": false, "has_testing": false, "has_logging": false, "function_count": 1, "class_count": 0, "line_count": 3}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create REST API endpoint with proper request/response handling using Flask/FastAPI decorators\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# let's import the flask\nfrom flask import Flask, render_template, request, redirect, url_for\nimport os # importing operating system module\n\napp = Flask(__name__)\n# to stop caching static file\napp.config['SEND_FILE_MAX_AGE_DEFAULT'] = 0\n\n\n\n@app.route('/') # this decorator create the home route\ndef home ():\n    techs = ['HTML', 'CSS', 'Flask', 'Python']\n    name = '30 Days Of Python Programming'\n    return render_template('home.html', techs=techs, name = name, title = 'Home')\n\n@app.route('/about')\ndef about():\n    name = '30 Days Of Python Programming'\n    return render_template('about.html', name = name, title = 'About Us')\n\n@app.route('/result')\ndef result():\n    return render_template('result.html')\n\n@app.route('/post', methods= ['GET','POST'])\ndef post():\n    name = 'Text Analyzer'\n    if request.method == 'GET':\n         return render_template('post.html', name = name, title = name)\n    if request.method =='POST':\n        content = request.form['content']\n        return redirect(url_for('result'))\n   \nif __name__ == '__main__':\n    # for deployment\n    # to make it work for both production and development\n    port = int(os.environ.get(\"PORT\", 5000))\n    app.run(debug=True, host='0.0.0.0', port=port)", "metadata": {"source": "github.com/Asabeneh/30-Days-Of-Python", "file_path": "python_for_web/app.py", "language": "python", "complexity": "simple", "star_count": 47914, "quality_score": 0.92, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": false, "has_testing": false, "has_logging": false, "function_count": 4, "class_count": 0, "line_count": 39}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create REST API endpoint with proper request/response handling\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import io\nimport ntpath\nimport base64\n\n# region Utils for Download component\n\n\ndef send_file(path, filename=None, type=None):\n    \"\"\"\n    Convert a file into the format expected by the Download component.\n    :param path: path to the file to be sent\n    :param filename: name of the file, if not provided the original filename is used\n    :param type: type of the file (optional, passed to Blob in the javascript layer)\n    :return: dict of file content (base64 encoded) and meta data used by the Download component\n    \"\"\"\n    # If filename is not set, read it from the path.\n    if filename is None:\n        filename = ntpath.basename(path)\n    # Read the file contents and send it.\n    with open(path, \"rb\") as f:\n        return send_bytes(f.read(), filename, type)\n\n\ndef send_bytes(src, filename, type=None, **kwargs):\n    \"\"\"\n    Convert data written to BytesIO into the format expected by the Download component.\n    :param src: array of bytes or a writer that can write to BytesIO\n    :param filename: the name of the file\n    :param type: type of the file (optional, passed to Blob in the javascript layer)\n    :return: dict of data frame content (base64 encoded) and meta data used by the Download component\n    \"\"\"\n    content = src if isinstance(src, bytes) else _io_to_str(io.BytesIO(), src, **kwargs)\n    return dict(\n        content=base64.b64encode(content).decode(),\n        filename=filename,\n        type=type,\n        base64=True,\n    )\n\n\ndef send_string(src, filename, type=None, **kwargs):\n    \"\"\"\n    Convert data written to StringIO into the format expected by the Download component.\n    :param src: a string or a writer that can write to StringIO\n    :param filename: the name of the file\n    :param type: type of the file (optional, passed to Blob in the javascript layer)\n    :return: dict of data frame content (NOT base64 encoded) and meta data used by the Download component\n    \"\"\"\n    content = src if isinstance(src, str) else _io_to_str(io.StringIO(), src, **kwargs)\n    return dict(content=content, filename=filename, type=type, base64=False)\n\n\ndef _io_to_str(data_io, writer, **kwargs):\n    # Some pandas writers try to close the IO, we do not want that.\n    data_io_close = data_io.close\n    data_io.close = lambda: None\n    # Write data content.\n    writer(data_io, **kwargs)\n    data_value = data_io.getvalue()\n    data_io_close()\n    return data_value\n\n\ndef send_data_frame(writer, filename, type=None, **kwargs):\n    \"\"\"\n    Convert data frame into the format expected by the Download component.\n    :param writer: a data frame writer\n    :param filename: the name of the file\n    :param type: type of the file (optional, passed to Blob in the javascript layer)\n    :return: dict of data frame content (base64 encoded) and meta data used by the Download component\n\n    Examples\n    --------\n\n    >>> df = pd.DataFrame({'a': [1, 2, 3, 4], 'b': [2, 1, 5, 6], 'c': ['x', 'x', 'y', 'y']})\n    ...\n    >>> send_data_frame(df.to_csv, \"mydf.csv\")  # download as csv\n    >>> send_data_frame(df.to_json, \"mydf.json\")  # download as json\n    >>> send_data_frame(df.to_excel, \"mydf.xls\", index=False) # download as excel\n    >>> send_data_frame(df.to_pickle, \"mydf.pkl\") # download as pickle\n\n    \"\"\"\n    name = writer.__name__\n    # Check if the provided writer is known.\n    if name not in _data_frame_senders.keys():\n        raise ValueError(\n            \"The provided writer ({}) is not supported, \"\n            \"try calling send_string or send_bytes directly.\".format(name)\n        )\n    # Send data frame using the appropriate send function.\n    return _data_frame_senders[name](writer, filename, type, **kwargs)\n\n\n_data_frame_senders = {\n    \"to_csv\": send_string,\n    \"to_json\": send_string,\n    \"to_html\": send_string,\n    \"to_excel\": send_bytes,\n    \"to_feather\": send_bytes,\n    \"to_parquet\": send_bytes,\n    \"to_msgpack\": send_bytes,\n    \"to_stata\": send_bytes,\n    \"to_pickle\": send_bytes,\n}\n\n# endregion\n", "metadata": {"source": "github.com/plotly/dash", "file_path": "components/dash-core-components/dash_core_components_base/express.py", "language": "python", "complexity": "medium", "star_count": 23523, "quality_score": 0.92, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": true, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 5, "class_count": 0, "line_count": 107}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create REST API endpoint with proper request/response handling\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# This file is automatically loaded on build time to generate types.\n\ndef generate_plotly_figure(*_):\n    return \"typing.Union[Figure, dict]\"\n\n\ndef generate_datetime_prop(array=False):\n\n    def generator(*_):\n        datetime_type = \"typing.Union[str, datetime.datetime]\"\n        if array:\n            datetime_type = f\"typing.Sequence[{datetime_type}]\"\n        return datetime_type\n\n    return generator\n\n\ncustom_imports = {\n    \"Graph\": [\"from plotly.graph_objects import Figure\"],\n    \"DatePickerRange\": [\"import datetime\"],\n    \"DatePickerSingle\": [\"import datetime\"],\n}\n\ncustom_props = {\n    \"Graph\": {\"figure\": generate_plotly_figure},\n    \"DatePickerRange\": {\n        \"start_date\": generate_datetime_prop(),\n        \"end_date\": generate_datetime_prop(),\n        \"min_date_allowed\": generate_datetime_prop(),\n        \"max_date_allowed\": generate_datetime_prop(),\n        \"disabled_days\": generate_datetime_prop(True),\n    },\n    \"DatePickerSingle\": {\n        \"date\": generate_datetime_prop(),\n        \"min_date_allowed\": generate_datetime_prop(),\n        \"max_date_allowed\": generate_datetime_prop(),\n        \"disabled_days\": generate_datetime_prop(True),\n        \"initial_visible_month\": generate_datetime_prop(),\n    },\n}\n", "metadata": {"source": "github.com/plotly/dash", "file_path": "components/dash-core-components/dash_prop_typing.py", "language": "python", "complexity": "simple", "star_count": 23523, "quality_score": 0.89, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 3, "class_count": 0, "line_count": 41}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create REST API endpoint with proper request/response handling\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import pytest\nfrom dash_core_components_page import DashCoreComponentsMixin\nfrom dash.testing.browser import Browser\n\n\nclass DashCoreComponentsComposite(Browser, DashCoreComponentsMixin):\n    def __init__(self, server, **kwargs):\n        super(DashCoreComponentsComposite, self).__init__(**kwargs)\n        self.server = server\n\n    def start_server(self, app, **kwargs):\n        \"\"\"start the local server with app\"\"\"\n\n        # start server with app and pass Dash arguments\n        self.server(app, **kwargs)\n\n        # set the default server_url, it implicitly call wait_for_page\n        self.server_url = self.server.url\n\n\n@pytest.fixture\ndef dash_dcc(request, dash_thread_server, tmpdir):\n    with DashCoreComponentsComposite(\n        dash_thread_server,\n        browser=request.config.getoption(\"webdriver\"),\n        remote=request.config.getoption(\"remote\"),\n        remote_url=request.config.getoption(\"remote_url\"),\n        headless=request.config.getoption(\"headless\"),\n        options=request.config.hook.pytest_setup_options(),\n        download_path=tmpdir.mkdir(\"download\").strpath,\n        percy_assets_root=request.config.getoption(\"percy_assets\"),\n        percy_finalize=request.config.getoption(\"nopercyfinalize\"),\n        pause=request.config.getoption(\"pause\"),\n    ) as dc:\n        yield dc\n\n\n@pytest.fixture\ndef dash_dcc_headed(request, dash_thread_server, tmpdir):\n    with DashCoreComponentsComposite(\n        dash_thread_server,\n        browser=request.config.getoption(\"webdriver\"),\n        remote=request.config.getoption(\"remote\"),\n        remote_url=request.config.getoption(\"remote_url\"),\n        headless=False,\n        options=request.config.hook.pytest_setup_options(),\n        download_path=tmpdir.mkdir(\"download\").strpath,\n        percy_assets_root=request.config.getoption(\"percy_assets\"),\n        percy_finalize=request.config.getoption(\"nopercyfinalize\"),\n        pause=request.config.getoption(\"pause\"),\n    ) as dc:\n        yield dc\n", "metadata": {"source": "github.com/plotly/dash", "file_path": "components/dash-core-components/tests/conftest.py", "language": "python", "complexity": "simple", "star_count": 23523, "quality_score": 1.0, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": true, "has_type_hints": false, "has_testing": true, "has_logging": false, "function_count": 4, "class_count": 1, "line_count": 53}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create REST API endpoint with proper request/response handling\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import logging\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support import expected_conditions as EC\nfrom selenium.webdriver.common.action_chains import ActionChains\n\nlogger = logging.getLogger(__name__)\n\n\nclass DashCoreComponentsMixin(object):\n    def select_date_single(self, compid, index=0, day=\"\", outside_month=False):\n        \"\"\"Select Date in DPS component with either index or day\n        compid: the id defined for component\n        index: the index for all visibles in the popup calendar\n        day: a number or string; if set, use this to select instead of index\n        outside_month: used in conjunction with day. indicates if the day out\n            the scope of current month. default False.\n        \"\"\"\n        date = self.find_element(f\"#{compid} input\")\n        date.click()\n\n        def is_month_valid(elem):\n            return (\n                \"__outside\" in elem.get_attribute(\"class\")\n                if outside_month\n                else \"__outside\" not in elem.get_attribute(\"class\")\n            )\n\n        self._wait_until_day_is_clickable()\n        days = self.find_elements(self.date_picker_day_locator)\n        if day:\n            filtered = [_ for _ in days if _.text == str(day) and is_month_valid(_)]\n            if not filtered or len(filtered) > 1:\n                logger.error(\n                    \"cannot find the matched day with index=%s, day=%s\",\n                    index,\n                    day,\n                )\n            matched = filtered[0]\n        else:\n            matched = days[index]\n\n        matched.click()\n        return date.get_attribute(\"value\")\n\n    def select_date_range(self, compid, day_range, start_first=True):\n        \"\"\"Select Date in DPR component with a day_range tuple\n        compid: the id defined for component\n        day_range: a tuple or list, defines the start and end date you want to\n            select, the tuple must be length of 1 or 2, i.e.\n            (start, ) or (start, end)\n        start_first: boolean value decides clicking start or end date.\n            default True\n        \"\"\"\n\n        if (\n            not day_range\n            or not isinstance(day_range, (tuple, list))\n            or not set(range(1, 32)).issuperset(day_range)\n            or len(day_range) > 2\n        ):\n            logger.error(\n                \"data_range is provided with an invalid value %s\\n\"\n                \"the accepted range is range(1, 32)\",\n                day_range,\n            )\n            return\n\n        prefix = \"Start\" if start_first else \"End\"\n        date = self.find_element(f'#{compid} input[aria-label=\"{prefix} Date\"]')\n        date.click()\n        for day in day_range:\n            self._wait_until_day_is_clickable()\n            matched = [\n                _\n                for _ in self.find_elements(self.date_picker_day_locator)\n                if _.text == str(day)\n            ]\n            matched[0].click()\n\n        return self.get_date_range(compid)\n\n    def get_date_range(self, compid):\n        return tuple(\n            _.get_attribute(\"value\") for _ in self.find_elements(f\"#{compid} input\")\n        )\n\n    def _wait_until_day_is_clickable(self, timeout=1):\n        WebDriverWait(self.driver, timeout).until(\n            EC.element_to_be_clickable((By.CSS_SELECTOR, self.date_picker_day_locator))\n        )\n\n    @property\n    def date_picker_day_locator(self):\n        return 'div[data-visible=\"true\"] td.CalendarDay'\n\n    def click_and_hold_at_coord_fractions(self, elem_or_selector, fx, fy):\n        elem = self._get_element(elem_or_selector)\n\n        ActionChains(self.driver).move_to_element_with_offset(\n            elem, elem.size[\"width\"] * fx, elem.size[\"height\"] * fy\n        ).click_and_hold().perform()\n\n    def move_to_coord_fractions(self, elem_or_selector, fx, fy):\n        elem = self._get_element(elem_or_selector)\n\n        ActionChains(self.driver).move_to_element_with_offset(\n            elem, elem.size[\"width\"] * fx, elem.size[\"height\"] * fy\n        ).perform()\n\n    def release(self):\n        ActionChains(self.driver).release().perform()\n\n    def click_and_drag_at_coord_fractions(self, elem_or_selector, fx1, fy1, fx2, fy2):\n        elem = self._get_element(elem_or_selector)\n\n        ActionChains(self.driver).move_to_element_with_offset(\n            elem, elem.size[\"width\"] * fx1, elem.size[\"height\"] * fy1\n        ).click_and_hold().move_to_element_with_offset(\n            elem, elem.size[\"width\"] * fx2, elem.size[\"height\"] * fy2\n        ).release().perform()\n", "metadata": {"source": "github.com/plotly/dash", "file_path": "components/dash-core-components/tests/dash_core_components_page.py", "language": "python", "complexity": "medium", "star_count": 23523, "quality_score": 1.0, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": true, "has_type_hints": true, "has_testing": false, "has_logging": true, "function_count": 10, "class_count": 1, "line_count": 122}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create REST API endpoint with proper request/response handling with pytest testing framework\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import itertools\nimport pytest\n\nfrom dash import Dash, Input, Output, html, dcc\nimport dash.testing.wait as wait\n\n\n@pytest.mark.DCC594\ndef test_cdpr001_date_clearable_true_works(dash_dcc):\n\n    app = Dash(__name__)\n    app.layout = html.Div(\n        [\n            dcc.DatePickerRange(id=\"dpr\", clearable=True),\n            dcc.DatePickerSingle(id=\"dps\", clearable=True),\n        ]\n    )\n\n    dash_dcc.start_server(app)\n\n    # DPR\n    start_date, end_date = dash_dcc.select_date_range(\"dpr\", (1, 28))\n    close_btn = dash_dcc.wait_for_element('button[aria-label=\"Clear Dates\"]')\n\n    assert (\n        \"1\" in start_date and \"28\" in end_date\n    ), \"both start date and end date should match the selected day\"\n\n    close_btn.click()\n    start_date, end_date = dash_dcc.get_date_range(\"dpr\")\n    assert not start_date and not end_date, \"both start and end dates should be cleared\"\n\n    # DPS\n    selected = dash_dcc.select_date_single(\"dps\", day=\"1\")\n\n    assert selected, \"single date should get a value\"\n    close_btn = dash_dcc.wait_for_element(\"#dps button\")\n    close_btn.click()\n    (single_date,) = dash_dcc.get_date_range(\"dps\")\n    assert not single_date, \"date should be cleared\"\n\n    assert dash_dcc.get_logs() == []\n\n\ndef test_cdpr002_updatemodes(dash_dcc):\n    app = Dash(__name__)\n\n    app.layout = html.Div(\n        [\n            dcc.DatePickerRange(\n                id=\"date-picker-range\",\n                start_date_id=\"startDate\",\n                end_date_id=\"endDate\",\n                start_date_placeholder_text=\"Select a start date!\",\n                end_date_placeholder_text=\"Select an end date!\",\n                updatemode=\"bothdates\",\n            ),\n            html.Div(id=\"date-picker-range-output\"),\n        ]\n    )\n\n    @app.callback(\n        Output(\"date-picker-range-output\", \"children\"),\n        [\n            Input(\"date-picker-range\", \"start_date\"),\n            Input(\"date-picker-range\", \"end_date\"),\n        ],\n    )\n    def update_output(start_date, end_date):\n        return f\"{start_date} - {end_date}\"\n\n    dash_dcc.start_server(app=app)\n\n    start_date = dash_dcc.find_element(\"#startDate\")\n    start_date.click()\n\n    end_date = dash_dcc.find_element(\"#endDate\")\n    end_date.click()\n\n    assert (\n        dash_dcc.find_element(\"#date-picker-range-output\").text == \"None - None\"\n    ), \"the output should not update when both clicked but no selection happen\"\n\n    start_date.click()\n\n    dash_dcc.find_elements(dash_dcc.date_picker_day_locator)[4].click()\n    assert (\n        dash_dcc.find_element(\"#date-picker-range-output\").text == \"None - None\"\n    ), \"the output should not update when only one is selected\"\n\n    eday = dash_dcc.find_elements(dash_dcc.date_picker_day_locator)[-4]\n    wait.until(lambda: eday.is_displayed() and eday.is_enabled(), timeout=2)\n    eday.click()\n\n    date_tokens = set(start_date.get_attribute(\"value\").split(\"/\"))\n    date_tokens.update(end_date.get_attribute(\"value\").split(\"/\"))\n\n    assert (\n        set(\n            itertools.chain(\n                *[\n                    _.split(\"-\")\n                    for _ in dash_dcc.find_element(\n                        \"#date-picker-range-output\"\n                    ).text.split(\" - \")\n                ]\n            )\n        )\n        == date_tokens\n    ), \"date should match the callback output\"\n\n    assert dash_dcc.get_logs() == []\n", "metadata": {"source": "github.com/plotly/dash", "file_path": "components/dash-core-components/tests/integration/calendar/test_calendar_props.py", "language": "python", "complexity": "medium", "star_count": 23523, "quality_score": 0.9500000000000001, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": true, "has_logging": true, "function_count": 3, "class_count": 0, "line_count": 113}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create REST API endpoint with proper request/response handling with pytest testing framework\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from datetime import datetime\n\nfrom dash import Dash, Input, Output, html, dcc\n\n\ndef test_rdpr001_persisted_dps(dash_dcc):\n    app = Dash(__name__, suppress_callback_exceptions=True)\n    app.layout = html.Div(\n        [\n            html.Button(\"fire callback\", id=\"btn\", n_clicks=1),\n            html.Div(children=[html.Div(id=\"container\"), html.P(\"dps\", id=\"dps-p\")]),\n        ]\n    )\n\n    # changing value of date with each callback to verify\n    # persistenceTransforms is stripping the time-part from the date-time\n    @app.callback(Output(\"container\", \"children\"), [Input(\"btn\", \"n_clicks\")])\n    def update_output(value):\n        return dcc.DatePickerSingle(\n            id=\"dps\",\n            min_date_allowed=datetime(2020, 1, 1),\n            max_date_allowed=datetime(2020, 1, 7),\n            date=datetime(2020, 1, 3, 1, 1, 1, value),\n            persistence=True,\n            persistence_type=\"session\",\n        )\n\n    @app.callback(Output(\"dps-p\", \"children\"), [Input(\"dps\", \"date\")])\n    def display_dps(value):\n        return value\n\n    dash_dcc.start_server(app)\n\n    dash_dcc.select_date_single(\"dps\", day=\"2\")\n    dash_dcc.wait_for_text_to_equal(\"#dps-p\", \"2020-01-02\")\n    dash_dcc.find_element(\"#btn\").click()\n    dash_dcc.wait_for_text_to_equal(\"#dps-p\", \"2020-01-02\")\n\n    assert dash_dcc.get_logs() == []\n\n\ndef test_rdpr002_persisted_dpr(dash_dcc):\n    app = Dash(__name__, suppress_callback_exceptions=True)\n    app.layout = html.Div(\n        [\n            html.Button(\"fire callback\", id=\"btn\", n_clicks=1),\n            html.Div(\n                children=[\n                    html.Div(id=\"container\"),\n                    html.P(\"dpr\", id=\"dpr-p-start\"),\n                    html.P(\"dpr\", id=\"dpr-p-end\"),\n                ]\n            ),\n        ]\n    )\n\n    # changing value of start_date and end_date with each callback to verify\n    # persistenceTransforms is stripping the time-part from the date-time\n    @app.callback(Output(\"container\", \"children\"), [Input(\"btn\", \"n_clicks\")])\n    def update_output(value):\n        return dcc.DatePickerRange(\n            id=\"dpr\",\n            min_date_allowed=datetime(2020, 1, 1),\n            max_date_allowed=datetime(2020, 1, 7),\n            start_date=datetime(2020, 1, 3, 1, 1, 1, value),\n            end_date=datetime(2020, 1, 4, 1, 1, 1, value),\n            persistence=True,\n            persistence_type=\"session\",\n        )\n\n    @app.callback(Output(\"dpr-p-start\", \"children\"), [Input(\"dpr\", \"start_date\")])\n    def display_dpr_start(value):\n        return value\n\n    @app.callback(Output(\"dpr-p-end\", \"children\"), [Input(\"dpr\", \"end_date\")])\n    def display_dpr_end(value):\n        return value\n\n    dash_dcc.start_server(app)\n\n    dash_dcc.select_date_range(\"dpr\", (2, 5))\n    dash_dcc.wait_for_text_to_equal(\"#dpr-p-start\", \"2020-01-02\")\n    dash_dcc.wait_for_text_to_equal(\"#dpr-p-end\", \"2020-01-05\")\n    dash_dcc.find_element(\"#btn\").click()\n    dash_dcc.wait_for_text_to_equal(\"#dpr-p-start\", \"2020-01-02\")\n    dash_dcc.wait_for_text_to_equal(\"#dpr-p-end\", \"2020-01-05\")\n\n    assert dash_dcc.get_logs() == []\n", "metadata": {"source": "github.com/plotly/dash", "file_path": "components/dash-core-components/tests/integration/calendar/test_date_picker_persistence.py", "language": "python", "complexity": "medium", "star_count": 23523, "quality_score": 0.9500000000000001, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": false, "has_testing": true, "has_logging": true, "function_count": 7, "class_count": 0, "line_count": 89}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create REST API endpoint with proper request/response handling with pytest testing framework\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from datetime import datetime\n\nfrom dash import Dash, html, dcc\n\n\ndef test_dtpr001_initial_month_provided(dash_dcc):\n    app = Dash(__name__)\n    app.layout = html.Div(\n        [\n            dcc.DatePickerRange(\n                id=\"dps-initial-month\",\n                min_date_allowed=datetime(2010, 1, 1),\n                max_date_allowed=datetime(2099, 12, 31),\n                initial_visible_month=datetime(2019, 10, 28),\n            )\n        ]\n    )\n\n    dash_dcc.start_server(app)\n\n    date_picker_start = dash_dcc.find_element(\n        '#dps-initial-month .DateInput_input.DateInput_input_1[placeholder=\"Start Date\"]'\n    )\n    date_picker_start.click()\n\n    dash_dcc.wait_for_text_to_equal(\n        \"#dps-initial-month .CalendarMonth.CalendarMonth_1[data-visible=true] strong\",\n        \"October 2019\",\n        1,\n    )\n\n    assert dash_dcc.get_logs() == []\n\n\ndef test_dtpr002_no_initial_month_min_date(dash_dcc):\n    app = Dash(__name__)\n    app.layout = html.Div(\n        [\n            dcc.DatePickerRange(\n                id=\"dps-initial-month\",\n                min_date_allowed=datetime(2010, 1, 1),\n                max_date_allowed=datetime(2099, 12, 31),\n            )\n        ]\n    )\n\n    dash_dcc.start_server(app)\n\n    date_picker_start = dash_dcc.find_element(\n        '#dps-initial-month .DateInput_input.DateInput_input_1[placeholder=\"Start Date\"]'\n    )\n    date_picker_start.click()\n\n    dash_dcc.wait_for_text_to_equal(\n        \"#dps-initial-month .CalendarMonth.CalendarMonth_1[data-visible=true] strong\",\n        \"January 2010\",\n    )\n\n    assert dash_dcc.get_logs() == []\n\n\ndef test_dtpr003_no_initial_month_no_min_date_start_date(dash_dcc):\n    app = Dash(__name__)\n    app.layout = html.Div(\n        [\n            dcc.DatePickerRange(\n                id=\"dps-initial-month\",\n                start_date=datetime(2019, 8, 13),\n                max_date_allowed=datetime(2099, 12, 31),\n            )\n        ]\n    )\n\n    dash_dcc.start_server(app)\n\n    date_picker_start = dash_dcc.find_element(\n        '#dps-initial-month .DateInput_input.DateInput_input_1[placeholder=\"Start Date\"]'\n    )\n    date_picker_start.click()\n\n    dash_dcc.wait_for_text_to_equal(\n        \"#dps-initial-month .CalendarMonth.CalendarMonth_1[data-visible=true] strong\",\n        \"August 2019\",\n    )\n\n    assert dash_dcc.get_logs() == []\n\n\ndef test_dtpr004_max_and_min_dates_are_clickable(dash_dcc):\n    app = Dash(__name__)\n    app.layout = html.Div(\n        [\n            dcc.DatePickerRange(\n                id=\"dps-initial-month\",\n                start_date=datetime(2021, 1, 11),\n                end_date=datetime(2021, 1, 19),\n                max_date_allowed=datetime(2021, 1, 20),\n                min_date_allowed=datetime(2021, 1, 10),\n            )\n        ]\n    )\n\n    dash_dcc.start_server(app)\n\n    dash_dcc.select_date_range(\"dps-initial-month\", (10, 20))\n\n    dash_dcc.wait_for_text_to_equal(\n        '#dps-initial-month .DateInput_input.DateInput_input_1[placeholder=\"Start Date\"]',\n        \"01/10/2021\",\n    )\n\n    dash_dcc.wait_for_text_to_equal(\n        '#dps-initial-month .DateInput_input.DateInput_input_1[placeholder=\"End Date\"]',\n        \"01/20/2021\",\n    )\n\n    assert dash_dcc.get_logs() == []\n\n\ndef test_dtpr005_disabled_days_arent_clickable(dash_dcc):\n    app = Dash(__name__)\n    app.layout = html.Div(\n        [\n            html.Label(\"Operating Date\"),\n            dcc.DatePickerRange(\n                id=\"dpr\",\n                min_date_allowed=datetime(2021, 1, 1),\n                max_date_allowed=datetime(2021, 1, 31),\n                initial_visible_month=datetime(2021, 1, 1),\n                disabled_days=[datetime(2021, 1, 10), datetime(2021, 1, 11)],\n            ),\n        ],\n        style={\n            \"width\": \"10%\",\n            \"display\": \"inline-block\",\n            \"marginLeft\": 10,\n            \"marginRight\": 10,\n            \"marginBottom\": 10,\n        },\n    )\n    dash_dcc.start_server(app)\n    date = dash_dcc.find_element(\"#dpr input\")\n    assert not date.get_attribute(\"value\")\n    assert not any(\n        dash_dcc.select_date_range(\"dpr\", day_range=(10, 11))\n    ), \"Disabled days should not be clickable\"\n    assert all(\n        dash_dcc.select_date_range(\"dpr\", day_range=(1, 2))\n    ), \"Other days should be clickable\"\n\n    # open datepicker to take snapshot\n    date.click()\n    dash_dcc.percy_snapshot(\"dtpr005 - disabled days\")\n", "metadata": {"source": "github.com/plotly/dash", "file_path": "components/dash-core-components/tests/integration/calendar/test_date_picker_range.py", "language": "python", "complexity": "complex", "star_count": 23523, "quality_score": 0.9500000000000001, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": true, "has_logging": true, "function_count": 5, "class_count": 0, "line_count": 154}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create REST API endpoint with proper request/response handling with pytest testing framework\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from datetime import datetime, timedelta\nimport pandas as pd\nimport time\n\nimport pytest\n\nfrom dash import Dash, Input, Output, html, dcc, no_update\n\n\n@pytest.mark.DCC652\ndef test_dtps001_simple_click(dash_dcc):\n    app = Dash(__name__)\n    app.layout = html.Div(\n        [\n            html.Label(\"Operating Date\"),\n            dcc.DatePickerSingle(\n                id=\"dps\",\n                min_date_allowed=datetime(2010, 1, 1),\n                max_date_allowed=datetime(2099, 12, 31),\n                initial_visible_month=datetime.today().date() - timedelta(days=1),\n                day_size=47,\n            ),\n        ],\n        style={\n            \"width\": \"10%\",\n            \"display\": \"inline-block\",\n            \"marginLeft\": 10,\n            \"marginRight\": 10,\n            \"marginBottom\": 10,\n        },\n    )\n    dash_dcc.start_server(app)\n    date = dash_dcc.find_element(\"#dps input\")\n    assert not date.get_attribute(\"value\")\n    assert dash_dcc.select_date_single(\n        \"dps\", index=3\n    ), \"Component should be clickable to choose a valid date\"\n\n    assert dash_dcc.get_logs() == []\n\n\ndef test_dtps010_local_and_session_persistence(dash_dcc):\n    app = Dash(__name__)\n    app.layout = html.Div(\n        [\n            dcc.DatePickerSingle(id=\"dps-local\", persistence=True, day_size=47),\n            dcc.DatePickerSingle(\n                id=\"dps-session\",\n                persistence=True,\n                persistence_type=\"session\",\n                day_size=47,\n            ),\n        ]\n    )\n\n    dash_dcc.start_server(app)\n\n    assert not dash_dcc.find_element(\"#dps-local input\").get_attribute(\n        \"value\"\n    ) and not dash_dcc.find_element(\"#dps-session input\").get_attribute(\n        \"value\"\n    ), \"component should contain no initial date\"\n\n    for idx in range(3):\n        local = dash_dcc.select_date_single(\"dps-local\", index=idx)\n        session = dash_dcc.select_date_single(\"dps-session\", index=idx)\n        dash_dcc.wait_for_page()\n        assert (\n            dash_dcc.find_element(\"#dps-local input\").get_attribute(\"value\") == local\n            and dash_dcc.find_element(\"#dps-session input\").get_attribute(\"value\")\n            == session\n        ), \"the date value should be consistent after refresh\"\n\n    assert dash_dcc.get_logs() == []\n\n\ndef test_dtps011_memory_persistence(dash_dcc):\n    app = Dash(__name__)\n    app.layout = html.Div(\n        [html.Button(id=\"switch\", children=\"Switch\"), html.Div(id=\"out\")]\n    )\n\n    @app.callback(Output(\"out\", \"children\"), [Input(\"switch\", \"n_clicks\")])\n    def cb(clicks):\n        if clicks is None:\n            return no_update\n        if clicks % 2 == 1:\n            return [\n                dcc.DatePickerSingle(\n                    id=\"dps-memory\",\n                    min_date_allowed=datetime(2010, 1, 1),\n                    max_date_allowed=datetime(2099, 12, 31),\n                    initial_visible_month=datetime.today().date() - timedelta(days=1),\n                    persistence=True,\n                    persistence_type=\"memory\",\n                    day_size=47,\n                ),\n                dcc.DatePickerSingle(\n                    id=\"dps-none\",\n                    min_date_allowed=datetime(2010, 1, 1),\n                    max_date_allowed=datetime(2099, 12, 31),\n                    initial_visible_month=datetime.today().date() - timedelta(days=1),\n                    day_size=47,\n                ),\n            ]\n        else:\n            return \"switched\"\n\n    dash_dcc.start_server(app)\n\n    switch = dash_dcc.find_element(\"#switch\")\n    switch.click()\n\n    memorized = dash_dcc.select_date_single(\"dps-memory\", day=\"4\")\n    amnesiaed = dash_dcc.select_date_single(\"dps-none\", day=\"11\")\n\n    switch.click()\n    assert dash_dcc.wait_for_text_to_equal(\"#out\", \"switched\")\n    switch.click()\n    assert (\n        dash_dcc.find_element(\"#dps-memory input\").get_attribute(\"value\") == memorized\n    )\n    switched = dash_dcc.find_element(\"#dps-none input\").get_attribute(\"value\")\n    assert switched != amnesiaed and switched == \"\"\n\n    assert dash_dcc.get_logs() == []\n\n\ndef test_dtps012_initial_month(dash_dcc):\n    app = Dash(__name__)\n    app.layout = html.Div(\n        [\n            dcc.DatePickerSingle(\n                id=\"dps-initial-month\",\n                min_date_allowed=datetime(2010, 1, 1),\n                max_date_allowed=datetime(2099, 12, 31),\n            )\n        ]\n    )\n\n    dash_dcc.start_server(app)\n\n    date_picker = dash_dcc.find_element(\"#dps-initial-month\")\n    date_picker.click()\n    dash_dcc.wait_for_text_to_equal(\n        \"#dps-initial-month .CalendarMonth.CalendarMonth_1[data-visible=true] strong\",\n        \"January 2010\",\n    )\n\n    assert dash_dcc.get_logs() == []\n\n\ndef test_dtps013_disabled_days_arent_clickable(dash_dcc):\n    app = Dash(__name__)\n    app.layout = html.Div(\n        [\n            html.Label(\"Operating Date\"),\n            dcc.DatePickerSingle(\n                id=\"dps\",\n                min_date_allowed=datetime(2021, 1, 1),\n                max_date_allowed=datetime(2021, 1, 31),\n                initial_visible_month=datetime(2021, 1, 1),\n                disabled_days=[datetime(2021, 1, 10)],\n            ),\n        ],\n        style={\n            \"width\": \"10%\",\n            \"display\": \"inline-block\",\n            \"marginLeft\": 10,\n            \"marginRight\": 10,\n            \"marginBottom\": 10,\n        },\n    )\n    dash_dcc.start_server(app)\n    date = dash_dcc.find_element(\"#dps input\")\n    assert not date.get_attribute(\"value\")\n    assert not dash_dcc.select_date_single(\n        \"dps\", day=10\n    ), \"Disabled days should not be clickable\"\n    assert dash_dcc.select_date_single(\"dps\", day=1), \"Other days should be clickable\"\n\n    # open datepicker to take snapshot\n    date.click()\n    dash_dcc.percy_snapshot(\"dtps013 - disabled days\")\n\n\ndef test_dtps0014_disabed_days_timeout(dash_dcc):\n    app = Dash(__name__)\n\n    min_date = pd.to_datetime(\"2010-01-01\")\n    max_date = pd.to_datetime(\"2099-01-01\")\n    disabled_days = [\n        x for x in pd.date_range(min_date, max_date, freq=\"D\") if x.day != 1\n    ]\n\n    app.layout = html.Div(\n        [\n            html.Label(\"Operating Date\"),\n            dcc.DatePickerSingle(\n                id=\"dps\",\n                min_date_allowed=min_date,\n                max_date_allowed=max_date,\n                disabled_days=disabled_days,\n            ),\n        ]\n    )\n    dash_dcc.start_server(app)\n    date = dash_dcc.wait_for_element(\"#dps\", timeout=5)\n\n    \"\"\"\n    WebDriver click() function hangs at the time of the react code\n    execution, so it necessary to check execution time.\n    \"\"\"\n    start_time = time.time()\n    date.click()\n    assert time.time() - start_time < 5\n\n    dash_dcc.wait_for_element(\".SingleDatePicker_picker\", timeout=5)\n    assert dash_dcc.get_logs() == []\n", "metadata": {"source": "github.com/plotly/dash", "file_path": "components/dash-core-components/tests/integration/calendar/test_date_picker_single.py", "language": "python", "complexity": "complex", "star_count": 23523, "quality_score": 0.98, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": true, "has_type_hints": true, "has_testing": true, "has_logging": true, "function_count": 7, "class_count": 0, "line_count": 220}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create REST API endpoint with proper request/response handling\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from dash import Dash, html, dcc, callback, Output, Input, State\n\nimport dash.testing.wait as wait\nimport time\n\n\nfrom selenium.webdriver.common.action_chains import ActionChains\nfrom selenium.webdriver.common.keys import Keys\n\n\ndef test_clp001_clipboard_text(dash_dcc_headed):\n    copy_text = \"Hello, Dash!\"\n    app = Dash(__name__, prevent_initial_callbacks=True)\n    app.layout = html.Div(\n        [\n            html.Div(copy_text, id=\"copy\"),\n            dcc.Clipboard(id=\"copy_icon\", target_id=\"copy\"),\n            dcc.Textarea(id=\"paste\"),\n        ]\n    )\n    dash_dcc_headed.start_server(app)\n\n    dash_dcc_headed.find_element(\"#copy_icon\").click()\n    #  time.sleep(2)\n    dash_dcc_headed.find_element(\"#paste\").click()\n    ActionChains(dash_dcc_headed.driver).key_down(Keys.CONTROL).send_keys(\"v\").key_up(\n        Keys.CONTROL\n    ).perform()\n\n    wait.until(\n        lambda: dash_dcc_headed.find_element(\"#paste\").get_attribute(\"value\")\n        == copy_text,\n        timeout=3,\n    )\n\n\ndef test_clp002_clipboard_text(dash_dcc_headed):\n    copy_text = \"Copy this text to the clipboard\"\n    app = Dash(__name__, prevent_initial_callbacks=True)\n    app.layout = html.Div(\n        [dcc.Clipboard(id=\"copy_icon\", content=copy_text), dcc.Textarea(id=\"paste\")]\n    )\n    dash_dcc_headed.start_server(app)\n\n    dash_dcc_headed.find_element(\"#copy_icon\").click()\n    time.sleep(1)\n    dash_dcc_headed.find_element(\"#paste\").click()\n    ActionChains(dash_dcc_headed.driver).key_down(Keys.CONTROL).send_keys(\"v\").key_up(\n        Keys.CONTROL\n    ).perform()\n\n    wait.until(\n        lambda: dash_dcc_headed.find_element(\"#paste\").get_attribute(\"value\")\n        == copy_text,\n        timeout=3,\n    )\n\n\ndef test_clp003_clipboard_text(dash_dcc_headed):\n    copy_text = \"Copy this text to the clipboard using a separate button\"\n    app = Dash(__name__, prevent_initial_callbacks=True)\n    app.layout = html.Div(\n        [\n            dcc.Clipboard(id=\"copy_icon\", content=copy_text, n_clicks=0),\n            dcc.Textarea(id=\"paste\"),\n            html.Button(\"Copy\", id=\"copy_button\", n_clicks=0),\n        ]\n    )\n\n    @callback(\n        Output(\"copy_icon\", \"n_clicks\"),\n        State(\"copy_icon\", \"n_clicks\"),\n        Input(\"copy_button\", \"n_clicks\"),\n        prevent_initial_call=True,\n    )\n    def selected(icon_clicks, button_clicks):\n        return icon_clicks + 1\n\n    dash_dcc_headed.start_server(app)\n\n    dash_dcc_headed.find_element(\"#copy_button\").click()\n    time.sleep(1)\n    dash_dcc_headed.find_element(\"#paste\").click()\n    ActionChains(dash_dcc_headed.driver).key_down(Keys.CONTROL).send_keys(\"v\").key_up(\n        Keys.CONTROL\n    ).perform()\n\n    wait.until(\n        lambda: dash_dcc_headed.find_element(\"#paste\").get_attribute(\"value\")\n        == copy_text,\n        timeout=3,\n    )\n", "metadata": {"source": "github.com/plotly/dash", "file_path": "components/dash-core-components/tests/integration/clipboard/test_clipboard.py", "language": "python", "complexity": "medium", "star_count": 23523, "quality_score": 0.92, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": true, "has_logging": false, "function_count": 4, "class_count": 0, "line_count": 93}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create REST API endpoint with proper request/response handling with pytest testing framework\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from multiprocessing import Value\n\nfrom dash.testing import wait\nfrom dash import Dash, Input, Output, State, dcc, html, callback_context\nimport pytest\nimport time\n\n\n@pytest.mark.parametrize(\n    \"confirms\", [[False, False], [False, True], [True, False], [True, True]]\n)\n@pytest.mark.parametrize(\"confirm_callback\", [True, False])\n@pytest.mark.parametrize(\n    \"components\",\n    [\n        [\n            html.Button(id=\"button\", children=\"Send confirm\", n_clicks=0),\n            dcc.ConfirmDialog(id=\"confirm\", message=\"Please confirm.\"),\n        ],\n        [\n            dcc.ConfirmDialogProvider(\n                html.Button(\"click me\", id=\"button\"),\n                id=\"confirm\",\n                message=\"Please confirm.\",\n            )\n        ],\n    ],\n)\ndef test_cnfd001_dialog(dash_dcc, confirm_callback, confirms, components):\n    app = Dash(__name__)\n    app.layout = html.Div(components + [html.Div(id=\"confirmed\")])\n\n    @app.callback(Output(\"confirm\", \"displayed\"), [Input(\"button\", \"n_clicks\")])\n    def on_click_confirm(n_clicks):\n        if n_clicks:\n            return True\n\n    count = Value(\"i\", 0)\n\n    if confirm_callback:\n\n        @app.callback(\n            Output(\"confirmed\", \"children\"),\n            [\n                Input(\"confirm\", \"submit_n_clicks\"),\n                Input(\"confirm\", \"cancel_n_clicks\"),\n            ],\n            [\n                State(\"confirm\", \"submit_n_clicks_timestamp\"),\n                State(\"confirm\", \"cancel_n_clicks_timestamp\"),\n            ],\n        )\n        def on_confirmed(\n            submit_n_clicks,\n            cancel_n_clicks,\n            submit_timestamp,\n            cancel_timestamp,\n        ):\n            count.value += 1\n\n            if not submit_n_clicks and not cancel_n_clicks:\n                return \"\"\n\n            trigger = callback_context.triggered[0][\"prop_id\"].split(\".\")[1]\n\n            return \"confirmed\" if trigger == \"submit_n_clicks\" else \"canceled\"\n\n    dash_dcc.start_server(app)\n    dash_dcc.wait_for_element(\"#button\")\n\n    for confirm in confirms:\n        dash_dcc.find_element(\"#button\").click()\n        time.sleep(0.5)\n        if confirm:\n            dash_dcc.driver.switch_to.alert.accept()\n        else:\n            dash_dcc.driver.switch_to.alert.dismiss()\n        time.sleep(0.5)\n\n        if confirm_callback:\n            dash_dcc.wait_for_text_to_equal(\n                \"#confirmed\", \"confirmed\" if confirm else \"canceled\"\n            )\n\n    if confirm_callback:\n        assert wait.until(lambda: count.value == 1 + len(confirms), 3)\n\n    assert dash_dcc.get_logs() == []\n\n\ndef test_cnfd002_injected_confirm(dash_dcc):\n    app = Dash(__name__)\n    app.layout = html.Div(\n        [\n            html.Button(id=\"button\", children=\"Send confirm\"),\n            html.Div(id=\"confirm-container\"),\n            dcc.Location(id=\"dummy-location\"),\n        ]\n    )\n\n    @app.callback(\n        Output(\"confirm-container\", \"children\"),\n        [Input(\"button\", \"n_clicks\")],\n    )\n    def on_click(n_clicks):\n        if n_clicks:\n            return dcc.ConfirmDialog(\n                displayed=True, id=\"confirm\", message=\"Please confirm.\"\n            )\n\n    dash_dcc.start_server(app)\n    dash_dcc.wait_for_element(\"#button\").click()\n\n    time.sleep(1)\n    dash_dcc.driver.switch_to.alert.accept()\n\n    assert dash_dcc.get_logs() == []\n", "metadata": {"source": "github.com/plotly/dash", "file_path": "components/dash-core-components/tests/integration/confirmdialog/test_confirm.py", "language": "python", "complexity": "medium", "star_count": 23523, "quality_score": 0.9500000000000001, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": true, "has_logging": true, "function_count": 5, "class_count": 0, "line_count": 118}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create REST API endpoint with proper request/response handling with pytest testing framework\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import os\nfrom dash import Dash, Input, Output, dcc, html\n\nfrom dash.testing.wait import until\n\n\ndef test_dltx001_download_text(dash_dcc):\n    text = \"Hello, world!\"\n    filename = \"hello.txt\"\n    # Create app.\n    app = Dash(__name__, prevent_initial_callbacks=True)\n    app.layout = html.Div([html.Button(\"Click\", id=\"btn\"), dcc.Download(id=\"download\")])\n\n    @app.callback(Output(\"download\", \"data\"), Input(\"btn\", \"n_clicks\"))\n    def download(_):\n        return dcc.send_string(text, filename)\n\n    dash_dcc.start_server(app)\n\n    # Check that there is nothing before clicking\n    fp = os.path.join(dash_dcc.download_path, filename)\n    assert not os.path.isfile(fp)\n\n    dash_dcc.find_element(\"#btn\").click()\n\n    # Check that a file has been download, and that it's content matches the original text.\n    until(lambda: os.path.exists(fp), 10)\n    with open(fp, \"r\") as f:\n        content = f.read()\n    assert content == text\n\n    assert dash_dcc.get_logs() == []\n", "metadata": {"source": "github.com/plotly/dash", "file_path": "components/dash-core-components/tests/integration/download/test_download.py", "language": "python", "complexity": "simple", "star_count": 23523, "quality_score": 0.9500000000000001, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": true, "has_logging": true, "function_count": 2, "class_count": 0, "line_count": 33}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create REST API endpoint with proper request/response handling with pytest testing framework\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import os\nfrom dash import Dash, Input, Output, dcc, html\n\nimport pytest\nimport pandas as pd\nimport numpy as np\n\nfrom dash.testing.wait import until\n\n\n@pytest.mark.parametrize(\n    \"fmt\", (\"csv\", \"json\", \"html\", \"feather\", \"parquet\", \"stata\", \"pickle\")\n)\ndef test_dldf001_download_dataframe(fmt, dash_dcc):\n    df = pd.DataFrame({\"a\": [1, 2, 3, 4], \"b\": [2, 1, 5, 6], \"c\": [\"x\", \"x\", \"y\", \"y\"]})\n    reader = getattr(pd, f\"read_{fmt}\")  # e.g. read_csv\n    writer = getattr(df, f\"to_{fmt}\")  # e.g. to_csv\n    filename = f\"df.{fmt}\"\n    # Create app.\n    app = Dash(__name__, prevent_initial_callbacks=True)\n    app.layout = html.Div(\n        [html.Button(\"Click me\", id=\"btn\"), dcc.Download(id=\"download\")]\n    )\n\n    @app.callback(Output(\"download\", \"data\"), Input(\"btn\", \"n_clicks\"))\n    def download(_):\n        # For csv and html, the index must be removed to preserve the structure.\n        if fmt in [\"csv\", \"html\", \"excel\"]:\n            return dcc.send_data_frame(writer, filename, index=False)\n        # For csv and html, the index must be removed to preserve the structure.\n        if fmt in [\"stata\"]:\n            a = dcc.send_data_frame(writer, filename, write_index=False)\n            return a\n        # For other formats, no modifications are needed.\n        return dcc.send_data_frame(writer, filename)\n\n    dash_dcc.start_server(app)\n\n    # Check that there is nothing before clicking\n    fp = os.path.join(dash_dcc.download_path, filename)\n    assert not os.path.isfile(fp)\n\n    dash_dcc.find_element(\"#btn\").click()\n\n    # Check that a file has been download, and that it's content matches the original data frame.\n    until(lambda: os.path.exists(fp), 10)\n    df_download = reader(fp)\n    if isinstance(df_download, list):\n        df_download = df_download[0]\n    # For stata data, pandas equals fails. Hence, a custom equals is used instead.\n    assert df.columns.equals(df_download.columns)\n    assert df.index.equals(df_download.index)\n    np.testing.assert_array_equal(df.values, df_download.values)\n\n    assert dash_dcc.get_logs() == []\n", "metadata": {"source": "github.com/plotly/dash", "file_path": "components/dash-core-components/tests/integration/download/test_download_dataframe.py", "language": "python", "complexity": "simple", "star_count": 23523, "quality_score": 0.9500000000000001, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": true, "has_logging": true, "function_count": 2, "class_count": 0, "line_count": 56}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create REST API endpoint with proper request/response handling with pytest testing framework\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import os\nfrom dash import Dash, Input, Output, dcc, html\n\nfrom dash.testing.wait import until\n\n\ndef test_dlfi001_download_file(dash_dcc):\n    filename = \"chuck.jpg\"\n    asset_folder = os.path.join(os.path.dirname(__file__), \"download-assets\")\n    # Create app.\n    app = Dash(__name__, prevent_initial_callbacks=True)\n    app.layout = html.Div([html.Button(\"Click\", id=\"btn\"), dcc.Download(id=\"download\")])\n\n    @app.callback(Output(\"download\", \"data\"), Input(\"btn\", \"n_clicks\"))\n    def download(_):\n        return dcc.send_file(os.path.join(asset_folder, filename))\n\n    dash_dcc.start_server(app)\n\n    # Check that there is nothing before clicking\n    fp = os.path.join(dash_dcc.download_path, filename)\n    assert not os.path.isfile(fp)\n\n    dash_dcc.find_element(\"#btn\").click()\n\n    # Check that a file has been download, and that it's content matches the original.\n    until(lambda: os.path.exists(fp), 10)\n    with open(fp, \"rb\") as f:\n        content = f.read()\n    with open(os.path.join(asset_folder, filename), \"rb\") as f:\n        original = f.read()\n    assert content == original\n\n    assert dash_dcc.get_logs() == []\n", "metadata": {"source": "github.com/plotly/dash", "file_path": "components/dash-core-components/tests/integration/download/test_download_file.py", "language": "python", "complexity": "simple", "star_count": 23523, "quality_score": 0.9500000000000001, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": true, "has_logging": true, "function_count": 2, "class_count": 0, "line_count": 35}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create REST API endpoint with proper request/response handling with pytest testing framework\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from dash import Dash, Input, Output, dcc, html\n\nfrom selenium.webdriver.common.keys import Keys\n\n\ndef test_ddcf001_clearable_false_single(dash_duo):\n    app = Dash(__name__)\n    app.layout = html.Div(\n        [\n            dcc.Dropdown(\n                id=\"my-unclearable-dropdown\",\n                options=[\n                    {\"label\": \"New York City\", \"value\": \"NYC\"},\n                    {\"label\": \"Montreal\", \"value\": \"MTL\"},\n                    {\"label\": \"San Francisco\", \"value\": \"SF\"},\n                ],\n                value=\"MTL\",\n                clearable=False,\n            ),\n            html.Div(id=\"dropdown-value\", style={\"height\": \"10px\", \"width\": \"10px\"}),\n        ]\n    )\n\n    @app.callback(\n        Output(\"dropdown-value\", \"children\"),\n        [Input(\"my-unclearable-dropdown\", \"value\")],\n    )\n    def update_value(val):\n        return val\n\n    dash_duo.start_server(app)\n\n    dropdown = dash_duo.find_element(\"#my-unclearable-dropdown input\")\n    dropdown.send_keys(Keys.BACKSPACE)\n    dash_duo.find_element(\"#dropdown-value\").click()\n\n    assert len(dash_duo.find_element(\"#dropdown-value\").text) > 0\n\n    assert dash_duo.get_logs() == []\n\n\ndef test_ddcf002_clearable_false_multi(dash_duo):\n    app = Dash(__name__)\n    app.layout = html.Div(\n        [\n            dcc.Dropdown(\n                id=\"my-unclearable-dropdown\",\n                options=[\n                    {\"label\": \"New York City\", \"value\": \"NYC\"},\n                    {\"label\": \"Montreal\", \"value\": \"MTL\"},\n                    {\"label\": \"San Francisco\", \"value\": \"SF\"},\n                ],\n                value=[\"MTL\", \"SF\"],\n                multi=True,\n                clearable=False,\n            ),\n            html.Div(id=\"dropdown-value\", style={\"height\": \"10px\", \"width\": \"10px\"}),\n        ]\n    )\n\n    @app.callback(\n        Output(\"dropdown-value\", \"children\"),\n        [Input(\"my-unclearable-dropdown\", \"value\")],\n    )\n    def update_value(val):\n        return \", \".join(val)\n\n    dash_duo.start_server(app)\n\n    dropdown = dash_duo.find_element(\"#my-unclearable-dropdown input\")\n    dropdown.send_keys(Keys.BACKSPACE)\n    dropdown.send_keys(Keys.BACKSPACE)\n    dash_duo.find_element(\"#dropdown-value\").click()\n\n    assert len(dash_duo.find_element(\"#dropdown-value\").text) > 0\n\n    assert dash_duo.get_logs() == []\n", "metadata": {"source": "github.com/plotly/dash", "file_path": "components/dash-core-components/tests/integration/dropdown/test_clearable_false.py", "language": "python", "complexity": "medium", "star_count": 23523, "quality_score": 0.9500000000000001, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": true, "has_logging": true, "function_count": 4, "class_count": 0, "line_count": 78}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create REST API endpoint with proper request/response handling\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from dash import Dash, dcc, html\n\n\ndef test_ddsh001_test_dropdown_radioitems_checklist_shorthands(dash_dcc):\n    app = Dash(__name__)\n\n    TEST_OPTIONS_N_VALUES = [\n        [[\"a\", \"b\", \"c\"]],\n        [[\"a\", \"b\", \"c\"], \"b\"],\n        [[\"a\", 3, \"c\"]],\n        [[\"a\", 3, \"c\"], 3],\n        [[\"a\", True, \"c\"]],\n        [[\"a\", True, \"c\"], True],\n        [[\"a\", 3, \"c\", True, False]],\n        [[\"a\", 3, \"c\", True, False], False],\n        # {`value1`: `label1`, `value2`, `label2`, ...}\n        [{\"one\": \"One\", \"two\": \"Two\", \"three\": \"Three\"}],\n        [{\"one\": \"One\", \"two\": \"Two\", \"three\": \"Three\"}, \"two\"],\n        [{\"one\": 1, \"two\": 2, \"three\": False}],\n        [{\"one\": 1, \"two\": 2, \"three\": False}, \"three\"],\n        [{\"one\": 1, \"two\": True, \"three\": 3}],\n        [{\"one\": 1, \"two\": True, \"three\": 3}, \"two\"],\n        # original options format\n        [\n            [\n                {\"label\": \"one\", \"value\": 1},\n                {\"label\": \"two\", \"value\": True},\n                {\"label\": \"three\", \"value\": 3},\n            ]\n        ],\n        [\n            [\n                {\"label\": \"one\", \"value\": 1},\n                {\"label\": \"two\", \"value\": True},\n                {\"label\": \"three\", \"value\": 3},\n            ],\n            True,\n        ],\n    ]\n\n    layout = []\n    for definition in TEST_OPTIONS_N_VALUES:\n        (option, value) = definition if len(definition) > 1 else [definition[0], None]\n        layout.extend(\n            [\n                html.Div(\n                    [\n                        html.Div(\n                            f\"Options={option}, Value={value}\",\n                            style={\"marginBottom\": 15, \"marginTop\": 25},\n                        ),\n                        dcc.Dropdown(option, value),\n                    ]\n                ),\n                html.Div(\n                    [\n                        html.Div(\n                            f\"Options={option}, Value={value}\",\n                            style={\"marginBottom\": 15, \"marginTop\": 25},\n                        ),\n                        dcc.RadioItems(option, value=value),\n                    ]\n                ),\n                html.Div(\n                    [\n                        html.Div(\n                            f\"Options={option}, Value={value}\",\n                            style={\"marginBottom\": 15, \"marginTop\": 25},\n                        ),\n                        dcc.Checklist(option, value=[value]),\n                    ]\n                ),\n            ]\n        )\n\n    app.layout = html.Div(layout)\n\n    dash_dcc.start_server(app)\n    dash_dcc.wait_for_element(\".dash-dropdown\")\n    dash_dcc.percy_snapshot(\n        \"ddsh001 - test_ddsh001_test_dropdown_radioitems_checklist_shorthands\"\n    )\n", "metadata": {"source": "github.com/plotly/dash", "file_path": "components/dash-core-components/tests/integration/dropdown/test_dropdown_radioitems_checklist_shorthands.py", "language": "python", "complexity": "medium", "star_count": 23523, "quality_score": 0.92, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": true, "has_logging": false, "function_count": 1, "class_count": 0, "line_count": 83}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create REST API endpoint with proper request/response handling using Flask/FastAPI decorators\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# -*- coding: utf-8 -*-\n\n\"\"\"\nhttpbin.core\n~~~~~~~~~~~~\n\nThis module provides the core HttpBin experience.\n\"\"\"\n\nimport base64\nimport json\nimport os\nimport random\nimport time\nimport uuid\nimport argparse\n\nfrom flask import (\n    Flask,\n    Response,\n    request,\n    render_template,\n    redirect,\n    jsonify as flask_jsonify,\n    make_response,\n    url_for,\n    abort,\n)\nfrom six.moves import range as xrange\nfrom werkzeug.datastructures import WWWAuthenticate, MultiDict\nfrom werkzeug.http import http_date\nfrom werkzeug.wrappers import BaseResponse\nfrom werkzeug.http import parse_authorization_header\nfrom flasgger import Swagger, NO_SANITIZER\n\nfrom . import filters\nfrom .helpers import (\n    get_headers,\n    status_code,\n    get_dict,\n    get_request_range,\n    check_basic_auth,\n    check_digest_auth,\n    secure_cookie,\n    H,\n    ROBOT_TXT,\n    ANGRY_ASCII,\n    parse_multi_value_header,\n    next_stale_after_value,\n    digest_challenge_response,\n)\nfrom .utils import weighted_choice\nfrom .structures import CaseInsensitiveDict\n\nwith open(\n    os.path.join(os.path.realpath(os.path.dirname(__file__)), \"VERSION\")\n) as version_file:\n    version = version_file.read().strip()\n\nENV_COOKIES = (\n    \"_gauges_unique\",\n    \"_gauges_unique_year\",\n    \"_gauges_unique_month\",\n    \"_gauges_unique_day\",\n    \"_gauges_unique_hour\",\n    \"__utmz\",\n    \"__utma\",\n    \"__utmb\",\n)\n\n\ndef jsonify(*args, **kwargs):\n    response = flask_jsonify(*args, **kwargs)\n    if not response.data.endswith(b\"\\n\"):\n        response.data += b\"\\n\"\n    return response\n\n\n# Prevent WSGI from correcting the casing of the Location header\nBaseResponse.autocorrect_location_header = False\n\n# Find the correct template folder when running from a different location\ntmpl_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), \"templates\")\n\napp = Flask(__name__, template_folder=tmpl_dir)\napp.debug = bool(os.environ.get(\"DEBUG\"))\napp.config[\"JSONIFY_PRETTYPRINT_REGULAR\"] = True\n\napp.add_template_global(\"HTTPBIN_TRACKING\" in os.environ, name=\"tracking_enabled\")\n\napp.config[\"SWAGGER\"] = {\"title\": \"httpbin.org\", \"uiversion\": 3}\n\ntemplate = {\n    \"swagger\": \"2.0\",\n    \"info\": {\n        \"title\": \"httpbin.org\",\n        \"description\": (\n            \"A simple HTTP Request & Response Service.\"\n            \"<br/> <br/> <b>Run locally: </b> <code>$ docker run -p 80:80 kennethreitz/httpbin</code>\"\n        ),\n        \"contact\": {\n            \"responsibleOrganization\": \"Kenneth Reitz\",\n            \"responsibleDeveloper\": \"Kenneth Reitz\",\n            \"email\": \"me@kennethreitz.org\",\n            \"url\": \"https://kennethreitz.org\",\n        },\n        # \"termsOfService\": \"http://me.com/terms\",\n        \"version\": version,\n    },\n    \"host\": \"httpbin.org\",  # overrides localhost:5000\n    \"basePath\": \"/\",  # base bash for blueprint registration\n    \"schemes\": [\"https\"],\n    \"protocol\": \"https\",\n    \"tags\": [\n        {\n            \"name\": \"HTTP Methods\",\n            \"description\": \"Testing different HTTP verbs\",\n            # 'externalDocs': {'description': 'Learn more', 'url': 'https://www.w3.org/Protocols/rfc2616/rfc2616-sec9.html'}\n        },\n        {\"name\": \"Auth\", \"description\": \"Auth methods\"},\n        {\n            \"name\": \"Status codes\",\n            \"description\": \"Generates responses with given status code\",\n        },\n        {\"name\": \"Request inspection\", \"description\": \"Inspect the request data\"},\n        {\n            \"name\": \"Response inspection\",\n            \"description\": \"Inspect the response data like caching and headers\",\n        },\n        {\n            \"name\": \"Response formats\",\n            \"description\": \"Returns responses in different data formats\",\n        },\n        {\"name\": \"Dynamic data\", \"description\": \"Generates random and dynamic data\"},\n        {\"name\": \"Cookies\", \"description\": \"Creates, reads and deletes Cookies\"},\n        {\"name\": \"Images\", \"description\": \"Returns different image formats\"},\n        {\"name\": \"Redirects\", \"description\": \"Returns different redirect responses\"},\n        {\n            \"name\": \"Anything\",\n            \"description\": \"Returns anything that is passed to request\",\n        },\n    ],\n}\n\nswagger_config = {\n    \"headers\": [],\n    \"specs\": [\n        {\n            \"endpoint\": \"spec\",\n            \"route\": \"/spec.json\",\n            \"rule_filter\": lambda rule: True,  # all in\n            \"model_filter\": lambda tag: True,  # all in\n        }\n    ],\n    \"static_url_path\": \"/flasgger_static\",\n    # \"static_folder\": \"static\",  # must be set by user\n    \"swagger_ui\": True,\n    \"specs_route\": \"/\",\n}\n\nswagger = Swagger(app, sanitizer=NO_SANITIZER, template=template, config=swagger_config)\n\n# Set up Bugsnag exception tracking, if desired. To use Bugsnag, install the\n# Bugsnag Python client with the command \"pip install bugsnag\", and set the\n# environment variable BUGSNAG_API_KEY. You can also optionally set\n# BUGSNAG_RELEASE_STAGE.\nif os.environ.get(\"BUGSNAG_API_KEY\") is not None:\n    try:\n        import bugsnag\n        import bugsnag.flask\n\n        release_stage = os.environ.get(\"BUGSNAG_RELEASE_STAGE\") or \"production\"\n        bugsnag.configure(\n            api_key=os.environ.get(\"BUGSNAG_API_KEY\"),\n            project_root=os.path.dirname(os.path.abspath(__file__)),\n            use_ssl=True,\n            release_stage=release_stage,\n            ignore_classes=[\"werkzeug.exceptions.NotFound\"],\n        )\n        bugsnag.flask.handle_exceptions(app)\n    except:\n        app.logger.warning(\"Unable to initialize Bugsnag exception handling.\")\n\n# -----------\n# Middlewares\n# -----------\n\"\"\"\nhttps://github.com/kennethreitz/httpbin/issues/340\nAdds a middleware to provide chunked request encoding support running under\ngunicorn only.\nWerkzeug required environ 'wsgi.input_terminated' to be set otherwise it\nempties the input request stream.\n- gunicorn seems to support input_terminated but does not add the environ,\n  so we add it here.\n- flask will hang and does not seem to properly terminate the request, so\n  we explicitly deny chunked requests.\n\"\"\"\n\n\n@app.before_request\ndef before_request():\n    if request.environ.get(\"HTTP_TRANSFER_ENCODING\", \"\").lower() == \"chunked\":\n        server = request.environ.get(\"SERVER_SOFTWARE\", \"\")\n        if server.lower().startswith(\"gunicorn/\"):\n            if \"wsgi.input_terminated\" in request.environ:\n                app.logger.debug(\n                    \"environ wsgi.input_terminated already set, keeping: %s\"\n                    % request.environ[\"wsgi.input_terminated\"]\n                )\n            else:\n                request.environ[\"wsgi.input_terminated\"] = 1\n        else:\n            abort(501, \"Chunked requests are not supported for server %s\" % server)\n\n\n@app.after_request\ndef set_cors_headers(response):\n    response.headers[\"Access-Control-Allow-Origin\"] = request.headers.get(\"Origin\", \"*\")\n    response.headers[\"Access-Control-Allow-Credentials\"] = \"true\"\n\n    if request.method == \"OPTIONS\":\n        # Both of these headers are only used for the \"preflight request\"\n        # http://www.w3.org/TR/cors/#access-control-allow-methods-response-header\n        response.headers[\n            \"Access-Control-Allow-Methods\"\n        ] = \"GET, POST, PUT, DELETE, PATCH, OPTIONS\"\n        response.headers[\"Access-Control-Max-Age\"] = \"3600\"  # 1 hour cache\n        if request.headers.get(\"Access-Control-Request-Headers\") is not None:\n            response.headers[\"Access-Control-Allow-Headers\"] = request.headers[\n                \"Access-Control-Request-Headers\"\n            ]\n    return response\n\n\n# ------\n# Routes\n# ------\n\n\n@app.route(\"/legacy\")\ndef view_landing_page():\n    \"\"\"Generates Landing Page in legacy layout.\"\"\"\n    return render_template(\"index.html\")\n\n\n@app.route(\"/html\")\ndef view_html_page():\n    \"\"\"Returns a simple HTML document.\n    ---\n    tags:\n      - Response formats\n    produces:\n      - text/html\n    responses:\n      200:\n        description: An HTML page.\n    \"\"\"\n\n    return render_template(\"moby.html\")\n\n\n@app.route(\"/robots.txt\")\ndef view_robots_page():\n    \"\"\"Returns some robots.txt rules.\n    ---\n    tags:\n      - Response formats\n    produces:\n      - text/plain\n    responses:\n      200:\n        description: Robots file\n    \"\"\"\n\n    response = make_response()\n    response.data = ROBOT_TXT\n    response.content_type = \"text/plain\"\n    return response\n\n\n@app.route(\"/deny\")\ndef view_deny_page():\n    \"\"\"Returns page denied by robots.txt rules.\n    ---\n    tags:\n      - Response formats\n    produces:\n      - text/plain\n    responses:\n      200:\n        description: Denied message\n    \"\"\"\n    response = make_response()\n    response.data = ANGRY_ASCII\n    response.content_type = \"text/plain\"\n    return response\n    # return \"YOU SHOULDN'T BE HERE\"\n\n\n@app.route(\"/ip\")\ndef view_origin():\n    \"\"\"Returns the requester's IP Address.\n    ---\n    tags:\n      - Request inspection\n    produces:\n      - application/json\n    responses:\n      200:\n        description: The Requester's IP Address.\n    \"\"\"\n\n    return jsonify(origin=request.headers.get(\"X-Forwarded-For\", request.remote_addr))\n\n\n@app.route(\"/uuid\")\ndef view_uuid():\n    \"\"\"Return a UUID4.\n    ---\n    tags:\n      - Dynamic data\n    produces:\n      - application/json\n    responses:\n      200:\n        description: A UUID4.\n    \"\"\"\n\n    return jsonify(uuid=str(uuid.uuid4()))\n\n\n@app.route(\"/headers\")\ndef view_headers():\n    \"\"\"Return the incoming request's HTTP headers.\n    ---\n    tags:\n      - Request inspection\n    produces:\n      - application/json\n    responses:\n      200:\n        description: The request's headers.\n    \"\"\"\n\n    return jsonify(get_dict('headers'))\n\n\n@app.route(\"/user-agent\")\ndef view_user_agent():\n    \"\"\"Return the incoming requests's User-Agent header.\n    ---\n    tags:\n      - Request inspection\n    produces:\n      - application/json\n    responses:\n      200:\n        description: The request's User-Agent header.\n    \"\"\"\n\n    headers = get_headers()\n\n    return jsonify({\"user-agent\": headers[\"user-agent\"]})\n\n\n@app.route(\"/get\", methods=(\"GET\",))\ndef view_get():\n    \"\"\"The request's query parameters.\n    ---\n    tags:\n      - HTTP Methods\n    produces:\n      - application/json\n    responses:\n      200:\n        description: The request's query parameters.\n    \"\"\"\n\n    return jsonify(get_dict(\"url\", \"args\", \"headers\", \"origin\"))\n\n\n@app.route(\"/anything\", methods=[\"GET\", \"POST\", \"PUT\", \"DELETE\", \"PATCH\", \"TRACE\"])\n@app.route(\n    \"/anything/<path:anything>\",\n    methods=[\"GET\", \"POST\", \"PUT\", \"DELETE\", \"PATCH\", \"TRACE\"],\n)\ndef view_anything(anything=None):\n    \"\"\"Returns anything passed in request data.\n    ---\n    tags:\n      - Anything\n    produces:\n      - application/json\n    responses:\n      200:\n        description: Anything passed in request\n    \"\"\"\n\n    return jsonify(\n        get_dict(\n            \"url\",\n            \"args\",\n            \"headers\",\n            \"origin\",\n            \"method\",\n            \"form\",\n            \"data\",\n            \"files\",\n            \"json\",\n        )\n    )\n\n\n@app.route(\"/post\", methods=(\"POST\",))\ndef view_post():\n    \"\"\"The request's POST parameters.\n    ---\n    tags:\n      - HTTP Methods\n    produces:\n      - application/json\n    responses:\n      200:\n        description: The request's POST parameters.\n    \"\"\"\n\n    return jsonify(\n        get_dict(\"url\", \"args\", \"form\", \"data\", \"origin\", \"headers\", \"files\", \"json\")\n    )\n\n\n@app.route(\"/put\", methods=(\"PUT\",))\ndef view_put():\n    \"\"\"The request's PUT parameters.\n    ---\n    tags:\n      - HTTP Methods\n    produces:\n      - application/json\n    responses:\n      200:\n        description: The request's PUT parameters.\n    \"\"\"\n\n    return jsonify(\n        get_dict(\"url\", \"args\", \"form\", \"data\", \"origin\", \"headers\", \"files\", \"json\")\n    )\n\n\n@app.route(\"/patch\", methods=(\"PATCH\",))\ndef view_patch():\n    \"\"\"The request's PATCH parameters.\n    ---\n    tags:\n      - HTTP Methods\n    produces:\n      - application/json\n    responses:\n      200:\n        description: The request's PATCH parameters.\n    \"\"\"\n\n    return jsonify(\n        get_dict(\"url\", \"args\", \"form\", \"data\", \"origin\", \"headers\", \"files\", \"json\")\n    )\n\n\n@app.route(\"/delete\", methods=(\"DELETE\",))\ndef view_delete():\n    \"\"\"The request's DELETE parameters.\n    ---\n    tags:\n      - HTTP Methods\n    produces:\n      - application/json\n    responses:\n      200:\n        description: The request's DELETE parameters.\n    \"\"\"\n\n    return jsonify(\n        get_dict(\"url\", \"args\", \"form\", \"data\", \"origin\", \"headers\", \"files\", \"json\")\n    )\n\n\n@app.route(\"/gzip\")\n@filters.gzip\ndef view_gzip_encoded_content():\n    \"\"\"Returns GZip-encoded data.\n    ---\n    tags:\n      - Response formats\n    produces:\n      - application/json\n    responses:\n      200:\n        description: GZip-encoded data.\n    \"\"\"\n\n    return jsonify(get_dict(\"origin\", \"headers\", method=request.method, gzipped=True))\n\n\n@app.route(\"/deflate\")\n@filters.deflate\ndef view_deflate_encoded_content():\n    \"\"\"Returns Deflate-encoded data.\n    ---\n    tags:\n      - Response formats\n    produces:\n      - application/json\n    responses:\n      200:\n        description: Defalte-encoded data.\n    \"\"\"\n\n    return jsonify(get_dict(\"origin\", \"headers\", method=request.method, deflated=True))\n\n\n@app.route(\"/brotli\")\n@filters.brotli\ndef view_brotli_encoded_content():\n    \"\"\"Returns Brotli-encoded data.\n    ---\n    tags:\n      - Response formats\n    produces:\n      - application/json\n    responses:\n      200:\n        description: Brotli-encoded data.\n    \"\"\"\n\n    return jsonify(get_dict(\"origin\", \"headers\", method=request.method, brotli=True))\n\n\n@app.route(\"/redirect/<int:n>\")\ndef redirect_n_times(n):\n    \"\"\"302 Redirects n times.\n    ---\n    tags:\n      - Redirects\n    parameters:\n      - in: path\n        name: n\n        type: int\n    produces:\n      - text/html\n    responses:\n      302:\n        description: A redirection.\n    \"\"\"\n    assert n > 0\n\n    absolute = request.args.get(\"absolute\", \"false\").lower() == \"true\"\n\n    if n == 1:\n        return redirect(url_for(\"view_get\", _external=absolute))\n\n    if absolute:\n        return _redirect(\"absolute\", n, True)\n    else:\n        return _redirect(\"relative\", n, False)\n\n\ndef _redirect(kind, n, external):\n    return redirect(\n        url_for(\"{0}_redirect_n_times\".format(kind), n=n - 1, _external=external)\n    )\n\n\n@app.route(\"/redirect-to\", methods=[\"GET\", \"POST\", \"PUT\", \"DELETE\", \"PATCH\", \"TRACE\"])\ndef redirect_to():\n    \"\"\"302/3XX Redirects to the given URL.\n    ---\n    tags:\n      - Redirects\n    produces:\n      - text/html\n    get:\n      parameters:\n        - in: query\n          name: url\n          type: string\n          required: true\n        - in: query\n          name: status_code\n          type: int\n    post:\n      consumes:\n        - application/x-www-form-urlencoded\n      parameters:\n        - in: formData\n          name: url\n          type: string\n          required: true\n        - in: formData\n          name: status_code\n          type: int\n          required: false\n    patch:\n      consumes:\n        - application/x-www-form-urlencoded\n      parameters:\n        - in: formData\n          name: url\n          type: string\n          required: true\n        - in: formData\n          name: status_code\n          type: int\n          required: false\n    put:\n      consumes:\n        - application/x-www-form-urlencoded\n      parameters:\n        - in: formData\n          name: url\n          type: string\n          required: true\n        - in: formData\n          name: status_code\n          type: int\n          required: false\n    responses:\n      302:\n        description: A redirection.\n    \"\"\"\n\n    args_dict = request.args.items()\n    args = CaseInsensitiveDict(args_dict)\n\n    # We need to build the response manually and convert to UTF-8 to prevent\n    # werkzeug from \"fixing\" the URL. This endpoint should set the Location\n    # header to the exact string supplied.\n    response = app.make_response(\"\")\n    response.status_code = 302\n    if \"status_code\" in args:\n        status_code = int(args[\"status_code\"])\n        if status_code >= 300 and status_code < 400:\n            response.status_code = status_code\n    response.headers[\"Location\"] = args[\"url\"].encode(\"utf-8\")\n\n    return response\n\n\n@app.route(\"/relative-redirect/<int:n>\")\ndef relative_redirect_n_times(n):\n    \"\"\"Relatively 302 Redirects n times.\n    ---\n    tags:\n      - Redirects\n    parameters:\n      - in: path\n        name: n\n        type: int\n    produces:\n      - text/html\n    responses:\n      302:\n        description: A redirection.\n    \"\"\"\n\n    assert n > 0\n\n    response = app.make_response(\"\")\n    response.status_code = 302\n\n    if n == 1:\n        response.headers[\"Location\"] = url_for(\"view_get\")\n        return response\n\n    response.headers[\"Location\"] = url_for(\"relative_redirect_n_times\", n=n - 1)\n    return response\n\n\n@app.route(\"/absolute-redirect/<int:n>\")\ndef absolute_redirect_n_times(n):\n    \"\"\"Absolutely 302 Redirects n times.\n    ---\n    tags:\n      - Redirects\n    parameters:\n      - in: path\n        name: n\n        type: int\n    produces:\n      - text/html\n    responses:\n      302:\n        description: A redirection.\n    \"\"\"\n\n    assert n > 0\n\n    if n == 1:\n        return redirect(url_for(\"view_get\", _external=True))\n\n    return _redirect(\"absolute\", n, True)\n\n\n@app.route(\"/stream/<int:n>\")\ndef stream_n_messages(n):\n    \"\"\"Stream n JSON responses\n    ---\n    tags:\n      - Dynamic data\n    parameters:\n      - in: path\n        name: n\n        type: int\n    produces:\n      - application/json\n    responses:\n      200:\n        description: Streamed JSON responses.\n    \"\"\"\n    response = get_dict(\"url\", \"args\", \"headers\", \"origin\")\n    n = min(n, 100)\n\n    def generate_stream():\n        for i in range(n):\n            response[\"id\"] = i\n            yield json.dumps(response) + \"\\n\"\n\n    return Response(generate_stream(), headers={\"Content-Type\": \"application/json\"})\n\n\n@app.route(\n    \"/status/<codes>\", methods=[\"GET\", \"POST\", \"PUT\", \"DELETE\", \"PATCH\", \"TRACE\"]\n)\ndef view_status_code(codes):\n    \"\"\"Return status code or random status code if more than one are given\n    ---\n    tags:\n      - Status codes\n    parameters:\n      - in: path\n        name: codes\n    produces:\n      - text/plain\n    responses:\n      100:\n        description: Informational responses\n      200:\n        description: Success\n      300:\n        description: Redirection\n      400:\n        description: Client Errors\n      500:\n        description: Server Errors\n    \"\"\"\n\n    if \",\" not in codes:\n        try:\n            code = int(codes)\n        except ValueError:\n            return Response(\"Invalid status code\", status=400)\n        return status_code(code)\n\n    choices = []\n    for choice in codes.split(\",\"):\n        if \":\" not in choice:\n            code = choice\n            weight = 1\n        else:\n            code, weight = choice.split(\":\")\n\n        try:\n            choices.append((int(code), float(weight)))\n        except ValueError:\n            return Response(\"Invalid status code\", status=400)\n\n    code = weighted_choice(choices)\n\n    return status_code(code)\n\n\n@app.route(\"/response-headers\", methods=[\"GET\", \"POST\"])\ndef response_headers():\n    \"\"\"Returns a set of response headers from the query string.\n    ---\n    tags:\n      - Response inspection\n    parameters:\n      - in: query\n        name: freeform\n        explode: true\n        allowEmptyValue: true\n        schema:\n          type: object\n          additionalProperties:\n            type: string\n        style: form\n    produces:\n      - application/json\n    responses:\n      200:\n        description: Response headers\n    \"\"\"\n    # Pending swaggerUI update\n    # https://github.com/swagger-api/swagger-ui/issues/3850\n    headers = MultiDict(request.args.items(multi=True))\n    response = jsonify(list(headers.lists()))\n\n    while True:\n        original_data = response.data\n        d = {}\n        for key in response.headers.keys():\n            value = response.headers.get_all(key)\n            if len(value) == 1:\n                value = value[0]\n            d[key] = value\n        response = jsonify(d)\n        for key, value in headers.items(multi=True):\n            response.headers.add(key, value)\n        response_has_changed = response.data != original_data\n        if not response_has_changed:\n            break\n    return response\n\n\n@app.route(\"/cookies\")\ndef view_cookies(hide_env=True):\n    \"\"\"Returns cookie data.\n    ---\n    tags:\n      - Cookies\n    produces:\n      - application/json\n    responses:\n      200:\n        description: Set cookies.\n    \"\"\"\n\n    cookies = dict(request.cookies.items())\n\n    if hide_env and (\"show_env\" not in request.args):\n        for key in ENV_COOKIES:\n            try:\n                del cookies[key]\n            except KeyError:\n                pass\n\n    return jsonify(cookies=cookies)\n\n\n@app.route(\"/forms/post\")\ndef view_forms_post():\n    \"\"\"Simple HTML form.\"\"\"\n\n    return render_template(\"forms-post.html\")\n\n\n@app.route(\"/cookies/set/<name>/<value>\")\ndef set_cookie(name, value):\n    \"\"\"Sets a cookie and redirects to cookie list.\n    ---\n    tags:\n      - Cookies\n    parameters:\n      - in: path\n        name: name\n        type: string\n      - in: path\n        name: value\n        type: string\n    produces:\n      - text/plain\n    responses:\n      200:\n        description: Set cookies and redirects to cookie list.\n    \"\"\"\n\n    r = app.make_response(redirect(url_for(\"view_cookies\")))\n    r.set_cookie(key=name, value=value, secure=secure_cookie())\n\n    return r\n\n\n@app.route(\"/cookies/set\")\ndef set_cookies():\n    \"\"\"Sets cookie(s) as provided by the query string and redirects to cookie list.\n    ---\n    tags:\n      - Cookies\n    parameters:\n      - in: query\n        name: freeform\n        explode: true\n        allowEmptyValue: true\n        schema:\n          type: object\n          additionalProperties:\n            type: string\n        style: form\n    produces:\n      - text/plain\n    responses:\n      200:\n        description: Redirect to cookie list\n    \"\"\"\n\n    cookies = dict(request.args.items())\n    r = app.make_response(redirect(url_for(\"view_cookies\")))\n    for key, value in cookies.items():\n        r.set_cookie(key=key, value=value, secure=secure_cookie())\n\n    return r\n\n\n@app.route(\"/cookies/delete\")\ndef delete_cookies():\n    \"\"\"Deletes cookie(s) as provided by the query string and redirects to cookie list.\n    ---\n    tags:\n      - Cookies\n    parameters:\n      - in: query\n        name: freeform\n        explode: true\n        allowEmptyValue: true\n        schema:\n          type: object\n          additionalProperties:\n            type: string\n        style: form\n    produces:\n      - text/plain\n    responses:\n      200:\n        description: Redirect to cookie list\n    \"\"\"\n\n    cookies = dict(request.args.items())\n    r = app.make_response(redirect(url_for(\"view_cookies\")))\n    for key, value in cookies.items():\n        r.delete_cookie(key=key)\n\n    return r\n\n\n@app.route(\"/basic-auth/<user>/<passwd>\")\ndef basic_auth(user=\"user\", passwd=\"passwd\"):\n    \"\"\"Prompts the user for authorization using HTTP Basic Auth.\n    ---\n    tags:\n      - Auth\n    parameters:\n      - in: path\n        name: user\n        type: string\n      - in: path\n        name: passwd\n        type: string\n    produces:\n      - application/json\n    responses:\n      200:\n        description: Sucessful authentication.\n      401:\n        description: Unsuccessful authentication.\n    \"\"\"\n\n    if not check_basic_auth(user, passwd):\n        return status_code(401)\n\n    return jsonify(authenticated=True, user=user)\n\n\n@app.route(\"/hidden-basic-auth/<user>/<passwd>\")\ndef hidden_basic_auth(user=\"user\", passwd=\"passwd\"):\n    \"\"\"Prompts the user for authorization using HTTP Basic Auth.\n    ---\n    tags:\n      - Auth\n    parameters:\n      - in: path\n        name: user\n        type: string\n      - in: path\n        name: passwd\n        type: string\n    produces:\n      - application/json\n    responses:\n      200:\n        description: Sucessful authentication.\n      404:\n        description: Unsuccessful authentication.\n    \"\"\"\n\n    if not check_basic_auth(user, passwd):\n        return status_code(404)\n    return jsonify(authenticated=True, user=user)\n\n\n@app.route(\"/bearer\")\ndef bearer_auth():\n    \"\"\"Prompts the user for authorization using bearer authentication.\n    ---\n    tags:\n      - Auth\n    parameters:\n      - in: header\n        name: Authorization\n        schema:\n          type: string\n    produces:\n      - application/json\n    responses:\n      200:\n        description: Sucessful authentication.\n      401:\n        description: Unsuccessful authentication.\n    \"\"\"\n    authorization = request.headers.get(\"Authorization\")\n    if not (authorization and authorization.startswith(\"Bearer \")):\n        response = app.make_response(\"\")\n        response.headers[\"WWW-Authenticate\"] = \"Bearer\"\n        response.status_code = 401\n        return response\n    slice_start = len(\"Bearer \")\n    token = authorization[slice_start:]\n\n    return jsonify(authenticated=True, token=token)\n\n\n@app.route(\"/digest-auth/<qop>/<user>/<passwd>\")\ndef digest_auth_md5(qop=None, user=\"user\", passwd=\"passwd\"):\n    \"\"\"Prompts the user for authorization using Digest Auth.\n    ---\n    tags:\n      - Auth\n    parameters:\n      - in: path\n        name: qop\n        type: string\n        description: auth or auth-int\n      - in: path\n        name: user\n        type: string\n      - in: path\n        name: passwd\n        type: string\n    produces:\n      - application/json\n    responses:\n      200:\n        description: Sucessful authentication.\n      401:\n        description: Unsuccessful authentication.\n    \"\"\"\n    return digest_auth(qop, user, passwd, \"MD5\", \"never\")\n\n\n@app.route(\"/digest-auth/<qop>/<user>/<passwd>/<algorithm>\")\ndef digest_auth_nostale(qop=None, user=\"user\", passwd=\"passwd\", algorithm=\"MD5\"):\n    \"\"\"Prompts the user for authorization using Digest Auth + Algorithm.\n    ---\n    tags:\n      - Auth\n    parameters:\n      - in: path\n        name: qop\n        type: string\n        description: auth or auth-int\n      - in: path\n        name: user\n        type: string\n      - in: path\n        name: passwd\n        type: string\n      - in: path\n        name: algorithm\n        type: string\n        description: MD5, SHA-256, SHA-512\n        default: MD5\n    produces:\n      - application/json\n    responses:\n      200:\n        description: Sucessful authentication.\n      401:\n        description: Unsuccessful authentication.\n    \"\"\"\n    return digest_auth(qop, user, passwd, algorithm, \"never\")\n\n\n@app.route(\"/digest-auth/<qop>/<user>/<passwd>/<algorithm>/<stale_after>\")\ndef digest_auth(\n    qop=None, user=\"user\", passwd=\"passwd\", algorithm=\"MD5\", stale_after=\"never\"\n):\n    \"\"\"Prompts the user for authorization using Digest Auth + Algorithm.\n    allow settings the stale_after argument.\n    ---\n    tags:\n      - Auth\n    parameters:\n      - in: path\n        name: qop\n        type: string\n        description: auth or auth-int\n      - in: path\n        name: user\n        type: string\n      - in: path\n        name: passwd\n        type: string\n      - in: path\n        name: algorithm\n        type: string\n        description: MD5, SHA-256, SHA-512\n        default: MD5\n      - in: path\n        name: stale_after\n        type: string\n        default: never\n    produces:\n      - application/json\n    responses:\n      200:\n        description: Sucessful authentication.\n      401:\n        description: Unsuccessful authentication.\n    \"\"\"\n    require_cookie_handling = request.args.get(\"require-cookie\", \"\").lower() in (\n        \"1\",\n        \"t\",\n        \"true\",\n    )\n    if algorithm not in (\"MD5\", \"SHA-256\", \"SHA-512\"):\n        algorithm = \"MD5\"\n\n    if qop not in (\"auth\", \"auth-int\"):\n        qop = None\n\n    authorization = request.headers.get(\"Authorization\")\n    credentials = None\n    if authorization:\n        credentials = parse_authorization_header(authorization)\n\n    if (\n        not authorization\n        or not credentials\n        or credentials.type.lower() != \"digest\"\n        or (require_cookie_handling and \"Cookie\" not in request.headers)\n    ):\n        response = digest_challenge_response(app, qop, algorithm)\n        response.set_cookie(\"stale_after\", value=stale_after)\n        response.set_cookie(\"fake\", value=\"fake_value\")\n        return response\n\n    if require_cookie_handling and request.cookies.get(\"fake\") != \"fake_value\":\n        response = jsonify({\"errors\": [\"missing cookie set on challenge\"]})\n        response.set_cookie(\"fake\", value=\"fake_value\")\n        response.status_code = 403\n        return response\n\n    current_nonce = credentials.get(\"nonce\")\n\n    stale_after_value = None\n    if \"stale_after\" in request.cookies:\n        stale_after_value = request.cookies.get(\"stale_after\")\n\n    if (\n        \"last_nonce\" in request.cookies\n        and current_nonce == request.cookies.get(\"last_nonce\")\n        or stale_after_value == \"0\"\n    ):\n        response = digest_challenge_response(app, qop, algorithm, True)\n        response.set_cookie(\"stale_after\", value=stale_after)\n        response.set_cookie(\"last_nonce\", value=current_nonce)\n        response.set_cookie(\"fake\", value=\"fake_value\")\n        return response\n\n    if not check_digest_auth(user, passwd):\n        response = digest_challenge_response(app, qop, algorithm, False)\n        response.set_cookie(\"stale_after\", value=stale_after)\n        response.set_cookie(\"last_nonce\", value=current_nonce)\n        response.set_cookie(\"fake\", value=\"fake_value\")\n        return response\n\n    response = jsonify(authenticated=True, user=user)\n    response.set_cookie(\"fake\", value=\"fake_value\")\n    if stale_after_value:\n        response.set_cookie(\n            \"stale_after\", value=next_stale_after_value(stale_after_value)\n        )\n\n    return response\n\n\n@app.route(\"/delay/<delay>\", methods=[\"GET\", \"POST\", \"PUT\", \"DELETE\", \"PATCH\", \"TRACE\"])\ndef delay_response(delay):\n    \"\"\"Returns a delayed response (max of 10 seconds).\n    ---\n    tags:\n      - Dynamic data\n    parameters:\n      - in: path\n        name: delay\n        type: int\n    produces:\n      - application/json\n    responses:\n      200:\n        description: A delayed response.\n    \"\"\"\n    delay = min(float(delay), 10)\n\n    time.sleep(delay)\n\n    return jsonify(\n        get_dict(\"url\", \"args\", \"form\", \"data\", \"origin\", \"headers\", \"files\")\n    )\n\n\n@app.route(\"/drip\")\ndef drip():\n    \"\"\"Drips data over a duration after an optional initial delay.\n    ---\n    tags:\n      - Dynamic data\n    parameters:\n      - in: query\n        name: duration\n        type: number\n        description: The amount of time (in seconds) over which to drip each byte\n        default: 2\n        required: false\n      - in: query\n        name: numbytes\n        type: integer\n        description: The number of bytes to respond with\n        default: 10\n        required: false\n      - in: query\n        name: code\n        type: integer\n        description: The response code that will be returned\n        default: 200\n        required: false\n      - in: query\n        name: delay\n        type: number\n        description: The amount of time (in seconds) to delay before responding\n        default: 2\n        required: false\n    produces:\n      - application/octet-stream\n    responses:\n      200:\n        description: A dripped response.\n    \"\"\"\n    args = CaseInsensitiveDict(request.args.items())\n    duration = float(args.get(\"duration\", 2))\n    numbytes = min(int(args.get(\"numbytes\", 10)), (10 * 1024 * 1024))  # set 10MB limit\n    code = int(args.get(\"code\", 200))\n\n    if numbytes <= 0:\n        response = Response(\"number of bytes must be positive\", status=400)\n        return response\n\n    delay = float(args.get(\"delay\", 0))\n    if delay > 0:\n        time.sleep(delay)\n\n    pause = duration / numbytes\n\n    def generate_bytes():\n        for i in xrange(numbytes):\n            yield b\"*\"\n            time.sleep(pause)\n\n    response = Response(\n        generate_bytes(),\n        headers={\n            \"Content-Type\": \"application/octet-stream\",\n            \"Content-Length\": str(numbytes),\n        },\n    )\n\n    response.status_code = code\n\n    return response\n\n\n@app.route(\"/base64/<value>\")\ndef decode_base64(value):\n    \"\"\"Decodes base64url-encoded string.\n    ---\n    tags:\n      - Dynamic data\n    parameters:\n      - in: path\n        name: value\n        type: string\n        default: SFRUUEJJTiBpcyBhd2Vzb21l\n    produces:\n      - text/html\n    responses:\n      200:\n        description: Decoded base64 content.\n    \"\"\"\n    encoded = value.encode(\"utf-8\")  # base64 expects binary string as input\n    try:\n        return base64.urlsafe_b64decode(encoded).decode(\"utf-8\")\n    except:\n        return \"Incorrect Base64 data try: SFRUUEJJTiBpcyBhd2Vzb21l\"\n\n\n@app.route(\"/cache\", methods=(\"GET\",))\ndef cache():\n    \"\"\"Returns a 304 if an If-Modified-Since header or If-None-Match is present. Returns the same as a GET otherwise.\n    ---\n    tags:\n      - Response inspection\n    parameters:\n      - in: header\n        name: If-Modified-Since\n      - in: header\n        name: If-None-Match\n    produces:\n      - application/json\n    responses:\n      200:\n        description: Cached response\n      304:\n        description: Modified\n\n    \"\"\"\n    is_conditional = request.headers.get(\"If-Modified-Since\") or request.headers.get(\n        \"If-None-Match\"\n    )\n\n    if is_conditional is None:\n        response = view_get()\n        response.headers[\"Last-Modified\"] = http_date()\n        response.headers[\"ETag\"] = uuid.uuid4().hex\n        return response\n    else:\n        return status_code(304)\n\n\n@app.route(\"/etag/<etag>\", methods=(\"GET\",))\ndef etag(etag):\n    \"\"\"Assumes the resource has the given etag and responds to If-None-Match and If-Match headers appropriately.\n    ---\n    tags:\n      - Response inspection\n    parameters:\n      - in: header\n        name: If-None-Match\n      - in: header\n        name: If-Match\n    produces:\n      - application/json\n    responses:\n      200:\n        description: Normal response\n      412:\n        description: match\n\n    \"\"\"\n    if_none_match = parse_multi_value_header(request.headers.get(\"If-None-Match\"))\n    if_match = parse_multi_value_header(request.headers.get(\"If-Match\"))\n\n    if if_none_match:\n        if etag in if_none_match or \"*\" in if_none_match:\n            response = status_code(304)\n            response.headers[\"ETag\"] = etag\n            return response\n    elif if_match:\n        if etag not in if_match and \"*\" not in if_match:\n            return status_code(412)\n\n    # Special cases don't apply, return normal response\n    response = view_get()\n    response.headers[\"ETag\"] = etag\n    return response\n\n\n@app.route(\"/cache/<int:value>\")\ndef cache_control(value):\n    \"\"\"Sets a Cache-Control header for n seconds.\n    ---\n    tags:\n      - Response inspection\n    parameters:\n      - in: path\n        name: value\n        type: integer\n    produces:\n      - application/json\n    responses:\n      200:\n        description: Cache control set\n    \"\"\"\n    response = view_get()\n    response.headers[\"Cache-Control\"] = \"public, max-age={0}\".format(value)\n    return response\n\n\n@app.route(\"/encoding/utf8\")\ndef encoding():\n    \"\"\"Returns a UTF-8 encoded body.\n    ---\n    tags:\n      - Response formats\n    produces:\n      - text/html\n    responses:\n      200:\n        description: Encoded UTF-8 content.\n    \"\"\"\n\n    return render_template(\"UTF-8-demo.txt\")\n\n\n@app.route(\"/bytes/<int:n>\")\ndef random_bytes(n):\n    \"\"\"Returns n random bytes generated with given seed\n    ---\n    tags:\n      - Dynamic data\n    parameters:\n      - in: path\n        name: n\n        type: int\n    produces:\n      - application/octet-stream\n    responses:\n      200:\n        description: Bytes.\n    \"\"\"\n\n    n = min(n, 100 * 1024)  # set 100KB limit\n\n    params = CaseInsensitiveDict(request.args.items())\n    if \"seed\" in params:\n        random.seed(int(params[\"seed\"]))\n\n    response = make_response()\n\n    # Note: can't just use os.urandom here because it ignores the seed\n    response.data = bytearray(random.randint(0, 255) for i in range(n))\n    response.content_type = \"application/octet-stream\"\n    return response\n\n\n@app.route(\"/stream-bytes/<int:n>\")\ndef stream_random_bytes(n):\n    \"\"\"Streams n random bytes generated with given seed, at given chunk size per packet.\n    ---\n    tags:\n      - Dynamic data\n    parameters:\n      - in: path\n        name: n\n        type: int\n    produces:\n      - application/octet-stream\n    responses:\n      200:\n        description: Bytes.\n    \"\"\"\n    n = min(n, 100 * 1024)  # set 100KB limit\n\n    params = CaseInsensitiveDict(request.args.items())\n    if \"seed\" in params:\n        random.seed(int(params[\"seed\"]))\n\n    if \"chunk_size\" in params:\n        chunk_size = max(1, int(params[\"chunk_size\"]))\n    else:\n        chunk_size = 10 * 1024\n\n    def generate_bytes():\n        chunks = bytearray()\n\n        for i in xrange(n):\n            chunks.append(random.randint(0, 255))\n            if len(chunks) == chunk_size:\n                yield (bytes(chunks))\n                chunks = bytearray()\n\n        if chunks:\n            yield (bytes(chunks))\n\n    headers = {\"Content-Type\": \"application/octet-stream\"}\n\n    return Response(generate_bytes(), headers=headers)\n\n\n@app.route(\"/range/<int:numbytes>\")\ndef range_request(numbytes):\n    \"\"\"Streams n random bytes generated with given seed, at given chunk size per packet.\n    ---\n    tags:\n      - Dynamic data\n    parameters:\n      - in: path\n        name: numbytes\n        type: int\n    produces:\n      - application/octet-stream\n    responses:\n      200:\n        description: Bytes.\n    \"\"\"\n\n    if numbytes <= 0 or numbytes > (100 * 1024):\n        response = Response(\n            headers={\"ETag\": \"range%d\" % numbytes, \"Accept-Ranges\": \"bytes\"}\n        )\n        response.status_code = 404\n        response.data = \"number of bytes must be in the range (0, 102400]\"\n        return response\n\n    params = CaseInsensitiveDict(request.args.items())\n    if \"chunk_size\" in params:\n        chunk_size = max(1, int(params[\"chunk_size\"]))\n    else:\n        chunk_size = 10 * 1024\n\n    duration = float(params.get(\"duration\", 0))\n    pause_per_byte = duration / numbytes\n\n    request_headers = get_headers()\n    first_byte_pos, last_byte_pos = get_request_range(request_headers, numbytes)\n    range_length = (last_byte_pos + 1) - first_byte_pos\n\n    if (\n        first_byte_pos > last_byte_pos\n        or first_byte_pos not in xrange(0, numbytes)\n        or last_byte_pos not in xrange(0, numbytes)\n    ):\n        response = Response(\n            headers={\n                \"ETag\": \"range%d\" % numbytes,\n                \"Accept-Ranges\": \"bytes\",\n                \"Content-Range\": \"bytes */%d\" % numbytes,\n                \"Content-Length\": \"0\",\n            }\n        )\n        response.status_code = 416\n        return response\n\n    def generate_bytes():\n        chunks = bytearray()\n\n        for i in xrange(first_byte_pos, last_byte_pos + 1):\n\n            # We don't want the resource to change across requests, so we need\n            # to use a predictable data generation function\n            chunks.append(ord(\"a\") + (i % 26))\n            if len(chunks) == chunk_size:\n                yield (bytes(chunks))\n                time.sleep(pause_per_byte * chunk_size)\n                chunks = bytearray()\n\n        if chunks:\n            time.sleep(pause_per_byte * len(chunks))\n            yield (bytes(chunks))\n\n    content_range = \"bytes %d-%d/%d\" % (first_byte_pos, last_byte_pos, numbytes)\n    response_headers = {\n        \"Content-Type\": \"application/octet-stream\",\n        \"ETag\": \"range%d\" % numbytes,\n        \"Accept-Ranges\": \"bytes\",\n        \"Content-Length\": str(range_length),\n        \"Content-Range\": content_range,\n    }\n\n    response = Response(generate_bytes(), headers=response_headers)\n\n    if (first_byte_pos == 0) and (last_byte_pos == (numbytes - 1)):\n        response.status_code = 200\n    else:\n        response.status_code = 206\n\n    return response\n\n\n@app.route(\"/links/<int:n>/<int:offset>\")\ndef link_page(n, offset):\n    \"\"\"Generate a page containing n links to other pages which do the same.\n    ---\n    tags:\n      - Dynamic data\n    parameters:\n      - in: path\n        name: n\n        type: int\n      - in: path\n        name: offset\n        type: int\n    produces:\n      - text/html\n    responses:\n      200:\n        description: HTML links.\n    \"\"\"\n    n = min(max(1, n), 200)  # limit to between 1 and 200 links\n\n    link = \"<a href='{0}'>{1}</a> \"\n\n    html = [\"<html><head><title>Links</title></head><body>\"]\n    for i in xrange(n):\n        if i == offset:\n            html.append(\"{0} \".format(i))\n        else:\n            html.append(link.format(url_for(\"link_page\", n=n, offset=i), i))\n    html.append(\"</body></html>\")\n\n    return \"\".join(html)\n\n\n@app.route(\"/links/<int:n>\")\ndef links(n):\n    \"\"\"Redirect to first links page.\"\"\"\n    return redirect(url_for(\"link_page\", n=n, offset=0))\n\n\n@app.route(\"/image\")\ndef image():\n    \"\"\"Returns a simple image of the type suggest by the Accept header.\n    ---\n    tags:\n      - Images\n    produces:\n      - image/webp\n      - image/svg+xml\n      - image/jpeg\n      - image/png\n      - image/*\n    responses:\n      200:\n        description: An image.\n    \"\"\"\n\n    headers = get_headers()\n    if \"accept\" not in headers:\n        return image_png()  # Default media type to png\n\n    accept = headers[\"accept\"].lower()\n\n    if \"image/webp\" in accept:\n        return image_webp()\n    elif \"image/svg+xml\" in accept:\n        return image_svg()\n    elif \"image/jpeg\" in accept:\n        return image_jpeg()\n    elif \"image/png\" in accept or \"image/*\" in accept:\n        return image_png()\n    else:\n        return status_code(406)  # Unsupported media type\n\n\n@app.route(\"/image/png\")\ndef image_png():\n    \"\"\"Returns a simple PNG image.\n    ---\n    tags:\n      - Images\n    produces:\n      - image/png\n    responses:\n      200:\n        description: A PNG image.\n    \"\"\"\n    data = resource(\"images/pig_icon.png\")\n    return Response(data, headers={\"Content-Type\": \"image/png\"})\n\n\n@app.route(\"/image/jpeg\")\ndef image_jpeg():\n    \"\"\"Returns a simple JPEG image.\n    ---\n    tags:\n      - Images\n    produces:\n      - image/jpeg\n    responses:\n      200:\n        description: A JPEG image.\n    \"\"\"\n    data = resource(\"images/jackal.jpg\")\n    return Response(data, headers={\"Content-Type\": \"image/jpeg\"})\n\n\n@app.route(\"/image/webp\")\ndef image_webp():\n    \"\"\"Returns a simple WEBP image.\n    ---\n    tags:\n      - Images\n    produces:\n      - image/webp\n    responses:\n      200:\n        description: A WEBP image.\n    \"\"\"\n    data = resource(\"images/wolf_1.webp\")\n    return Response(data, headers={\"Content-Type\": \"image/webp\"})\n\n\n@app.route(\"/image/svg\")\ndef image_svg():\n    \"\"\"Returns a simple SVG image.\n    ---\n    tags:\n      - Images\n    produces:\n      - image/svg+xml\n    responses:\n      200:\n        description: An SVG image.\n    \"\"\"\n    data = resource(\"images/svg_logo.svg\")\n    return Response(data, headers={\"Content-Type\": \"image/svg+xml\"})\n\n\ndef resource(filename):\n    path = os.path.join(tmpl_dir, filename)\n    with open(path, \"rb\") as f:\n      return f.read()\n\n\n@app.route(\"/xml\")\ndef xml():\n    \"\"\"Returns a simple XML document.\n    ---\n    tags:\n      - Response formats\n    produces:\n      - application/xml\n    responses:\n      200:\n        description: An XML document.\n    \"\"\"\n    response = make_response(render_template(\"sample.xml\"))\n    response.headers[\"Content-Type\"] = \"application/xml\"\n    return response\n\n\n@app.route(\"/json\")\ndef a_json_endpoint():\n    \"\"\"Returns a simple JSON document.\n    ---\n    tags:\n      - Response formats\n    produces:\n      - application/json\n    responses:\n      200:\n        description: An JSON document.\n    \"\"\"\n    return flask_jsonify(\n        slideshow={\n            \"title\": \"Sample Slide Show\",\n            \"date\": \"date of publication\",\n            \"author\": \"Yours Truly\",\n            \"slides\": [\n                {\"type\": \"all\", \"title\": \"Wake up to WonderWidgets!\"},\n                {\n                    \"type\": \"all\",\n                    \"title\": \"Overview\",\n                    \"items\": [\n                        \"Why <em>WonderWidgets</em> are great\",\n                        \"Who <em>buys</em> WonderWidgets\",\n                    ],\n                },\n            ],\n        }\n    )\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--port\", type=int, default=5000)\n    parser.add_argument(\"--host\", default=\"127.0.0.1\")\n    args = parser.parse_args()\n    app.run(port=args.port, host=args.host)\n", "metadata": {"source": "github.com/postmanlabs/httpbin", "file_path": "httpbin/core.py", "language": "python", "complexity": "complex", "star_count": 13214, "quality_score": 1.0, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": true, "has_documentation": true, "has_type_hints": true, "has_testing": true, "has_logging": true, "function_count": 63, "class_count": 0, "line_count": 1787}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create REST API endpoint with proper request/response handling\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# -*- coding: utf-8 -*-\n\n\"\"\"\nhttpbin.filters\n~~~~~~~~~~~~~~~\n\nThis module provides response filter decorators.\n\"\"\"\n\nimport gzip as gzip2\nimport zlib\n\nimport brotli as _brotli\n\nfrom six import BytesIO\nfrom decimal import Decimal\nfrom time import time as now\n\nfrom decorator import decorator\nfrom flask import Flask, Response\n\n\napp = Flask(__name__)\n\n\n@decorator\ndef x_runtime(f, *args, **kwargs):\n    \"\"\"X-Runtime Flask Response Decorator.\"\"\"\n\n    _t0 = now()\n    r = f(*args, **kwargs)\n    _t1 = now()\n    r.headers['X-Runtime'] = '{0}s'.format(Decimal(str(_t1 - _t0)))\n\n    return r\n\n\n@decorator\ndef gzip(f, *args, **kwargs):\n    \"\"\"GZip Flask Response Decorator.\"\"\"\n\n    data = f(*args, **kwargs)\n\n    if isinstance(data, Response):\n        content = data.data\n    else:\n        content = data\n\n    gzip_buffer = BytesIO()\n    gzip_file = gzip2.GzipFile(\n        mode='wb',\n        compresslevel=4,\n        fileobj=gzip_buffer\n    )\n    gzip_file.write(content)\n    gzip_file.close()\n\n    gzip_data = gzip_buffer.getvalue()\n\n    if isinstance(data, Response):\n        data.data = gzip_data\n        data.headers['Content-Encoding'] = 'gzip'\n        data.headers['Content-Length'] = str(len(data.data))\n\n        return data\n\n    return gzip_data\n\n\n@decorator\ndef deflate(f, *args, **kwargs):\n    \"\"\"Deflate Flask Response Decorator.\"\"\"\n\n    data = f(*args, **kwargs)\n\n    if isinstance(data, Response):\n        content = data.data\n    else:\n        content = data\n\n    deflater = zlib.compressobj()\n    deflated_data = deflater.compress(content)\n    deflated_data += deflater.flush()\n\n    if isinstance(data, Response):\n        data.data = deflated_data\n        data.headers['Content-Encoding'] = 'deflate'\n        data.headers['Content-Length'] = str(len(data.data))\n\n        return data\n\n    return deflated_data\n\n\n@decorator\ndef brotli(f, *args, **kwargs):\n    \"\"\"Brotli Flask Response Decorator\"\"\"\n\n    data = f(*args, **kwargs)\n\n    if isinstance(data, Response):\n        content = data.data\n    else:\n        content = data\n\n    deflated_data = _brotli.compress(content)\n\n    if isinstance(data, Response):\n        data.data = deflated_data\n        data.headers['Content-Encoding'] = 'br'\n        data.headers['Content-Length'] = str(len(data.data))\n\n        return data\n\n    return deflated_data\n", "metadata": {"source": "github.com/postmanlabs/httpbin", "file_path": "httpbin/filters.py", "language": "python", "complexity": "medium", "star_count": 13214, "quality_score": 0.92, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": true, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 4, "class_count": 0, "line_count": 116}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create REST API endpoint with proper request/response handling\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# -*- coding: utf-8 -*-\n\n\"\"\"\nhttpbin.helpers\n~~~~~~~~~~~~~~~\n\nThis module provides helper functions for httpbin.\n\"\"\"\n\nimport json\nimport base64\nimport re\nimport time\nimport os\nfrom hashlib import md5, sha256, sha512\nfrom werkzeug.http import parse_authorization_header\nfrom werkzeug.datastructures import WWWAuthenticate\n\nfrom flask import request, make_response\nfrom six.moves.urllib.parse import urlparse, urlunparse\n\n\nfrom .structures import CaseInsensitiveDict\n\n\nASCII_ART = \"\"\"\n    -=[ teapot ]=-\n\n       _...._\n     .'  _ _ `.\n    | .\"` ^ `\". _,\n    \\_;`\"---\"`|//\n      |       ;/\n      \\_     _/\n        `\\\"\\\"\\\"`\n\"\"\"\n\nREDIRECT_LOCATION = '/redirect/1'\n\nENV_HEADERS = (\n    'X-Varnish',\n    'X-Request-Start',\n    'X-Heroku-Queue-Depth',\n    'X-Real-Ip',\n    'X-Forwarded-Proto',\n    'X-Forwarded-Protocol',\n    'X-Forwarded-Ssl',\n    'X-Heroku-Queue-Wait-Time',\n    'X-Forwarded-For',\n    'X-Heroku-Dynos-In-Use',\n    'X-Forwarded-Protocol',\n    'X-Forwarded-Port',\n    'X-Request-Id',\n    'Via',\n    'Total-Route-Time',\n    'Connect-Time'\n)\n\nROBOT_TXT = \"\"\"User-agent: *\nDisallow: /deny\n\"\"\"\n\nACCEPTED_MEDIA_TYPES = [\n    'image/webp',\n    'image/svg+xml',\n    'image/jpeg',\n    'image/png',\n    'image/*'\n]\n\nANGRY_ASCII =\"\"\"\n          .-''''''-.\n        .' _      _ '.\n       /   O      O   \\\\\n      :                :\n      |                |\n      :       __       :\n       \\  .-\"`  `\"-.  /\n        '.          .'\n          '-......-'\n     YOU SHOULDN'T BE HERE\n\"\"\"\n\n\ndef json_safe(string, content_type='application/octet-stream'):\n    \"\"\"Returns JSON-safe version of `string`.\n\n    If `string` is a Unicode string or a valid UTF-8, it is returned unmodified,\n    as it can safely be encoded to JSON string.\n\n    If `string` contains raw/binary data, it is Base64-encoded, formatted and\n    returned according to \"data\" URL scheme (RFC2397). Since JSON is not\n    suitable for binary data, some additional encoding was necessary; \"data\"\n    URL scheme was chosen for its simplicity.\n    \"\"\"\n    try:\n        string = string.decode('utf-8')\n        json.dumps(string)\n        return string\n    except (ValueError, TypeError):\n        return b''.join([\n            b'data:',\n            content_type.encode('utf-8'),\n            b';base64,',\n            base64.b64encode(string)\n        ]).decode('utf-8')\n\n\ndef get_files():\n    \"\"\"Returns files dict from request context.\"\"\"\n\n    files = dict()\n\n    for k, v in request.files.items():\n        content_type = request.files[k].content_type or 'application/octet-stream'\n        val = json_safe(v.read(), content_type)\n        if files.get(k):\n            if not isinstance(files[k], list):\n                files[k] = [files[k]]\n            files[k].append(val)\n        else:\n            files[k] = val\n\n    return files\n\n\ndef get_headers(hide_env=True):\n    \"\"\"Returns headers dict from request context.\"\"\"\n\n    headers = dict(request.headers.items())\n\n    if hide_env and ('show_env' not in request.args):\n        for key in ENV_HEADERS:\n            try:\n                del headers[key]\n            except KeyError:\n                pass\n\n    return CaseInsensitiveDict(headers.items())\n\n\ndef semiflatten(multi):\n    \"\"\"Convert a MutiDict into a regular dict. If there are more than one value\n    for a key, the result will have a list of values for the key. Otherwise it\n    will have the plain value.\"\"\"\n    if multi:\n        result = multi.to_dict(flat=False)\n        for k, v in result.items():\n            if len(v) == 1:\n                result[k] = v[0]\n        return result\n    else:\n        return multi\n\ndef get_url(request):\n    \"\"\"\n    Since we might be hosted behind a proxy, we need to check the\n    X-Forwarded-Proto, X-Forwarded-Protocol, or X-Forwarded-SSL headers\n    to find out what protocol was used to access us.\n    \"\"\"\n    protocol = request.headers.get('X-Forwarded-Proto') or request.headers.get('X-Forwarded-Protocol')\n    if protocol is None and request.headers.get('X-Forwarded-Ssl') == 'on':\n        protocol = 'https'\n    if protocol is None:\n        return request.url\n    url = list(urlparse(request.url))\n    url[0] = protocol\n    return urlunparse(url)\n\n\ndef get_dict(*keys, **extras):\n    \"\"\"Returns request dict of given keys.\"\"\"\n\n    _keys = ('url', 'args', 'form', 'data', 'origin', 'headers', 'files', 'json', 'method')\n\n    assert all(map(_keys.__contains__, keys))\n    data = request.data\n    form = semiflatten(request.form)\n\n    try:\n        _json = json.loads(data.decode('utf-8'))\n    except (ValueError, TypeError):\n        _json = None\n\n    d = dict(\n        url=get_url(request),\n        args=semiflatten(request.args),\n        form=form,\n        data=json_safe(data),\n        origin=request.headers.get('X-Forwarded-For', request.remote_addr),\n        headers=get_headers(),\n        files=get_files(),\n        json=_json,\n        method=request.method,\n    )\n\n    out_d = dict()\n\n    for key in keys:\n        out_d[key] = d.get(key)\n\n    out_d.update(extras)\n\n    return out_d\n\n\ndef status_code(code):\n    \"\"\"Returns response object of given status code.\"\"\"\n\n    redirect = dict(headers=dict(location=REDIRECT_LOCATION))\n\n    code_map = {\n        301: redirect,\n        302: redirect,\n        303: redirect,\n        304: dict(data=''),\n        305: redirect,\n        307: redirect,\n        401: dict(headers={'WWW-Authenticate': 'Basic realm=\"Fake Realm\"'}),\n        402: dict(\n            data='Fuck you, pay me!',\n            headers={\n                'x-more-info': 'http://vimeo.com/22053820'\n            }\n        ),\n        406: dict(data=json.dumps({\n                'message': 'Client did not request a supported media type.',\n                'accept': ACCEPTED_MEDIA_TYPES\n            }),\n            headers={\n                'Content-Type': 'application/json'\n            }),\n        407: dict(headers={'Proxy-Authenticate': 'Basic realm=\"Fake Realm\"'}),\n        418: dict(  # I'm a teapot!\n            data=ASCII_ART,\n            headers={\n                'x-more-info': 'http://tools.ietf.org/html/rfc2324'\n            }\n        ),\n\n    }\n\n    r = make_response()\n    r.status_code = code\n\n    if code in code_map:\n\n        m = code_map[code]\n\n        if 'data' in m:\n            r.data = m['data']\n        if 'headers' in m:\n            r.headers = m['headers']\n\n    return r\n\n\ndef check_basic_auth(user, passwd):\n    \"\"\"Checks user authentication using HTTP Basic Auth.\"\"\"\n\n    auth = request.authorization\n    return auth and auth.username == user and auth.password == passwd\n\n\n\n# Digest auth helpers\n# qop is a quality of protection\n\ndef H(data, algorithm):\n    if algorithm == 'SHA-256':\n        return sha256(data).hexdigest()\n    elif algorithm == 'SHA-512':\n        return sha512(data).hexdigest()\n    else:\n        return md5(data).hexdigest()\n\n\ndef HA1(realm, username, password, algorithm):\n    \"\"\"Create HA1 hash by realm, username, password\n\n    HA1 = md5(A1) = MD5(username:realm:password)\n    \"\"\"\n    if not realm:\n        realm = u''\n    return H(b\":\".join([username.encode('utf-8'),\n                           realm.encode('utf-8'),\n                           password.encode('utf-8')]), algorithm)\n\n\ndef HA2(credentials, request, algorithm):\n    \"\"\"Create HA2 md5 hash\n\n    If the qop directive's value is \"auth\" or is unspecified, then HA2:\n        HA2 = md5(A2) = MD5(method:digestURI)\n    If the qop directive's value is \"auth-int\" , then HA2 is\n        HA2 = md5(A2) = MD5(method:digestURI:MD5(entityBody))\n    \"\"\"\n    if credentials.get(\"qop\") == \"auth\" or credentials.get('qop') is None:\n        return H(b\":\".join([request['method'].encode('utf-8'), request['uri'].encode('utf-8')]), algorithm)\n    elif credentials.get(\"qop\") == \"auth-int\":\n        for k in 'method', 'uri', 'body':\n            if k not in request:\n                raise ValueError(\"%s required\" % k)\n        A2 = b\":\".join([request['method'].encode('utf-8'),\n                        request['uri'].encode('utf-8'),\n                        H(request['body'], algorithm).encode('utf-8')])\n        return H(A2, algorithm)\n    raise ValueError\n\n\ndef response(credentials, password, request):\n    \"\"\"Compile digest auth response\n\n    If the qop directive's value is \"auth\" or \"auth-int\" , then compute the response as follows:\n       RESPONSE = MD5(HA1:nonce:nonceCount:clienNonce:qop:HA2)\n    Else if the qop directive is unspecified, then compute the response as follows:\n       RESPONSE = MD5(HA1:nonce:HA2)\n\n    Arguments:\n    - `credentials`: credentials dict\n    - `password`: request user password\n    - `request`: request dict\n    \"\"\"\n    response = None\n    algorithm = credentials.get('algorithm')\n    HA1_value = HA1(\n        credentials.get('realm'),\n        credentials.get('username'),\n        password,\n        algorithm\n    )\n    HA2_value = HA2(credentials, request, algorithm)\n    if credentials.get('qop') is None:\n        response = H(b\":\".join([\n            HA1_value.encode('utf-8'),\n            credentials.get('nonce', '').encode('utf-8'),\n            HA2_value.encode('utf-8')\n        ]), algorithm)\n    elif credentials.get('qop') == 'auth' or credentials.get('qop') == 'auth-int':\n        for k in 'nonce', 'nc', 'cnonce', 'qop':\n            if k not in credentials:\n                raise ValueError(\"%s required for response H\" % k)\n        response = H(b\":\".join([HA1_value.encode('utf-8'),\n                               credentials.get('nonce').encode('utf-8'),\n                               credentials.get('nc').encode('utf-8'),\n                               credentials.get('cnonce').encode('utf-8'),\n                               credentials.get('qop').encode('utf-8'),\n                               HA2_value.encode('utf-8')]), algorithm)\n    else:\n        raise ValueError(\"qop value are wrong\")\n\n    return response\n\n\ndef check_digest_auth(user, passwd):\n    \"\"\"Check user authentication using HTTP Digest auth\"\"\"\n\n    if request.headers.get('Authorization'):\n        credentials = parse_authorization_header(request.headers.get('Authorization'))\n        if not credentials:\n            return\n        request_uri = request.script_root + request.path\n        if request.query_string:\n            request_uri +=  '?' + request.query_string\n        response_hash = response(credentials, passwd, dict(uri=request_uri,\n                                                           body=request.data,\n                                                           method=request.method))\n        if credentials.get('response') == response_hash:\n            return True\n    return False\n\ndef secure_cookie():\n    \"\"\"Return true if cookie should have secure attribute\"\"\"\n    return request.environ['wsgi.url_scheme'] == 'https'\n\ndef __parse_request_range(range_header_text):\n    \"\"\" Return a tuple describing the byte range requested in a GET request\n    If the range is open ended on the left or right side, then a value of None\n    will be set.\n    RFC7233: http://svn.tools.ietf.org/svn/wg/httpbis/specs/rfc7233.html#header.range\n    Examples:\n      Range : bytes=1024-\n      Range : bytes=10-20\n      Range : bytes=-999\n    \"\"\"\n\n    left = None\n    right = None\n\n    if not range_header_text:\n        return left, right\n\n    range_header_text = range_header_text.strip()\n    if not range_header_text.startswith('bytes'):\n        return left, right\n\n    components = range_header_text.split(\"=\")\n    if len(components) != 2:\n        return left, right\n\n    components = components[1].split(\"-\")\n\n    try:\n        right = int(components[1])\n    except:\n        pass\n\n    try:\n        left = int(components[0])\n    except:\n        pass\n\n    return left, right\n\ndef get_request_range(request_headers, upper_bound):\n    first_byte_pos, last_byte_pos = __parse_request_range(request_headers['range'])\n\n    if first_byte_pos is None and last_byte_pos is None:\n        # Request full range\n        first_byte_pos = 0\n        last_byte_pos = upper_bound - 1\n    elif first_byte_pos is None:\n        # Request the last X bytes\n        first_byte_pos = max(0, upper_bound - last_byte_pos)\n        last_byte_pos = upper_bound - 1\n    elif last_byte_pos is None:\n        # Request the last X bytes\n        last_byte_pos = upper_bound - 1\n\n    return first_byte_pos, last_byte_pos\n\ndef parse_multi_value_header(header_str):\n    \"\"\"Break apart an HTTP header string that is potentially a quoted, comma separated list as used in entity headers in RFC2616.\"\"\"\n    parsed_parts = []\n    if header_str:\n        parts = header_str.split(',')\n        for part in parts:\n            match = re.search('\\s*(W/)?\\\"?([^\"]*)\\\"?\\s*', part)\n            if match is not None:\n                parsed_parts.append(match.group(2))\n    return parsed_parts\n\n\ndef next_stale_after_value(stale_after):\n    try:\n        stal_after_count = int(stale_after) - 1\n        return str(stal_after_count)\n    except ValueError:\n        return 'never'\n\n\ndef digest_challenge_response(app, qop, algorithm, stale = False):\n    response = app.make_response('')\n    response.status_code = 401\n\n    # RFC2616 Section4.2: HTTP headers are ASCII.  That means\n    # request.remote_addr was originally ASCII, so I should be able to\n    # encode it back to ascii.  Also, RFC2617 says about nonces: \"The\n    # contents of the nonce are implementation dependent\"\n    nonce = H(b''.join([\n        getattr(request, 'remote_addr', u'').encode('ascii'),\n        b':',\n        str(time.time()).encode('ascii'),\n        b':',\n        os.urandom(10)\n    ]), algorithm)\n    opaque = H(os.urandom(10), algorithm)\n\n    auth = WWWAuthenticate(\"digest\")\n    auth.set_digest('me@kennethreitz.com', nonce, opaque=opaque,\n                    qop=('auth', 'auth-int') if qop is None else (qop,), algorithm=algorithm)\n    auth.stale = stale\n    response.headers['WWW-Authenticate'] = auth.to_header()\n    return response\n", "metadata": {"source": "github.com/postmanlabs/httpbin", "file_path": "httpbin/helpers.py", "language": "python", "complexity": "complex", "star_count": 13214, "quality_score": 1.0, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": true, "has_documentation": true, "has_type_hints": true, "has_testing": true, "has_logging": false, "function_count": 19, "class_count": 0, "line_count": 475}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create REST API endpoint with proper request/response handling\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# -*- coding: utf-8 -*-\n\n\"\"\"\nhttpbin.structures\n~~~~~~~~~~~~~~~~~~~\n\nData structures that power httpbin.\n\"\"\"\n\n\nclass CaseInsensitiveDict(dict):\n    \"\"\"Case-insensitive Dictionary for headers.\n\n    For example, ``headers['content-encoding']`` will return the\n    value of a ``'Content-Encoding'`` response header.\n    \"\"\"\n\n    def _lower_keys(self):\n        return [k.lower() for k in self.keys()]\n\n    def __contains__(self, key):\n        return key.lower() in self._lower_keys()\n\n    def __getitem__(self, key):\n        # We allow fall-through here, so values default to None\n        if key in self:\n            return list(self.items())[self._lower_keys().index(key.lower())][1]\n", "metadata": {"source": "github.com/postmanlabs/httpbin", "file_path": "httpbin/structures.py", "language": "python", "complexity": "simple", "star_count": 13214, "quality_score": 0.89, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": false, "has_error_handling": false, "has_documentation": true, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 3, "class_count": 1, "line_count": 28}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create REST API endpoint with proper request/response handling\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# -*- coding: utf-8 -*-\n\n\"\"\"\nhttpbin.utils\n~~~~~~~~~~~~~~~\n\nUtility functions.\n\"\"\"\n\nimport random\nimport bisect\n\n\ndef weighted_choice(choices):\n    \"\"\"Returns a value from choices chosen by weighted random selection\n\n    choices should be a list of (value, weight) tuples.\n\n    eg. weighted_choice([('val1', 5), ('val2', 0.3), ('val3', 1)])\n\n    \"\"\"\n    values, weights = zip(*choices)\n    total = 0\n    cum_weights = []\n    for w in weights:\n        total += w\n        cum_weights.append(total)\n    x = random.uniform(0, total)\n    i = bisect.bisect(cum_weights, x)\n    return values[i]\n", "metadata": {"source": "github.com/postmanlabs/httpbin", "file_path": "httpbin/utils.py", "language": "python", "complexity": "simple", "star_count": 13214, "quality_score": 0.92, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": true, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 1, "class_count": 0, "line_count": 31}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate module integration for Create REST API endpoint with proper request/response handling\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from setuptools import setup, find_packages\nimport os\nimport io\n\n\nwith open(os.path.join(os.path.realpath(os.path.dirname(__file__)), 'httpbin', 'VERSION')) as version_file:\n    version = version_file.read().strip()\n\nsetup(\n    name=\"httpbin\",\n    version=version,\n    description=\"HTTP Request and Response Service\",\n    long_description=\"A simple HTTP Request & Response Service, written in Python + Flask.\",\n\n    # The project URL.\n    url='https://github.com/requests/httpbin',\n\n    # Author details\n    author='Kenneth Reitz',\n    author_email='me@kennethreitz.org',\n\n    # Choose your license\n    license='MIT',\n\n    classifiers=[\n         'Development Status :: 5 - Production/Stable',\n         'Intended Audience :: Developers',\n         'Natural Language :: English',\n         'License :: OSI Approved :: MIT License',\n         'Programming Language :: Python',\n         'Programming Language :: Python :: 2.7',\n         'Programming Language :: Python :: 3.6',\n    ],\n    test_suite=\"test_httpbin\",\n    packages=find_packages(),\n    include_package_data = True, # include files listed in MANIFEST.in\n    install_requires=[\n        'Flask', 'MarkupSafe', 'decorator', 'itsdangerous', 'six', 'brotlipy',\n        'raven[flask]', 'werkzeug>=0.14.1', 'gevent', 'flasgger'\n    ],\n)\n", "metadata": {"source": "github.com/postmanlabs/httpbin", "file_path": "setup.py", "language": "python", "complexity": "simple", "star_count": 13214, "quality_score": 0.89, "features": {"has_classes": false, "has_functions": false, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": true, "has_logging": false, "function_count": 0, "class_count": 0, "line_count": 42}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create REST API endpoint with proper request/response handling with pytest testing framework\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\nimport os\nimport base64\nimport unittest\nimport contextlib\nimport six\nimport json\nfrom werkzeug.http import parse_dict_header\nfrom hashlib import md5, sha256, sha512\nfrom six import BytesIO\n\nimport httpbin\nfrom httpbin.helpers import parse_multi_value_header\n\n\n@contextlib.contextmanager\ndef _setenv(key, value):\n    \"\"\"Context manager to set an environment variable temporarily.\"\"\"\n    old_value = os.environ.get(key, None)\n    if value is None:\n        os.environ.pop(key, None)\n    else:\n        os.environ[key] = value\n\n    yield\n\n    if old_value is None:\n        os.environ.pop(key, None)\n    else:\n        os.environ[key] = value\n\n\n\ndef _string_to_base64(string):\n    \"\"\"Encodes string to utf-8 and then base64\"\"\"\n    utf8_encoded = string.encode('utf-8')\n    return base64.urlsafe_b64encode(utf8_encoded)\n\ndef _hash(data, algorithm):\n    \"\"\"Encode binary data according to specified algorithm, use MD5 by default\"\"\"\n    if algorithm == 'SHA-256':\n        return sha256(data).hexdigest()\n    elif algorithm == 'SHA-512':\n        return sha512(data).hexdigest()\n    else:\n        return md5(data).hexdigest()\n\ndef _make_digest_auth_header(username, password, method, uri, nonce,\n                             realm=None, opaque=None, algorithm=None,\n                             qop=None, cnonce=None, nc=None, body=None):\n    \"\"\"Compile a digest authentication header string.\n\n    Arguments:\n    - `nonce`: nonce string, received within \"WWW-Authenticate\" header\n    - `realm`: realm string, received within \"WWW-Authenticate\" header\n    - `opaque`: opaque string, received within \"WWW-Authenticate\" header\n    - `algorithm`: type of hashing algorithm, used by the client\n    - `qop`: type of quality-of-protection, used by the client\n    - `cnonce`: client nonce, required if qop is \"auth\" or \"auth-int\"\n    - `nc`: client nonce count, required if qop is \"auth\" or \"auth-int\"\n    - `body`: body of the outgoing request (bytes), used if qop is \"auth-int\"\n    \"\"\"\n\n    assert username\n    assert password\n    assert nonce\n    assert method\n    assert uri\n    assert algorithm in ('MD5', 'SHA-256', 'SHA-512', None)\n\n    a1 = ':'.join([username, realm or '', password])\n    ha1 = _hash(a1.encode('utf-8'), algorithm)\n\n    a2 = ':'.join([method, uri])\n    if qop == 'auth-int':\n        a2 = ':'.join([a2, _hash(body or b'', algorithm)])\n    ha2 = _hash(a2.encode('utf-8'), algorithm)\n\n    a3 = ':'.join([ha1, nonce])\n    if qop in ('auth', 'auth-int'):\n        assert cnonce\n        assert nc\n        a3 = ':'.join([a3, nc, cnonce, qop])\n\n    a3 = ':'.join([a3, ha2])\n    auth_response = _hash(a3.encode('utf-8'), algorithm)\n\n    auth_header = \\\n        'Digest username=\"{0}\", response=\"{1}\", uri=\"{2}\", nonce=\"{3}\"'\\\n            .format(username, auth_response, uri, nonce)\n\n    # 'realm' and 'opaque' should be returned unchanged, even if empty\n    if realm != None:\n        auth_header += ', realm=\"{0}\"'.format(realm)\n    if opaque != None:\n        auth_header += ', opaque=\"{0}\"'.format(opaque)\n\n    if algorithm:\n        auth_header += ', algorithm=\"{0}\"'.format(algorithm)\n    if cnonce:\n        auth_header += ', cnonce=\"{0}\"'.format(cnonce)\n    if nc:\n        auth_header += ', nc={0}'.format(nc)\n    if qop:\n        auth_header += ', qop={0}'.format(qop)\n\n    return auth_header\n\nclass HttpbinTestCase(unittest.TestCase):\n    \"\"\"Httpbin tests\"\"\"\n\n    def setUp(self):\n        httpbin.app.debug = True\n        self.app = httpbin.app.test_client()\n\n    def test_index(self):   \n        response = self.app.get('/', headers={'User-Agent': 'test'})\n        self.assertEqual(response.status_code, 200)\n \n    def get_data(self, response):\n        if 'get_data' in dir(response):\n            return response.get_data()\n        else:\n            return response.data\n\n    def test_response_headers_simple(self):\n        supported_verbs = ['get', 'post']\n        for verb in supported_verbs:\n            method = getattr(self.app, verb)\n            response = method('/response-headers?animal=dog')\n            self.assertEqual(response.status_code, 200)\n            self.assertEqual(response.headers.get_all('animal'), ['dog'])\n            assert json.loads(response.data.decode('utf-8'))['animal'] == 'dog'\n\n    def test_response_headers_multi(self):\n        supported_verbs = ['get', 'post']\n        for verb in supported_verbs:\n            method = getattr(self.app, verb)\n            response = method('/response-headers?animal=dog&animal=cat')\n            self.assertEqual(response.status_code, 200)\n            self.assertEqual(response.headers.get_all('animal'), ['dog', 'cat'])\n            assert json.loads(response.data.decode('utf-8'))['animal'] == ['dog', 'cat']\n\n    def test_get(self):\n        response = self.app.get('/get', headers={'User-Agent': 'test'})\n        self.assertEqual(response.status_code, 200)\n        data = json.loads(response.data.decode('utf-8'))\n        self.assertEqual(data['args'], {})\n        self.assertEqual(data['headers']['Host'], 'localhost')\n        self.assertEqual(data['headers']['Content-Length'], '0')\n        self.assertEqual(data['headers']['User-Agent'], 'test')\n        # self.assertEqual(data['origin'], None)\n        self.assertEqual(data['url'], 'http://localhost/get')\n        self.assertTrue(response.data.endswith(b'\\n'))\n\n    def test_anything(self):\n        response = self.app.get('/anything')\n        self.assertEqual(response.status_code, 200)\n        response = self.app.get('/anything/foo/bar')\n        self.assertEqual(response.status_code, 200)\n        data = json.loads(response.data.decode('utf-8'))\n        self.assertEqual(data['args'], {})\n        self.assertEqual(data['headers']['Host'], 'localhost')\n        self.assertEqual(data['headers']['Content-Length'], '0')\n        self.assertEqual(data['url'], 'http://localhost/anything/foo/bar')\n        self.assertEqual(data['method'], 'GET')\n        self.assertTrue(response.data.endswith(b'\\n'))\n\n    def test_base64(self):\n        greeting = u'Здравствуй, мир!'\n        b64_encoded = _string_to_base64(greeting)\n        response = self.app.get(b'/base64/' + b64_encoded)\n        content = response.data.decode('utf-8')\n        self.assertEqual(greeting, content)\n\n    def test_post_binary(self):\n        response = self.app.post('/post',\n                                 data=b'\\x01\\x02\\x03\\x81\\x82\\x83',\n                                 content_type='application/octet-stream')\n        self.assertEqual(response.status_code, 200)\n\n    def test_post_body_text(self):\n        with open('httpbin/core.py') as f:\n            response = self.app.post('/post', data={\"file\": f.read()})\n        self.assertEqual(response.status_code, 200)\n\n    def test_post_body_binary(self):\n        response = self.app.post(\n            '/post',\n            data={\"file\": b'\\x01\\x02\\x03\\x81\\x82\\x83'})\n        self.assertEqual(response.status_code, 200)\n\n    def test_post_body_unicode(self):\n        response = self.app.post('/post', data=u'оживлённым'.encode('utf-8'))\n        self.assertEqual(json.loads(response.data.decode('utf-8'))['data'], u'оживлённым')\n\n    def test_post_file_with_missing_content_type_header(self):\n        # I built up the form data manually here because I couldn't find a way\n        # to convince the werkzeug test client to send files without the\n        # content-type of the file set.\n        data = '--bound\\r\\nContent-Disposition: form-data; name=\"media\"; '\n        data += 'filename=\"test.bin\"\\r\\n\\r\\n\\xa5\\xc6\\n--bound--\\r\\n'\n        response = self.app.post(\n            '/post',\n            content_type='multipart/form-data; boundary=bound',\n            data=data,\n        )\n        self.assertEqual(response.status_code, 200)\n\n    \"\"\"\n    This is currently a sort of negative-test.\n    We validate that when running Flask-only server that\n    Transfer-Encoding: chunked requests are unsupported and\n    we return 501 Not Implemented\n    \"\"\"\n    def test_post_chunked(self):\n        data = '{\"animal\":\"dog\"}'\n        response = self.app.post(\n            '/post',\n            content_type='application/json',\n            headers=[('Transfer-Encoding', 'chunked')],\n            data=data,\n        )\n        self.assertEqual(response.status_code, 501)\n        #self.assertEqual(response.status_code, 200)\n        #self.assertEqual(json.loads(response.data.decode('utf-8'))['data'], '{\"animal\":\"dog\"}')\n        #self.assertEqual(json.loads(response.data.decode('utf-8'))['json'], {\"animal\": \"dog\"})\n\n    def test_set_cors_headers_after_request(self):\n        response = self.app.get('/get')\n        self.assertEqual(\n            response.headers.get('Access-Control-Allow-Origin'), '*'\n        )\n\n    def test_set_cors_credentials_headers_after_auth_request(self):\n        response = self.app.get('/basic-auth/foo/bar')\n        self.assertEqual(\n            response.headers.get('Access-Control-Allow-Credentials'), 'true'\n        )\n\n    def test_set_cors_headers_after_request_with_request_origin(self):\n        response = self.app.get('/get', headers={'Origin': 'origin'})\n        self.assertEqual(\n            response.headers.get('Access-Control-Allow-Origin'), 'origin'\n        )\n\n    def test_set_cors_headers_with_options_verb(self):\n        response = self.app.open('/get', method='OPTIONS')\n        self.assertEqual(\n            response.headers.get('Access-Control-Allow-Origin'), '*'\n        )\n        self.assertEqual(\n            response.headers.get('Access-Control-Allow-Credentials'), 'true'\n        )\n        self.assertEqual(\n            response.headers.get('Access-Control-Allow-Methods'),\n            'GET, POST, PUT, DELETE, PATCH, OPTIONS'\n        )\n        self.assertEqual(\n            response.headers.get('Access-Control-Max-Age'), '3600'\n        )\n        # FIXME should we add any extra headers?\n        self.assertNotIn(\n            'Access-Control-Allow-Headers', response.headers\n        )\n    def test_set_cors_allow_headers(self):\n        response = self.app.open('/get', method='OPTIONS', headers={'Access-Control-Request-Headers': 'X-Test-Header'})\n        self.assertEqual(\n            response.headers.get('Access-Control-Allow-Headers'), 'X-Test-Header'\n        )\n\n    def test_headers(self):\n        headers = {\n            \"Accept\": \"*/*\",\n            \"Host\": \"localhost:1234\",\n            \"User-Agent\": \"curl/7.54.0\",\n            \"Via\": \"bar\"\n        }\n        response = self.app.get('/headers', headers=headers)\n        self.assertEqual(response.status_code, 200)\n        self.assertTrue({'Accept', 'Host', 'User-Agent'}.issubset(set(response.json['headers'].keys())))\n        self.assertNotIn('Via', response.json)\n\n    def test_headers_show_env(self):\n        headers = {\n            \"Accept\": \"*/*\",\n            \"Host\": \"localhost:1234\",\n            \"User-Agent\": \"curl/7.54.0\",\n            \"Via\": \"bar\"\n        }\n        response = self.app.get('/headers?show_env=true', headers=headers)\n        self.assertEqual(response.status_code, 200)\n        self.assertTrue({'Accept', 'Host', 'User-Agent', 'Via'}.issubset(set(response.json['headers'].keys())))\n\n    def test_user_agent(self):\n        response = self.app.get(\n            '/user-agent', headers={'User-Agent': 'test'}\n        )\n        self.assertIn('test', response.data.decode('utf-8'))\n        self.assertEqual(response.status_code, 200)\n\n    def test_gzip(self):\n        response = self.app.get('/gzip')\n        self.assertEqual(response.status_code, 200)\n\n    def test_brotli(self):\n        response = self.app.get('/brotli')\n        self.assertEqual(response.status_code, 200)\n\n    def test_bearer_auth(self):\n        token = 'abcd1234'\n        response = self.app.get(\n            '/bearer',\n            headers={'Authorization': 'Bearer ' + token}\n        )\n        self.assertEqual(response.status_code, 200)\n        assert json.loads(response.data.decode('utf-8'))['token'] == token\n\n    def test_bearer_auth_with_wrong_authorization_type(self):\n        \"\"\"Sending an non-Bearer Authorization header to /bearer should return a 401\"\"\"\n        auth_headers = (\n            ('Authorization', 'Basic 1234abcd'),\n            ('Authorization', ''),\n            ('',  '')\n        )\n        for header in auth_headers:\n            response = self.app.get(\n                '/bearer',\n                headers={header[0]: header[1]}\n            )\n            self.assertEqual(response.status_code, 401)\n\n    def test_bearer_auth_with_missing_token(self):\n        \"\"\"Sending an 'Authorization: Bearer' header with no token to /bearer should return a 401\"\"\"\n        response = self.app.get(\n            '/bearer',\n            headers={'Authorization': 'Bearer'}\n        )\n        self.assertEqual(response.status_code, 401)\n\n    def test_digest_auth_with_wrong_password(self):\n        auth_header = 'Digest username=\"user\",realm=\"wrong\",nonce=\"wrong\",uri=\"/digest-auth/user/passwd/MD5\",response=\"wrong\",opaque=\"wrong\"'\n        response = self.app.get(\n            '/digest-auth/auth/user/passwd/MD5',\n            environ_base={\n                # httpbin's digest auth implementation uses the remote addr to\n                # build the nonce\n                'REMOTE_ADDR': '127.0.0.1',\n            },\n            headers={\n                'Authorization': auth_header,\n            }\n        )\n        self.assertTrue('Digest' in response.headers.get('WWW-Authenticate'))\n        self.assertEqual(response.status_code, 401)\n\n    def test_digest_auth(self):\n        \"\"\"Test different combinations of digest auth parameters\"\"\"\n        username = 'user'\n        password = 'passwd'\n        for qop in None, 'auth', 'auth-int',:\n            for algorithm in None, 'MD5', 'SHA-256', 'SHA-512':\n                for body in None, b'', b'request payload':\n                    for stale_after in (None, 1, 4) if algorithm else (None,) :\n                        self._test_digest_auth(username, password, qop, algorithm, body, stale_after)\n\n    def test_digest_auth_with_wrong_authorization_type(self):\n        \"\"\"Sending an non-digest Authorization header to /digest-auth should return a 401\"\"\"\n        auth_headers = (\n            ('Authorization', 'Basic 1234abcd'),\n            ('Authorization', ''),\n            ('',  '')\n        )\n        for header in auth_headers:\n            response = self.app.get(\n                '/digest-auth/auth/myname/mysecret',\n                headers={header[0]: header[1]}\n            )\n            self.assertEqual(response.status_code, 401)\n\n    def _test_digest_auth(self, username, password, qop, algorithm=None, body=None, stale_after=None):\n        uri = self._digest_auth_create_uri(username, password, qop, algorithm, stale_after)\n\n        unauthorized_response = self._test_digest_auth_first_challenge(uri)\n\n        header = unauthorized_response.headers.get('WWW-Authenticate')\n\n        authorized_response, nonce = self._test_digest_response_for_auth_request(header, username, password, qop, uri, body)\n        self.assertEqual(authorized_response.status_code, 200)\n\n        if None == stale_after :\n            return\n\n        # test stale after scenerio\n        self._digest_auth_stale_after_check(header, username, password, uri, body, qop, stale_after)\n\n    def _test_digest_auth_first_challenge(self, uri):\n        unauthorized_response = self.app.get(\n            uri,\n            environ_base={\n                # digest auth uses the remote addr to build the nonce\n                'REMOTE_ADDR': '127.0.0.1',\n            }\n        )\n        # make sure it returns a 401\n        self.assertEqual(unauthorized_response.status_code, 401)\n        return unauthorized_response\n\n    def _digest_auth_create_uri(self, username, password, qop, algorithm, stale_after):\n        uri = '/digest-auth/{0}/{1}/{2}'.format(qop or 'wrong-qop', username, password)\n        if algorithm:\n            uri += '/' + algorithm\n        if stale_after:\n            uri += '/{0}'.format(stale_after)\n        return uri\n\n    def _digest_auth_stale_after_check(self, header, username, password, uri, body, qop, stale_after):\n        for nc in range(2, stale_after + 1):\n            authorized_response, nonce = self._test_digest_response_for_auth_request(header, username, password, qop, uri, \\\n                                                                              body, nc)\n            self.assertEqual(authorized_response.status_code, 200)\n        stale_response, nonce = self._test_digest_response_for_auth_request(header, username, password, qop, uri, \\\n                                                                     body, stale_after + 1)\n        self.assertEqual(stale_response.status_code, 401)\n        header = stale_response.headers.get('WWW-Authenticate')\n        self.assertIn('stale=TRUE', header)\n\n    def _test_digest_response_for_auth_request(self, header, username, password, qop, uri, body, nc=1, nonce=None):\n        auth_type, auth_info = header.split(None, 1)\n        self.assertEqual(auth_type, 'Digest')\n\n        d = parse_dict_header(auth_info)\n\n        nonce = nonce or d['nonce']\n        realm = d['realm']\n        opaque = d['opaque']\n        if qop :\n            self.assertIn(qop, [x.strip() for x in d['qop'].split(',')], 'Challenge should contains expected qop')\n        algorithm = d['algorithm']\n\n        cnonce, nc = (_hash(os.urandom(10), \"MD5\"), '{:08}'.format(nc)) if qop in ('auth', 'auth-int') else (None, None)\n\n        auth_header = _make_digest_auth_header(\n            username, password, 'GET', uri, nonce, realm, opaque, algorithm, qop, cnonce, nc, body)\n\n        # make second request\n        return self.app.get(\n            uri,\n            environ_base={\n                # httpbin's digest auth implementation uses the remote addr to\n                # build the nonce\n                'REMOTE_ADDR': '127.0.0.1',\n            },\n            headers={\n                'Authorization': auth_header,\n            },\n            data=body\n        ), nonce\n\n    def test_digest_auth_wrong_pass(self):\n        \"\"\"Test different combinations of digest auth parameters\"\"\"\n        username = 'user'\n        password = 'passwd'\n        for qop in None, 'auth', 'auth-int',:\n            for algorithm in None, 'MD5', 'SHA-256', 'SHA-512':\n                for body in None, b'', b'request payload':\n                    self._test_digest_auth_wrong_pass(username, password, qop, algorithm, body, 3)\n\n    def _test_digest_auth_wrong_pass(self, username, password, qop, algorithm=None, body=None, stale_after=None):\n        uri = self._digest_auth_create_uri(username, password, qop, algorithm, stale_after)\n        unauthorized_response = self._test_digest_auth_first_challenge(uri)\n\n        header = unauthorized_response.headers.get('WWW-Authenticate')\n\n        wrong_pass_response, nonce = self._test_digest_response_for_auth_request(header, username, \"wrongPassword\", qop, uri, body)\n        self.assertEqual(wrong_pass_response.status_code, 401)\n        header = wrong_pass_response.headers.get('WWW-Authenticate')\n        self.assertNotIn('stale=TRUE', header)\n\n        reused_nonce_response, nonce =  self._test_digest_response_for_auth_request(header, username, password, qop, uri, \\\n                                                                              body, nonce=nonce)\n        self.assertEqual(reused_nonce_response.status_code, 401)\n        header = reused_nonce_response.headers.get('WWW-Authenticate')\n        self.assertIn('stale=TRUE', header)\n\n    def test_drip(self):\n        response = self.app.get('/drip?numbytes=400&duration=2&delay=1')\n        self.assertEqual(response.content_length, 400)\n        self.assertEqual(len(self.get_data(response)), 400)\n        self.assertEqual(response.status_code, 200)\n\n    def test_drip_with_invalid_numbytes(self):\n        for bad_num in -1, 0:\n            uri = '/drip?numbytes={0}&duration=2&delay=1'.format(bad_num)\n            response = self.app.get(uri)\n            self.assertEqual(response.status_code, 400)\n\n    def test_drip_with_custom_code(self):\n        response = self.app.get('/drip?numbytes=400&duration=2&code=500')\n        self.assertEqual(response.content_length, 400)\n        self.assertEqual(len(self.get_data(response)), 400)\n        self.assertEqual(response.status_code, 500)\n\n    def test_get_bytes(self):\n        response = self.app.get('/bytes/1024')\n        self.assertEqual(len(self.get_data(response)), 1024)\n        self.assertEqual(response.status_code, 200)\n\n    def test_bytes_with_seed(self):\n        response = self.app.get('/bytes/10?seed=0')\n        # The RNG changed in python3, so even though we are\n        # setting the seed, we can't expect the value to be the\n        # same across both interpreters.\n        if six.PY3:\n            self.assertEqual(\n                response.data, b'\\xc5\\xd7\\x14\\x84\\xf8\\xcf\\x9b\\xf4\\xb7o'\n            )\n        else:\n            self.assertEqual(\n                response.data, b'\\xd8\\xc2kB\\x82g\\xc8Mz\\x95'\n            )\n\n    def test_stream_bytes(self):\n        response = self.app.get('/stream-bytes/1024')\n        self.assertEqual(len(self.get_data(response)), 1024)\n        self.assertEqual(response.status_code, 200)\n\n    def test_stream_bytes_with_seed(self):\n        response = self.app.get('/stream-bytes/10?seed=0')\n        # The RNG changed in python3, so even though we are\n        # setting the seed, we can't expect the value to be the\n        # same across both interpreters.\n        if six.PY3:\n            self.assertEqual(\n                response.data, b'\\xc5\\xd7\\x14\\x84\\xf8\\xcf\\x9b\\xf4\\xb7o'\n            )\n        else:\n            self.assertEqual(\n                response.data, b'\\xd8\\xc2kB\\x82g\\xc8Mz\\x95'\n            )\n\n    def test_delete_endpoint_returns_body(self):\n        response = self.app.delete(\n            '/delete',\n            data={'name': 'kevin'},\n            content_type='application/x-www-form-urlencoded'\n        )\n        form_data = json.loads(response.data.decode('utf-8'))['form']\n        self.assertEqual(form_data, {'name': 'kevin'})\n\n    def test_methods__to_status_endpoint(self):\n        methods = [\n            'GET',\n            'HEAD',\n            'POST',\n            'PUT',\n            'DELETE',\n            'PATCH',\n            'TRACE',\n        ]\n        for m in methods:\n            response = self.app.open(path='/status/418', method=m)\n            self.assertEqual(response.status_code, 418)\n\n    def test_status_endpoint_invalid_code(self):\n        response = self.app.get(path='/status/4!9')\n        self.assertEqual(response.status_code, 400)\n\n    def test_status_endpoint_invalid_codes(self):\n        response = self.app.get(path='/status/200,402,foo')\n        self.assertEqual(response.status_code, 400)\n\n    def test_xml_endpoint(self):\n        response = self.app.get(path='/xml')\n        self.assertEqual(\n            response.headers.get('Content-Type'), 'application/xml'\n        )\n\n    def test_x_forwarded_proto(self):\n        response = self.app.get(path='/get', headers={\n            'X-Forwarded-Proto':'https'\n        })\n        assert json.loads(response.data.decode('utf-8'))['url'].startswith('https://')\n\n    def test_redirect_n_higher_than_1(self):\n        response = self.app.get('/redirect/5')\n        self.assertEqual(\n            response.headers.get('Location'), '/relative-redirect/4'\n        )\n\n    def test_redirect_to_post(self):\n        response = self.app.post('/redirect-to?url=/post&status_code=307',\n                                 data=b'\\x01\\x02\\x03\\x81\\x82\\x83',\n                                 content_type='application/octet-stream')\n        self.assertEqual(response.status_code, 307)\n        self.assertEqual(\n            response.headers.get('Location'), '/post'\n        )\n\n    def test_redirect_absolute_param_n_higher_than_1(self):\n        response = self.app.get('/redirect/5?absolute=true')\n        self.assertEqual(\n            response.headers.get('Location'), 'http://localhost/absolute-redirect/4'\n        )\n\n    def test_redirect_n_equals_to_1(self):\n        response = self.app.get('/redirect/1')\n        self.assertEqual(response.status_code, 302)\n        self.assertEqual(\n            response.headers.get('Location'), '/get'\n        )\n\n    def test_relative_redirect_n_equals_to_1(self):\n        response = self.app.get('/relative-redirect/1')\n        self.assertEqual(\n            response.headers.get('Location'), '/get'\n        )\n\n    def test_relative_redirect_n_higher_than_1(self):\n        response = self.app.get('/relative-redirect/7')\n        self.assertEqual(response.status_code, 302)\n        self.assertEqual(\n            response.headers.get('Location'), '/relative-redirect/6'\n        )\n\n    def test_absolute_redirect_n_higher_than_1(self):\n        response = self.app.get('/absolute-redirect/5')\n        self.assertEqual(\n            response.headers.get('Location'), 'http://localhost/absolute-redirect/4'\n        )\n\n    def test_absolute_redirect_n_equals_to_1(self):\n        response = self.app.get('/absolute-redirect/1')\n        self.assertEqual(response.status_code, 302)\n        self.assertEqual(\n            response.headers.get('Location'), 'http://localhost/get'\n        )\n\n    def test_request_range(self):\n        response1 = self.app.get('/range/1234')\n        self.assertEqual(response1.status_code, 200)\n        self.assertEqual(response1.headers.get('ETag'), 'range1234')\n        self.assertEqual(response1.headers.get('Content-range'), 'bytes 0-1233/1234')\n        self.assertEqual(response1.headers.get('Accept-ranges'), 'bytes')\n        self.assertEqual(len(self.get_data(response1)), 1234)\n\n        response2 = self.app.get('/range/1234')\n        self.assertEqual(response2.status_code, 200)\n        self.assertEqual(response2.headers.get('ETag'), 'range1234')\n        self.assertEqual(self.get_data(response1), self.get_data(response2))\n\n    def test_request_range_with_parameters(self):\n        response = self.app.get(\n            '/range/100?duration=1.5&chunk_size=5',\n            headers={ 'Range': 'bytes=10-24' }\n        )\n\n        self.assertEqual(response.status_code, 206)\n        self.assertEqual(response.headers.get('ETag'), 'range100')\n        self.assertEqual(response.headers.get('Content-range'), 'bytes 10-24/100')\n        self.assertEqual(response.headers.get('Accept-ranges'), 'bytes')\n        self.assertEqual(response.headers.get('Content-Length'), '15')\n        self.assertEqual(self.get_data(response), 'klmnopqrstuvwxy'.encode('utf8'))\n\n    def test_request_range_first_15_bytes(self):\n        response = self.app.get(\n            '/range/1000',\n            headers={ 'Range': 'bytes=0-15' }\n        )\n\n        self.assertEqual(response.status_code, 206)\n        self.assertEqual(response.headers.get('ETag'), 'range1000')\n        self.assertEqual(self.get_data(response), 'abcdefghijklmnop'.encode('utf8'))\n        self.assertEqual(response.headers.get('Content-range'), 'bytes 0-15/1000')\n\n    def test_request_range_open_ended_last_6_bytes(self):\n        response = self.app.get(\n            '/range/26',\n            headers={ 'Range': 'bytes=20-' }\n        )\n\n        self.assertEqual(response.status_code, 206)\n        self.assertEqual(response.headers.get('ETag'), 'range26')\n        self.assertEqual(self.get_data(response), 'uvwxyz'.encode('utf8'))\n        self.assertEqual(response.headers.get('Content-range'), 'bytes 20-25/26')\n        self.assertEqual(response.headers.get('Content-Length'), '6')\n\n    def test_request_range_suffix(self):\n        response = self.app.get(\n            '/range/26',\n            headers={ 'Range': 'bytes=-5' }\n        )\n\n        self.assertEqual(response.status_code, 206)\n        self.assertEqual(response.headers.get('ETag'), 'range26')\n        self.assertEqual(self.get_data(response), 'vwxyz'.encode('utf8'))\n        self.assertEqual(response.headers.get('Content-range'), 'bytes 21-25/26')\n        self.assertEqual(response.headers.get('Content-Length'), '5')\n\n    def test_request_out_of_bounds(self):\n        response = self.app.get(\n            '/range/26',\n            headers={ 'Range': 'bytes=10-5',\n            }\n        )\n\n        self.assertEqual(response.status_code, 416)\n        self.assertEqual(response.headers.get('ETag'), 'range26')\n        self.assertEqual(len(self.get_data(response)), 0)\n        self.assertEqual(response.headers.get('Content-range'), 'bytes */26')\n        self.assertEqual(response.headers.get('Content-Length'), '0')\n\n        response = self.app.get(\n            '/range/26',\n            headers={ 'Range': 'bytes=32-40',\n            }\n        )\n\n        self.assertEqual(response.status_code, 416)\n        response = self.app.get(\n            '/range/26',\n            headers={ 'Range': 'bytes=0-40',\n            }\n        )\n        self.assertEqual(response.status_code, 416)\n\n    def test_etag_if_none_match_matches(self):\n        response = self.app.get(\n            '/etag/abc',\n            headers={ 'If-None-Match': 'abc' }\n        )\n        self.assertEqual(response.status_code, 304)\n        self.assertEqual(response.headers.get('ETag'), 'abc')\n\n    def test_etag_if_none_match_matches_list(self):\n        response = self.app.get(\n            '/etag/abc',\n            headers={ 'If-None-Match': '\"123\", \"abc\"' }\n        )\n        self.assertEqual(response.status_code, 304)\n        self.assertEqual(response.headers.get('ETag'), 'abc')\n\n    def test_etag_if_none_match_matches_star(self):\n        response = self.app.get(\n            '/etag/abc',\n            headers={ 'If-None-Match': '*' }\n        )\n        self.assertEqual(response.status_code, 304)\n        self.assertEqual(response.headers.get('ETag'), 'abc')\n\n    def test_etag_if_none_match_w_prefix(self):\n        response = self.app.get(\n            '/etag/c3piozzzz',\n            headers={ 'If-None-Match': 'W/\"xyzzy\", W/\"r2d2xxxx\", W/\"c3piozzzz\"' }\n        )\n        self.assertEqual(response.status_code, 304)\n        self.assertEqual(response.headers.get('ETag'), 'c3piozzzz')\n\n    def test_etag_if_none_match_has_no_match(self):\n        response = self.app.get(\n            '/etag/abc',\n            headers={ 'If-None-Match': '123' }\n        )\n        self.assertEqual(response.status_code, 200)\n        self.assertEqual(response.headers.get('ETag'), 'abc')\n\n    def test_etag_if_match_matches(self):\n        response = self.app.get(\n            '/etag/abc',\n            headers={ 'If-Match': 'abc' }\n        )\n        self.assertEqual(response.status_code, 200)\n        self.assertEqual(response.headers.get('ETag'), 'abc')\n\n    def test_etag_if_match_matches_list(self):\n        response = self.app.get(\n            '/etag/abc',\n            headers={ 'If-Match': '\"123\", \"abc\"' }\n        )\n        self.assertEqual(response.status_code, 200)\n        self.assertEqual(response.headers.get('ETag'), 'abc')\n\n    def test_etag_if_match_matches_star(self):\n        response = self.app.get(\n            '/etag/abc',\n            headers={ 'If-Match': '*' }\n        )\n        self.assertEqual(response.status_code, 200)\n        self.assertEqual(response.headers.get('ETag'), 'abc')\n\n    def test_etag_if_match_has_no_match(self):\n        response = self.app.get(\n            '/etag/abc',\n            headers={ 'If-Match': '123' }\n        )\n        self.assertEqual(response.status_code, 412)\n        self.assertNotIn('ETag', response.headers)\n\n    def test_etag_with_no_headers(self):\n        response = self.app.get(\n            '/etag/abc'\n        )\n        self.assertEqual(response.status_code, 200)\n        self.assertEqual(response.headers.get('ETag'), 'abc')\n\n    def test_parse_multi_value_header(self):\n        self.assertEqual(parse_multi_value_header('xyzzy'), [ \"xyzzy\" ])\n        self.assertEqual(parse_multi_value_header('\"xyzzy\"'), [ \"xyzzy\" ])\n        self.assertEqual(parse_multi_value_header('W/\"xyzzy\"'), [ \"xyzzy\" ])\n        self.assertEqual(parse_multi_value_header('\"xyzzy\", \"r2d2xxxx\", \"c3piozzzz\"'), [ \"xyzzy\", \"r2d2xxxx\", \"c3piozzzz\" ])\n        self.assertEqual(parse_multi_value_header('W/\"xyzzy\", W/\"r2d2xxxx\", W/\"c3piozzzz\"'), [ \"xyzzy\", \"r2d2xxxx\", \"c3piozzzz\" ])\n        self.assertEqual(parse_multi_value_header('*'), [ \"*\" ])\n\nif __name__ == '__main__':\n    unittest.main()\n", "metadata": {"source": "github.com/postmanlabs/httpbin", "file_path": "test_httpbin.py", "language": "python", "complexity": "complex", "star_count": 13214, "quality_score": 1.0, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": true, "has_type_hints": true, "has_testing": true, "has_logging": false, "function_count": 79, "class_count": 1, "line_count": 817}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create REST API endpoint with proper request/response handling using Flask/FastAPI decorators\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import logging\nfrom flask import Flask\n\napp = Flask(__name__)\nlogging.basicConfig()\nlogger = logging.getLogger(__name__)\nlogger.setLevel(logging.DEBUG)\n\n@app.route('/', methods=['GET', 'POST'])\ndef lambda_handler(event=None, context=None):\n    logger.info('Lambda function invoked index()')\n\n    return 'Flask says Hello!!'\n\nif __name__ == '__main__':\n    app.run(debug=True)\n", "metadata": {"source": "github.com/Miserlou/Zappa", "file_path": "example/app.py", "language": "python", "complexity": "simple", "star_count": 11883, "quality_score": 0.9500000000000001, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": false, "has_testing": false, "has_logging": true, "function_count": 1, "class_count": 0, "line_count": 17}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create REST API endpoint with proper request/response handling\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"\nCopyright 2015-2016 Amazon.com, Inc. or its affiliates. All Rights Reserved.\nLicensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at\n     http://aws.amazon.com/apache2.0/\nor in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.\n\"\"\"\nimport re\nimport time\nimport pprint\nimport json\n\n\ndef lambda_handler(event, context):\n    print(\"Client token: \" + event['authorizationToken'])\n    print(\"Method ARN: \" + event['methodArn'])\n    \"\"\"validate the incoming token\"\"\"\n    \"\"\"and produce the principal user identifier associated with the token\"\"\"\n\n    \"\"\"this could be accomplished in a number of ways:\"\"\"\n    \"\"\"1. Call out to OAuth provider\"\"\"\n    \"\"\"2. Decode a JWT token inline\"\"\"\n    \"\"\"3. Lookup in a self-managed DB\"\"\"\n    principalId = \"user|a1b2c3d4\"\n\n    \"\"\"you can send a 401 Unauthorized response to the client by failing like so:\"\"\"\n    \"\"\"raise Exception('Unauthorized')\"\"\"\n\n    \"\"\"if the token is valid, a policy must be generated which will allow or deny access to the client\"\"\"\n\n    \"\"\"if access is denied, the client will receive a 403 Access Denied response\"\"\"\n    \"\"\"if access is allowed, API Gateway will proceed with the backend integration configured on the method that was called\"\"\"\n\n    \"\"\"this function must generate a policy that is associated with the recognized principal user identifier.\"\"\"\n    \"\"\"depending on your use case, you might store policies in a DB, or generate them on the fly\"\"\"\n\n    \"\"\"keep in mind, the policy is cached for 5 minutes by default (TTL is configurable in the authorizer)\"\"\"\n    \"\"\"and will apply to subsequent calls to any method/resource in the RestApi\"\"\"\n    \"\"\"made with the same token\"\"\"\n\n    \"\"\"the example policy below denies access to all resources in the RestApi\"\"\"\n    tmp = event['methodArn'].split(':')\n    apiGatewayArnTmp = tmp[5].split('/')\n    awsAccountId = tmp[4]\n\n    policy = AuthPolicy(principalId, awsAccountId)\n    policy.restApiId = apiGatewayArnTmp[0]\n    policy.region = tmp[3]\n    policy.stage = apiGatewayArnTmp[1]\n\n    # Blueprint denies all methods by default\n    # policy.denyAllMethods()\n\n    # Example allows all methods\n    policy.allowAllMethods()\n\n    \"\"\"policy.allowMethod(HttpVerb.GET, \"/pets/*\")\"\"\"\n\n    \"\"\"finally, build the policy and exit the function using return\"\"\"\n    return policy.build()\n\nclass HttpVerb:\n    GET     = \"GET\"\n    POST    = \"POST\"\n    PUT     = \"PUT\"\n    PATCH   = \"PATCH\"\n    HEAD    = \"HEAD\"\n    DELETE  = \"DELETE\"\n    OPTIONS = \"OPTIONS\"\n    ALL     = \"*\"\n\nclass AuthPolicy:\n    awsAccountId = \"\"\n    \"\"\"The AWS account id the policy will be generated for. This is used to create the method ARNs.\"\"\"\n    principalId = \"\"\n    \"\"\"The principal used for the policy, this should be a unique identifier for the end user.\"\"\"\n    version = \"2012-10-17\"\n    \"\"\"The policy version used for the evaluation. This should always be '2012-10-17'\"\"\"\n    pathRegex = r\"^[/.a-zA-Z0-9-\\*]+$\"\n    \"\"\"The regular expression used to validate resource paths for the policy\"\"\"\n\n    \"\"\"these are the internal lists of allowed and denied methods. These are lists\n    of objects and each object has 2 properties: A resource ARN and a nullable\n    conditions statement.\n    the build method processes these lists and generates the appropriate\n    statements for the final policy\"\"\"\n    allowMethods = []\n    denyMethods = []\n\n    restApiId = \"*\"\n    \"\"\"The API Gateway API id. By default this is set to '*'\"\"\"\n    region = \"*\"\n    \"\"\"The region where the API is deployed. By default this is set to '*'\"\"\"\n    stage = \"*\"\n    \"\"\"The name of the stage used in the policy. By default this is set to '*'\"\"\"\n\n    def __init__(self, principal, awsAccountId):\n        self.awsAccountId = awsAccountId\n        self.principalId = principal\n        self.allowMethods = []\n        self.denyMethods = []\n\n    def _addMethod(self, effect, verb, resource, conditions):\n        \"\"\"Adds a method to the internal lists of allowed or denied methods. Each object in\n        the internal list contains a resource ARN and a condition statement. The condition\n        statement can be null.\"\"\"\n        if verb != \"*\" and not hasattr(HttpVerb, verb):\n            raise NameError(\"Invalid HTTP verb \" + verb + \". Allowed verbs in HttpVerb class\")\n        resourcePattern = re.compile(self.pathRegex)\n        if not resourcePattern.match(resource):\n            raise NameError(\"Invalid resource path: \" + resource + \". Path should match \" + self.pathRegex)\n\n        if resource[:1] == \"/\":\n            resource = resource[1:]\n\n        resourceArn = (\"arn:aws:execute-api:\" +\n            self.region + \":\" +\n            self.awsAccountId + \":\" +\n            self.restApiId + \"/\" +\n            self.stage + \"/\" +\n            verb + \"/\" +\n            resource)\n\n        if effect.lower() == \"allow\":\n            self.allowMethods.append({\n                'resourceArn' : resourceArn,\n                'conditions' : conditions\n            })\n        elif effect.lower() == \"deny\":\n            self.denyMethods.append({\n                'resourceArn' : resourceArn,\n                'conditions' : conditions\n            })\n\n    def _getEmptyStatement(self, effect):\n        \"\"\"Returns an empty statement object prepopulated with the correct action and the\n        desired effect.\"\"\"\n        statement = {\n            'Action': 'execute-api:Invoke',\n            'Effect': effect[:1].upper() + effect[1:].lower(),\n            'Resource': []\n        }\n\n        return statement\n\n    def _getStatementForEffect(self, effect, methods):\n        \"\"\"This function loops over an array of objects containing a resourceArn and\n        conditions statement and generates the array of statements for the policy.\"\"\"\n        statements = []\n\n        if len(methods) > 0:\n            statement = self._getEmptyStatement(effect)\n\n            for curMethod in methods:\n                if curMethod['conditions'] is None or len(curMethod['conditions']) == 0:\n                    statement['Resource'].append(curMethod['resourceArn'])\n                else:\n                    conditionalStatement = self._getEmptyStatement(effect)\n                    conditionalStatement['Resource'].append(curMethod['resourceArn'])\n                    conditionalStatement['Condition'] = curMethod['conditions']\n                    statements.append(conditionalStatement)\n\n            statements.append(statement)\n\n        return statements\n\n    def allowAllMethods(self):\n        \"\"\"Adds a '*' allow to the policy to authorize access to all methods of an API\"\"\"\n        self._addMethod(\"Allow\", HttpVerb.ALL, \"*\", [])\n\n    def denyAllMethods(self):\n        \"\"\"Adds a '*' allow to the policy to deny access to all methods of an API\"\"\"\n        self._addMethod(\"Deny\", HttpVerb.ALL, \"*\", [])\n\n    def allowMethod(self, verb, resource):\n        \"\"\"Adds an API Gateway method (Http verb + Resource path) to the list of allowed\n        methods for the policy\"\"\"\n        self._addMethod(\"Allow\", verb, resource, [])\n\n    def denyMethod(self, verb, resource):\n        \"\"\"Adds an API Gateway method (Http verb + Resource path) to the list of denied\n        methods for the policy\"\"\"\n        self._addMethod(\"Deny\", verb, resource, [])\n\n    def allowMethodWithConditions(self, verb, resource, conditions):\n        \"\"\"Adds an API Gateway method (Http verb + Resource path) to the list of allowed\n        methods and includes a condition for the policy statement. More on AWS policy\n        conditions here: http://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_elements.html#Condition\"\"\"\n        self._addMethod(\"Allow\", verb, resource, conditions)\n\n    def denyMethodWithConditions(self, verb, resource, conditions):\n        \"\"\"Adds an API Gateway method (Http verb + Resource path) to the list of denied\n        methods and includes a condition for the policy statement. More on AWS policy\n        conditions here: http://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_elements.html#Condition\"\"\"\n        self._addMethod(\"Deny\", verb, resource, conditions)\n\n    def build(self):\n        \"\"\"Generates the policy document based on the internal lists of allowed and denied\n        conditions. This will generate a policy with two main statements for the effect:\n        one statement for Allow and one statement for Deny.\n        Methods that includes conditions will have their own statement in the policy.\"\"\"\n        if ((self.allowMethods is None or len(self.allowMethods) == 0) and\n            (self.denyMethods is None or len(self.denyMethods) == 0)):\n            raise NameError(\"No statements defined for the policy\")\n\n        policy = {\n            'principalId' : self.principalId,\n            'policyDocument' : {\n                'Version' : self.version,\n                'Statement' : []\n            }\n        }\n\n        policy['policyDocument']['Statement'].extend(self._getStatementForEffect(\"Allow\", self.allowMethods))\n        policy['policyDocument']['Statement'].extend(self._getStatementForEffect(\"Deny\", self.denyMethods))\n\n        return policy\n", "metadata": {"source": "github.com/Miserlou/Zappa", "file_path": "example/authmodule.py", "language": "python", "complexity": "complex", "star_count": 11883, "quality_score": 0.9500000000000001, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": true, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 12, "class_count": 2, "line_count": 217}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create REST API endpoint with proper request/response handling\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "def myfunc():\n    print('Running my function in a schedule!')\n\n\ndef myfunc_with_events(event, context):\n    print('Event time was', event['time'])\n    print('This log is', context.log_group_name, context.log_stream_name)\n    print('Time left for execution:', context.get_remaining_time_in_millis())\n", "metadata": {"source": "github.com/Miserlou/Zappa", "file_path": "example/mymodule.py", "language": "python", "complexity": "simple", "star_count": 11883, "quality_score": 0.6300000000000001, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": false, "has_error_handling": false, "has_documentation": false, "has_type_hints": false, "has_testing": false, "has_logging": true, "function_count": 2, "class_count": 0, "line_count": 9}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate module integration for Create REST API endpoint with proper request/response handling\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import os\nimport sys\nfrom setuptools import setup\nfrom io import open\nfrom zappa import __version__\n\nwith open('README.md') as readme_file:\n    long_description = readme_file.read()\n\nwith open(os.path.join(os.path.dirname(__file__), 'requirements.in')) as f:\n    required = f.read().splitlines()\n\nwith open(os.path.join(os.path.dirname(__file__), 'test_requirements.in')) as f:\n    test_required = f.read().splitlines()\n\nsetup(\n    name='zappa',\n    version=__version__,\n    packages=['zappa'],\n    install_requires=required,\n    tests_require=test_required,\n    test_suite='nose.collector',\n    include_package_data=True,\n    license='MIT License',\n    description='Server-less Python Web Services for AWS Lambda and API Gateway',\n    long_description=long_description,\n    long_description_content_type='text/markdown',\n    url='https://github.com/Miserlou/Zappa',\n    author='Rich Jones',\n    author_email='rich@openwatch.net',\n    entry_points={\n        'console_scripts': [\n            'zappa=zappa.cli:handle',\n            'z=zappa.cli:handle',\n        ]\n    },\n    classifiers=[\n        'Environment :: Console',\n        'License :: OSI Approved :: Apache Software License',\n        'Operating System :: OS Independent',\n        'Programming Language :: Python',\n        'Programming Language :: Python :: 3.6',\n        'Programming Language :: Python :: 3.7',\n        'Programming Language :: Python :: 3.8',\n        'Framework :: Django',\n        'Framework :: Django :: 1.11',\n        'Framework :: Django :: 2.0',\n        'Framework :: Django :: 3.0',\n        'Topic :: Internet :: WWW/HTTP',\n        'Topic :: Internet :: WWW/HTTP :: Dynamic Content',\n    ],\n)\n", "metadata": {"source": "github.com/Miserlou/Zappa", "file_path": "setup.py", "language": "python", "complexity": "simple", "star_count": 11883, "quality_score": 0.89, "features": {"has_classes": false, "has_functions": false, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": true, "has_logging": false, "function_count": 0, "class_count": 0, "line_count": 53}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create REST API endpoint with proper request/response handling\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "APP_MODULE = 'tests.test_app'\nAPP_FUNCTION = 'hello_world'\nDJANGO_SETTINGS = None\nDEBUG = 'True'\nLOG_LEVEL = 'DEBUG'\nSCRIPT_NAME = 'hello_world'\nDOMAIN = None\nAPI_STAGE = 'ttt888'\nPROJECT_NAME = 'ttt888'\n\nREMOTE_ENV='s3://lmbda/test_env.json'\n## test_env.json\n#{\n#    \"hello\": \"world\"\n#}\n#\n\nAWS_EVENT_MAPPING = {\n    'arn:aws:s3:1': 'test_settings.aws_s3_event',\n    'arn:aws:sns:1': 'test_settings.aws_sns_event',\n    'arn:aws:dynamodb:1': 'test_settings.aws_dynamodb_event',\n    'arn:aws:kinesis:1': 'test_settings.aws_kinesis_event',\n    'arn:aws:sqs:1': 'test_settings.aws_sqs_event'\n}\n\nENVIRONMENT_VARIABLES={'testenv': 'envtest'}\n\nAUTHORIZER_FUNCTION='test_settings.authorizer_event'\n\n\ndef prebuild_me():\n    print(\"This is a prebuild script!\")\n\n\ndef callback(self):\n    print(\"this is a callback\")\n\n\ndef aws_s3_event(event, content):\n    return \"AWS S3 EVENT\"\n\n\ndef aws_sns_event(event, content):\n    return \"AWS SNS EVENT\"\n\ndef aws_async_sns_event(arg1, arg2, arg3):\n    return \"AWS ASYNC SNS EVENT\"\n\n\ndef aws_dynamodb_event(event, content):\n    return \"AWS DYNAMODB EVENT\"\n\n\ndef aws_kinesis_event(event, content):\n    return \"AWS KINESIS EVENT\"\n\n\ndef aws_sqs_event(event, content):\n    return \"AWS SQS EVENT\"\n\n\ndef authorizer_event(event, content):\n    return \"AUTHORIZER_EVENT\"\n\n\ndef command():\n    print(\"command\")\n", "metadata": {"source": "github.com/Miserlou/Zappa", "file_path": "test_settings.py", "language": "python", "complexity": "medium", "star_count": 11883, "quality_score": 0.89, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": false, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": true, "has_logging": true, "function_count": 10, "class_count": 0, "line_count": 68}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create REST API endpoint with proper request/response handling\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\n# this is a module\n\n\ndef greet():\n    print('There is more stupidity than hydrogen in the universe, and it has a longer shelf life.')\n", "metadata": {"source": "github.com/Miserlou/Zappa", "file_path": "tests/data/test1.py", "language": "python", "complexity": "simple", "star_count": 11883, "quality_score": 0.6300000000000001, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": false, "has_error_handling": false, "has_documentation": false, "has_type_hints": false, "has_testing": false, "has_logging": false, "function_count": 1, "class_count": 0, "line_count": 7}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create REST API endpoint with proper request/response handling\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\n# this is a module, so test2 should get an __init__.py\n\ndef get_some_wisdom():\n    return 'Art is making something out of nothing, and selling it.'", "metadata": {"source": "github.com/Miserlou/Zappa", "file_path": "tests/data/test2/test.py", "language": "python", "complexity": "simple", "star_count": 11883, "quality_score": 0.6300000000000001, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": false, "has_error_handling": false, "has_documentation": false, "has_type_hints": false, "has_testing": false, "has_logging": false, "function_count": 1, "class_count": 0, "line_count": 5}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create REST API endpoint with proper request/response handling\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from zappa.asynchronous import task\ntry:\n    from urllib.parse import parse_qs\nexcept ImportError:\n    from cgi import parse_qs\n\ntry:\n    from html import escape\nexcept ImportError:\n    from cgi import escape\n\n\ndef hello_world(environ, start_response):\n    parameters = parse_qs(environ.get('QUERY_STRING', ''))\n    if 'subject' in parameters:\n        subject = escape(parameters['subject'][0])\n    else:\n        subject = 'World'\n    start_response('200 OK', [('Content-Type', 'text/html')])\n    return ['''Hello {subject!s}\n    Hello {subject!s}!\n\n'''.format(**{'subject': subject})]\n\n\ndef schedule_me():\n    return \"Hello!\"\n\n@task\ndef async_me(arg1, **kwargs):\n    return \"run async when on lambda %s%s\" % (arg1, kwargs.get('foo', ''))\n\n\n@task(remote_aws_lambda_function_name='test-app-dev', remote_aws_region='us-east-1')\ndef remote_async_me(arg1, **kwargs):\n    return \"run async always on lambda %s%s\" % (arg1, kwargs.get('foo', ''))\n\n\ndef callback(self):\n    print(\"this is a callback\")\n\n\ndef prebuild_me():\n    print(\"this is a prebuild script\")\n\n", "metadata": {"source": "github.com/Miserlou/Zappa", "file_path": "tests/test_app.py", "language": "python", "complexity": "simple", "star_count": 11883, "quality_score": 0.9500000000000001, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": true, "has_documentation": true, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 6, "class_count": 0, "line_count": 46}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate configuration for Create REST API endpoint with proper request/response handling\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "API_STAGE = 'dev'\nAPP_FUNCTION = 'app'\nAPP_MODULE = 'tests.test_wsgi_script_name_app'\nBINARY_SUPPORT = False\nCONTEXT_HEADER_MAPPINGS = {}\nDEBUG = 'True'\nDJANGO_SETTINGS = None\nDOMAIN = 'api.example.com'\nENVIRONMENT_VARIABLES = {}\nLOG_LEVEL = 'DEBUG'\nPROJECT_NAME = 'wsgi_script_name_settings'\nCOGNITO_TRIGGER_MAPPING = {}\nAWS_BOT_EVENT_MAPPING = {'intent-name:DialogCodeHook': 'tests.test_handler.raises_exception'}\nEXCEPTION_HANDLER = 'tests.test_handler.mocked_exception_handler'\n", "metadata": {"source": "github.com/Miserlou/Zappa", "file_path": "tests/test_bot_exception_handler_settings.py", "language": "python", "complexity": "simple", "star_count": 11883, "quality_score": 0.8600000000000001, "features": {"has_classes": false, "has_functions": false, "has_async": false, "has_decorators": false, "has_imports": false, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": true, "has_logging": true, "function_count": 0, "class_count": 0, "line_count": 15}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate configuration for Create REST API endpoint with proper request/response handling\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "API_STAGE = 'dev'\nAPP_FUNCTION = 'app'\nAPP_MODULE = 'tests.test_wsgi_script_name_app'\nBINARY_SUPPORT = False\nCONTEXT_HEADER_MAPPINGS = {}\nDEBUG = 'True'\nDJANGO_SETTINGS = None\nDOMAIN = 'api.example.com'\nENVIRONMENT_VARIABLES = {}\nLOG_LEVEL = 'DEBUG'\nPROJECT_NAME = 'wsgi_script_name_settings'\nCOGNITO_TRIGGER_MAPPING = {}\nAWS_BOT_EVENT_MAPPING = {'intent-name:DialogCodeHook': 'tests.test_handler.handle_bot_intent'}\n", "metadata": {"source": "github.com/Miserlou/Zappa", "file_path": "tests/test_bot_handler_being_triggered.py", "language": "python", "complexity": "simple", "star_count": 11883, "quality_score": 0.8600000000000001, "features": {"has_classes": false, "has_functions": false, "has_async": false, "has_decorators": false, "has_imports": false, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": true, "has_logging": true, "function_count": 0, "class_count": 0, "line_count": 14}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create REST API endpoint with proper request/response handling\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from __future__ import print_function\n\n\ndef handler_for_events(event, context):\n    print('Event:', event)\n    return True\n", "metadata": {"source": "github.com/Miserlou/Zappa", "file_path": "tests/test_event_script_app.py", "language": "python", "complexity": "simple", "star_count": 11883, "quality_score": 0.69, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": false, "has_testing": false, "has_logging": false, "function_count": 1, "class_count": 0, "line_count": 7}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate configuration for Create REST API endpoint with proper request/response handling\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "API_STAGE = 'dev'\nAPP_FUNCTION = 'handler_for_events'\nAPP_MODULE = 'tests.test_event_script_app'\nDEBUG = 'True'\nDJANGO_SETTINGS = None\nDOMAIN = 'api.example.com'\nENVIRONMENT_VARIABLES = {}\nLOG_LEVEL = 'DEBUG'\nPROJECT_NAME = 'test_event_script_app'\nCOGNITO_TRIGGER_MAPPING = {}\n", "metadata": {"source": "github.com/Miserlou/Zappa", "file_path": "tests/test_event_script_settings.py", "language": "python", "complexity": "simple", "star_count": 11883, "quality_score": 0.8300000000000001, "features": {"has_classes": false, "has_functions": false, "has_async": false, "has_decorators": false, "has_imports": false, "has_error_handling": false, "has_documentation": false, "has_type_hints": false, "has_testing": true, "has_logging": true, "function_count": 0, "class_count": 0, "line_count": 11}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate configuration for Create REST API endpoint with proper request/response handling\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "API_STAGE = 'dev'\nAPP_FUNCTION = 'raises_exception'\nAPP_MODULE = 'tests.test_handler'\nBINARY_SUPPORT = False\nCONTEXT_HEADER_MAPPINGS = {}\nDEBUG = 'True'\nDJANGO_SETTINGS = None\nDOMAIN = 'api.example.com'\nENVIRONMENT_VARIABLES = {}\nLOG_LEVEL = 'DEBUG'\nPROJECT_NAME = 'raises_exception'\nCOGNITO_TRIGGER_MAPPING = {}\nEXCEPTION_HANDLER = 'tests.test_handler.mocked_exception_handler'\n", "metadata": {"source": "github.com/Miserlou/Zappa", "file_path": "tests/test_exception_handler_settings.py", "language": "python", "complexity": "simple", "star_count": 11883, "quality_score": 0.8300000000000001, "features": {"has_classes": false, "has_functions": false, "has_async": false, "has_decorators": false, "has_imports": false, "has_error_handling": false, "has_documentation": false, "has_type_hints": false, "has_testing": true, "has_logging": true, "function_count": 0, "class_count": 0, "line_count": 14}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create REST API endpoint with proper request/response handling with pytest testing framework\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from mock import Mock\nimport sys\nimport unittest\nfrom zappa.handler import LambdaHandler\nfrom zappa.utilities import merge_headers\n\n\ndef no_args():\n    return\n\n\ndef one_arg(first):\n    return first\n\n\ndef two_args(first, second):\n    return first, second\n\n\ndef var_args(*args):\n    return args\n\n\ndef var_args_with_one(first, *args):\n    return first, args[0]\n\n\ndef unsupported(first, second, third):\n    return first, second, third\n\n\ndef raises_exception(*args, **kwargs):\n    raise Exception('app exception')\n\n\ndef handle_bot_intent(event, context):\n    return \"Success\"\n\n\nmocked_exception_handler = Mock()\n\n\nclass TestZappa(unittest.TestCase):\n\n    def setUp(self):\n        mocked_exception_handler.reset_mock()\n\n    def tearDown(self):\n        LambdaHandler._LambdaHandler__instance = None\n        LambdaHandler.settings = None\n        LambdaHandler.settings_name = None\n\n    def test_run_function(self):\n        self.assertIsNone(LambdaHandler.run_function(no_args, 'e', 'c'))\n        self.assertEqual(LambdaHandler.run_function(one_arg, 'e', 'c'), 'e')\n        self.assertEqual(LambdaHandler.run_function(two_args, 'e', 'c'), ('e', 'c'))\n        self.assertEqual(LambdaHandler.run_function(var_args, 'e', 'c'), ('e', 'c'))\n        self.assertEqual(LambdaHandler.run_function(var_args_with_one, 'e', 'c'), ('e', 'c'))\n\n        try:\n            LambdaHandler.run_function(unsupported, 'e', 'c')\n            self.fail('Exception expected')\n        except RuntimeError as e:\n            pass\n\n    def test_run_fuction_with_type_hint(self):\n        scope = {}\n        exec('def f_with_type_hint() -> None: return', scope)\n        f_with_type_hint = scope['f_with_type_hint']\n        self.assertIsNone(LambdaHandler.run_function(f_with_type_hint, 'e', 'c'))\n\n    def test_wsgi_script_name_on_aws_url(self):\n        \"\"\"\n        Ensure that requests to the amazonaws.com host for an API with a\n        domain have the correct request.url\n        \"\"\"\n        lh = LambdaHandler('tests.test_wsgi_script_name_settings')\n\n        event = {\n            'body': '',\n            'resource': '/{proxy+}',\n            'requestContext': {},\n            'queryStringParameters': {},\n            'headers': {\n                'Host': '1234567890.execute-api.us-east-1.amazonaws.com',\n            },\n            'pathParameters': {\n                'proxy': 'return/request/url'\n            },\n            'httpMethod': 'GET',\n            'stageVariables': {},\n            'path': '/return/request/url'\n        }\n        response = lh.handler(event, None)\n\n        self.assertEqual(response['statusCode'], 200)\n        self.assertEqual(\n            response['body'],\n            'https://1234567890.execute-api.us-east-1.amazonaws.com/dev/return/request/url'\n        )\n\n    def test_wsgi_script_name_on_domain_url(self):\n        \"\"\"\n        Ensure that requests to the amazonaws.com host for an API with a\n        domain have the correct request.url\n        \"\"\"\n        lh = LambdaHandler('tests.test_wsgi_script_name_settings')\n\n        event = {\n            'body': '',\n            'resource': '/{proxy+}',\n            'requestContext': {},\n            'queryStringParameters': {},\n            'headers': {\n                'Host': 'example.com',\n            },\n            'pathParameters': {\n                'proxy': 'return/request/url'\n            },\n            'httpMethod': 'GET',\n            'stageVariables': {},\n            'path': '/return/request/url'\n        }\n        response = lh.handler(event, None)\n\n        self.assertEqual(response['statusCode'], 200)\n        self.assertEqual(\n            response['body'],\n            'https://example.com/return/request/url'\n        )\n\n    def test_wsgi_script_name_with_multi_value_header(self):\n        \"\"\"\n        Ensure that requests generated with multivalued headers (such as\n        from an ALB with Multi Valued Headers enabled) succeed.\n        \"\"\"\n        lh = LambdaHandler('tests.test_wsgi_script_name_settings')\n\n        event = {\n            'body': '',\n            'resource': '/{proxy+}',\n            'requestContext': {},\n            'queryStringParameters': {},\n            'multiValueHeaders': {\n                'Host': ['example.com'],\n            },\n            'pathParameters': {\n                'proxy': 'return/request/url'\n            },\n            'httpMethod': 'GET',\n            'stageVariables': {},\n            'path': '/return/request/url'\n        }\n        response = lh.handler(event, None)\n        self.assertEqual(response['statusCode'], 200)\n        self.assertIn('multiValueHeaders', response)\n\n    def test_wsgi_script_name_with_multi_value_querystring(self):\n        \"\"\"\n        Ensure that requests generated with multivalue querystrings succeed.\n        \"\"\"\n        lh = LambdaHandler('tests.test_wsgi_script_name_settings')\n\n        event = {\n            'body': '',\n            'resource': '/{proxy+}',\n            'requestContext': {},\n            'multiValueQueryStringParameters': {\n                'multi': ['value', 'qs']\n            },\n            'multiValueHeaders': {\n                'Host': ['example.com'],\n            },\n            'pathParameters': {\n                'proxy': 'return/request/url'\n            },\n            'httpMethod': 'GET',\n            'stageVariables': {},\n            'path': '/return/request/url'\n        }\n        response = lh.handler(event, None)\n\n        self.assertEqual(response['statusCode'], 200)\n        self.assertEqual(\n            response['body'],\n            'https://example.com/return/request/url?multi=value&multi=qs'\n        )\n\n    def test_wsgi_script_name_on_test_request(self):\n        \"\"\"\n        Ensure that requests sent by the \"Send test request\" button behaves\n        sensibly\n        \"\"\"\n        lh = LambdaHandler('tests.test_wsgi_script_name_settings')\n\n        event = {\n            'body': '',\n            'resource': '/{proxy+}',\n            'requestContext': {},\n            'queryStringParameters': {},\n            'headers': {},\n            'pathParameters': {\n                'proxy': 'return/request/url'\n            },\n            'httpMethod': 'GET',\n            'stageVariables': {},\n            'path': '/return/request/url'\n        }\n        response = lh.handler(event, None)\n\n        self.assertEqual(response['statusCode'], 200)\n        self.assertEqual(\n            response['body'],\n            'https://zappa:80/return/request/url'\n        )\n\n    def test_exception_handler_on_web_request(self):\n        \"\"\"\n        Ensure that app exceptions triggered by web requests use the exception_handler.\n        \"\"\"\n        lh = LambdaHandler('tests.test_exception_handler_settings')\n\n        event = {\n            'body': '',\n            'resource': '/{proxy+}',\n            'requestContext': {},\n            'queryStringParameters': {},\n            'headers': {\n                'Host': '1234567890.execute-api.us-east-1.amazonaws.com',\n            },\n            'pathParameters': {\n                'proxy': 'return/request/url'\n            },\n            'httpMethod': 'GET',\n            'stageVariables': {},\n            'path': '/return/request/url'\n        }\n\n        mocked_exception_handler.assert_not_called()\n        response = lh.handler(event, None)\n\n        self.assertEqual(response['statusCode'], 500)\n        mocked_exception_handler.assert_called()\n\n    def test_wsgi_script_on_cognito_event_request(self):\n        \"\"\"\n        Ensure that requests sent by cognito behave sensibly\n        \"\"\"\n        lh = LambdaHandler('tests.test_wsgi_script_name_settings')\n\n        event = {'version': '1',\n                 'region': 'eu-west-1',\n                 'userPoolId': 'region_poolID',\n                 'userName': 'uuu-id-here',\n                 'callerContext': {'awsSdkVersion': 'aws-sdk-js-2.149.0',\n                                   'clientId': 'client-id-here'},\n                 'triggerSource': 'PreSignUp_SignUp',\n                 'request': {'userAttributes':\n                                 {'email': 'email@example.com'}, 'validationData': None},\n                 'response': {'autoConfirmUser': False,\n                              'autoVerifyEmail': False,\n                              'autoVerifyPhone': False}}\n\n        response = lh.handler(event, None)\n\n        self.assertEqual(response['response']['autoConfirmUser'], False)\n\n    def test_bot_triggered_event(self):\n        \"\"\"\n        Ensure that bot triggered events are handled as in the settings\n        \"\"\"\n        lh = LambdaHandler('tests.test_bot_handler_being_triggered')\n        # from : https://docs.aws.amazon.com/lambda/latest/dg/eventsources.html#eventsources-lex\n        event = {\n            \"messageVersion\": \"1.0\",\n            \"invocationSource\": \"DialogCodeHook\",\n            \"userId\": \"user-id specified in the POST request to Amazon Lex.\",\n            \"sessionAttributes\": {\n                \"key1\": \"value1\",\n                \"key2\": \"value2\",\n            },\n            \"bot\": {\n                \"name\": \"bot-name\",\n                \"alias\": \"bot-alias\",\n                \"version\": \"bot-version\"\n            },\n            \"outputDialogMode\": \"Text or Voice, based on ContentType request header in runtime API request\",\n            \"currentIntent\": {\n                \"name\": \"intent-name\",\n                \"slots\": {\n                    \"slot-name\": \"value\",\n                    \"slot-name\": \"value\",\n                    \"slot-name\": \"value\"\n                },\n                \"confirmationStatus\": \"None, Confirmed, or Denied (intent confirmation, if configured)\"\n            }\n        }\n\n        response = lh.handler(event, None)\n\n        self.assertEqual(response, 'Success')\n\n    def test_exception_in_bot_triggered_event(self):\n        \"\"\"\n        Ensure that bot triggered exceptions are handled as defined in the settings.\n        \"\"\"\n        lh = LambdaHandler('tests.test_bot_exception_handler_settings')\n        # from : https://docs.aws.amazon.com/lambda/latest/dg/eventsources.html#eventsources-lex\n        event = {\n            \"messageVersion\": \"1.0\",\n            \"invocationSource\": \"DialogCodeHook\",\n            \"userId\": \"user-id specified in the POST request to Amazon Lex.\",\n            \"sessionAttributes\": {\n                \"key1\": \"value1\",\n                \"key2\": \"value2\",\n            },\n            \"bot\": {\n                \"name\": \"bot-name\",\n                \"alias\": \"bot-alias\",\n                \"version\": \"bot-version\"\n            },\n            \"outputDialogMode\": \"Text or Voice, based on ContentType request header in runtime API request\",\n            \"currentIntent\": {\n                \"name\": \"intent-name\",\n                \"slots\": {\n                    \"slot-name\": \"value\",\n                    \"slot-name\": \"value\",\n                    \"slot-name\": \"value\"\n                },\n                \"confirmationStatus\": \"None, Confirmed, or Denied (intent confirmation, if configured)\"\n            }\n        }\n\n        response = lh.lambda_handler(event, None)\n        mocked_exception_handler.assert_called\n\n    def test_wsgi_script_name_on_alb_event(self):\n        \"\"\"\n        Ensure ALB-triggered events are properly handled by LambdaHandler\n        ALB-forwarded events have a slightly different request structure than API-Gateway\n        https://docs.aws.amazon.com/elasticloadbalancing/latest/application/lambda-functions.html\n        \"\"\"\n        lh = LambdaHandler('tests.test_wsgi_script_name_settings')\n\n        event = {\n            'requestContext': {\n                'elb': {\n                    'targetGroupArn': 'arn:aws:elasticloadbalancing:region:123456789012:targetgroup/my-target-group/6d0ecf831eec9f09'\n                }\n            },\n            'httpMethod': 'GET',\n            'path': '/return/request/url',\n            'queryStringParameters': {},\n            'headers': {\n                'accept': 'text/html,application/xhtml+xml',\n                'accept-language': 'en-US,en;q=0.8',\n                'content-type': 'text/plain',\n                'cookie': 'cookies',\n                'host': '1234567890.execute-api.us-east-1.amazonaws.com',\n                'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6)',\n                'x-amzn-trace-id': 'Root=1-5bdb40ca-556d8b0c50dc66f0511bf520',\n                'x-forwarded-for': '72.21.198.66',\n                'x-forwarded-port': '443',\n                'x-forwarded-proto': 'https'\n            },\n            'isBase64Encoded': False,\n            'body': ''\n        }\n        response = lh.handler(event, None)\n\n        self.assertEqual(response['statusCode'], 200)\n        self.assertEqual(response['statusDescription'], '200 OK')\n        self.assertEqual(response['isBase64Encoded'], False)\n        self.assertEqual(\n            response['body'],\n            'https://1234567890.execute-api.us-east-1.amazonaws.com/return/request/url'\n        )\n\n    def test_merge_headers_no_multi_value(self):\n        event = {\n            'headers': {\n                'a': 'b'\n            }\n        }\n\n        merged = merge_headers(event)\n        self.assertEqual(merged['a'], 'b')\n\n    def test_merge_headers_combine_values(self):\n        event = {\n            'headers': {\n                'a': 'b',\n                'z': 'q'\n            },\n            'multiValueHeaders': {\n                'a': ['c'],\n                'x': ['y']\n            }\n        }\n\n        merged = merge_headers(event)\n        self.assertEqual(merged['a'], 'c')\n        self.assertEqual(merged['x'], 'y')\n        self.assertEqual(merged['z'], 'q')\n\n    def test_merge_headers_no_single_value(self):\n        event = {\n            'multiValueHeaders': {\n                'a': ['c', 'd'],\n                'x': ['y', 'z', 'f']\n            }\n        }\n        merged = merge_headers(event)\n        self.assertEqual(merged['a'], 'c, d')\n        self.assertEqual(merged['x'], 'y, z, f')\n\n    def test_cloudwatch_subscription_event(self):\n        \"\"\"\n        Test that events sent in the format used by CloudWatch logs via\n        subscription filters are handled properly.\n        The actual payload that Lambda receives is in the following format\n        { \"awslogs\": {\"data\": \"BASE64ENCODED_GZIP_COMPRESSED_DATA\"} }\n        https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/SubscriptionFilters.html\n        \"\"\"\n        lh = LambdaHandler('tests.test_event_script_settings')\n\n        event = {\n            'awslogs': {\n                'data': \"some-data-not-important-for-test\"\n            }\n        }\n        response = lh.handler(event, None)\n\n        self.assertEqual(response, True)\n", "metadata": {"source": "github.com/Miserlou/Zappa", "file_path": "tests/test_handler.py", "language": "python", "complexity": "complex", "star_count": 11883, "quality_score": 1.0, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": true, "has_documentation": true, "has_type_hints": true, "has_testing": true, "has_logging": true, "function_count": 27, "class_count": 1, "line_count": 435}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "api_development", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create REST API endpoint with proper request/response handling using Flask/FastAPI decorators\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from flask import Flask, request\n\napp = Flask(__name__)\n\n\n@app.route('/return/request/url', methods=['GET', 'POST'])\ndef return_request_url():\n    return request.url\n", "metadata": {"source": "github.com/Miserlou/Zappa", "file_path": "tests/test_wsgi_script_name_app.py", "language": "python", "complexity": "simple", "star_count": 11883, "quality_score": 0.69, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": false, "has_testing": false, "has_logging": false, "function_count": 1, "class_count": 0, "line_count": 9}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "database_integration", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create database model with ORM relationships and queries with SQLAlchemy/Django ORM patterns\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from decimal import Decimal\nfrom typing import Optional\n\nfrom sqlmodel import Field, Session, SQLModel, create_engine, select\n\n\nclass Hero(SQLModel, table=True):\n    id: Optional[int] = Field(default=None, primary_key=True)\n    name: str = Field(index=True)\n    secret_name: str\n    age: Optional[int] = Field(default=None, index=True)\n    money: Decimal = Field(default=0, max_digits=5, decimal_places=3)\n\n\nsqlite_file_name = \"database.db\"\nsqlite_url = f\"sqlite:///{sqlite_file_name}\"\n\nengine = create_engine(sqlite_url, echo=True)\n\n\ndef create_db_and_tables():\n    SQLModel.metadata.create_all(engine)\n\n\ndef create_heroes():\n    hero_1 = Hero(name=\"Deadpond\", secret_name=\"Dive Wilson\", money=1.1)\n    hero_2 = Hero(name=\"Spider-Boy\", secret_name=\"Pedro Parqueador\", money=0.001)\n    hero_3 = Hero(name=\"Rusty-Man\", secret_name=\"Tommy Sharp\", age=48, money=2.2)\n\n    with Session(engine) as session:\n        session.add(hero_1)\n        session.add(hero_2)\n        session.add(hero_3)\n\n        session.commit()\n\n\ndef select_heroes():\n    with Session(engine) as session:\n        statement = select(Hero).where(Hero.name == \"Deadpond\")\n        results = session.exec(statement)\n        hero_1 = results.one()\n        print(\"Hero 1:\", hero_1)\n\n        statement = select(Hero).where(Hero.name == \"Rusty-Man\")\n        results = session.exec(statement)\n        hero_2 = results.one()\n        print(\"Hero 2:\", hero_2)\n\n        total_money = hero_1.money + hero_2.money\n        print(f\"Total money: {total_money}\")\n\n\ndef main():\n    create_db_and_tables()\n    create_heroes()\n    select_heroes()\n\n\nif __name__ == \"__main__\":\n    main()\n", "metadata": {"source": "github.com/fastapi/sqlmodel", "file_path": "docs_src/advanced/decimal/tutorial001.py", "language": "python", "complexity": "simple", "star_count": 16410, "quality_score": 0.92, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 4, "class_count": 1, "line_count": 62}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "database_integration", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create database model with ORM relationships and queries with SQLAlchemy/Django ORM patterns\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from decimal import Decimal\n\nfrom sqlmodel import Field, Session, SQLModel, create_engine, select\n\n\nclass Hero(SQLModel, table=True):\n    id: int | None = Field(default=None, primary_key=True)\n    name: str = Field(index=True)\n    secret_name: str\n    age: int | None = Field(default=None, index=True)\n    money: Decimal = Field(default=0, max_digits=5, decimal_places=3)\n\n\nsqlite_file_name = \"database.db\"\nsqlite_url = f\"sqlite:///{sqlite_file_name}\"\n\nengine = create_engine(sqlite_url, echo=True)\n\n\ndef create_db_and_tables():\n    SQLModel.metadata.create_all(engine)\n\n\ndef create_heroes():\n    hero_1 = Hero(name=\"Deadpond\", secret_name=\"Dive Wilson\", money=1.1)\n    hero_2 = Hero(name=\"Spider-Boy\", secret_name=\"Pedro Parqueador\", money=0.001)\n    hero_3 = Hero(name=\"Rusty-Man\", secret_name=\"Tommy Sharp\", age=48, money=2.2)\n\n    with Session(engine) as session:\n        session.add(hero_1)\n        session.add(hero_2)\n        session.add(hero_3)\n\n        session.commit()\n\n\ndef select_heroes():\n    with Session(engine) as session:\n        statement = select(Hero).where(Hero.name == \"Deadpond\")\n        results = session.exec(statement)\n        hero_1 = results.one()\n        print(\"Hero 1:\", hero_1)\n\n        statement = select(Hero).where(Hero.name == \"Rusty-Man\")\n        results = session.exec(statement)\n        hero_2 = results.one()\n        print(\"Hero 2:\", hero_2)\n\n        total_money = hero_1.money + hero_2.money\n        print(f\"Total money: {total_money}\")\n\n\ndef main():\n    create_db_and_tables()\n    create_heroes()\n    select_heroes()\n\n\nif __name__ == \"__main__\":\n    main()\n", "metadata": {"source": "github.com/fastapi/sqlmodel", "file_path": "docs_src/advanced/decimal/tutorial001_py310.py", "language": "python", "complexity": "simple", "star_count": 16410, "quality_score": 0.92, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 4, "class_count": 1, "line_count": 61}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "database_integration", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create database model with ORM relationships and queries with SQLAlchemy/Django ORM patterns\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import uuid\nfrom typing import Union\n\nfrom sqlmodel import Field, Session, SQLModel, create_engine, select\n\n\nclass Hero(SQLModel, table=True):\n    id: uuid.UUID = Field(default_factory=uuid.uuid4, primary_key=True)\n    name: str = Field(index=True)\n    secret_name: str\n    age: Union[int, None] = Field(default=None, index=True)\n\n\nsqlite_file_name = \"database.db\"\nsqlite_url = f\"sqlite:///{sqlite_file_name}\"\n\nengine = create_engine(sqlite_url, echo=True)\n\n\ndef create_db_and_tables():\n    SQLModel.metadata.create_all(engine)\n\n\ndef create_hero():\n    with Session(engine) as session:\n        hero = Hero(name=\"Deadpond\", secret_name=\"Dive Wilson\")\n        print(\"The hero before saving in the DB\")\n        print(hero)\n        print(\"The hero ID was already set\")\n        print(hero.id)\n        session.add(hero)\n        session.commit()\n        session.refresh(hero)\n        print(\"After saving in the DB\")\n        print(hero)\n\n\ndef select_hero():\n    with Session(engine) as session:\n        hero_2 = Hero(name=\"Spider-Boy\", secret_name=\"Pedro Parqueador\")\n        session.add(hero_2)\n        session.commit()\n        session.refresh(hero_2)\n        hero_id = hero_2.id\n        print(\"Created hero:\")\n        print(hero_2)\n        print(\"Created hero ID:\")\n        print(hero_id)\n\n        statement = select(Hero).where(Hero.id == hero_id)\n        selected_hero = session.exec(statement).one()\n        print(\"Selected hero:\")\n        print(selected_hero)\n        print(\"Selected hero ID:\")\n        print(selected_hero.id)\n\n\ndef main() -> None:\n    create_db_and_tables()\n    create_hero()\n    select_hero()\n\n\nif __name__ == \"__main__\":\n    main()\n", "metadata": {"source": "github.com/fastapi/sqlmodel", "file_path": "docs_src/advanced/uuid/tutorial001.py", "language": "python", "complexity": "simple", "star_count": 16410, "quality_score": 0.92, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 4, "class_count": 1, "line_count": 66}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "database_integration", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create database model with ORM relationships and queries with SQLAlchemy/Django ORM patterns\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import uuid\n\nfrom sqlmodel import Field, Session, SQLModel, create_engine, select\n\n\nclass Hero(SQLModel, table=True):\n    id: uuid.UUID = Field(default_factory=uuid.uuid4, primary_key=True)\n    name: str = Field(index=True)\n    secret_name: str\n    age: int | None = Field(default=None, index=True)\n\n\nsqlite_file_name = \"database.db\"\nsqlite_url = f\"sqlite:///{sqlite_file_name}\"\n\nengine = create_engine(sqlite_url, echo=True)\n\n\ndef create_db_and_tables():\n    SQLModel.metadata.create_all(engine)\n\n\ndef create_hero():\n    with Session(engine) as session:\n        hero = Hero(name=\"Deadpond\", secret_name=\"Dive Wilson\")\n        print(\"The hero before saving in the DB\")\n        print(hero)\n        print(\"The hero ID was already set\")\n        print(hero.id)\n        session.add(hero)\n        session.commit()\n        session.refresh(hero)\n        print(\"After saving in the DB\")\n        print(hero)\n\n\ndef select_hero():\n    with Session(engine) as session:\n        hero_2 = Hero(name=\"Spider-Boy\", secret_name=\"Pedro Parqueador\")\n        session.add(hero_2)\n        session.commit()\n        session.refresh(hero_2)\n        hero_id = hero_2.id\n        print(\"Created hero:\")\n        print(hero_2)\n        print(\"Created hero ID:\")\n        print(hero_id)\n\n        statement = select(Hero).where(Hero.id == hero_id)\n        selected_hero = session.exec(statement).one()\n        print(\"Selected hero:\")\n        print(selected_hero)\n        print(\"Selected hero ID:\")\n        print(selected_hero.id)\n\n\ndef main() -> None:\n    create_db_and_tables()\n    create_hero()\n    select_hero()\n\n\nif __name__ == \"__main__\":\n    main()\n", "metadata": {"source": "github.com/fastapi/sqlmodel", "file_path": "docs_src/advanced/uuid/tutorial001_py310.py", "language": "python", "complexity": "simple", "star_count": 16410, "quality_score": 0.92, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 4, "class_count": 1, "line_count": 65}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "database_integration", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create database model with ORM relationships and queries with SQLAlchemy/Django ORM patterns\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import uuid\nfrom typing import Union\n\nfrom sqlmodel import Field, Session, SQLModel, create_engine\n\n\nclass Hero(SQLModel, table=True):\n    id: uuid.UUID = Field(default_factory=uuid.uuid4, primary_key=True)\n    name: str = Field(index=True)\n    secret_name: str\n    age: Union[int, None] = Field(default=None, index=True)\n\n\nsqlite_file_name = \"database.db\"\nsqlite_url = f\"sqlite:///{sqlite_file_name}\"\n\nengine = create_engine(sqlite_url, echo=True)\n\n\ndef create_db_and_tables():\n    SQLModel.metadata.create_all(engine)\n\n\ndef create_hero():\n    with Session(engine) as session:\n        hero_1 = Hero(name=\"Deadpond\", secret_name=\"Dive Wilson\")\n        print(\"The hero before saving in the DB\")\n        print(hero_1)\n        print(\"The hero ID was already set\")\n        print(hero_1.id)\n        session.add(hero_1)\n        session.commit()\n        session.refresh(hero_1)\n        print(\"After saving in the DB\")\n        print(hero_1)\n\n\ndef select_hero():\n    with Session(engine) as session:\n        hero_2 = Hero(name=\"Spider-Boy\", secret_name=\"Pedro Parqueador\")\n        session.add(hero_2)\n        session.commit()\n        session.refresh(hero_2)\n        hero_id = hero_2.id\n        print(\"Created hero:\")\n        print(hero_2)\n        print(\"Created hero ID:\")\n        print(hero_id)\n\n        selected_hero = session.get(Hero, hero_id)\n        print(\"Selected hero:\")\n        print(selected_hero)\n        print(\"Selected hero ID:\")\n        print(selected_hero.id)\n\n\ndef main() -> None:\n    create_db_and_tables()\n    create_hero()\n    select_hero()\n\n\nif __name__ == \"__main__\":\n    main()\n", "metadata": {"source": "github.com/fastapi/sqlmodel", "file_path": "docs_src/advanced/uuid/tutorial002.py", "language": "python", "complexity": "simple", "star_count": 16410, "quality_score": 0.92, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 4, "class_count": 1, "line_count": 65}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "database_integration", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create database model with ORM relationships and queries with SQLAlchemy/Django ORM patterns\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import uuid\n\nfrom sqlmodel import Field, Session, SQLModel, create_engine\n\n\nclass Hero(SQLModel, table=True):\n    id: uuid.UUID = Field(default_factory=uuid.uuid4, primary_key=True)\n    name: str = Field(index=True)\n    secret_name: str\n    age: int | None = Field(default=None, index=True)\n\n\nsqlite_file_name = \"database.db\"\nsqlite_url = f\"sqlite:///{sqlite_file_name}\"\n\nengine = create_engine(sqlite_url, echo=True)\n\n\ndef create_db_and_tables():\n    SQLModel.metadata.create_all(engine)\n\n\ndef create_hero():\n    with Session(engine) as session:\n        hero = Hero(name=\"Deadpond\", secret_name=\"Dive Wilson\")\n        print(\"The hero before saving in the DB\")\n        print(hero)\n        print(\"The hero ID was already set\")\n        print(hero.id)\n        session.add(hero)\n        session.commit()\n        session.refresh(hero)\n        print(\"After saving in the DB\")\n        print(hero)\n\n\ndef select_hero():\n    with Session(engine) as session:\n        hero_2 = Hero(name=\"Spider-Boy\", secret_name=\"Pedro Parqueador\")\n        session.add(hero_2)\n        session.commit()\n        session.refresh(hero_2)\n        hero_id = hero_2.id\n        print(\"Created hero:\")\n        print(hero_2)\n        print(\"Created hero ID:\")\n        print(hero_id)\n\n        selected_hero = session.get(Hero, hero_id)\n        print(\"Selected hero:\")\n        print(selected_hero)\n        print(\"Selected hero ID:\")\n        print(selected_hero.id)\n\n\ndef main() -> None:\n    create_db_and_tables()\n    create_hero()\n    select_hero()\n\n\nif __name__ == \"__main__\":\n    main()\n", "metadata": {"source": "github.com/fastapi/sqlmodel", "file_path": "docs_src/advanced/uuid/tutorial002_py310.py", "language": "python", "complexity": "simple", "star_count": 16410, "quality_score": 0.92, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 4, "class_count": 1, "line_count": 64}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "database_integration", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create database model with ORM relationships and queries with SQLAlchemy/Django ORM patterns\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from typing import Optional\n\nfrom sqlmodel import Field, Session, SQLModel, create_engine\n\n\nclass Hero(SQLModel, table=True):\n    id: Optional[int] = Field(default=None, primary_key=True)\n    name: str\n    secret_name: str\n    age: Optional[int] = None\n\n\nsqlite_file_name = \"database.db\"\nsqlite_url = f\"sqlite:///{sqlite_file_name}\"\n\nengine = create_engine(sqlite_url, echo=True)\n\n\ndef create_db_and_tables():\n    SQLModel.metadata.create_all(engine)\n\n\ndef create_heroes():\n    hero_1 = Hero(name=\"Deadpond\", secret_name=\"Dive Wilson\")\n    hero_2 = Hero(name=\"Spider-Boy\", secret_name=\"Pedro Parqueador\")\n    hero_3 = Hero(name=\"Rusty-Man\", secret_name=\"Tommy Sharp\", age=48)\n\n    print(\"Before interacting with the database\")\n    print(\"Hero 1:\", hero_1)\n    print(\"Hero 2:\", hero_2)\n    print(\"Hero 3:\", hero_3)\n\n    with Session(engine) as session:\n        session.add(hero_1)\n        session.add(hero_2)\n        session.add(hero_3)\n\n        print(\"After adding to the session\")\n        print(\"Hero 1:\", hero_1)\n        print(\"Hero 2:\", hero_2)\n        print(\"Hero 3:\", hero_3)\n\n        session.commit()\n\n        print(\"After committing the session\")\n        print(\"Hero 1:\", hero_1)\n        print(\"Hero 2:\", hero_2)\n        print(\"Hero 3:\", hero_3)\n\n        print(\"After committing the session, show IDs\")\n        print(\"Hero 1 ID:\", hero_1.id)\n        print(\"Hero 2 ID:\", hero_2.id)\n        print(\"Hero 3 ID:\", hero_3.id)\n\n        print(\"After committing the session, show names\")\n        print(\"Hero 1 name:\", hero_1.name)\n        print(\"Hero 2 name:\", hero_2.name)\n        print(\"Hero 3 name:\", hero_3.name)\n\n        session.refresh(hero_1)\n        session.refresh(hero_2)\n        session.refresh(hero_3)\n\n        print(\"After refreshing the heroes\")\n        print(\"Hero 1:\", hero_1)\n        print(\"Hero 2:\", hero_2)\n        print(\"Hero 3:\", hero_3)\n\n    print(\"After the session closes\")\n    print(\"Hero 1:\", hero_1)\n    print(\"Hero 2:\", hero_2)\n    print(\"Hero 3:\", hero_3)\n\n\ndef main():\n    create_db_and_tables()\n    create_heroes()\n\n\nif __name__ == \"__main__\":\n    main()\n", "metadata": {"source": "github.com/fastapi/sqlmodel", "file_path": "docs_src/tutorial/automatic_id_none_refresh/tutorial001.py", "language": "python", "complexity": "medium", "star_count": 16410, "quality_score": 0.92, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 3, "class_count": 1, "line_count": 82}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "database_integration", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create database model with ORM relationships and queries with SQLAlchemy/Django ORM patterns\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from sqlmodel import Field, Session, SQLModel, create_engine\n\n\nclass Hero(SQLModel, table=True):\n    id: int | None = Field(default=None, primary_key=True)\n    name: str\n    secret_name: str\n    age: int | None = None\n\n\nsqlite_file_name = \"database.db\"\nsqlite_url = f\"sqlite:///{sqlite_file_name}\"\n\nengine = create_engine(sqlite_url, echo=True)\n\n\ndef create_db_and_tables():\n    SQLModel.metadata.create_all(engine)\n\n\ndef create_heroes():\n    hero_1 = Hero(name=\"Deadpond\", secret_name=\"Dive Wilson\")\n    hero_2 = Hero(name=\"Spider-Boy\", secret_name=\"Pedro Parqueador\")\n    hero_3 = Hero(name=\"Rusty-Man\", secret_name=\"Tommy Sharp\", age=48)\n\n    print(\"Before interacting with the database\")\n    print(\"Hero 1:\", hero_1)\n    print(\"Hero 2:\", hero_2)\n    print(\"Hero 3:\", hero_3)\n\n    with Session(engine) as session:\n        session.add(hero_1)\n        session.add(hero_2)\n        session.add(hero_3)\n\n        print(\"After adding to the session\")\n        print(\"Hero 1:\", hero_1)\n        print(\"Hero 2:\", hero_2)\n        print(\"Hero 3:\", hero_3)\n\n        session.commit()\n\n        print(\"After committing the session\")\n        print(\"Hero 1:\", hero_1)\n        print(\"Hero 2:\", hero_2)\n        print(\"Hero 3:\", hero_3)\n\n        print(\"After committing the session, show IDs\")\n        print(\"Hero 1 ID:\", hero_1.id)\n        print(\"Hero 2 ID:\", hero_2.id)\n        print(\"Hero 3 ID:\", hero_3.id)\n\n        print(\"After committing the session, show names\")\n        print(\"Hero 1 name:\", hero_1.name)\n        print(\"Hero 2 name:\", hero_2.name)\n        print(\"Hero 3 name:\", hero_3.name)\n\n        session.refresh(hero_1)\n        session.refresh(hero_2)\n        session.refresh(hero_3)\n\n        print(\"After refreshing the heroes\")\n        print(\"Hero 1:\", hero_1)\n        print(\"Hero 2:\", hero_2)\n        print(\"Hero 3:\", hero_3)\n\n    print(\"After the session closes\")\n    print(\"Hero 1:\", hero_1)\n    print(\"Hero 2:\", hero_2)\n    print(\"Hero 3:\", hero_3)\n\n\ndef main():\n    create_db_and_tables()\n    create_heroes()\n\n\nif __name__ == \"__main__\":\n    main()\n", "metadata": {"source": "github.com/fastapi/sqlmodel", "file_path": "docs_src/tutorial/automatic_id_none_refresh/tutorial001_py310.py", "language": "python", "complexity": "medium", "star_count": 16410, "quality_score": 0.92, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 3, "class_count": 1, "line_count": 80}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "database_integration", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create database model with ORM relationships and queries with SQLAlchemy/Django ORM patterns\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from typing import Optional\n\nfrom sqlmodel import Field, Session, SQLModel, create_engine\n\n\nclass Hero(SQLModel, table=True):\n    id: Optional[int] = Field(default=None, primary_key=True)\n    name: str\n    secret_name: str\n    age: Optional[int] = None\n\n\nsqlite_file_name = \"database.db\"\nsqlite_url = f\"sqlite:///{sqlite_file_name}\"\n\nengine = create_engine(sqlite_url, echo=True)\n\n\ndef create_db_and_tables():\n    SQLModel.metadata.create_all(engine)\n\n\ndef create_heroes():\n    hero_1 = Hero(name=\"Deadpond\", secret_name=\"Dive Wilson\")  # (1)!\n    hero_2 = Hero(name=\"Spider-Boy\", secret_name=\"Pedro Parqueador\")  # (2)!\n    hero_3 = Hero(name=\"Rusty-Man\", secret_name=\"Tommy Sharp\", age=48)  # (3)!\n\n    print(\"Before interacting with the database\")  # (4)!\n    print(\"Hero 1:\", hero_1)  # (5)!\n    print(\"Hero 2:\", hero_2)  # (6)!\n    print(\"Hero 3:\", hero_3)  # (7)!\n\n    with Session(engine) as session:  # (8)!\n        session.add(hero_1)  # (9)!\n        session.add(hero_2)  # (10)!\n        session.add(hero_3)  # (11)!\n\n        print(\"After adding to the session\")  # (12)!\n        print(\"Hero 1:\", hero_1)  # (13)!\n        print(\"Hero 2:\", hero_2)  # (14)!\n        print(\"Hero 3:\", hero_3)  # (15)!\n\n        session.commit()  # (16)!\n\n        print(\"After committing the session\")  # (17)!\n        print(\"Hero 1:\", hero_1)  # (18)!\n        print(\"Hero 2:\", hero_2)  # (19)!\n        print(\"Hero 3:\", hero_3)  # (20)!\n\n        print(\"After committing the session, show IDs\")  # (21)!\n        print(\"Hero 1 ID:\", hero_1.id)  # (22)!\n        print(\"Hero 2 ID:\", hero_2.id)  # (23)!\n        print(\"Hero 3 ID:\", hero_3.id)  # (24)!\n\n        print(\"After committing the session, show names\")  # (25)!\n        print(\"Hero 1 name:\", hero_1.name)  # (26)!\n        print(\"Hero 2 name:\", hero_2.name)  # (27)!\n        print(\"Hero 3 name:\", hero_3.name)  # (28)!\n\n        session.refresh(hero_1)  # (29)!\n        session.refresh(hero_2)  # (30)!\n        session.refresh(hero_3)  # (31)!\n\n        print(\"After refreshing the heroes\")  # (32)!\n        print(\"Hero 1:\", hero_1)  # (33)!\n        print(\"Hero 2:\", hero_2)  # (34)!\n        print(\"Hero 3:\", hero_3)  # (35)!\n    # (36)!\n\n    print(\"After the session closes\")  # (37)!\n    print(\"Hero 1:\", hero_1)  # (38)!\n    print(\"Hero 2:\", hero_2)  # (39)!\n    print(\"Hero 3:\", hero_3)  # (40)!\n\n\ndef main():\n    create_db_and_tables()\n    create_heroes()\n\n\nif __name__ == \"__main__\":\n    main()\n", "metadata": {"source": "github.com/fastapi/sqlmodel", "file_path": "docs_src/tutorial/automatic_id_none_refresh/tutorial002.py", "language": "python", "complexity": "medium", "star_count": 16410, "quality_score": 0.92, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 3, "class_count": 1, "line_count": 83}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "database_integration", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create database model with ORM relationships and queries with SQLAlchemy/Django ORM patterns\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from sqlmodel import Field, Session, SQLModel, create_engine\n\n\nclass Hero(SQLModel, table=True):\n    id: int | None = Field(default=None, primary_key=True)\n    name: str\n    secret_name: str\n    age: int | None = None\n\n\nsqlite_file_name = \"database.db\"\nsqlite_url = f\"sqlite:///{sqlite_file_name}\"\n\nengine = create_engine(sqlite_url, echo=True)\n\n\ndef create_db_and_tables():\n    SQLModel.metadata.create_all(engine)\n\n\ndef create_heroes():\n    hero_1 = Hero(name=\"Deadpond\", secret_name=\"Dive Wilson\")  # (1)!\n    hero_2 = Hero(name=\"Spider-Boy\", secret_name=\"Pedro Parqueador\")  # (2)!\n    hero_3 = Hero(name=\"Rusty-Man\", secret_name=\"Tommy Sharp\", age=48)  # (3)!\n\n    print(\"Before interacting with the database\")  # (4)!\n    print(\"Hero 1:\", hero_1)  # (5)!\n    print(\"Hero 2:\", hero_2)  # (6)!\n    print(\"Hero 3:\", hero_3)  # (7)!\n\n    with Session(engine) as session:  # (8)!\n        session.add(hero_1)  # (9)!\n        session.add(hero_2)  # (10)!\n        session.add(hero_3)  # (11)!\n\n        print(\"After adding to the session\")  # (12)!\n        print(\"Hero 1:\", hero_1)  # (13)!\n        print(\"Hero 2:\", hero_2)  # (14)!\n        print(\"Hero 3:\", hero_3)  # (15)!\n\n        session.commit()  # (16)!\n\n        print(\"After committing the session\")  # (17)!\n        print(\"Hero 1:\", hero_1)  # (18)!\n        print(\"Hero 2:\", hero_2)  # (19)!\n        print(\"Hero 3:\", hero_3)  # (20)!\n\n        print(\"After committing the session, show IDs\")  # (21)!\n        print(\"Hero 1 ID:\", hero_1.id)  # (22)!\n        print(\"Hero 2 ID:\", hero_2.id)  # (23)!\n        print(\"Hero 3 ID:\", hero_3.id)  # (24)!\n\n        print(\"After committing the session, show names\")  # (25)!\n        print(\"Hero 1 name:\", hero_1.name)  # (26)!\n        print(\"Hero 2 name:\", hero_2.name)  # (27)!\n        print(\"Hero 3 name:\", hero_3.name)  # (28)!\n\n        session.refresh(hero_1)  # (29)!\n        session.refresh(hero_2)  # (30)!\n        session.refresh(hero_3)  # (31)!\n\n        print(\"After refreshing the heroes\")  # (32)!\n        print(\"Hero 1:\", hero_1)  # (33)!\n        print(\"Hero 2:\", hero_2)  # (34)!\n        print(\"Hero 3:\", hero_3)  # (35)!\n    # (36)!\n\n    print(\"After the session closes\")  # (37)!\n    print(\"Hero 1:\", hero_1)  # (38)!\n    print(\"Hero 2:\", hero_2)  # (39)!\n    print(\"Hero 3:\", hero_3)  # (40)!\n\n\ndef main():\n    create_db_and_tables()\n    create_heroes()\n\n\nif __name__ == \"__main__\":\n    main()\n", "metadata": {"source": "github.com/fastapi/sqlmodel", "file_path": "docs_src/tutorial/automatic_id_none_refresh/tutorial002_py310.py", "language": "python", "complexity": "medium", "star_count": 16410, "quality_score": 0.92, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 3, "class_count": 1, "line_count": 81}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "database_integration", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create database model with ORM relationships and queries\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from sqlmodel import Session\n\nfrom .database import create_db_and_tables, engine\nfrom .models import Hero, Team\n\n\ndef create_heroes():\n    with Session(engine) as session:\n        team_z_force = Team(name=\"Z-Force\", headquarters=\"Sister Margaret's Bar\")\n\n        hero_deadpond = Hero(\n            name=\"Deadpond\", secret_name=\"Dive Wilson\", team=team_z_force\n        )\n        session.add(hero_deadpond)\n        session.commit()\n\n        session.refresh(hero_deadpond)\n\n        print(\"Created hero:\", hero_deadpond)\n        print(\"Hero's team:\", hero_deadpond.team)\n\n\ndef main():\n    create_db_and_tables()\n    create_heroes()\n\n\nif __name__ == \"__main__\":\n    main()\n", "metadata": {"source": "github.com/fastapi/sqlmodel", "file_path": "docs_src/tutorial/code_structure/tutorial001/app.py", "language": "python", "complexity": "simple", "star_count": 16410, "quality_score": 0.89, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": false, "has_testing": false, "has_logging": false, "function_count": 2, "class_count": 0, "line_count": 30}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "database_integration", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create database model with ORM relationships and queries\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from sqlmodel import SQLModel, create_engine\n\nsqlite_file_name = \"database.db\"\nsqlite_url = f\"sqlite:///{sqlite_file_name}\"\n\nengine = create_engine(sqlite_url)\n\n\ndef create_db_and_tables():\n    SQLModel.metadata.create_all(engine)\n", "metadata": {"source": "github.com/fastapi/sqlmodel", "file_path": "docs_src/tutorial/code_structure/tutorial001/database.py", "language": "python", "complexity": "simple", "star_count": 16410, "quality_score": 0.89, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": false, "has_testing": false, "has_logging": false, "function_count": 1, "class_count": 0, "line_count": 11}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "database_integration", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create database model with ORM relationships and queries with SQLAlchemy/Django ORM patterns\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from typing import List, Optional\n\nfrom sqlmodel import Field, Relationship, SQLModel\n\n\nclass Team(SQLModel, table=True):\n    id: Optional[int] = Field(default=None, primary_key=True)\n    name: str = Field(index=True)\n    headquarters: str\n\n    heroes: List[\"Hero\"] = Relationship(back_populates=\"team\")\n\n\nclass Hero(SQLModel, table=True):\n    id: Optional[int] = Field(default=None, primary_key=True)\n    name: str = Field(index=True)\n    secret_name: str\n    age: Optional[int] = Field(default=None, index=True)\n\n    team_id: Optional[int] = Field(default=None, foreign_key=\"team.id\")\n    team: Optional[Team] = Relationship(back_populates=\"heroes\")\n", "metadata": {"source": "github.com/fastapi/sqlmodel", "file_path": "docs_src/tutorial/code_structure/tutorial001/models.py", "language": "python", "complexity": "medium", "star_count": 16410, "quality_score": 0.89, "features": {"has_classes": true, "has_functions": false, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 0, "class_count": 2, "line_count": 22}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "database_integration", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create database model with ORM relationships and queries\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from sqlmodel import Session\n\nfrom .database import create_db_and_tables, engine\nfrom .models import Hero, Team\n\n\ndef create_heroes():\n    with Session(engine) as session:\n        team_z_force = Team(name=\"Z-Force\", headquarters=\"Sister Margaret's Bar\")\n\n        hero_deadpond = Hero(\n            name=\"Deadpond\", secret_name=\"Dive Wilson\", team=team_z_force\n        )\n        session.add(hero_deadpond)\n        session.commit()\n\n        session.refresh(hero_deadpond)\n\n        print(\"Created hero:\", hero_deadpond)\n        print(\"Hero's team:\", hero_deadpond.team)\n\n\ndef main():\n    create_db_and_tables()\n    create_heroes()\n\n\nif __name__ == \"__main__\":\n    main()\n", "metadata": {"source": "github.com/fastapi/sqlmodel", "file_path": "docs_src/tutorial/code_structure/tutorial001_py310/app.py", "language": "python", "complexity": "simple", "star_count": 16410, "quality_score": 0.89, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": false, "has_testing": false, "has_logging": false, "function_count": 2, "class_count": 0, "line_count": 30}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "database_integration", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create database model with ORM relationships and queries\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from sqlmodel import SQLModel, create_engine\n\nsqlite_file_name = \"database.db\"\nsqlite_url = f\"sqlite:///{sqlite_file_name}\"\n\nengine = create_engine(sqlite_url)\n\n\ndef create_db_and_tables():\n    SQLModel.metadata.create_all(engine)\n", "metadata": {"source": "github.com/fastapi/sqlmodel", "file_path": "docs_src/tutorial/code_structure/tutorial001_py310/database.py", "language": "python", "complexity": "simple", "star_count": 16410, "quality_score": 0.89, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": false, "has_testing": false, "has_logging": false, "function_count": 1, "class_count": 0, "line_count": 11}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "database_integration", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate module integration for Create database model with ORM relationships and queries\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import sys\n\nfrom packaging import tags\n\nto_check = \"--\"\nfound = False\nif len(sys.argv) > 1:\n    to_check = sys.argv[1]\n    for t in tags.sys_tags():\n        start = \"-\".join(str(t).split(\"-\")[:2])\n        if to_check.lower() == start:\n            print(\n                \"Wheel tag {0} matches installed version {1}.\".format(\n                    to_check, t\n                )\n            )\n            found = True\n            break\nif not found:\n    print(\n        \"Wheel tag {0} not found in installed version tags {1}.\".format(\n            to_check, [str(t) for t in tags.sys_tags()]\n        )\n    )\n    exit(1)\n", "metadata": {"source": "github.com/sqlalchemy/sqlalchemy", "file_path": ".github/workflows/scripts/can_install.py", "language": "python", "complexity": "simple", "star_count": 10679, "quality_score": 0.8600000000000001, "features": {"has_classes": false, "has_functions": false, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": false, "has_testing": false, "has_logging": false, "function_count": 0, "class_count": 0, "line_count": 26}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "database_integration", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create database model with ORM relationships and queries\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# -*- coding: utf-8 -*-\n#\n# SQLAlchemy documentation build configuration file, created by\n# sphinx-quickstart on Wed Nov 26 19:50:10 2008.\n#\n# This file is execfile()d with the current directory set to its containing\n# dir.\n#\n# Note that not all possible configuration values are present in this\n# autogenerated file.\n#\n# All configuration values have a default; values that are commented out\n# serve to show the default.\n\nimport os\nimport sys\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\nsys.path.insert(0, os.path.abspath(\"../../lib\"))\nsys.path.insert(0, os.path.abspath(\"../..\"))  # examples\n\n# was never needed, does not work as of python 3.12 due to conflicts\n# sys.path.insert(0, os.path.abspath(\".\"))\n\n\n# -- General configuration --------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\nneeds_sphinx = \"5.0.1\"\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named 'sphinx.ext.*') or your custom ones.\n\nextensions = [\n    \"sphinx.ext.autodoc\",\n    \"zzzeeksphinx\",\n    \"changelog\",\n    \"sphinx_paramlinks\",\n    \"sphinx_copybutton\",\n]\nneeds_extensions = {\"zzzeeksphinx\": \"1.6.1\"}\n\n# Add any paths that contain templates here, relative to this directory.\n# not sure why abspath() is needed here, some users\n# have reported this.\ntemplates_path = [os.path.abspath(\"templates\")]\n\n# https://sphinx-copybutton.readthedocs.io/en/latest/use.html#strip-and-configure-input-prompts-for-code-cells\ncopybutton_prompt_text = (\n    r\">>> |\\.\\.\\. |\\$ |In \\[\\d*\\]: | {2,5}\\.\\.\\.: | {5,8}: \"\n)\ncopybutton_prompt_is_regexp = True\n\n# workaround\n# https://sphinx-copybutton-exclude-issue.readthedocs.io/en/v0.5.1-go/\n# https://github.com/executablebooks/sphinx-copybutton/issues/185\n# while we're at it, add our SQL css classes to also not be copied\ncopybutton_exclude = \".linenos .show_sql .show_sql_print .popup_sql\"\n\nnitpicky = False\n\n# The suffix of source filenames.\nsource_suffix = \".rst\"\n\n\n# section names used by the changelog extension.\nchangelog_sections = [\n    \"general\",\n    \"platform\",\n    \"orm\",\n    \"orm declarative\",\n    \"orm querying\",\n    \"orm configuration\",\n    \"orm extensions\",\n    \"examples\",\n    \"engine\",\n    \"sql\",\n    \"schema\",\n    \"extensions\",\n    \"typing\",\n    \"mypy\",\n    \"asyncio\",\n    \"postgresql\",\n    \"mysql\",\n    \"mariadb\",\n    \"sqlite\",\n    \"mssql\",\n    \"oracle\",\n    \"tests\",\n]\n# tags to sort on inside of sections\nchangelog_inner_tag_sort = [\n    \"feature\",\n    \"improvement\",\n    \"usecase\",\n    \"change\",\n    \"changed\",\n    \"performance\",\n    \"bug\",\n    \"deprecated\",\n    \"removed\",\n    \"renamed\",\n    \"moved\",\n]\n\n\n# how to render changelog links\nchangelog_render_ticket = \"https://www.sqlalchemy.org/trac/ticket/%s\"\n\nchangelog_render_pullreq = {\n    \"default\": \"https://github.com/sqlalchemy/sqlalchemy/pull/%s\",\n    \"github\": \"https://github.com/sqlalchemy/sqlalchemy/pull/%s\",\n}\n\nchangelog_render_changeset = \"https://www.sqlalchemy.org/trac/changeset/%s\"\n\nexclude_patterns = [\"build\", \"**/unreleased*/*\", \"**/*_include.rst\", \".venv\"]\n\nautodoc_class_signature = \"separated\"\n\nautodoc_default_options = {\n    \"exclude-members\": \"__new__\",\n    \"no-undoc-members\": True,\n}\n\n# enable \"annotation\" indicator.  doesn't actually use this\n# link right now, it's just a png image\nzzzeeksphinx_annotation_key = \"glossary#annotated-example\"\n\n# to use this, we need:\n# 1. fix sphinx-paramlinks to work with \"description\" typing\n# 2. we need a huge autodoc_type_aliases map as we have extensive type aliasing\n# present, and typing is largely not very legible w/ the aliases\n# autodoc_typehints = \"description\"\n# autodoc_typehints_format = \"short\"\n# autodoc_typehints_description_target = \"documented\"\n\n# zzzeeksphinx makes these conversions when it is rendering the\n# docstrings classes, methods, and functions within the scope of\n# Sphinx autodoc\nautodocmods_convert_modname = {\n    \"sqlalchemy.sql.sqltypes\": \"sqlalchemy.types\",\n    \"sqlalchemy.sql.type_api\": \"sqlalchemy.types\",\n    \"sqlalchemy.sql.schema\": \"sqlalchemy.schema\",\n    \"sqlalchemy.sql.elements\": \"sqlalchemy.sql.expression\",\n    \"sqlalchemy.sql.selectable\": \"sqlalchemy.sql.expression\",\n    \"sqlalchemy.sql.dml\": \"sqlalchemy.sql.expression\",\n    \"sqlalchemy.sql.ddl\": \"sqlalchemy.schema\",\n    \"sqlalchemy.sql.base\": \"sqlalchemy.sql.expression\",\n    \"sqlalchemy.sql.operators\": \"sqlalchemy.sql.expression\",\n    \"sqlalchemy.event.base\": \"sqlalchemy.event\",\n    \"sqlalchemy.engine.base\": \"sqlalchemy.engine\",\n    \"sqlalchemy.engine.url\": \"sqlalchemy.engine\",\n    \"sqlalchemy.engine.row\": \"sqlalchemy.engine\",\n    \"sqlalchemy.engine.cursor\": \"sqlalchemy.engine\",\n    \"sqlalchemy.engine.result\": \"sqlalchemy.engine\",\n    \"sqlalchemy.ext.asyncio.result\": \"sqlalchemy.ext.asyncio\",\n    \"sqlalchemy.ext.asyncio.engine\": \"sqlalchemy.ext.asyncio\",\n    \"sqlalchemy.ext.asyncio.session\": \"sqlalchemy.ext.asyncio\",\n    \"sqlalchemy.util._collections\": \"sqlalchemy.util\",\n    \"sqlalchemy.orm.attributes\": \"sqlalchemy.orm\",\n    \"sqlalchemy.orm.relationships\": \"sqlalchemy.orm\",\n    \"sqlalchemy.orm.interfaces\": \"sqlalchemy.orm\",\n    \"sqlalchemy.orm.query\": \"sqlalchemy.orm\",\n    \"sqlalchemy.orm.util\": \"sqlalchemy.orm\",\n}\n\n# on the referencing side, a newer zzzeeksphinx extension\n# applies shorthand symbols to references so that we can have short\n# names that are still using absolute references.\nzzzeeksphinx_module_prefixes = {\n    \"_sa\": \"sqlalchemy\",\n    \"_engine\": \"sqlalchemy.engine\",\n    \"_url\": \"sqlalchemy.engine\",\n    \"_result\": \"sqlalchemy.engine\",\n    \"_row\": \"sqlalchemy.engine\",\n    \"_schema\": \"sqlalchemy.schema\",\n    \"_types\": \"sqlalchemy.types\",\n    \"_sqltypes\": \"sqlalchemy.types\",\n    \"_asyncio\": \"sqlalchemy.ext.asyncio\",\n    \"_expression\": \"sqlalchemy.sql.expression\",\n    \"_sql\": \"sqlalchemy.sql.expression\",\n    \"_dml\": \"sqlalchemy.sql.expression\",\n    \"_ddl\": \"sqlalchemy.schema\",\n    \"_functions\": \"sqlalchemy.sql.functions\",\n    \"_pool\": \"sqlalchemy.pool\",\n    # base event API, like listen() etc.\n    \"_event\": \"sqlalchemy.event\",\n    # core events like PoolEvents, ConnectionEvents\n    \"_events\": \"sqlalchemy.events\",\n    # note Core events are linked as sqlalchemy.event.<cls>\n    # ORM is sqlalchemy.orm.<cls>.\n    \"_ormevent\": \"sqlalchemy.orm\",\n    \"_ormevents\": \"sqlalchemy.orm\",\n    \"_scoping\": \"sqlalchemy.orm.scoping\",\n    \"_exc\": \"sqlalchemy.exc\",\n    \"_reflection\": \"sqlalchemy.engine.reflection\",\n    \"_orm\": \"sqlalchemy.orm\",\n    \"_query\": \"sqlalchemy.orm\",\n    \"_ormexc\": \"sqlalchemy.orm.exc\",\n    \"_roles\": \"sqlalchemy.sql.roles\",\n    \"_baked\": \"sqlalchemy.ext.baked\",\n    \"_horizontal\": \"sqlalchemy.ext.horizontal_shard\",\n    \"_associationproxy\": \"sqlalchemy.ext.associationproxy\",\n    \"_automap\": \"sqlalchemy.ext.automap\",\n    \"_hybrid\": \"sqlalchemy.ext.hybrid\",\n    \"_compilerext\": \"sqlalchemy.ext.compiler\",\n    \"_mutable\": \"sqlalchemy.ext.mutable\",\n    \"_declarative\": \"sqlalchemy.ext.declarative\",\n    \"_future\": \"sqlalchemy.future\",\n    \"_futureorm\": \"sqlalchemy.future.orm\",\n    \"_postgresql\": \"sqlalchemy.dialects.postgresql\",\n    \"_mysql\": \"sqlalchemy.dialects.mysql\",\n    \"_mssql\": \"sqlalchemy.dialects.mssql\",\n    \"_oracle\": \"sqlalchemy.dialects.oracle\",\n    \"_sqlite\": \"sqlalchemy.dialects.sqlite\",\n    \"_util\": \"sqlalchemy.util\",\n}\n\n\n# The encoding of source files.\n# source_encoding = 'utf-8-sig'\n\n# The master toctree document.\nmaster_doc = \"contents\"\n\n# General information about the project.\nproject = \"SQLAlchemy\"\ncopyright = \"2007-2025, the SQLAlchemy authors and contributors\"  # noqa\n\n# The version info for the project you're documenting, acts as replacement for\n# |version| and |release|, also used in various other places throughout the\n# built documents.\n#\n# The short X.Y version.\nversion = \"2.1\"\n# The full version, including alpha/beta/rc tags.\nrelease = \"2.1.0b1\"\n\nrelease_date = None\n\nsite_base = os.environ.get(\"RTD_SITE_BASE\", \"https://www.sqlalchemy.org\")\nsite_adapter_template = \"docs_adapter.mako\"\nsite_adapter_py = \"docs_adapter.py\"\n\n# arbitrary number recognized by builders.py, incrementing this\n# will force a rebuild\nbuild_number = \"3\"\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n# language = None\n\n# There are two options for replacing |today|: either, you set today to some\n# non-false value, then it is used:\n# today = ''\n# Else, today_fmt is used as the format for a strftime call.\n# today_fmt = '%B %d, %Y'\n\n# The reST default role (used for this markup: `text`) to use for all\n# documents.\n# default_role = None\n\n# If true, '()' will be appended to :func: etc. cross-reference text.\n# add_function_parentheses = True\n\n# If true, the current module name will be prepended to all description\n# unit titles (such as .. function::).\n# add_module_names = True\n\n# If true, sectionauthor and moduleauthor directives will be shown in the\n# output. They are ignored by default.\n# show_authors = False\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = \"sphinx\"\n\n# A list of ignored prefixes for module index sorting.\n# modindex_common_prefix = []\n\n# have the \"gettext\" build generate .pot for each individual\n# .rst\ngettext_compact = False\n\n# -- Options for HTML output -------------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\nhtml_theme = \"zzzeeksphinx\"\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\n# html_theme_options = {}\n\n# Add any paths that contain custom themes here, relative to this directory.\n# html_theme_path = []\n\n# The style sheet to use for HTML and HTML Help pages. A file of that name\n# must exist either in Sphinx' static/ path, or in one of the custom paths\n# given in html_static_path.\nhtml_style = \"default.css\"\n\n# The name for this set of Sphinx documents.  If None, it defaults to\n# \"<project> v<release> documentation\".\nhtml_title = \"%s %s Documentation\" % (project, version)\n\n# A shorter title for the navigation bar.  Default is the same as html_title.\n# html_short_title = None\n\n# The name of an image file (relative to this directory) to place at the top\n# of the sidebar.\n# html_logo = None\n\n# The name of an image file (within the static path) to use as favicon of the\n# docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32\n# pixels large.\n# html_favicon = None\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named \"default.css\" will overwrite the builtin \"default.css\".\nhtml_static_path = []\n\n# If not '', a 'Last updated on:' timestamp is inserted at every page bottom,\n# using the given strftime format.\nhtml_last_updated_fmt = \"%m/%d/%Y %H:%M:%S\"\n\n# If true, SmartyPants will be used to convert quotes and dashes to\n# typographically correct entities.\n# html_use_smartypants = True\n\n# Custom sidebar templates, maps document names to template names.\n# html_sidebars = {}\n\n# Additional templates that should be rendered to pages, maps page names to\n# template names.\nhtml_additional_pages = {\"notfound\": \"notfound.html\"}\n\n# If false, no module index is generated.\nhtml_domain_indices = False\n\n# If false, no index is generated.\n# html_use_index = True\n\n# If true, the index is split into individual pages for each letter.\n# html_split_index = False\n\n# If true, the reST sources are included in the HTML build as _sources/<name>.\n# html_copy_source = True\nhtml_copy_source = False\n\n# If true, links to the reST sources are added to the pages.\n# html_show_sourcelink = True\nhtml_show_sourcelink = False\n\n# If true, \"Created using Sphinx\" is shown in the HTML footer. Default is True.\n# html_show_sphinx = True\n\n# If true, \"(C) Copyright ...\" is shown in the HTML footer. Default is True.\n# html_show_copyright = True\n\n# If true, an OpenSearch description file will be output, and all pages will\n# contain a <link> tag referring to it.  The value of this option must be the\n# base URL from which the finished HTML is served.\n# html_use_opensearch = ''\n\n# This is the file name suffix for HTML files (e.g. \".xhtml\").\n# html_file_suffix = None\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = \"SQLAlchemydoc\"\n\n# autoclass_content = 'both'\n\n# -- Options for LaTeX output ------------------------------------------------\n\n# The paper size ('letter' or 'a4').\n# latex_paper_size = 'letter'\n\n# The font size ('10pt', '11pt' or '12pt').\n# latex_font_size = '10pt'\n\n# Grouping the document tree into LaTeX files. List of tuples (source start\n# file, target name, title, author, documentclass [howto/manual]).\nlatex_documents = [\n    (\n        \"contents\",\n        \"sqlalchemy_%s.tex\" % release.replace(\".\", \"_\"),\n        \"SQLAlchemy Documentation\",\n        \"Mike Bayer\",\n        \"manual\",\n    )\n]\n\n# The name of an image file (relative to this directory) to place at the top of\n# the title page.\n# latex_logo = None\n\n# For \"manual\" documents, if this is true, then toplevel headings are parts,\n# not chapters.\n# latex_use_parts = False\n\n# If true, show page references after internal links.\n# latex_show_pagerefs = False\n\n# If true, show URL addresses after external links.\n# latex_show_urls = False\n\n# Additional stuff for the LaTeX preamble.\n# sets TOC depth to 2.\nlatex_preamble = r\"\\setcounter{tocdepth}{3}\"\n\n# Documents to append as an appendix to all manuals.\n# latex_appendices = []\n\n# If false, no module index is generated.\n# latex_domain_indices = True\n\n# latex_elements = {\n#    'papersize': 'letterpaper',\n#    'pointsize': '10pt',\n# }\n\n# -- Options for manual page output ------------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [\n    (\n        \"index\",\n        \"sqlalchemy\",\n        \"SQLAlchemy Documentation\",\n        [\"SQLAlchemy authors\"],\n        1,\n    )\n]\n\n\n# -- Options for Epub output -------------------------------------------------\n\n# Bibliographic Dublin Core info.\nepub_title = \"SQLAlchemy\"\nepub_author = \"SQLAlchemy authors\"\nepub_publisher = \"SQLAlchemy authors\"\nepub_copyright = \"2007-2015, SQLAlchemy authors\"\n\n# The language of the text. It defaults to the language option\n# or en if the language is not set.\n# epub_language = ''\n\n# The scheme of the identifier. Typical schemes are ISBN or URL.\n# epub_scheme = ''\n\n# The unique identifier of the text. This can be a ISBN number\n# or the project homepage.\n# epub_identifier = ''\n\n# A unique identification for the text.\n# epub_uid = ''\n\n# HTML files that should be inserted before the pages created by sphinx.\n# The format is a list of tuples containing the path and title.\n# epub_pre_files = []\n\n# HTML files that should be inserted after the pages created by sphinx.\n# The format is a list of tuples containing the path and title.\n# epub_post_files = []\n\n# A list of files that should not be packed into the epub file.\n# epub_exclude_files = []\n\n# The depth of the table of contents in toc.ncx.\n# epub_tocdepth = 3\n\n# Allow duplicate toc entries.\n# epub_tocdup = True\n", "metadata": {"source": "github.com/sqlalchemy/sqlalchemy", "file_path": "doc/build/conf.py", "language": "python", "complexity": "complex", "star_count": 10679, "quality_score": 0.92, "features": {"has_classes": true, "has_functions": false, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": false, "has_logging": true, "function_count": 0, "class_count": 1, "line_count": 480}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "database_integration", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate configuration for Create database model with ORM relationships and queries\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"\nAn example of a dictionary-of-dictionaries structure mapped using\nan adjacency list model.\n\nE.g.::\n\n    node = TreeNode(\"rootnode\")\n    node.append(\"node1\")\n    node.append(\"node3\")\n    session.add(node)\n    session.commit()\n\n    dump_tree(node)\n\n.. autosource::\n\n\"\"\"\n", "metadata": {"source": "github.com/sqlalchemy/sqlalchemy", "file_path": "examples/adjacency_list/__init__.py", "language": "python", "complexity": "simple", "star_count": 10679, "quality_score": 0.8300000000000001, "features": {"has_classes": false, "has_functions": false, "has_async": false, "has_decorators": false, "has_imports": false, "has_error_handling": false, "has_documentation": true, "has_type_hints": false, "has_testing": false, "has_logging": false, "function_count": 0, "class_count": 0, "line_count": 18}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "database_integration", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create database model with ORM relationships and queries\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from __future__ import annotations\n\nfrom typing import Dict\nfrom typing import Optional\n\nfrom sqlalchemy import create_engine\nfrom sqlalchemy import ForeignKey\nfrom sqlalchemy import select\nfrom sqlalchemy.orm import DeclarativeBase\nfrom sqlalchemy.orm import Mapped\nfrom sqlalchemy.orm import mapped_column\nfrom sqlalchemy.orm import MappedAsDataclass\nfrom sqlalchemy.orm import relationship\nfrom sqlalchemy.orm import selectinload\nfrom sqlalchemy.orm import Session\nfrom sqlalchemy.orm.collections import attribute_keyed_dict\n\n\nclass Base(DeclarativeBase):\n    pass\n\n\nclass TreeNode(MappedAsDataclass, Base):\n    __tablename__ = \"tree\"\n\n    id: Mapped[int] = mapped_column(primary_key=True, init=False)\n    parent_id: Mapped[Optional[int]] = mapped_column(\n        ForeignKey(\"tree.id\"), init=False\n    )\n    name: Mapped[str]\n\n    children: Mapped[Dict[str, TreeNode]] = relationship(\n        cascade=\"all, delete-orphan\",\n        back_populates=\"parent\",\n        collection_class=attribute_keyed_dict(\"name\"),\n        init=False,\n        repr=False,\n    )\n\n    parent: Mapped[Optional[TreeNode]] = relationship(\n        back_populates=\"children\", remote_side=id, default=None\n    )\n\n    def dump(self, _indent: int = 0) -> str:\n        return (\n            \"   \" * _indent\n            + repr(self)\n            + \"\\n\"\n            + \"\".join([c.dump(_indent + 1) for c in self.children.values()])\n        )\n\n\nif __name__ == \"__main__\":\n    engine = create_engine(\"sqlite://\", echo=True)\n\n    print(\"Creating Tree Table:\")\n\n    Base.metadata.create_all(engine)\n\n    with Session(engine) as session:\n        node = TreeNode(\"rootnode\")\n        TreeNode(\"node1\", parent=node)\n        TreeNode(\"node3\", parent=node)\n\n        node2 = TreeNode(\"node2\")\n        TreeNode(\"subnode1\", parent=node2)\n        node.children[\"node2\"] = node2\n        TreeNode(\"subnode2\", parent=node.children[\"node2\"])\n\n        print(f\"Created new tree structure:\\n{node.dump()}\")\n\n        print(\"flush + commit:\")\n\n        session.add(node)\n        session.commit()\n\n        print(f\"Tree after save:\\n{node.dump()}\")\n\n        session.add_all(\n            [\n                TreeNode(\"node4\", parent=node),\n                TreeNode(\"subnode3\", parent=node.children[\"node4\"]),\n                TreeNode(\"subnode4\", parent=node.children[\"node4\"]),\n                TreeNode(\n                    \"subsubnode1\",\n                    parent=node.children[\"node4\"].children[\"subnode3\"],\n                ),\n            ]\n        )\n\n        # remove node1 from the parent, which will trigger a delete\n        # via the delete-orphan cascade.\n        del node.children[\"node1\"]\n\n        print(\"Removed node1.  flush + commit:\")\n        session.commit()\n\n        print(\"Tree after save, will unexpire all nodes:\\n\")\n        print(f\"{node.dump()}\")\n\n    with Session(engine) as session:\n        print(\n            \"Perform a full select of the root node, eagerly loading \"\n            \"up to a depth of four\"\n        )\n        node = session.scalars(\n            select(TreeNode)\n            .options(selectinload(TreeNode.children, recursion_depth=4))\n            .filter(TreeNode.name == \"rootnode\")\n        ).one()\n\n        print(f\"Full Tree:\\n{node.dump()}\")\n\n        print(\"Marking root node as deleted, flush + commit:\")\n\n        session.delete(node)\n        session.commit()\n", "metadata": {"source": "github.com/sqlalchemy/sqlalchemy", "file_path": "examples/adjacency_list/adjacency_list.py", "language": "python", "complexity": "medium", "star_count": 10679, "quality_score": 0.92, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 1, "class_count": 2, "line_count": 118}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "database_integration", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create database model with ORM relationships and queries\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"\nExamples illustrating the usage of the \"association object\" pattern,\nwhere an intermediary class mediates the relationship between two\nclasses that are associated in a many-to-many pattern.\n\n.. autosource::\n\n\"\"\"\n", "metadata": {"source": "github.com/sqlalchemy/sqlalchemy", "file_path": "examples/association/__init__.py", "language": "python", "complexity": "simple", "star_count": 10679, "quality_score": 0.6600000000000001, "features": {"has_classes": true, "has_functions": false, "has_async": false, "has_decorators": false, "has_imports": false, "has_error_handling": false, "has_documentation": true, "has_type_hints": false, "has_testing": false, "has_logging": false, "function_count": 0, "class_count": 1, "line_count": 9}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "database_integration", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create database model with ORM relationships and queries\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"Illustrate a many-to-many relationship between an\n\"Order\" and a collection of \"Item\" objects, associating a purchase price\nwith each via an association object called \"OrderItem\"\n\nThe association object pattern is a form of many-to-many which\nassociates additional data with each association between parent/child.\n\nThe example illustrates an \"order\", referencing a collection\nof \"items\", with a particular price paid associated with each \"item\".\n\n\"\"\"\n\nfrom __future__ import annotations\n\nfrom datetime import datetime\n\nfrom sqlalchemy import create_engine\nfrom sqlalchemy import ForeignKey\nfrom sqlalchemy import select\nfrom sqlalchemy import String\nfrom sqlalchemy.orm import DeclarativeBase\nfrom sqlalchemy.orm import Mapped\nfrom sqlalchemy.orm import mapped_column\nfrom sqlalchemy.orm import relationship\nfrom sqlalchemy.orm import Session\n\n\nclass Base(DeclarativeBase):\n    pass\n\n\nclass Order(Base):\n    __tablename__ = \"order\"\n\n    order_id: Mapped[int] = mapped_column(primary_key=True)\n    customer_name: Mapped[str] = mapped_column(String(30))\n    order_date: Mapped[datetime] = mapped_column(default=datetime.now())\n    order_items: Mapped[list[OrderItem]] = relationship(\n        cascade=\"all, delete-orphan\", backref=\"order\"\n    )\n\n    def __init__(self, customer_name: str) -> None:\n        self.customer_name = customer_name\n\n\nclass Item(Base):\n    __tablename__ = \"item\"\n    item_id: Mapped[int] = mapped_column(primary_key=True)\n    description: Mapped[str] = mapped_column(String(30))\n    price: Mapped[float]\n\n    def __init__(self, description: str, price: float) -> None:\n        self.description = description\n        self.price = price\n\n    def __repr__(self) -> str:\n        return \"Item({!r}, {!r})\".format(self.description, self.price)\n\n\nclass OrderItem(Base):\n    __tablename__ = \"orderitem\"\n    order_id: Mapped[int] = mapped_column(\n        ForeignKey(\"order.order_id\"), primary_key=True\n    )\n    item_id: Mapped[int] = mapped_column(\n        ForeignKey(\"item.item_id\"), primary_key=True\n    )\n    price: Mapped[float]\n\n    def __init__(self, item: Item, price: float | None = None) -> None:\n        self.item = item\n        self.price = price or item.price\n\n    item: Mapped[Item] = relationship(lazy=\"joined\")\n\n\nif __name__ == \"__main__\":\n    engine = create_engine(\"sqlite://\")\n    Base.metadata.create_all(engine)\n\n    with Session(engine) as session:\n\n        # create catalog\n        tshirt, mug, hat, crowbar = (\n            Item(\"SA T-Shirt\", 10.99),\n            Item(\"SA Mug\", 6.50),\n            Item(\"SA Hat\", 8.99),\n            Item(\"MySQL Crowbar\", 16.99),\n        )\n        session.add_all([tshirt, mug, hat, crowbar])\n        session.commit()\n\n        # create an order\n        order = Order(\"john smith\")\n\n        # add three OrderItem associations to the Order and save\n        order.order_items.append(OrderItem(mug))\n        order.order_items.append(OrderItem(crowbar, 10.99))\n        order.order_items.append(OrderItem(hat))\n        session.add(order)\n        session.commit()\n\n        # query the order, print items\n        order = session.scalars(\n            select(Order).filter_by(customer_name=\"john smith\")\n        ).one()\n        print(\n            [\n                (order_item.item.description, order_item.price)\n                for order_item in order.order_items\n            ]\n        )\n\n        # print customers who bought 'MySQL Crowbar' on sale\n        q = (\n            select(Order)\n            .join(OrderItem)\n            .join(Item)\n            .where(\n                Item.description == \"MySQL Crowbar\",\n                Item.price > OrderItem.price,\n            )\n        )\n\n        print([order.customer_name for order in session.scalars(q)])\n", "metadata": {"source": "github.com/sqlalchemy/sqlalchemy", "file_path": "examples/association/basic_association.py", "language": "python", "complexity": "complex", "star_count": 10679, "quality_score": 0.9500000000000001, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": true, "has_type_hints": true, "has_testing": false, "has_logging": true, "function_count": 4, "class_count": 4, "line_count": 126}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "database_integration", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create database model with ORM relationships and queries\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"An advanced association proxy example which\nillustrates nesting of association proxies to produce multi-level Python\ncollections, in this case a dictionary with string keys and sets of integers\nas values, which conceal the underlying mapped classes.\n\nThis is a three table model which represents a parent table referencing a\ndictionary of string keys and sets as values, where each set stores a\ncollection of integers. The association proxy extension is used to hide the\ndetails of this persistence. The dictionary also generates new collections\nupon access of a non-existent key, in the same manner as Python's\n\"collections.defaultdict\" object.\n\n\"\"\"\n\nfrom __future__ import annotations\n\nimport operator\nfrom typing import Mapping\n\nfrom sqlalchemy import create_engine\nfrom sqlalchemy import ForeignKey\nfrom sqlalchemy import select\nfrom sqlalchemy.ext.associationproxy import association_proxy\nfrom sqlalchemy.ext.associationproxy import AssociationProxy\nfrom sqlalchemy.orm import DeclarativeBase\nfrom sqlalchemy.orm import Mapped\nfrom sqlalchemy.orm import mapped_column\nfrom sqlalchemy.orm import relationship\nfrom sqlalchemy.orm import Session\nfrom sqlalchemy.orm.collections import KeyFuncDict\n\n\nclass Base(DeclarativeBase):\n    id: Mapped[int] = mapped_column(primary_key=True)\n\n\nclass GenDefaultCollection(KeyFuncDict[str, \"B\"]):\n    def __missing__(self, key: str) -> B:\n        self[key] = b = B(key)\n        return b\n\n\nclass A(Base):\n    __tablename__ = \"a\"\n    associations: Mapped[Mapping[str, B]] = relationship(\n        \"B\",\n        collection_class=lambda: GenDefaultCollection(\n            operator.attrgetter(\"key\")\n        ),\n    )\n\n    collections: AssociationProxy[dict[str, set[int]]] = association_proxy(\n        \"associations\", \"values\"\n    )\n    \"\"\"Bridge the association from 'associations' over to the 'values'\n    association proxy of B.\n    \"\"\"\n\n\nclass B(Base):\n    __tablename__ = \"b\"\n    a_id: Mapped[int] = mapped_column(ForeignKey(\"a.id\"))\n    elements: Mapped[set[C]] = relationship(\"C\", collection_class=set)\n    key: Mapped[str]\n\n    values: AssociationProxy[set[int]] = association_proxy(\"elements\", \"value\")\n    \"\"\"Bridge the association from 'elements' over to the\n    'value' element of C.\"\"\"\n\n    def __init__(self, key: str, values: set[int] | None = None) -> None:\n        self.key = key\n        if values:\n            self.values = values\n\n\nclass C(Base):\n    __tablename__ = \"c\"\n    b_id: Mapped[int] = mapped_column(ForeignKey(\"b.id\"))\n    value: Mapped[int]\n\n    def __init__(self, value: int) -> None:\n        self.value = value\n\n\nif __name__ == \"__main__\":\n    engine = create_engine(\"sqlite://\", echo=True)\n    Base.metadata.create_all(engine)\n    session = Session(engine)\n\n    # only \"A\" is referenced explicitly.  Using \"collections\",\n    # we deal with a dict of key/sets of integers directly.\n\n    session.add_all([A(collections={\"1\": {1, 2, 3}})])\n    session.commit()\n\n    a1 = session.scalars(select(A)).one()\n    print(a1.collections[\"1\"])\n    a1.collections[\"1\"].add(4)\n    session.commit()\n\n    a1.collections[\"2\"].update([7, 8, 9])\n    session.commit()\n\n    print(a1.collections[\"2\"])\n", "metadata": {"source": "github.com/sqlalchemy/sqlalchemy", "file_path": "examples/association/dict_of_sets_with_default.py", "language": "python", "complexity": "complex", "star_count": 10679, "quality_score": 0.9500000000000001, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": true, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 3, "class_count": 5, "line_count": 105}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "database_integration", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create database model with ORM relationships and queries\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"Same example as basic_association, adding in\nusage of :mod:`sqlalchemy.ext.associationproxy` to make explicit references\nto ``OrderItem`` optional.\n\n\n\"\"\"\n\nfrom __future__ import annotations\n\nfrom datetime import datetime\n\nfrom sqlalchemy import create_engine\nfrom sqlalchemy import ForeignKey\nfrom sqlalchemy import select\nfrom sqlalchemy import String\nfrom sqlalchemy.ext.associationproxy import association_proxy\nfrom sqlalchemy.ext.associationproxy import AssociationProxy\nfrom sqlalchemy.orm import DeclarativeBase\nfrom sqlalchemy.orm import Mapped\nfrom sqlalchemy.orm import mapped_column\nfrom sqlalchemy.orm import relationship\nfrom sqlalchemy.orm import Session\n\n\nclass Base(DeclarativeBase):\n    pass\n\n\nclass Order(Base):\n    __tablename__ = \"order\"\n\n    order_id: Mapped[int] = mapped_column(primary_key=True)\n    customer_name: Mapped[str] = mapped_column(String(30))\n    order_date: Mapped[datetime] = mapped_column(default=datetime.now())\n    order_items: Mapped[list[OrderItem]] = relationship(\n        cascade=\"all, delete-orphan\", backref=\"order\"\n    )\n    items: AssociationProxy[list[Item]] = association_proxy(\n        \"order_items\", \"item\"\n    )\n\n    def __init__(self, customer_name: str) -> None:\n        self.customer_name = customer_name\n\n\nclass Item(Base):\n    __tablename__ = \"item\"\n    item_id: Mapped[int] = mapped_column(primary_key=True)\n    description: Mapped[str] = mapped_column(String(30))\n    price: Mapped[float]\n\n    def __init__(self, description: str, price: float) -> None:\n        self.description = description\n        self.price = price\n\n    def __repr__(self) -> str:\n        return \"Item({!r}, {!r})\".format(self.description, self.price)\n\n\nclass OrderItem(Base):\n    __tablename__ = \"orderitem\"\n    order_id: Mapped[int] = mapped_column(\n        ForeignKey(\"order.order_id\"), primary_key=True\n    )\n    item_id: Mapped[int] = mapped_column(\n        ForeignKey(\"item.item_id\"), primary_key=True\n    )\n    price: Mapped[float]\n\n    item: Mapped[Item] = relationship(lazy=\"joined\")\n\n    def __init__(self, item: Item, price: float | None = None):\n        self.item = item\n        self.price = price or item.price\n\n\nif __name__ == \"__main__\":\n    engine = create_engine(\"sqlite://\")\n    Base.metadata.create_all(engine)\n\n    with Session(engine) as session:\n\n        # create catalog\n        tshirt, mug, hat, crowbar = (\n            Item(\"SA T-Shirt\", 10.99),\n            Item(\"SA Mug\", 6.50),\n            Item(\"SA Hat\", 8.99),\n            Item(\"MySQL Crowbar\", 16.99),\n        )\n        session.add_all([tshirt, mug, hat, crowbar])\n        session.commit()\n\n        # create an order\n        order = Order(\"john smith\")\n\n        # add items via the association proxy.\n        # the OrderItem is created automatically.\n        order.items.append(mug)\n        order.items.append(hat)\n\n        # add an OrderItem explicitly.\n        order.order_items.append(OrderItem(crowbar, 10.99))\n\n        session.add(order)\n        session.commit()\n\n        # query the order, print items\n        order = session.scalars(\n            select(Order).filter_by(customer_name=\"john smith\")\n        ).one()\n\n        # print items based on the OrderItem collection directly\n        print(\n            [\n                (assoc.item.description, assoc.price, assoc.item.price)\n                for assoc in order.order_items\n            ]\n        )\n\n        # print items based on the \"proxied\" items collection\n        print([(item.description, item.price) for item in order.items])\n\n        # print customers who bought 'MySQL Crowbar' on sale\n        orders_stmt = (\n            select(Order)\n            .join(OrderItem)\n            .join(Item)\n            .filter(Item.description == \"MySQL Crowbar\")\n            .filter(Item.price > OrderItem.price)\n        )\n        print([o.customer_name for o in session.scalars(orders_stmt)])\n", "metadata": {"source": "github.com/sqlalchemy/sqlalchemy", "file_path": "examples/association/proxied_association.py", "language": "python", "complexity": "complex", "star_count": 10679, "quality_score": 0.9500000000000001, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": true, "has_type_hints": true, "has_testing": false, "has_logging": true, "function_count": 4, "class_count": 4, "line_count": 132}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "database_integration", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create database model with ORM relationships and queries with asyncio coroutines\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"Illustrates use of the ``sqlalchemy.ext.asyncio.AsyncSession`` object\nfor asynchronous ORM use.\n\n\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\nimport datetime\nfrom typing import List\nfrom typing import Optional\n\nfrom sqlalchemy import ForeignKey\nfrom sqlalchemy import func\nfrom sqlalchemy.ext.asyncio import async_sessionmaker\nfrom sqlalchemy.ext.asyncio import AsyncAttrs\nfrom sqlalchemy.ext.asyncio import create_async_engine\nfrom sqlalchemy.future import select\nfrom sqlalchemy.orm import DeclarativeBase\nfrom sqlalchemy.orm import Mapped\nfrom sqlalchemy.orm import mapped_column\nfrom sqlalchemy.orm import relationship\nfrom sqlalchemy.orm import selectinload\n\n\nclass Base(AsyncAttrs, DeclarativeBase):\n    pass\n\n\nclass A(Base):\n    __tablename__ = \"a\"\n\n    id: Mapped[int] = mapped_column(primary_key=True)\n    data: Mapped[Optional[str]]\n    create_date: Mapped[datetime.datetime] = mapped_column(\n        server_default=func.now()\n    )\n    bs: Mapped[List[B]] = relationship()\n\n\nclass B(Base):\n    __tablename__ = \"b\"\n    id: Mapped[int] = mapped_column(primary_key=True)\n    a_id: Mapped[int] = mapped_column(ForeignKey(\"a.id\"))\n    data: Mapped[Optional[str]]\n\n\nasync def async_main():\n    \"\"\"Main program function.\"\"\"\n\n    engine = create_async_engine(\n        \"postgresql+asyncpg://scott:tiger@localhost/test\",\n        echo=True,\n    )\n\n    async with engine.begin() as conn:\n        await conn.run_sync(Base.metadata.drop_all)\n    async with engine.begin() as conn:\n        await conn.run_sync(Base.metadata.create_all)\n\n    # expire_on_commit=False will prevent attributes from being expired\n    # after commit.\n    async_session = async_sessionmaker(engine, expire_on_commit=False)\n\n    async with async_session() as session:\n        async with session.begin():\n            session.add_all(\n                [\n                    A(bs=[B(), B()], data=\"a1\"),\n                    A(bs=[B()], data=\"a2\"),\n                    A(bs=[B(), B()], data=\"a3\"),\n                ]\n            )\n\n        # for relationship loading, eager loading should be applied.\n        stmt = select(A).options(selectinload(A.bs))\n\n        # AsyncSession.execute() is used for 2.0 style ORM execution\n        # (same as the synchronous API).\n        result = await session.scalars(stmt)\n\n        # result is a buffered Result object.\n        for a1 in result:\n            print(a1)\n            print(f\"created at: {a1.create_date}\")\n            for b1 in a1.bs:\n                print(b1)\n\n        # for streaming ORM results, AsyncSession.stream() may be used.\n        result = await session.stream(stmt)\n\n        # result is a streaming AsyncResult object.\n        async for a1 in result.scalars():\n            print(a1)\n            for b1 in a1.bs:\n                print(b1)\n\n        result = await session.scalars(select(A).order_by(A.id))\n\n        a1 = result.first()\n\n        a1.data = \"new data\"\n\n        await session.commit()\n\n        # use the AsyncAttrs interface to accommodate for a lazy load\n        for b1 in await a1.awaitable_attrs.bs:\n            print(b1)\n\n\nasyncio.run(async_main())\n", "metadata": {"source": "github.com/sqlalchemy/sqlalchemy", "file_path": "examples/asyncio/async_orm.py", "language": "python", "complexity": "medium", "star_count": 10679, "quality_score": 0.9500000000000001, "features": {"has_classes": true, "has_functions": true, "has_async": true, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": true, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 1, "class_count": 3, "line_count": 112}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "database_integration", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create database model with ORM relationships and queries with asyncio coroutines\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"Illustrates using **write only relationships** for simpler handling\nof ORM collections under asyncio.\n\n\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\nimport datetime\nfrom typing import Optional\n\nfrom sqlalchemy import ForeignKey\nfrom sqlalchemy import func\nfrom sqlalchemy.ext.asyncio import async_sessionmaker\nfrom sqlalchemy.ext.asyncio import AsyncAttrs\nfrom sqlalchemy.ext.asyncio import create_async_engine\nfrom sqlalchemy.future import select\nfrom sqlalchemy.orm import DeclarativeBase\nfrom sqlalchemy.orm import Mapped\nfrom sqlalchemy.orm import mapped_column\nfrom sqlalchemy.orm import relationship\nfrom sqlalchemy.orm import WriteOnlyMapped\n\n\nclass Base(AsyncAttrs, DeclarativeBase):\n    pass\n\n\nclass A(Base):\n    __tablename__ = \"a\"\n\n    id: Mapped[int] = mapped_column(primary_key=True)\n    data: Mapped[Optional[str]]\n    create_date: Mapped[datetime.datetime] = mapped_column(\n        server_default=func.now()\n    )\n\n    # collection relationships are declared with WriteOnlyMapped.  There\n    # is no separate collection type\n    bs: WriteOnlyMapped[B] = relationship()\n\n\nclass B(Base):\n    __tablename__ = \"b\"\n    id: Mapped[int] = mapped_column(primary_key=True)\n    a_id: Mapped[int] = mapped_column(ForeignKey(\"a.id\"))\n    data: Mapped[Optional[str]]\n\n\nasync def async_main():\n    \"\"\"Main program function.\"\"\"\n\n    engine = create_async_engine(\n        \"postgresql+asyncpg://scott:tiger@localhost/test\",\n        echo=True,\n    )\n\n    async with engine.begin() as conn:\n        await conn.run_sync(Base.metadata.drop_all)\n    async with engine.begin() as conn:\n        await conn.run_sync(Base.metadata.create_all)\n\n    async_session = async_sessionmaker(engine, expire_on_commit=False)\n\n    async with async_session() as session:\n        async with session.begin():\n            # WriteOnlyMapped may be populated using any iterable,\n            # e.g. lists, sets, etc.\n            session.add_all(\n                [\n                    A(bs=[B(), B()], data=\"a1\"),\n                    A(bs=[B()], data=\"a2\"),\n                    A(bs=[B(), B()], data=\"a3\"),\n                ]\n            )\n\n        stmt = select(A)\n\n        result = await session.scalars(stmt)\n\n        for a1 in result:\n            print(a1)\n            print(f\"created at: {a1.create_date}\")\n\n            # to iterate a collection, emit a SELECT statement\n            for b1 in await session.scalars(a1.bs.select()):\n                print(b1)\n\n        result = await session.stream(stmt)\n\n        async for a1 in result.scalars():\n            print(a1)\n\n            # similar using \"streaming\" (server side cursors)\n            async for b1 in (await session.stream(a1.bs.select())).scalars():\n                print(b1)\n\n        await session.commit()\n        result = await session.scalars(select(A).order_by(A.id))\n\n        a1 = result.first()\n\n        a1.data = \"new data\"\n\n\nasyncio.run(async_main())\n", "metadata": {"source": "github.com/sqlalchemy/sqlalchemy", "file_path": "examples/asyncio/async_orm_writeonly.py", "language": "python", "complexity": "medium", "star_count": 10679, "quality_score": 0.9500000000000001, "features": {"has_classes": true, "has_functions": true, "has_async": true, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": true, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 1, "class_count": 3, "line_count": 107}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "database_integration", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create database model with ORM relationships and queries with asyncio coroutines\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"Illustrates the asyncio engine / connection interface.\n\nIn this example, we have an async engine created by\n:func:`_engine.create_async_engine`.   We then use it using await\nwithin a coroutine.\n\n\"\"\"\n\nimport asyncio\n\nfrom sqlalchemy import Column\nfrom sqlalchemy import Integer\nfrom sqlalchemy import MetaData\nfrom sqlalchemy import String\nfrom sqlalchemy import Table\nfrom sqlalchemy.ext.asyncio import create_async_engine\n\n\nmeta = MetaData()\n\nt1 = Table(\n    \"t1\", meta, Column(\"id\", Integer, primary_key=True), Column(\"name\", String)\n)\n\n\nasync def async_main():\n    # engine is an instance of AsyncEngine\n    engine = create_async_engine(\n        \"postgresql+asyncpg://scott:tiger@localhost/test\",\n        echo=True,\n    )\n\n    # conn is an instance of AsyncConnection\n    async with engine.begin() as conn:\n        # to support SQLAlchemy DDL methods as well as legacy functions, the\n        # AsyncConnection.run_sync() awaitable method will pass a \"sync\"\n        # version of the AsyncConnection object to any synchronous method,\n        # where synchronous IO calls will be transparently translated for\n        # await.\n        await conn.run_sync(meta.drop_all)\n        await conn.run_sync(meta.create_all)\n\n        # for normal statement execution, a traditional \"await execute()\"\n        # pattern is used.\n        await conn.execute(\n            t1.insert(), [{\"name\": \"some name 1\"}, {\"name\": \"some name 2\"}]\n        )\n\n    async with engine.connect() as conn:\n        # the default result object is the\n        # sqlalchemy.engine.Result object\n        result = await conn.execute(t1.select())\n\n        # the results are buffered so no await call is necessary\n        # for this case.\n        print(result.fetchall())\n\n        # for a streaming result that buffers only segments of the\n        # result at time, the AsyncConnection.stream() method is used.\n        # this returns a sqlalchemy.ext.asyncio.AsyncResult object.\n        async_result = await conn.stream(t1.select())\n\n        # this object supports async iteration and awaitable\n        # versions of methods like .all(), fetchmany(), etc.\n        async for row in async_result:\n            print(row)\n\n\nasyncio.run(async_main())\n", "metadata": {"source": "github.com/sqlalchemy/sqlalchemy", "file_path": "examples/asyncio/basic.py", "language": "python", "complexity": "simple", "star_count": 10679, "quality_score": 0.92, "features": {"has_classes": false, "has_functions": true, "has_async": true, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": true, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 1, "class_count": 0, "line_count": 70}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "database_integration", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create database model with ORM relationships and queries with asyncio coroutines\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"\nIllustrates how to run many statements concurrently using ``asyncio.gather()``\nalong many asyncio database connections, merging ORM results into a single\n``AsyncSession``.\n\nNote that this pattern loses all transactional safety and is also not\nnecessarily any more performant than using a single Session, as it adds\nsignificant CPU-bound work both to maintain more database connections\nand sessions, as well as within the merging of results from external sessions\ninto one.\n\nPython is a CPU-intensive language even in trivial cases, so it is strongly\nrecommended that any workarounds for \"speed\" such as the one below are\ncarefully vetted to show that they do in fact improve performance vs a\ntraditional approach.\n\n\"\"\"\n\nimport asyncio\nimport random\n\nfrom sqlalchemy.ext.asyncio import async_sessionmaker\nfrom sqlalchemy.ext.asyncio import create_async_engine\nfrom sqlalchemy.future import select\nfrom sqlalchemy.orm import DeclarativeBase\nfrom sqlalchemy.orm import Mapped\nfrom sqlalchemy.orm import mapped_column\nfrom sqlalchemy.orm import merge_frozen_result\n\n\nclass Base(DeclarativeBase):\n    pass\n\n\nclass A(Base):\n    __tablename__ = \"a\"\n\n    id: Mapped[int] = mapped_column(primary_key=True)\n    data: Mapped[str]\n\n    def __repr__(self):\n        id_, data = self.id, self.data\n        return f\"A({id_=}, {data=})\"\n\n\nasync def run_out_of_band(async_sessionmaker, statement, merge_results=True):\n    \"\"\"run an ORM statement in a distinct session,\n    returning the frozen results\n    \"\"\"\n\n    async with async_sessionmaker() as oob_session:\n        # use AUTOCOMMIT for each connection to reduce transaction\n        # overhead / contention\n        await oob_session.connection(\n            execution_options={\"isolation_level\": \"AUTOCOMMIT\"}\n        )\n\n        result = await oob_session.execute(statement)\n\n        if merge_results:\n            return result.freeze()\n        else:\n            await result.close()\n\n\nasync def async_main():\n    engine = create_async_engine(\n        \"postgresql+asyncpg://scott:tiger@localhost/test\",\n        echo=True,\n    )\n\n    async with engine.begin() as conn:\n        await conn.run_sync(Base.metadata.drop_all)\n        await conn.run_sync(Base.metadata.create_all)\n\n    async_session = async_sessionmaker(engine, expire_on_commit=False)\n\n    async with async_session() as session, session.begin():\n        session.add_all([A(data=\"a_%d\" % i) for i in range(100)])\n\n    statements = [\n        select(A).where(A.data == \"a_%d\" % random.choice(range(100)))\n        for i in range(30)\n    ]\n\n    frozen_results = await asyncio.gather(\n        *(\n            run_out_of_band(async_session, statement)\n            for statement in statements\n        )\n    )\n    results = [\n        # merge_results means the ORM objects from the result\n        # will be merged back into the original session.\n        # load=False means we can use the objects directly without\n        # re-selecting them.  however this merge operation is still\n        # more expensive CPU-wise than a regular ORM load because the\n        # objects are copied into new instances\n        (\n            await session.run_sync(\n                merge_frozen_result, statement, result, load=False\n            )\n        )()\n        for statement, result in zip(statements, frozen_results)\n    ]\n\n    print(f\"results: {[r.all() for r in results]}\")\n\n\nasyncio.run(async_main())\n", "metadata": {"source": "github.com/sqlalchemy/sqlalchemy", "file_path": "examples/asyncio/gather_orm_statements.py", "language": "python", "complexity": "medium", "star_count": 10679, "quality_score": 0.9500000000000001, "features": {"has_classes": true, "has_functions": true, "has_async": true, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": true, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 3, "class_count": 2, "line_count": 111}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "database_integration", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create database model with ORM relationships and queries with asyncio coroutines\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"Illustrates use of the sqlalchemy.ext.asyncio.AsyncSession object\nfor asynchronous ORM use, including the optional run_sync() method.\n\n\n\"\"\"\n\nimport asyncio\n\nfrom sqlalchemy import Column\nfrom sqlalchemy import ForeignKey\nfrom sqlalchemy import Integer\nfrom sqlalchemy import String\nfrom sqlalchemy.ext.asyncio import AsyncAttrs\nfrom sqlalchemy.ext.asyncio import AsyncSession\nfrom sqlalchemy.ext.asyncio import create_async_engine\nfrom sqlalchemy.future import select\nfrom sqlalchemy.orm import DeclarativeBase\nfrom sqlalchemy.orm import relationship\n\n\nclass Base(AsyncAttrs, DeclarativeBase):\n    pass\n\n\nclass A(Base):\n    __tablename__ = \"a\"\n\n    id = Column(Integer, primary_key=True)\n    data = Column(String)\n    bs = relationship(\"B\")\n\n\nclass B(Base):\n    __tablename__ = \"b\"\n    id = Column(Integer, primary_key=True)\n    a_id = Column(ForeignKey(\"a.id\"))\n    data = Column(String)\n\n\ndef run_queries(session):\n    \"\"\"A function written in \"synchronous\" style that will be invoked\n    within the asyncio event loop.\n\n    The session object passed is a traditional orm.Session object with\n    synchronous interface.\n\n    \"\"\"\n\n    stmt = select(A)\n\n    result = session.execute(stmt)\n\n    for a1 in result.scalars():\n        print(a1)\n        # lazy loads\n        for b1 in a1.bs:\n            print(b1)\n\n    result = session.execute(select(A).order_by(A.id))\n\n    a1 = result.scalars().first()\n\n    a1.data = \"new data\"\n\n\nasync def async_main():\n    \"\"\"Main program function.\"\"\"\n\n    engine = create_async_engine(\n        \"postgresql+asyncpg://scott:tiger@localhost/test\",\n        echo=True,\n    )\n    async with engine.begin() as conn:\n        await conn.run_sync(Base.metadata.drop_all)\n        await conn.run_sync(Base.metadata.create_all)\n\n    async with AsyncSession(engine) as session:\n        async with session.begin():\n            session.add_all(\n                [\n                    A(bs=[B(), B()], data=\"a1\"),\n                    A(bs=[B()], data=\"a2\"),\n                    A(bs=[B(), B()], data=\"a3\"),\n                ]\n            )\n\n        # we have the option to run a function written in sync style\n        # within the AsyncSession.run_sync() method.  The function will\n        # be passed a synchronous-style Session object and the function\n        # can use traditional ORM patterns.\n        await session.run_sync(run_queries)\n\n        await session.commit()\n\n\nasyncio.run(async_main())\n", "metadata": {"source": "github.com/sqlalchemy/sqlalchemy", "file_path": "examples/asyncio/greenlet_orm.py", "language": "python", "complexity": "medium", "star_count": 10679, "quality_score": 0.9500000000000001, "features": {"has_classes": true, "has_functions": true, "has_async": true, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": true, "has_type_hints": false, "has_testing": false, "has_logging": false, "function_count": 2, "class_count": 3, "line_count": 97}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "database_integration", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create database model with ORM relationships and queries\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"Illustrates use of the :meth:`.AttributeEvents.init_scalar`\nevent, in conjunction with Core column defaults to provide\nORM objects that automatically produce the default value\nwhen an un-set attribute is accessed.\n\n\"\"\"\n\nimport datetime\n\nfrom sqlalchemy import Column\nfrom sqlalchemy import create_engine\nfrom sqlalchemy import DateTime\nfrom sqlalchemy import event\nfrom sqlalchemy import Integer\nfrom sqlalchemy.ext.declarative import declarative_base\nfrom sqlalchemy.orm import Session\n\n\ndef configure_listener(mapper, class_):\n    \"\"\"Establish attribute setters for every default-holding column on the\n    given mapper.\"\"\"\n\n    # iterate through ColumnProperty objects\n    for col_attr in mapper.column_attrs:\n        # look at the Column mapped by the ColumnProperty\n        # (we look at the first column in the less common case\n        # of a property mapped to multiple columns at once)\n        column = col_attr.columns[0]\n\n        # if the Column has a \"default\", set up a listener\n        if column.default is not None:\n            default_listener(col_attr, column.default)\n\n\ndef default_listener(col_attr, default):\n    \"\"\"Establish a default-setting listener.\n\n    Given a class attribute and a :class:`.DefaultGenerator` instance.\n    The default generator should be a :class:`.ColumnDefault` object with a\n    plain Python value or callable default; otherwise, the appropriate behavior\n    for SQL functions and defaults should be determined here by the\n    user integrating this feature.\n\n    \"\"\"\n\n    @event.listens_for(col_attr, \"init_scalar\", retval=True, propagate=True)\n    def init_scalar(target, value, dict_):\n        if default.is_callable:\n            # the callable of ColumnDefault always accepts a context\n            # argument; we can pass it as None here.\n            value = default.arg(None)\n        elif default.is_scalar:\n            value = default.arg\n        else:\n            # default is a Sequence, a SQL expression, server\n            # side default generator, or other non-Python-evaluable\n            # object.  The feature here can't easily support this.   This\n            # can be made to return None, rather than raising,\n            # or can procure a connection from an Engine\n            # or Session and actually run the SQL, if desired.\n            raise NotImplementedError(\n                \"Can't invoke pre-default for a SQL-level column default\"\n            )\n\n        # set the value in the given dict_; this won't emit any further\n        # attribute set events or create attribute \"history\", but the value\n        # will be used in the INSERT statement\n        dict_[col_attr.key] = value\n\n        # return the value as well\n        return value\n\n\nif __name__ == \"__main__\":\n    Base = declarative_base()\n\n    event.listen(Base, \"mapper_configured\", configure_listener, propagate=True)\n\n    class Widget(Base):\n        __tablename__ = \"widget\"\n\n        id = Column(Integer, primary_key=True)\n\n        radius = Column(Integer, default=30)\n        timestamp = Column(DateTime, default=datetime.datetime.now)\n\n    e = create_engine(\"sqlite://\", echo=True)\n    Base.metadata.create_all(e)\n\n    w1 = Widget()\n\n    # not persisted at all, default values are present the moment\n    # we access them\n    assert w1.radius == 30\n\n    # this line will invoke the datetime.now() function, and establish\n    # its return value upon the w1 instance, such that the\n    # Column-level default for the \"timestamp\" column will no longer fire\n    # off.\n    current_time = w1.timestamp\n    assert current_time > datetime.datetime.now() - datetime.timedelta(\n        seconds=5\n    )\n\n    # persist\n    sess = Session(e)\n    sess.add(w1)\n    sess.commit()\n\n    # data is persisted.  The timestamp is also the one we generated above;\n    # e.g. the default wasn't re-invoked later.\n    assert sess.query(Widget.radius, Widget.timestamp).first() == (\n        30,\n        current_time,\n    )\n", "metadata": {"source": "github.com/sqlalchemy/sqlalchemy", "file_path": "examples/custom_attributes/active_column_defaults.py", "language": "python", "complexity": "medium", "star_count": 10679, "quality_score": 1.0, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": true, "has_type_hints": false, "has_testing": true, "has_logging": false, "function_count": 3, "class_count": 2, "line_count": 116}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "database_integration", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create database model with ORM relationships and queries\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"Illustrates customized class instrumentation, using\nthe :mod:`sqlalchemy.ext.instrumentation` extension package.\n\nIn this example, mapped classes are modified to\nstore their state in a dictionary attached to an attribute\nnamed \"_goofy_dict\", instead of using __dict__.\nthis example illustrates how to replace SQLAlchemy's class\ndescriptors with a user-defined system.\n\n\n\"\"\"\n\nfrom sqlalchemy import Column\nfrom sqlalchemy import create_engine\nfrom sqlalchemy import ForeignKey\nfrom sqlalchemy import Integer\nfrom sqlalchemy import MetaData\nfrom sqlalchemy import Table\nfrom sqlalchemy import Text\nfrom sqlalchemy.ext.instrumentation import InstrumentationManager\nfrom sqlalchemy.orm import registry as _reg\nfrom sqlalchemy.orm import relationship\nfrom sqlalchemy.orm import Session\nfrom sqlalchemy.orm.attributes import del_attribute\nfrom sqlalchemy.orm.attributes import get_attribute\nfrom sqlalchemy.orm.attributes import set_attribute\nfrom sqlalchemy.orm.instrumentation import is_instrumented\n\n\nregistry = _reg()\n\n\nclass MyClassState(InstrumentationManager):\n    def get_instance_dict(self, class_, instance):\n        return instance._goofy_dict\n\n    def initialize_instance_dict(self, class_, instance):\n        instance.__dict__[\"_goofy_dict\"] = {}\n\n    def install_state(self, class_, instance, state):\n        instance.__dict__[\"_goofy_dict\"][\"state\"] = state\n\n    def state_getter(self, class_):\n        def find(instance):\n            return instance.__dict__[\"_goofy_dict\"][\"state\"]\n\n        return find\n\n\nclass MyClass:\n    __sa_instrumentation_manager__ = MyClassState\n\n    def __init__(self, **kwargs):\n        for k in kwargs:\n            setattr(self, k, kwargs[k])\n\n    def __getattr__(self, key):\n        if is_instrumented(self, key):\n            return get_attribute(self, key)\n        else:\n            try:\n                return self._goofy_dict[key]\n            except KeyError:\n                raise AttributeError(key)\n\n    def __setattr__(self, key, value):\n        if is_instrumented(self, key):\n            set_attribute(self, key, value)\n        else:\n            self._goofy_dict[key] = value\n\n    def __delattr__(self, key):\n        if is_instrumented(self, key):\n            del_attribute(self, key)\n        else:\n            del self._goofy_dict[key]\n\n\nif __name__ == \"__main__\":\n    engine = create_engine(\"sqlite://\")\n    meta = MetaData()\n\n    table1 = Table(\n        \"table1\",\n        meta,\n        Column(\"id\", Integer, primary_key=True),\n        Column(\"name\", Text),\n    )\n    table2 = Table(\n        \"table2\",\n        meta,\n        Column(\"id\", Integer, primary_key=True),\n        Column(\"name\", Text),\n        Column(\"t1id\", Integer, ForeignKey(\"table1.id\")),\n    )\n    meta.create_all(engine)\n\n    class A(MyClass):\n        pass\n\n    class B(MyClass):\n        pass\n\n    registry.map_imperatively(A, table1, properties={\"bs\": relationship(B)})\n\n    registry.map_imperatively(B, table2)\n\n    a1 = A(name=\"a1\", bs=[B(name=\"b1\"), B(name=\"b2\")])\n\n    assert a1.name == \"a1\"\n    assert a1.bs[0].name == \"b1\"\n\n    sess = Session(engine)\n    sess.add(a1)\n\n    sess.commit()\n\n    a1 = sess.query(A).get(a1.id)\n\n    assert a1.name == \"a1\"\n    assert a1.bs[0].name == \"b1\"\n\n    a1.bs.remove(a1.bs[0])\n\n    sess.commit()\n\n    a1 = sess.query(A).get(a1.id)\n    assert len(a1.bs) == 1\n", "metadata": {"source": "github.com/sqlalchemy/sqlalchemy", "file_path": "examples/custom_attributes/custom_management.py", "language": "python", "complexity": "complex", "star_count": 10679, "quality_score": 1.0, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": true, "has_documentation": true, "has_type_hints": true, "has_testing": true, "has_logging": false, "function_count": 9, "class_count": 5, "line_count": 129}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "database_integration", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate module integration for Create database model with ORM relationships and queries\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "#!/usr/bin/env python3\n# coding: utf-8\n\nimport json\nimport requests\nimport records\n\n# Fetch random user data from randomuser.me API\nresponse = requests.get('http://api.randomuser.me/0.6/?nat=us&results=10')\nuser_data = response.json()['results']\n\n# Database connection string\nDATABASE_URL = 'sqlite:///users.db'\n\n# Initialize the database\ndb = records.Database(DATABASE_URL)\n\n# Create the 'persons' table\ndb.query('DROP TABLE IF EXISTS persons')\ndb.query('CREATE TABLE persons (key INTEGER PRIMARY KEY, fname TEXT, lname TEXT, email TEXT)')\n\n# Insert user data into the 'persons' table\nfor record in user_data:\n    user = record['user']\n    key = user['registered']\n    fname = user['name']['first']\n    lname = user['name']['last']\n    email = user['email']\n    db.query(\n        'INSERT INTO persons (key, fname, lname, email) VALUES (:key, :fname, :lname, :email)',\n        key=key, fname=fname, lname=lname, email=email\n    )\n\n# Retrieve and print the contents of the 'persons' table as CSV\nrows = db.query('SELECT * FROM persons')\nprint(rows.export('csv'))\n", "metadata": {"source": "github.com/kennethreitz/records", "file_path": "examples/randomuser-sqlite.py", "language": "python", "complexity": "simple", "star_count": 7208, "quality_score": 0.76, "features": {"has_classes": false, "has_functions": false, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 0, "class_count": 0, "line_count": 37}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "database_integration", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create database model with ORM relationships and queries\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# -*- coding: utf-8 -*-\n\nimport os\nfrom sys import stdout\nfrom collections import OrderedDict\nfrom contextlib import contextmanager\nfrom inspect import isclass\n\nimport tablib\nfrom docopt import docopt\nfrom sqlalchemy import create_engine, exc, inspect, text\n\n\ndef isexception(obj):\n    \"\"\"Given an object, return a boolean indicating whether it is an instance\n    or subclass of :py:class:`Exception`.\n    \"\"\"\n    if isinstance(obj, Exception):\n        return True\n    if isclass(obj) and issubclass(obj, Exception):\n        return True\n    return False\n\n\nclass Record(object):\n    \"\"\"A row, from a query, from a database.\"\"\"\n\n    __slots__ = (\"_keys\", \"_values\")\n\n    def __init__(self, keys, values):\n        self._keys = keys\n        self._values = values\n\n        # Ensure that lengths match properly.\n        assert len(self._keys) == len(self._values)\n\n    def keys(self):\n        \"\"\"Returns the list of column names from the query.\"\"\"\n        return self._keys\n\n    def values(self):\n        \"\"\"Returns the list of values from the query.\"\"\"\n        return self._values\n\n    def __repr__(self):\n        return \"<Record {}>\".format(self.export(\"json\")[1:-1])\n\n    def __getitem__(self, key):\n        # Support for index-based lookup.\n        if isinstance(key, int):\n            return self.values()[key]\n\n        # Support for string-based lookup.\n        usekeys = self.keys()\n        if hasattr(\n            usekeys, \"_keys\"\n        ):  # sqlalchemy 2.x uses (result.RMKeyView which has wrapped _keys as list)\n            usekeys = usekeys._keys\n        if key in usekeys:\n            i = usekeys.index(key)\n            if usekeys.count(key) > 1:\n                raise KeyError(\"Record contains multiple '{}' fields.\".format(key))\n            return self.values()[i]\n\n        raise KeyError(\"Record contains no '{}' field.\".format(key))\n\n    def __getattr__(self, key):\n        try:\n            return self[key]\n        except KeyError as e:\n            raise AttributeError(e)\n\n    def __dir__(self):\n        standard = dir(super(Record, self))\n        # Merge standard attrs with generated ones (from column names).\n        return sorted(standard + [str(k) for k in self.keys()])\n\n    def get(self, key, default=None):\n        \"\"\"Returns the value for a given key, or default.\"\"\"\n        try:\n            return self[key]\n        except KeyError:\n            return default\n\n    def as_dict(self, ordered=False):\n        \"\"\"Returns the row as a dictionary, as ordered.\"\"\"\n        items = zip(self.keys(), self.values())\n\n        return OrderedDict(items) if ordered else dict(items)\n\n    @property\n    def dataset(self):\n        \"\"\"A Tablib Dataset containing the row.\"\"\"\n        data = tablib.Dataset()\n        data.headers = self.keys()\n\n        row = _reduce_datetimes(self.values())\n        data.append(row)\n\n        return data\n\n    def export(self, format, **kwargs):\n        \"\"\"Exports the row to the given format.\"\"\"\n        return self.dataset.export(format, **kwargs)\n\n\nclass RecordCollection(object):\n    \"\"\"A set of excellent Records from a query.\"\"\"\n\n    def __init__(self, rows):\n        self._rows = rows\n        self._all_rows = []\n        self.pending = True\n\n    def __repr__(self):\n        return \"<RecordCollection size={} pending={}>\".format(len(self), self.pending)\n\n    def __iter__(self):\n        \"\"\"Iterate over all rows, consuming the underlying generator\n        only when necessary.\"\"\"\n        i = 0\n        while True:\n            # Other code may have iterated between yields,\n            # so always check the cache.\n            if i < len(self):\n                yield self[i]\n            else:\n                # Throws StopIteration when done.\n                # Prevent StopIteration bubbling from generator, following https://www.python.org/dev/peps/pep-0479/\n                try:\n                    yield next(self)\n                except StopIteration:\n                    return\n            i += 1\n\n    def next(self):\n        return self.__next__()\n\n    def __next__(self):\n        try:\n            nextrow = next(self._rows)\n            self._all_rows.append(nextrow)\n            return nextrow\n        except StopIteration:\n            self.pending = False\n            raise StopIteration(\"RecordCollection contains no more rows.\")\n\n    def __getitem__(self, key):\n        is_int = isinstance(key, int)\n\n        # Convert RecordCollection[1] into slice.\n        if is_int:\n            key = slice(key, key + 1)\n\n        while key.stop is None or len(self) < key.stop:\n            try:\n                next(self)\n            except StopIteration:\n                break\n\n        rows = self._all_rows[key]\n        if is_int:\n            return rows[0]\n        else:\n            return RecordCollection(iter(rows))\n\n    def __len__(self):\n        return len(self._all_rows)\n\n    def export(self, format, **kwargs):\n        \"\"\"Export the RecordCollection to a given format (courtesy of Tablib).\"\"\"\n        return self.dataset.export(format, **kwargs)\n\n    @property\n    def dataset(self):\n        \"\"\"A Tablib Dataset representation of the RecordCollection.\"\"\"\n        # Create a new Tablib Dataset.\n        data = tablib.Dataset()\n\n        # If the RecordCollection is empty, just return the empty set\n        # Check number of rows by typecasting to list\n        if len(list(self)) == 0:\n            return data\n\n        # Set the column names as headers on Tablib Dataset.\n        first = self[0]\n\n        data.headers = first.keys()\n        for row in self.all():\n            row = _reduce_datetimes(row.values())\n            data.append(row)\n\n        return data\n\n    def all(self, as_dict=False, as_ordereddict=False):\n        \"\"\"Returns a list of all rows for the RecordCollection. If they haven't\n        been fetched yet, consume the iterator and cache the results.\"\"\"\n\n        # By calling list it calls the __iter__ method\n        rows = list(self)\n\n        if as_dict:\n            return [r.as_dict() for r in rows]\n        elif as_ordereddict:\n            return [r.as_dict(ordered=True) for r in rows]\n\n        return rows\n\n    def as_dict(self, ordered=False):\n        return self.all(as_dict=not (ordered), as_ordereddict=ordered)\n\n    def first(self, default=None, as_dict=False, as_ordereddict=False):\n        \"\"\"Returns a single record for the RecordCollection, or `default`. If\n        `default` is an instance or subclass of Exception, then raise it\n        instead of returning it.\"\"\"\n\n        # Try to get a record, or return/raise default.\n        try:\n            record = self[0]\n        except IndexError:\n            if isexception(default):\n                raise default\n            return default\n\n        # Cast and return.\n        if as_dict:\n            return record.as_dict()\n        elif as_ordereddict:\n            return record.as_dict(ordered=True)\n        else:\n            return record\n\n    def one(self, default=None, as_dict=False, as_ordereddict=False):\n        \"\"\"Returns a single record for the RecordCollection, ensuring that it\n        is the only record, or returns `default`. If `default` is an instance\n        or subclass of Exception, then raise it instead of returning it.\"\"\"\n\n        # Ensure that we don't have more than one row.\n        try:\n            self[1]\n        except IndexError:\n            return self.first(\n                default=default, as_dict=as_dict, as_ordereddict=as_ordereddict\n            )\n        else:\n            raise ValueError(\n                \"RecordCollection contained more than one row. \"\n                \"Expects only one row when using \"\n                \"RecordCollection.one\"\n            )\n\n    def scalar(self, default=None):\n        \"\"\"Returns the first column of the first row, or `default`.\"\"\"\n        row = self.one()\n        return row[0] if row else default\n\n\nclass Database(object):\n    \"\"\"A Database. Encapsulates a url and an SQLAlchemy engine with a pool of\n    connections.\n    \"\"\"\n\n    def __init__(self, db_url=None, **kwargs):\n        # If no db_url was provided, fallback to $DATABASE_URL.\n        self.db_url = db_url or os.environ.get(\"DATABASE_URL\")\n\n        if not self.db_url:\n            raise ValueError(\"You must provide a db_url.\")\n\n        # Create an engine.\n        self._engine = create_engine(self.db_url, **kwargs)\n        self.open = True\n\n    def get_engine(self):\n        # Return the engine if open\n        if not self.open:\n            raise exc.ResourceClosedError(\"Database closed.\")\n        return self._engine\n\n    def close(self):\n        \"\"\"Closes the Database.\"\"\"\n        self._engine.dispose()\n        self.open = False\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc, val, traceback):\n        self.close()\n\n    def __repr__(self):\n        return \"<Database open={}>\".format(self.open)\n\n    def get_table_names(self, internal=False, **kwargs):\n        \"\"\"Returns a list of table names for the connected database.\"\"\"\n\n        # Setup SQLAlchemy for Database inspection.\n        return inspect(self._engine).get_table_names(**kwargs)\n\n    def get_connection(self, close_with_result=False):\n        \"\"\"Get a connection to this Database. Connections are retrieved from a\n        pool.\n        \"\"\"\n        if not self.open:\n            raise exc.ResourceClosedError(\"Database closed.\")\n\n        return Connection(self._engine.connect(), close_with_result=close_with_result)\n\n    def query(self, query, fetchall=False, **params):\n        \"\"\"Executes the given SQL query against the Database. Parameters can,\n        optionally, be provided. Returns a RecordCollection, which can be\n        iterated over to get result rows as dictionaries.\n        \"\"\"\n        with self.get_connection(True) as conn:\n            return conn.query(query, fetchall, **params)\n\n    def bulk_query(self, query, *multiparams):\n        \"\"\"Bulk insert or update.\"\"\"\n\n        with self.get_connection() as conn:\n            conn.bulk_query(query, *multiparams)\n\n    def query_file(self, path, fetchall=False, **params):\n        \"\"\"Like Database.query, but takes a filename to load a query from.\"\"\"\n\n        with self.get_connection(True) as conn:\n            return conn.query_file(path, fetchall, **params)\n\n    def bulk_query_file(self, path, *multiparams):\n        \"\"\"Like Database.bulk_query, but takes a filename to load a query from.\"\"\"\n\n        with self.get_connection() as conn:\n            conn.bulk_query_file(path, *multiparams)\n\n    @contextmanager\n    def transaction(self):\n        \"\"\"A context manager for executing a transaction on this Database.\"\"\"\n\n        conn = self.get_connection()\n        tx = conn.transaction()\n        try:\n            yield conn\n            tx.commit()\n        except:\n            tx.rollback()\n        finally:\n            conn.close()\n\n\nclass Connection(object):\n    \"\"\"A Database connection.\"\"\"\n\n    def __init__(self, connection, close_with_result=False):\n        self._conn = connection\n        self.open = not connection.closed\n        self._close_with_result = close_with_result\n\n    def close(self):\n        # No need to close if this connection is used for a single result.\n        # The connection will close when the results are all consumed or GCed.\n        if not self._close_with_result:\n            self._conn.close()\n        self.open = False\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc, val, traceback):\n        self.close()\n\n    def __repr__(self):\n        return \"<Connection open={}>\".format(self.open)\n\n    def query(self, query, fetchall=False, **params):\n        \"\"\"Executes the given SQL query against the connected Database.\n        Parameters can, optionally, be provided. Returns a RecordCollection,\n        which can be iterated over to get result rows as dictionaries.\n        \"\"\"\n\n        # Execute the given query.\n        cursor = self._conn.execute(\n            text(query).bindparams(**params)\n        )  # TODO: PARAMS GO HERE\n\n        # Row-by-row Record generator.\n        row_gen = iter(Record([], []))\n\n        if cursor.returns_rows:\n            row_gen = (Record(cursor.keys(), row) for row in cursor)\n\n        # Convert psycopg2 results to RecordCollection.\n        results = RecordCollection(row_gen)\n\n        # Fetch all results if desired.\n        if fetchall:\n            results.all()\n\n        return results\n\n    def bulk_query(self, query, *multiparams):\n        \"\"\"Bulk insert or update.\"\"\"\n\n        self._conn.execute(text(query), *multiparams)\n\n    def query_file(self, path, fetchall=False, **params):\n        \"\"\"Like Connection.query, but takes a filename to load a query from.\"\"\"\n\n        # If path doesn't exists\n        if not os.path.exists(path):\n            raise IOError(\"File '{}' not found!\".format(path))\n\n        # If it's a directory\n        if os.path.isdir(path):\n            raise IOError(\"'{}' is a directory!\".format(path))\n\n        # Read the given .sql file into memory.\n        with open(path) as f:\n            query = f.read()\n\n        # Defer processing to self.query method.\n        return self.query(query=query, fetchall=fetchall, **params)\n\n    def bulk_query_file(self, path, *multiparams):\n        \"\"\"Like Connection.bulk_query, but takes a filename to load a query\n        from.\n        \"\"\"\n\n        # If path doesn't exists\n        if not os.path.exists(path):\n            raise IOError(\"File '{}'' not found!\".format(path))\n\n        # If it's a directory\n        if os.path.isdir(path):\n            raise IOError(\"'{}' is a directory!\".format(path))\n\n        # Read the given .sql file into memory.\n        with open(path) as f:\n            query = f.read()\n\n        self._conn.execute(text(query), *multiparams)\n\n    def transaction(self):\n        \"\"\"Returns a transaction object. Call ``commit`` or ``rollback``\n        on the returned object as appropriate.\"\"\"\n\n        return self._conn.begin()\n\n\ndef _reduce_datetimes(row):\n    \"\"\"Receives a row, converts datetimes to strings.\"\"\"\n\n    row = list(row)\n\n    for i, element in enumerate(row):\n        if hasattr(element, \"isoformat\"):\n            row[i] = element.isoformat()\n    return tuple(row)\n\n\ndef cli():\n    supported_formats = \"csv tsv json yaml html xls xlsx dbf latex ods\".split()\n    formats_lst = \", \".join(supported_formats)\n    cli_docs = \"\"\"Records: SQL for Humans™\nA Kenneth Reitz project.\n\nUsage:\n  records <query> [<format>] [<params>...] [--url=<url>]\n  records (-h | --help)\n\nOptions:\n  -h --help     Show this screen.\n  --url=<url>   The database URL to use. Defaults to $DATABASE_URL.\n\nSupported Formats:\n   %(formats_lst)s\n\n   Note: xls, xlsx, dbf, and ods formats are binary, and should only be\n         used with redirected output e.g. '$ records sql xls > sql.xls'.\n\nQuery Parameters:\n    Query parameters can be specified in key=value format, and injected\n    into your query in :key format e.g.:\n\n    $ records 'select * from repos where language ~= :lang' lang=python\n\nNotes:\n  - While you may specify a database connection string with --url, records\n    will automatically default to the value of $DATABASE_URL, if available.\n  - Query is intended to be the path of a SQL file, however a query string\n    can be provided instead. Use this feature discernfully; it's dangerous.\n  - Records is intended for report-style exports of database queries, and\n    has not yet been optimized for extremely large data dumps.\n    \"\"\" % dict(\n        formats_lst=formats_lst\n    )\n\n    # Parse the command-line arguments.\n    arguments = docopt(cli_docs)\n\n    query = arguments[\"<query>\"]\n    params = arguments[\"<params>\"]\n    format = arguments.get(\"<format>\")\n    if format and \"=\" in format:\n        del arguments[\"<format>\"]\n        arguments[\"<params>\"].append(format)\n        format = None\n    if format and format not in supported_formats:\n        print(\"%s format not supported.\" % format)\n        print(\"Supported formats are %s.\" % formats_lst)\n        exit(62)\n\n    # Can't send an empty list if params aren't expected.\n    try:\n        params = dict([i.split(\"=\") for i in params])\n    except ValueError:\n        print(\"Parameters must be given in key=value format.\")\n        exit(64)\n\n    # Be ready to fail on missing packages\n    try:\n        # Create the Database.\n        db = Database(arguments[\"--url\"])\n\n        # Execute the query, if it is a found file.\n        if os.path.isfile(query):\n            rows = db.query_file(query, **params)\n\n        # Execute the query, if it appears to be a query string.\n        elif len(query.split()) > 2:\n            rows = db.query(query, **params)\n\n        # Otherwise, say the file wasn't found.\n        else:\n            print(\"The given query could not be found.\")\n            exit(66)\n\n        # Print results in desired format.\n        if format:\n            content = rows.export(format)\n            if isinstance(content, bytes):\n                print_bytes(content)\n            else:\n                print(content)\n        else:\n            print(rows.dataset)\n    except ImportError as impexc:\n        print(impexc.msg)\n        print(\"Used database or format require a package, which is missing.\")\n        print(\"Try to install missing packages.\")\n        exit(60)\n\n\ndef print_bytes(content):\n    try:\n        stdout.buffer.write(content)\n    except AttributeError:\n        stdout.write(content)\n\n\n# Run the CLI when executed directly.\nif __name__ == \"__main__\":\n    cli()\n", "metadata": {"source": "github.com/kennethreitz/records", "file_path": "records.py", "language": "python", "complexity": "complex", "star_count": 7208, "quality_score": 0.97, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": true, "has_documentation": true, "has_type_hints": true, "has_testing": true, "has_logging": false, "function_count": 52, "class_count": 7, "line_count": 563}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "database_integration", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create database model with ORM relationships and queries\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "#!/usr/bin/env python\n\nimport io\nimport os\nimport sys\nfrom codecs import open\nfrom shutil import rmtree\n\nfrom setuptools import setup, Command\n\nhere = os.path.abspath(os.path.dirname(__file__))\nwith io.open(os.path.join(here, \"README.rst\"), encoding=\"utf-8\") as f:\n    long_description = \"\\n\" + f.read()\n\n\nclass PublishCommand(Command):\n    \"\"\"Support setup.py publish.\"\"\"\n\n    description = \"Build and publish the package.\"\n    user_options = []\n\n    @staticmethod\n    def status(s):\n        \"\"\"Prints things in bold.\"\"\"\n        print(\"\\033[1m{}\\033[0m\".format(s))\n\n    def initialize_options(self):\n        pass\n\n    def finalize_options(self):\n        pass\n\n    def run(self):\n        try:\n            self.status(\"Removing previous builds...\")\n            rmtree(os.path.join(here, \"dist\"))\n        except FileNotFoundError:\n            pass\n\n        self.status(\"Building Source and Wheel (universal) distribution...\")\n        os.system(\"{} setup.py sdist bdist_wheel --universal\".format(sys.executable))\n\n        self.status(\"Uploading the package to PyPi via Twine...\")\n        os.system(\"twine upload dist/*\")\n\n        sys.exit()\n\n\nrequires = [\n    \"SQLAlchemy>=2.0\",\n    \"tablib>=0.11.4\",\n    \"openpyxl>2.6.0\",  # https://github.com/kennethreitz-archive/records/pull/184#issuecomment-606207851\n    \"docopt\",\n]\nversion = \"0.6.0\"\n\n\ndef read(f):\n    return open(f, encoding=\"utf-8\").read()\n\n\nsetup(\n    name=\"records\",\n    version=version,\n    description=\"SQL for Humans\",\n    long_description=read(\"README.rst\") + \"\\n\\n\" + read(\"HISTORY.rst\"),\n    author=\"Kenneth Reitz\",\n    author_email=\"me@kennethreitz.org\",\n    url=\"https://github.com/kennethreitz/records\",\n    py_modules=[\"records\"],\n    package_data={\"\": [\"LICENSE\"]},\n    include_package_data=True,\n    entry_points={\n        \"console_scripts\": [\"records=records:cli\"],\n    },\n    install_requires=requires,\n    extras_require={\n        \"pandas\": [\"tablib[pandas]\"],\n        \"pg\": [\"psycopg2-binary\"],\n        \"redshift\": [\"sqlalchemy-redshift\", \"psycopg2\"],\n        # TODO: Add the rest.\n    },\n    license=\"ISC\",\n    zip_safe=False,\n    classifiers=(\n        \"Development Status :: 5 - Production/Stable\",\n        \"Intended Audience :: Developers\",\n        \"Natural Language :: English\",\n        \"License :: OSI Approved :: ISC License (ISCL)\",\n        \"Programming Language :: Python\",\n        \"Programming Language :: Python :: 3\",\n        \"Programming Language :: Python :: 3.4\",\n        \"Programming Language :: Python :: 3.5\",\n        \"Programming Language :: Python :: 3.6\",\n        \"Programming Language :: Python :: 3.7\",\n        \"Programming Language :: Python :: 3.8\",\n        \"Programming Language :: Python :: 3.9\",\n        \"Programming Language :: Python :: 3.10\",\n        \"Programming Language :: Python :: 3.11\",\n        \"Programming Language :: Python :: 3.12\",\n        \"Programming Language :: Python :: Implementation :: CPython\",\n    ),\n    cmdclass={\n        \"publish\": PublishCommand,\n    },\n)\n", "metadata": {"source": "github.com/kennethreitz/records", "file_path": "setup.py", "language": "python", "complexity": "medium", "star_count": 7208, "quality_score": 0.8799999999999999, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": true, "has_documentation": true, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 5, "class_count": 1, "line_count": 107}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "database_integration", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create database model with ORM relationships and queries\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"Shared pytest fixtures.\n\n\"\"\"\n\nimport pytest\n\nimport records\n\n\n@pytest.fixture(\n    params=[\n        # request: (sql_url_id, sql_url_template)\n        (\"sqlite_memory\", \"sqlite:///:memory:\"),\n        # ('sqlite_file', 'sqlite:///{dbfile}'),\n        # ('psql', 'postgresql://records:records@localhost/records')\n    ],\n    ids=lambda r: r[0],\n)\ndef db(request, tmpdir):\n    \"\"\"Instance of `records.Database(dburl)`\n\n    Ensure, it gets closed after being used in a test or fixture.\n\n    Parametrized with (sql_url_id, sql_url_template) tuple.\n    If `sql_url_template` contains `{dbfile}` it is replaced with path to a\n    temporary file.\n\n    Feel free to parametrize for other databases and experiment with them.\n    \"\"\"\n    id, url = request.param\n    # replace {dbfile} in url with temporary db file path\n    url = url.format(dbfile=str(tmpdir / \"db.sqlite\"))\n    db = records.Database(url)\n    yield db  # providing fixture value for a test case\n    # tear_down\n    db.close()\n\n\n@pytest.fixture\ndef foo_table(db):\n    \"\"\"Database with table `foo` created\n\n    tear_down drops the table.\n\n    Typically applied by `@pytest.mark.usefixtures('foo_table')`\n    \"\"\"\n    db.query(\"CREATE TABLE foo (a integer)\")\n    yield\n    db.query(\"DROP TABLE foo\")\n", "metadata": {"source": "github.com/kennethreitz/records", "file_path": "tests/conftest.py", "language": "python", "complexity": "simple", "star_count": 7208, "quality_score": 0.7899999999999999, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": true, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 2, "class_count": 0, "line_count": 50}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "database_integration", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create database model with ORM relationships and queries with pytest testing framework\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import pytest\n\n\n@pytest.mark.usefixtures(\"foo_table\")\ndef test_issue105(db):\n    assert db.query(\"select count(*) as n from foo\").scalar() == 0\n", "metadata": {"source": "github.com/kennethreitz/records", "file_path": "tests/test_105.py", "language": "python", "complexity": "simple", "star_count": 7208, "quality_score": 0.6499999999999999, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": false, "has_testing": true, "has_logging": false, "function_count": 1, "class_count": 0, "line_count": 7}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "database_integration", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create database model with ORM relationships and queries\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "def test_issue69(db):\n    db.query(\"CREATE table users (id text)\")\n    db.query(\"SELECT * FROM users WHERE id = :user\", user=\"Te'ArnaLambert\")\n", "metadata": {"source": "github.com/kennethreitz/records", "file_path": "tests/test_69.py", "language": "python", "complexity": "simple", "star_count": 7208, "quality_score": 0.56, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": false, "has_error_handling": false, "has_documentation": false, "has_type_hints": false, "has_testing": true, "has_logging": false, "function_count": 1, "class_count": 0, "line_count": 4}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "database_integration", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create database model with ORM relationships and queries with pytest testing framework\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from collections import namedtuple\n\nimport records\n\nfrom pytest import raises\n\n\nIdRecord = namedtuple('IdRecord', 'id')\n\n\ndef check_id(i, row):\n    assert row.id == i\n\n\nclass TestRecordCollection:\n    def test_iter(self):\n        rows = records.RecordCollection(IdRecord(i) for i in range(10))\n        for i, row in enumerate(rows):\n            check_id(i, row)\n\n    def test_next(self):\n        rows = records.RecordCollection(IdRecord(i) for i in range(10))\n        for i in range(10):\n            check_id(i, next(rows))\n\n    def test_iter_and_next(self):\n        rows = records.RecordCollection(IdRecord(i) for i in range(10))\n        i = enumerate(iter(rows))\n        check_id(*next(i))  # Cache first row.\n        next(rows)  # Cache second row.\n        check_id(*next(i))  # Read second row from cache.\n\n    def test_multiple_iter(self):\n        rows = records.RecordCollection(IdRecord(i) for i in range(10))\n        i = enumerate(iter(rows))\n        j = enumerate(iter(rows))\n\n        check_id(*next(i))  # Cache first row.\n\n        check_id(*next(j))  # Read first row from cache.\n        check_id(*next(j))  # Cache second row.\n\n        check_id(*next(i))  # Read second row from cache.\n\n    def test_slice_iter(self):\n        rows = records.RecordCollection(IdRecord(i) for i in range(10))\n        for i, row in enumerate(rows[:5]):\n            check_id(i, row)\n        for i, row in enumerate(rows):\n            check_id(i, row)\n        assert len(rows) == 10\n\n\n    # all\n\n    def test_all_returns_a_list_of_records(self):\n        rows = records.RecordCollection(IdRecord(i) for i in range(3))\n        assert rows.all() == [IdRecord(0), IdRecord(1), IdRecord(2)]\n\n\n    # first\n\n    def test_first_returns_a_single_record(self):\n        rows = records.RecordCollection(IdRecord(i) for i in range(1))\n        assert rows.first() == IdRecord(0)\n\n    def test_first_defaults_to_None(self):\n        rows = records.RecordCollection(iter([]))\n        assert rows.first() is None\n\n    def test_first_default_is_overridable(self):\n        rows = records.RecordCollection(iter([]))\n        assert rows.first('Cheese') == 'Cheese'\n\n    def test_first_raises_default_if_its_an_exception_subclass(self):\n        rows = records.RecordCollection(iter([]))\n        class Cheese(Exception): pass\n        raises(Cheese, rows.first, Cheese)\n\n    def test_first_raises_default_if_its_an_exception_instance(self):\n        rows = records.RecordCollection(iter([]))\n        class Cheese(Exception): pass\n        raises(Cheese, rows.first, Cheese('cheddar'))\n\n    # one\n\n    def test_one_returns_a_single_record(self):\n        rows = records.RecordCollection(IdRecord(i) for i in range(1))\n        assert rows.one() == IdRecord(0)\n\n    def test_one_defaults_to_None(self):\n        rows = records.RecordCollection(iter([]))\n        assert rows.one() is None\n\n    def test_one_default_is_overridable(self):\n        rows = records.RecordCollection(iter([]))\n        assert rows.one('Cheese') == 'Cheese'\n\n    def test_one_raises_when_more_than_one(self):\n        rows = records.RecordCollection(IdRecord(i) for i in range(3))\n        raises(ValueError, rows.one)\n\n    def test_one_raises_default_if_its_an_exception_subclass(self):\n        rows = records.RecordCollection(iter([]))\n        class Cheese(Exception): pass\n        raises(Cheese, rows.one, Cheese)\n\n    def test_one_raises_default_if_its_an_exception_instance(self):\n        rows = records.RecordCollection(iter([]))\n        class Cheese(Exception): pass\n        raises(Cheese, rows.one, Cheese('cheddar'))\n\n    # scalar\n\n    def test_scalar_returns_a_single_record(self):\n        rows = records.RecordCollection(IdRecord(i) for i in range(1))\n        assert rows.scalar() == 0\n\n    def test_scalar_defaults_to_None(self):\n        rows = records.RecordCollection(iter([]))\n        assert rows.scalar() is None\n\n    def test_scalar_default_is_overridable(self):\n        rows = records.RecordCollection(iter([]))\n        assert rows.scalar('Kaffe') == 'Kaffe'\n\n    def test_scalar_raises_when_more_than_one(self):\n        rows = records.RecordCollection(IdRecord(i) for i in range(3))\n        raises(ValueError, rows.scalar)\n\n\nclass TestRecord:\n\n    def test_record_dir(self):\n        keys, values = ['id', 'name', 'email'], [1, '', '']\n        record = records.Record(keys, values)\n        _dir = dir(record)\n        for key in keys:\n            assert key in _dir\n        for key in dir(object):\n            assert key in _dir\n\n    def test_record_duplicate_column(self):\n        keys, values = ['id', 'name', 'email', 'email'], [1, '', '', '']\n        record = records.Record(keys, values)\n        with raises(KeyError):\n            record['email']\n", "metadata": {"source": "github.com/kennethreitz/records", "file_path": "tests/test_records.py", "language": "python", "complexity": "complex", "star_count": 7208, "quality_score": 0.8799999999999999, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": true, "has_logging": false, "function_count": 24, "class_count": 6, "line_count": 148}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "database_integration", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create database model with ORM relationships and queries with pytest testing framework\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"Test manipulating database table in various transaction scenarios.\n\nVarying conditions:\n\n- db for different database backends (see `db` fixture)\n- query run via\n\n    - `db=records.Database(); db.query()\n    - `conn=db.get_connection(); conn.query()`\n\n- transaction\n    - not used at all\n    - used and created in different ways\n    - transaction succeeds\n    - transaction fails or raise\n\"\"\"\nimport pytest\n\n\n@pytest.mark.usefixtures('foo_table')\ndef test_plain_db(db):\n    \"\"\"Manipulate database by `db.query` without transactions.\n    \"\"\"\n    db.query('INSERT INTO foo VALUES (42)')\n    db.query('INSERT INTO foo VALUES (43)')\n    assert db.query('SELECT count(*) AS n FROM foo')[0].n == 2\n\n\n@pytest.mark.usefixtures('foo_table')\ndef test_plain_conn(db):\n    \"\"\"Manipulate database by `conn.query` without transactions.\n    \"\"\"\n    conn = db.get_connection()\n    conn.query('INSERT INTO foo VALUES (42)')\n    conn.query('INSERT INTO foo VALUES (43)')\n    assert conn.query('SELECT count(*) AS n FROM foo')[0].n == 2\n\n\n@pytest.mark.usefixtures('foo_table')\ndef test_failing_transaction_self_managed(db):\n    conn = db.get_connection()\n    tx = conn.transaction()\n    try:\n        conn.query('INSERT INTO foo VALUES (42)')\n        conn.query('INSERT INTO foo VALUES (43)')\n        raise ValueError()\n        tx.commit()\n        conn.query('INSERT INTO foo VALUES (44)')\n    except ValueError:\n        tx.rollback()\n    finally:\n        conn.close()\n        assert db.query('SELECT count(*) AS n FROM foo')[0].n == 0\n\n\n@pytest.mark.usefixtures('foo_table')\ndef test_failing_transaction(db):\n    with db.transaction() as conn:\n        conn.query('INSERT INTO foo VALUES (42)')\n        conn.query('INSERT INTO foo VALUES (43)')\n        raise ValueError()\n\n    assert db.query('SELECT count(*) AS n FROM foo')[0].n == 0\n\n\n@pytest.mark.usefixtures('foo_table')\ndef test_passing_transaction_self_managed(db):\n    conn = db.get_connection()\n    tx = conn.transaction()\n    conn.query('INSERT INTO foo VALUES (42)')\n    conn.query('INSERT INTO foo VALUES (43)')\n    tx.commit()\n    conn.close()\n    assert db.query('SELECT count(*) AS n FROM foo')[0].n == 2\n\n\n@pytest.mark.usefixtures('foo_table')\ndef test_passing_transaction(db):\n    with db.transaction() as conn:\n        conn.query('INSERT INTO foo VALUES (42)')\n        conn.query('INSERT INTO foo VALUES (43)')\n\n    assert db.query('SELECT count(*) AS n FROM foo')[0].n == 2\n", "metadata": {"source": "github.com/kennethreitz/records", "file_path": "tests/test_transactions.py", "language": "python", "complexity": "medium", "star_count": 7208, "quality_score": 0.9099999999999999, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": true, "has_documentation": true, "has_type_hints": false, "has_testing": true, "has_logging": false, "function_count": 6, "class_count": 0, "line_count": 84}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "database_integration", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create database model with ORM relationships and queries\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import os\n\nimport click\nfrom flask import Flask\nfrom flask.cli import with_appcontext\nfrom flask_sqlalchemy import SQLAlchemy\nfrom sqlalchemy.orm import DeclarativeBase\n\n__version__ = (1, 1, 0, \"dev\")\n\n\nclass Base(DeclarativeBase):\n    pass\n\n\ndb = SQLAlchemy(model_class=Base)\n\n\ndef create_app(test_config=None):\n    \"\"\"Create and configure an instance of the Flask application.\"\"\"\n    app = Flask(__name__, instance_relative_config=True)\n\n    # some deploy systems set the database url in the environ\n    db_url = os.environ.get(\"DATABASE_URL\")\n\n    if db_url is None:\n        # default to a sqlite database in the instance folder\n        db_url = \"sqlite:///flaskr.sqlite\"\n\n    app.config.from_mapping(\n        # default secret that should be overridden in environ or config\n        SECRET_KEY=os.environ.get(\"SECRET_KEY\", \"dev\"),\n        SQLALCHEMY_DATABASE_URI=db_url,\n    )\n\n    if test_config is None:\n        # load the instance config, if it exists, when not testing\n        app.config.from_pyfile(\"config.py\", silent=True)\n    else:\n        # load the test config if passed in\n        app.config.update(test_config)\n\n    # initialize Flask-SQLAlchemy and the init-db command\n    db.init_app(app)\n    app.cli.add_command(init_db_command)\n\n    # apply the blueprints to the app\n    from flaskr import auth, blog\n\n    app.register_blueprint(auth.bp)\n    app.register_blueprint(blog.bp)\n\n    # make \"index\" point at \"/\", which is handled by \"blog.index\"\n    app.add_url_rule(\"/\", endpoint=\"index\")\n\n    return app\n\n\ndef init_db():\n    db.drop_all()\n    db.create_all()\n\n\n@click.command(\"init-db\")\n@with_appcontext\ndef init_db_command():\n    \"\"\"Clear existing data and create new tables.\"\"\"\n    init_db()\n    click.echo(\"Initialized the database.\")\n", "metadata": {"source": "github.com/pallets-eco/flask-sqlalchemy", "file_path": "examples/flaskr/flaskr/__init__.py", "language": "python", "complexity": "simple", "star_count": 4297, "quality_score": 0.8099999999999999, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": true, "has_type_hints": false, "has_testing": true, "has_logging": true, "function_count": 3, "class_count": 1, "line_count": 70}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "database_integration", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create database model with ORM relationships and queries with SQLAlchemy/Django ORM patterns\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from __future__ import annotations\n\nfrom typing import TYPE_CHECKING\n\nfrom sqlalchemy.orm import Mapped, mapped_column, relationship\nfrom werkzeug.security import check_password_hash\nfrom werkzeug.security import generate_password_hash\n\nfrom flaskr import db\n\nif TYPE_CHECKING:\n    from ..blog.models import Post\n\n\nclass User(db.Model):\n    id: Mapped[int] = mapped_column(primary_key=True)\n    username: Mapped[str] = mapped_column(unique=True)\n    password_hash: Mapped[str]\n    posts: Mapped[list[Post]] = relationship(\"Post\", back_populates=\"author\")\n\n    def set_password(self, value: str) -> None:\n        \"\"\"Store the password as a hash for security.\"\"\"\n        self.password_hash = generate_password_hash(value)\n\n    # allow password = \"...\" to set a password\n    password = property(fset=set_password)\n\n    def check_password(self, value: str) -> bool:\n        return check_password_hash(self.password_hash, value)\n", "metadata": {"source": "github.com/pallets-eco/flask-sqlalchemy", "file_path": "examples/flaskr/flaskr/auth/models.py", "language": "python", "complexity": "simple", "star_count": 4297, "quality_score": 0.75, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": true, "has_type_hints": true, "has_testing": false, "has_logging": true, "function_count": 2, "class_count": 1, "line_count": 30}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "database_integration", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create database model with ORM relationships and queries\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import functools\n\nfrom flask import Blueprint\nfrom flask import flash\nfrom flask import g\nfrom flask import redirect\nfrom flask import render_template\nfrom flask import request\nfrom flask import session\nfrom flask import url_for\n\nfrom flaskr import db\nfrom flaskr.auth.models import User\n\nbp = Blueprint(\"auth\", __name__, url_prefix=\"/auth\")\n\n\ndef login_required(view):\n    \"\"\"View decorator that redirects anonymous users to the login page.\"\"\"\n\n    @functools.wraps(view)\n    def wrapped_view(**kwargs):\n        if g.user is None:\n            return redirect(url_for(\"auth.login\"))\n\n        return view(**kwargs)\n\n    return wrapped_view\n\n\n@bp.before_app_request\ndef load_logged_in_user():\n    \"\"\"If a user id is stored in the session, load the user object from\n    the database into ``g.user``.\"\"\"\n    user_id = session.get(\"user_id\")\n\n    if user_id is not None:\n        g.user = db.session.get(User, user_id)\n    else:\n        g.user = None\n\n\n@bp.route(\"/register\", methods=(\"GET\", \"POST\"))\ndef register():\n    \"\"\"Register a new user.\n\n    Validates that the username is not already taken. Hashes the\n    password for security.\n    \"\"\"\n    if request.method == \"POST\":\n        username = request.form[\"username\"]\n        password = request.form[\"password\"]\n        error = None\n\n        if not username:\n            error = \"Username is required.\"\n        elif not password:\n            error = \"Password is required.\"\n        elif db.session.execute(\n            db.select(db.select(User).filter_by(username=username).exists())\n        ).scalar():\n            error = f\"User {username} is already registered.\"\n\n        if error is None:\n            # the name is available, create the user and go to the login page\n            db.session.add(User(username=username, password=password))\n            db.session.commit()\n            return redirect(url_for(\"auth.login\"))\n\n        flash(error)\n\n    return render_template(\"auth/register.html\")\n\n\n@bp.route(\"/login\", methods=(\"GET\", \"POST\"))\ndef login():\n    \"\"\"Log in a registered user by adding the user id to the session.\"\"\"\n    if request.method == \"POST\":\n        username = request.form[\"username\"]\n        password = request.form[\"password\"]\n        error = None\n        select = db.select(User).filter_by(username=username)\n        user = db.session.execute(select).scalar()\n\n        if user is None:\n            error = \"Incorrect username.\"\n        elif not user.check_password(password):\n            error = \"Incorrect password.\"\n\n        if error is None:\n            # store the user id in a new session and return to the index\n            session.clear()\n            session[\"user_id\"] = user.id\n            return redirect(url_for(\"index\"))\n\n        flash(error)\n\n    return render_template(\"auth/login.html\")\n\n\n@bp.route(\"/logout\")\ndef logout():\n    \"\"\"Clear the current session, including the stored user id.\"\"\"\n    session.clear()\n    return redirect(url_for(\"index\"))\n", "metadata": {"source": "github.com/pallets-eco/flask-sqlalchemy", "file_path": "examples/flaskr/flaskr/auth/views.py", "language": "python", "complexity": "medium", "star_count": 4297, "quality_score": 0.72, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": true, "has_type_hints": false, "has_testing": false, "has_logging": true, "function_count": 6, "class_count": 0, "line_count": 106}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "database_integration", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create database model with ORM relationships and queries with SQLAlchemy/Django ORM patterns\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from sqlalchemy.orm import Mapped, mapped_column, relationship\nfrom sqlalchemy import ForeignKey\nfrom datetime import datetime\nfrom datetime import timezone\n\nfrom flask import url_for\n\nfrom flaskr import db\nfrom flaskr.auth.models import User\n\n\ndef now_utc() -> datetime:\n    return datetime.now(timezone.utc)\n\n\nclass Post(db.Model):\n    id: Mapped[int] = mapped_column(primary_key=True)\n    author_id: Mapped[int] = mapped_column(ForeignKey(\"user.id\"))\n    created: Mapped[datetime] = mapped_column(default=now_utc)\n    title: Mapped[str]\n    body: Mapped[str]\n\n    # User object backed by author_id\n    # lazy=\"joined\" means the user is returned with the post in one query\n    author: Mapped[User] = relationship(lazy=\"joined\", back_populates=\"posts\")\n\n    @property\n    def update_url(self) -> str:\n        return url_for(\"blog.update\", id=self.id)\n\n    @property\n    def delete_url(self) -> str:\n        return url_for(\"blog.delete\", id=self.id)\n", "metadata": {"source": "github.com/pallets-eco/flask-sqlalchemy", "file_path": "examples/flaskr/flaskr/blog/models.py", "language": "python", "complexity": "medium", "star_count": 4297, "quality_score": 0.72, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": false, "has_logging": true, "function_count": 3, "class_count": 1, "line_count": 34}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "database_integration", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create database model with ORM relationships and queries\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from flask import Blueprint\nfrom flask import flash\nfrom flask import g\nfrom flask import redirect\nfrom flask import render_template\nfrom flask import request\nfrom flask import url_for\nfrom werkzeug.exceptions import abort\n\nfrom flaskr import db\nfrom flaskr.auth.views import login_required\nfrom flaskr.blog.models import Post\n\nbp = Blueprint(\"blog\", __name__)\n\n\n@bp.route(\"/\")\ndef index():\n    \"\"\"Show all the posts, most recent first.\"\"\"\n    select = db.select(Post).order_by(Post.created.desc())\n    posts = db.session.execute(select).scalars()\n    return render_template(\"blog/index.html\", posts=posts)\n\n\ndef get_post(id, check_author=True):\n    \"\"\"Get a post and its author by id.\n\n    Checks that the id exists and optionally that the current user is\n    the author.\n\n    :param id: id of post to get\n    :param check_author: require the current user to be the author\n    :return: the post with author information\n    :raise 404: if a post with the given id doesn't exist\n    :raise 403: if the current user isn't the author\n    \"\"\"\n    post = db.get_or_404(Post, id, description=f\"Post id {id} doesn't exist.\")\n\n    if check_author and post.author != g.user:\n        abort(403)\n\n    return post\n\n\n@bp.route(\"/create\", methods=(\"GET\", \"POST\"))\n@login_required\ndef create():\n    \"\"\"Create a new post for the current user.\"\"\"\n    if request.method == \"POST\":\n        title = request.form[\"title\"]\n        body = request.form[\"body\"]\n        error = None\n\n        if not title:\n            error = \"Title is required.\"\n\n        if error is not None:\n            flash(error)\n        else:\n            db.session.add(Post(title=title, body=body, author=g.user))\n            db.session.commit()\n            return redirect(url_for(\"blog.index\"))\n\n    return render_template(\"blog/create.html\")\n\n\n@bp.route(\"/<int:id>/update\", methods=(\"GET\", \"POST\"))\n@login_required\ndef update(id):\n    \"\"\"Update a post if the current user is the author.\"\"\"\n    post = get_post(id)\n\n    if request.method == \"POST\":\n        title = request.form[\"title\"]\n        body = request.form[\"body\"]\n        error = None\n\n        if not title:\n            error = \"Title is required.\"\n\n        if error is not None:\n            flash(error)\n        else:\n            post.title = title\n            post.body = body\n            db.session.commit()\n            return redirect(url_for(\"blog.index\"))\n\n    return render_template(\"blog/update.html\", post=post)\n\n\n@bp.route(\"/<int:id>/delete\", methods=(\"POST\",))\n@login_required\ndef delete(id):\n    \"\"\"Delete a post.\n\n    Ensures that the post exists and that the logged in user is the\n    author of the post.\n    \"\"\"\n    post = get_post(id)\n    db.session.delete(post)\n    db.session.commit()\n    return redirect(url_for(\"blog.index\"))\n", "metadata": {"source": "github.com/pallets-eco/flask-sqlalchemy", "file_path": "examples/flaskr/flaskr/blog/views.py", "language": "python", "complexity": "medium", "star_count": 4297, "quality_score": 0.72, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": true, "has_type_hints": true, "has_testing": false, "has_logging": true, "function_count": 5, "class_count": 0, "line_count": 104}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "database_integration", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create database model with ORM relationships and queries\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from datetime import datetime\n\nimport pytest\nfrom werkzeug.security import generate_password_hash\n\nfrom flaskr import create_app\nfrom flaskr import db\nfrom flaskr import init_db\nfrom flaskr.auth.models import User\nfrom flaskr.blog.models import Post\n\n_user1_pass = generate_password_hash(\"test\")\n_user2_pass = generate_password_hash(\"other\")\n\n\n@pytest.fixture\ndef app():\n    \"\"\"Create and configure a new app instance for each test.\"\"\"\n    # create the app with common test config\n    app = create_app({\"TESTING\": True, \"SQLALCHEMY_DATABASE_URI\": \"sqlite:///:memory:\"})\n\n    # create the database and load test data\n    # set _password to pre-generated hashes, since hashing for each test is slow\n    with app.app_context():\n        init_db()\n        user = User(username=\"test\", password_hash=_user1_pass)\n        db.session.add_all(\n            (\n                user,\n                User(username=\"other\", password_hash=_user2_pass),\n                Post(\n                    title=\"test title\",\n                    body=\"test\\nbody\",\n                    author=user,\n                    created=datetime(2018, 1, 1),\n                ),\n            )\n        )\n        db.session.commit()\n\n    yield app\n\n\n@pytest.fixture\ndef client(app):\n    \"\"\"A test client for the app.\"\"\"\n    return app.test_client()\n\n\n@pytest.fixture\ndef runner(app):\n    \"\"\"A test runner for the app's Click commands.\"\"\"\n    return app.test_cli_runner()\n\n\nclass AuthActions:\n    def __init__(self, client):\n        self._client = client\n\n    def login(self, username=\"test\", password=\"test\"):\n        return self._client.post(\n            \"/auth/login\", data={\"username\": username, \"password\": password}\n        )\n\n    def logout(self):\n        return self._client.get(\"/auth/logout\")\n\n\n@pytest.fixture\ndef auth(client):\n    return AuthActions(client)\n", "metadata": {"source": "github.com/pallets-eco/flask-sqlalchemy", "file_path": "examples/flaskr/tests/conftest.py", "language": "python", "complexity": "medium", "star_count": 4297, "quality_score": 0.8099999999999999, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": true, "has_type_hints": true, "has_testing": true, "has_logging": true, "function_count": 7, "class_count": 1, "line_count": 72}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "database_integration", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create database model with ORM relationships and queries with pytest testing framework\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import pytest\nfrom flask import g\nfrom flask import session\n\nfrom flaskr import db\nfrom flaskr.auth.models import User\n\n\ndef test_register(client, app):\n    # test that viewing the page renders without template errors\n    assert client.get(\"/auth/register\").status_code == 200\n\n    # test that successful registration redirects to the login page\n    response = client.post(\"/auth/register\", data={\"username\": \"a\", \"password\": \"a\"})\n    assert response.headers[\"Location\"] == \"/auth/login\"\n\n    # test that the user was inserted into the database\n    with app.app_context():\n        select = db.select(User).filter_by(username=\"a\")\n        user = db.session.execute(select).scalar()\n        assert user is not None\n\n\ndef test_user_password(app):\n    user = User(username=\"a\", password=\"a\")\n    assert user.password_hash != \"a\"\n    assert user.check_password(\"a\")\n\n\n@pytest.mark.parametrize(\n    (\"username\", \"password\", \"message\"),\n    (\n        (\"\", \"\", \"Username is required.\"),\n        (\"a\", \"\", \"Password is required.\"),\n        (\"test\", \"test\", \"already registered\"),\n    ),\n)\ndef test_register_validate_input(client, username, password, message):\n    response = client.post(\n        \"/auth/register\", data={\"username\": username, \"password\": password}\n    )\n    assert message in response.text\n\n\ndef test_login(client, auth):\n    # test that viewing the page renders without template errors\n    assert client.get(\"/auth/login\").status_code == 200\n\n    # test that successful login redirects to the index page\n    response = auth.login()\n    assert response.headers[\"Location\"] == \"/\"\n\n    # login request set the user_id in the session\n    # check that the user is loaded from the session\n    with client:\n        client.get(\"/\")\n        assert session[\"user_id\"] == 1\n        assert g.user.username == \"test\"\n\n\n@pytest.mark.parametrize(\n    (\"username\", \"password\", \"message\"),\n    ((\"a\", \"test\", \"Incorrect username.\"), (\"test\", \"a\", \"Incorrect password.\")),\n)\ndef test_login_validate_input(auth, username, password, message):\n    response = auth.login(username, password)\n    assert message in response.text\n\n\ndef test_logout(client, auth):\n    auth.login()\n\n    with client:\n        auth.logout()\n        assert \"user_id\" not in session\n", "metadata": {"source": "github.com/pallets-eco/flask-sqlalchemy", "file_path": "examples/flaskr/tests/test_auth.py", "language": "python", "complexity": "medium", "star_count": 4297, "quality_score": 0.75, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": true, "has_logging": true, "function_count": 6, "class_count": 0, "line_count": 76}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "database_integration", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create database model with ORM relationships and queries with pytest testing framework\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import pytest\n\nfrom flaskr import db\nfrom flaskr.auth.models import User\nfrom flaskr.blog.models import Post\n\n\ndef test_index(client, auth):\n    text = client.get(\"/\").text\n    assert \"Log In\" in text\n    assert \"Register\" in text\n\n    auth.login()\n    text = client.get(\"/\").text\n    assert \"test title\" in text\n    assert \"by test on 2018-01-01\" in text\n    assert \"test\\nbody\" in text\n    assert 'href=\"/1/update\"' in text\n\n\n@pytest.mark.parametrize(\"path\", (\"/create\", \"/1/update\", \"/1/delete\"))\ndef test_login_required(client, path):\n    response = client.post(path)\n    assert response.headers[\"Location\"] == \"/auth/login\"\n\n\ndef test_author_required(app, client, auth):\n    # change the post author to another user\n    with app.app_context():\n        db.session.get(Post, 1).author = db.session.get(User, 2)\n        db.session.commit()\n\n    auth.login()\n    # current user can't modify other user's post\n    assert client.post(\"/1/update\").status_code == 403\n    assert client.post(\"/1/delete\").status_code == 403\n    # current user doesn't see edit link\n    assert 'href=\"/1/update\"' not in client.get(\"/\").text\n\n\n@pytest.mark.parametrize(\"path\", (\"/2/update\", \"/2/delete\"))\ndef test_exists_required(client, auth, path):\n    auth.login()\n    assert client.post(path).status_code == 404\n\n\ndef test_create(client, auth, app):\n    auth.login()\n    assert client.get(\"/create\").status_code == 200\n    client.post(\"/create\", data={\"title\": \"created\", \"body\": \"\"})\n\n    with app.app_context():\n        select = db.select(db.func.count(Post.id))\n        post_count = db.session.execute(select).scalar()\n        assert post_count == 2\n\n\ndef test_update(client, auth, app):\n    auth.login()\n    assert client.get(\"/1/update\").status_code == 200\n    client.post(\"/1/update\", data={\"title\": \"updated\", \"body\": \"\"})\n\n    with app.app_context():\n        assert db.session.get(Post, 1).title == \"updated\"\n\n\n@pytest.mark.parametrize(\"path\", (\"/create\", \"/1/update\"))\ndef test_create_update_validate(client, auth, path):\n    auth.login()\n    response = client.post(path, data={\"title\": \"\", \"body\": \"\"})\n    assert \"Title is required.\" in response.text\n\n\ndef test_delete(client, auth, app):\n    auth.login()\n    response = client.post(\"/1/delete\")\n    assert response.headers[\"Location\"] == \"/\"\n\n    with app.app_context():\n        assert db.session.get(Post, 1) is None\n", "metadata": {"source": "github.com/pallets-eco/flask-sqlalchemy", "file_path": "examples/flaskr/tests/test_blog.py", "language": "python", "complexity": "medium", "star_count": 4297, "quality_score": 0.75, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": true, "has_logging": true, "function_count": 8, "class_count": 0, "line_count": 81}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "database_integration", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create database model with ORM relationships and queries using Flask/FastAPI decorators\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from datetime import datetime\nfrom datetime import timezone\n\nfrom flask import flash\nfrom flask import Flask\nfrom flask import redirect\nfrom flask import render_template\nfrom flask import request\nfrom flask import url_for\nfrom flask_sqlalchemy import SQLAlchemy\nfrom sqlalchemy.orm import DeclarativeBase\nfrom sqlalchemy.orm import Mapped, mapped_column\n\n\napp = Flask(__name__)\napp.secret_key = \"Achee6phIexoh8dagiQuew0ephuga4Ih\"\napp.config[\"SQLALCHEMY_DATABASE_URI\"] = \"sqlite:///todo.sqlite\"\n\n\nclass Base(DeclarativeBase):\n    pass\n\n\ndb = SQLAlchemy(app, model_class=Base)\n\n\ndef now_utc() -> datetime:\n    return datetime.now(timezone.utc)\n\n\nclass Todo(db.Model):\n    id: Mapped[int] = mapped_column(primary_key=True)\n    title: Mapped[str]\n    text: Mapped[str]\n    done: Mapped[bool] = mapped_column(default=False)\n    pub_date: Mapped[datetime] = mapped_column(default=now_utc)\n\n\nwith app.app_context():\n    db.create_all()\n\n\n@app.route(\"/\")\ndef show_all():\n    select = db.select(Todo).order_by(Todo.pub_date.desc())\n    todos = db.session.execute(select).scalars()\n    return render_template(\"show_all.html\", todos=todos)\n\n\n@app.route(\"/new\", methods=[\"GET\", \"POST\"])\ndef new():\n    if request.method == \"POST\":\n        if not request.form[\"title\"]:\n            flash(\"Title is required\", \"error\")\n        elif not request.form[\"text\"]:\n            flash(\"Text is required\", \"error\")\n        else:\n            todo = Todo(title=request.form[\"title\"], text=request.form[\"text\"])\n            db.session.add(todo)\n            db.session.commit()\n            flash(\"Todo item was successfully created\")\n            return redirect(url_for(\"show_all\"))\n\n    return render_template(\"new.html\")\n\n\n@app.route(\"/update\", methods=[\"POST\"])\ndef update_done():\n    for todo in db.session.execute(db.select(Todo)).scalars():\n        todo.done = f\"done.{todo.id}\" in request.form\n\n    flash(\"Updated status\")\n    db.session.commit()\n    return redirect(url_for(\"show_all\"))\n", "metadata": {"source": "github.com/pallets-eco/flask-sqlalchemy", "file_path": "examples/todo/app.py", "language": "python", "complexity": "medium", "star_count": 4297, "quality_score": 0.75, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 4, "class_count": 2, "line_count": 75}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "database_integration", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create database model with ORM relationships and queries\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from __future__ import annotations\n\nimport typing as t\n\nfrom flask import current_app\n\n\ndef add_models_to_shell() -> dict[str, t.Any]:\n    \"\"\"Registered with :meth:`~flask.Flask.shell_context_processor` if\n    ``add_models_to_shell`` is enabled. Adds the ``db`` instance and all model classes\n    to ``flask shell``.\n    \"\"\"\n    db = current_app.extensions[\"sqlalchemy\"]\n    out = {m.class_.__name__: m.class_ for m in db.Model._sa_registry.mappers}\n    out[\"db\"] = db\n    return out\n", "metadata": {"source": "github.com/pallets-eco/flask-sqlalchemy", "file_path": "src/flask_sqlalchemy/cli.py", "language": "python", "complexity": "simple", "star_count": 4297, "quality_score": 0.72, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": true, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 1, "class_count": 0, "line_count": 17}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "database_integration", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create database model with ORM relationships and queries with SQLAlchemy/Django ORM patterns\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from __future__ import annotations\n\nimport os\nimport types\nimport typing as t\nimport warnings\nfrom weakref import WeakKeyDictionary\n\nimport sqlalchemy as sa\nimport sqlalchemy.event as sa_event\nimport sqlalchemy.exc as sa_exc\nimport sqlalchemy.orm as sa_orm\nfrom flask import abort\nfrom flask import current_app\nfrom flask import Flask\nfrom flask import has_app_context\n\nfrom .model import _QueryProperty\nfrom .model import BindMixin\nfrom .model import DefaultMeta\nfrom .model import DefaultMetaNoName\nfrom .model import Model\nfrom .model import NameMixin\nfrom .pagination import Pagination\nfrom .pagination import SelectPagination\nfrom .query import Query\nfrom .session import _app_ctx_id\nfrom .session import Session\nfrom .table import _Table\n\n_O = t.TypeVar(\"_O\", bound=object)  # Based on sqlalchemy.orm._typing.py\n\n\n# Type accepted for model_class argument\n_FSA_MCT = t.TypeVar(\n    \"_FSA_MCT\",\n    bound=t.Union[\n        t.Type[Model],\n        sa_orm.DeclarativeMeta,\n        t.Type[sa_orm.DeclarativeBase],\n        t.Type[sa_orm.DeclarativeBaseNoMeta],\n    ],\n)\n\n\n# Type returned by make_declarative_base\nclass _FSAModel(Model):\n    metadata: sa.MetaData\n\n\ndef _get_2x_declarative_bases(\n    model_class: _FSA_MCT,\n) -> list[type[sa_orm.DeclarativeBase | sa_orm.DeclarativeBaseNoMeta]]:\n    return [\n        b\n        for b in model_class.__bases__\n        if issubclass(b, (sa_orm.DeclarativeBase, sa_orm.DeclarativeBaseNoMeta))\n    ]\n\n\nclass SQLAlchemy:\n    \"\"\"Integrates SQLAlchemy with Flask. This handles setting up one or more engines,\n    associating tables and models with specific engines, and cleaning up connections and\n    sessions after each request.\n\n    Only the engine configuration is specific to each application, other things like\n    the model, table, metadata, and session are shared for all applications using that\n    extension instance. Call :meth:`init_app` to configure the extension on an\n    application.\n\n    After creating the extension, create model classes by subclassing :attr:`Model`, and\n    table classes with :attr:`Table`. These can be accessed before :meth:`init_app` is\n    called, making it possible to define the models separately from the application.\n\n    Accessing :attr:`session` and :attr:`engine` requires an active Flask application\n    context. This includes methods like :meth:`create_all` which use the engine.\n\n    This class also provides access to names in SQLAlchemy's ``sqlalchemy`` and\n    ``sqlalchemy.orm`` modules. For example, you can use ``db.Column`` and\n    ``db.relationship`` instead of importing ``sqlalchemy.Column`` and\n    ``sqlalchemy.orm.relationship``. This can be convenient when defining models.\n\n    :param app: Call :meth:`init_app` on this Flask application now.\n    :param metadata: Use this as the default :class:`sqlalchemy.schema.MetaData`. Useful\n        for setting a naming convention.\n    :param session_options: Arguments used by :attr:`session` to create each session\n        instance. A ``scopefunc`` key will be passed to the scoped session, not the\n        session instance. See :class:`sqlalchemy.orm.sessionmaker` for a list of\n        arguments.\n    :param query_class: Use this as the default query class for models and dynamic\n        relationships. The query interface is considered legacy in SQLAlchemy.\n    :param model_class: Use this as the model base class when creating the declarative\n        model class :attr:`Model`. Can also be a fully created declarative model class\n        for further customization.\n    :param engine_options: Default arguments used when creating every engine. These are\n        lower precedence than application config. See :func:`sqlalchemy.create_engine`\n        for a list of arguments.\n    :param add_models_to_shell: Add the ``db`` instance and all model classes to\n        ``flask shell``.\n\n    .. versionchanged:: 3.1.0\n        The ``metadata`` parameter can still be used with SQLAlchemy 1.x classes,\n        but is ignored when using SQLAlchemy 2.x style of declarative classes.\n        Instead, specify metadata on your Base class.\n\n    .. versionchanged:: 3.1.0\n        Added the ``disable_autonaming`` parameter.\n\n    .. versionchanged:: 3.1.0\n        Changed ``model_class`` parameter to accepta SQLAlchemy 2.x\n        declarative base subclass.\n\n    .. versionchanged:: 3.0\n        An active Flask application context is always required to access ``session`` and\n        ``engine``.\n\n    .. versionchanged:: 3.0\n        Separate ``metadata`` are used for each bind key.\n\n    .. versionchanged:: 3.0\n        The ``engine_options`` parameter is applied as defaults before per-engine\n        configuration.\n\n    .. versionchanged:: 3.0\n        The session class can be customized in ``session_options``.\n\n    .. versionchanged:: 3.0\n        Added the ``add_models_to_shell`` parameter.\n\n    .. versionchanged:: 3.0\n        Engines are created when calling ``init_app`` rather than the first time they\n        are accessed.\n\n    .. versionchanged:: 3.0\n        All parameters except ``app`` are keyword-only.\n\n    .. versionchanged:: 3.0\n        The extension instance is stored directly as ``app.extensions[\"sqlalchemy\"]``.\n\n    .. versionchanged:: 3.0\n        Setup methods are renamed with a leading underscore. They are considered\n        internal interfaces which may change at any time.\n\n    .. versionchanged:: 3.0\n        Removed the ``use_native_unicode`` parameter and config.\n\n    .. versionchanged:: 2.4\n        Added the ``engine_options`` parameter.\n\n    .. versionchanged:: 2.1\n        Added the ``metadata``, ``query_class``, and ``model_class`` parameters.\n\n    .. versionchanged:: 2.1\n        Use the same query class across ``session``, ``Model.query`` and\n        ``Query``.\n\n    .. versionchanged:: 0.16\n        ``scopefunc`` is accepted in ``session_options``.\n\n    .. versionchanged:: 0.10\n        Added the ``session_options`` parameter.\n    \"\"\"\n\n    def __init__(\n        self,\n        app: Flask | None = None,\n        *,\n        metadata: sa.MetaData | None = None,\n        session_options: dict[str, t.Any] | None = None,\n        query_class: type[Query] = Query,\n        model_class: _FSA_MCT = Model,  # type: ignore[assignment]\n        engine_options: dict[str, t.Any] | None = None,\n        add_models_to_shell: bool = True,\n        disable_autonaming: bool = False,\n    ):\n        if session_options is None:\n            session_options = {}\n\n        self.Query = query_class\n        \"\"\"The default query class used by ``Model.query`` and ``lazy=\"dynamic\"``\n        relationships.\n\n        .. warning::\n            The query interface is considered legacy in SQLAlchemy.\n\n        Customize this by passing the ``query_class`` parameter to the extension.\n        \"\"\"\n\n        self.session = self._make_scoped_session(session_options)\n        \"\"\"A :class:`sqlalchemy.orm.scoping.scoped_session` that creates instances of\n        :class:`.Session` scoped to the current Flask application context. The session\n        will be removed, returning the engine connection to the pool, when the\n        application context exits.\n\n        Customize this by passing ``session_options`` to the extension.\n\n        This requires that a Flask application context is active.\n\n        .. versionchanged:: 3.0\n            The session is scoped to the current app context.\n        \"\"\"\n\n        self.metadatas: dict[str | None, sa.MetaData] = {}\n        \"\"\"Map of bind keys to :class:`sqlalchemy.schema.MetaData` instances. The\n        ``None`` key refers to the default metadata, and is available as\n        :attr:`metadata`.\n\n        Customize the default metadata by passing the ``metadata`` parameter to the\n        extension. This can be used to set a naming convention. When metadata for\n        another bind key is created, it copies the default's naming convention.\n\n        .. versionadded:: 3.0\n        \"\"\"\n\n        if metadata is not None:\n            if len(_get_2x_declarative_bases(model_class)) > 0:\n                warnings.warn(\n                    \"When using SQLAlchemy 2.x style of declarative classes,\"\n                    \" the `metadata` should be an attribute of the base class.\"\n                    \"The metadata passed into SQLAlchemy() is ignored.\",\n                    DeprecationWarning,\n                    stacklevel=2,\n                )\n            else:\n                metadata.info[\"bind_key\"] = None\n                self.metadatas[None] = metadata\n\n        self.Table = self._make_table_class()\n        \"\"\"A :class:`sqlalchemy.schema.Table` class that chooses a metadata\n        automatically.\n\n        Unlike the base ``Table``, the ``metadata`` argument is not required. If it is\n        not given, it is selected based on the ``bind_key`` argument.\n\n        :param bind_key: Used to select a different metadata.\n        :param args: Arguments passed to the base class. These are typically the table's\n            name, columns, and constraints.\n        :param kwargs: Arguments passed to the base class.\n\n        .. versionchanged:: 3.0\n            This is a subclass of SQLAlchemy's ``Table`` rather than a function.\n        \"\"\"\n\n        self.Model = self._make_declarative_base(\n            model_class, disable_autonaming=disable_autonaming\n        )\n        \"\"\"A SQLAlchemy declarative model class. Subclass this to define database\n        models.\n\n        If a model does not set ``__tablename__``, it will be generated by converting\n        the class name from ``CamelCase`` to ``snake_case``. It will not be generated\n        if the model looks like it uses single-table inheritance.\n\n        If a model or parent class sets ``__bind_key__``, it will use that metadata and\n        database engine. Otherwise, it will use the default :attr:`metadata` and\n        :attr:`engine`. This is ignored if the model sets ``metadata`` or ``__table__``.\n\n        For code using the SQLAlchemy 1.x API, customize this model by subclassing\n        :class:`.Model` and passing the ``model_class`` parameter to the extension.\n        A fully created declarative model class can be\n        passed as well, to use a custom metaclass.\n\n        For code using the SQLAlchemy 2.x API, customize this model by subclassing\n        :class:`sqlalchemy.orm.DeclarativeBase` or\n        :class:`sqlalchemy.orm.DeclarativeBaseNoMeta`\n        and passing the ``model_class`` parameter to the extension.\n        \"\"\"\n\n        if engine_options is None:\n            engine_options = {}\n\n        self._engine_options = engine_options\n        self._app_engines: WeakKeyDictionary[Flask, dict[str | None, sa.engine.Engine]]\n        self._app_engines = WeakKeyDictionary()\n        self._add_models_to_shell = add_models_to_shell\n\n        if app is not None:\n            self.init_app(app)\n\n    def __repr__(self) -> str:\n        if not has_app_context():\n            return f\"<{type(self).__name__}>\"\n\n        num_default_engines = 1 if self.engines.get(None) else 0\n        engine_str = self.engine.url if num_default_engines else \"(No default engine)\"\n\n        num_other_engines = len(self.engines) - num_default_engines\n        if num_other_engines >= 1:\n            engine_str = f\"{engine_str} +{num_other_engines} engines\"\n\n        return f\"<{type(self).__name__} {engine_str}>\"\n\n    def init_app(self, app: Flask) -> None:\n        \"\"\"Initialize a Flask application for use with this extension instance. This\n        must be called before accessing the database engine or session with the app.\n\n        This sets default configuration values, then configures the extension on the\n        application and creates the engines for each bind key. Therefore, this must be\n        called after the application has been configured. Changes to application config\n        after this call will not be reflected.\n\n        The following keys from ``app.config`` are used:\n\n        - :data:`.SQLALCHEMY_DATABASE_URI`\n        - :data:`.SQLALCHEMY_ENGINE_OPTIONS`\n        - :data:`.SQLALCHEMY_ECHO`\n        - :data:`.SQLALCHEMY_BINDS`\n        - :data:`.SQLALCHEMY_RECORD_QUERIES`\n        - :data:`.SQLALCHEMY_TRACK_MODIFICATIONS`\n\n        :param app: The Flask application to initialize.\n        \"\"\"\n        if \"sqlalchemy\" in app.extensions:\n            raise RuntimeError(\n                \"A 'SQLAlchemy' instance has already been registered on this Flask app.\"\n                \" Import and use that instance instead.\"\n            )\n\n        app.extensions[\"sqlalchemy\"] = self\n        app.teardown_appcontext(self._teardown_session)\n\n        if self._add_models_to_shell:\n            from .cli import add_models_to_shell\n\n            app.shell_context_processor(add_models_to_shell)\n\n        basic_uri: str | sa.engine.URL | None = app.config.setdefault(\n            \"SQLALCHEMY_DATABASE_URI\", None\n        )\n        basic_engine_options = self._engine_options.copy()\n        basic_engine_options.update(\n            app.config.setdefault(\"SQLALCHEMY_ENGINE_OPTIONS\", {})\n        )\n        echo: bool = app.config.setdefault(\"SQLALCHEMY_ECHO\", False)\n        config_binds: dict[str | None, str | sa.engine.URL | dict[str, t.Any]] = (\n            app.config.setdefault(\"SQLALCHEMY_BINDS\", {})\n        )\n        engine_options: dict[str | None, dict[str, t.Any]] = {}\n\n        # Build the engine config for each bind key.\n        for key, value in config_binds.items():\n            engine_options[key] = self._engine_options.copy()\n\n            if isinstance(value, (str, sa.engine.URL)):\n                engine_options[key][\"url\"] = value\n            else:\n                engine_options[key].update(value)\n\n        # Build the engine config for the default bind key.\n        if basic_uri is not None:\n            basic_engine_options[\"url\"] = basic_uri\n\n        if \"url\" in basic_engine_options:\n            engine_options.setdefault(None, {}).update(basic_engine_options)\n\n        if not engine_options:\n            raise RuntimeError(\n                \"Either 'SQLALCHEMY_DATABASE_URI' or 'SQLALCHEMY_BINDS' must be set.\"\n            )\n\n        engines = self._app_engines.setdefault(app, {})\n\n        # Dispose existing engines in case init_app is called again.\n        if engines:\n            for engine in engines.values():\n                engine.dispose()\n\n            engines.clear()\n\n        # Create the metadata and engine for each bind key.\n        for key, options in engine_options.items():\n            self._make_metadata(key)\n            options.setdefault(\"echo\", echo)\n            options.setdefault(\"echo_pool\", echo)\n            self._apply_driver_defaults(options, app)\n            engines[key] = self._make_engine(key, options, app)\n\n        if app.config.setdefault(\"SQLALCHEMY_RECORD_QUERIES\", False):\n            from . import record_queries\n\n            for engine in engines.values():\n                record_queries._listen(engine)\n\n        if app.config.setdefault(\"SQLALCHEMY_TRACK_MODIFICATIONS\", False):\n            from . import track_modifications\n\n            track_modifications._listen(self.session)\n\n    def _make_scoped_session(\n        self, options: dict[str, t.Any]\n    ) -> sa_orm.scoped_session[Session]:\n        \"\"\"Create a :class:`sqlalchemy.orm.scoping.scoped_session` around the factory\n        from :meth:`_make_session_factory`. The result is available as :attr:`session`.\n\n        The scope function can be customized using the ``scopefunc`` key in the\n        ``session_options`` parameter to the extension. By default it uses the current\n        thread or greenlet id.\n\n        This method is used for internal setup. Its signature may change at any time.\n\n        :meta private:\n\n        :param options: The ``session_options`` parameter from ``__init__``. Keyword\n            arguments passed to the session factory. A ``scopefunc`` key is popped.\n\n        .. versionchanged:: 3.0\n            The session is scoped to the current app context.\n\n        .. versionchanged:: 3.0\n            Renamed from ``create_scoped_session``, this method is internal.\n        \"\"\"\n        scope = options.pop(\"scopefunc\", _app_ctx_id)\n        factory = self._make_session_factory(options)\n        return sa_orm.scoped_session(factory, scope)\n\n    def _make_session_factory(\n        self, options: dict[str, t.Any]\n    ) -> sa_orm.sessionmaker[Session]:\n        \"\"\"Create the SQLAlchemy :class:`sqlalchemy.orm.sessionmaker` used by\n        :meth:`_make_scoped_session`.\n\n        To customize, pass the ``session_options`` parameter to :class:`SQLAlchemy`. To\n        customize the session class, subclass :class:`.Session` and pass it as the\n        ``class_`` key.\n\n        This method is used for internal setup. Its signature may change at any time.\n\n        :meta private:\n\n        :param options: The ``session_options`` parameter from ``__init__``. Keyword\n            arguments passed to the session factory.\n\n        .. versionchanged:: 3.0\n            The session class can be customized.\n\n        .. versionchanged:: 3.0\n            Renamed from ``create_session``, this method is internal.\n        \"\"\"\n        options.setdefault(\"class_\", Session)\n        options.setdefault(\"query_cls\", self.Query)\n        return sa_orm.sessionmaker(db=self, **options)\n\n    def _teardown_session(self, exc: BaseException | None) -> None:\n        \"\"\"Remove the current session at the end of the request.\n\n        :meta private:\n\n        .. versionadded:: 3.0\n        \"\"\"\n        self.session.remove()\n\n    def _make_metadata(self, bind_key: str | None) -> sa.MetaData:\n        \"\"\"Get or create a :class:`sqlalchemy.schema.MetaData` for the given bind key.\n\n        This method is used for internal setup. Its signature may change at any time.\n\n        :meta private:\n\n        :param bind_key: The name of the metadata being created.\n\n        .. versionadded:: 3.0\n        \"\"\"\n        if bind_key in self.metadatas:\n            return self.metadatas[bind_key]\n\n        if bind_key is not None:\n            # Copy the naming convention from the default metadata.\n            naming_convention = self._make_metadata(None).naming_convention\n        else:\n            naming_convention = None\n\n        # Set the bind key in info to be used by session.get_bind.\n        metadata = sa.MetaData(\n            naming_convention=naming_convention, info={\"bind_key\": bind_key}\n        )\n        self.metadatas[bind_key] = metadata\n        return metadata\n\n    def _make_table_class(self) -> type[_Table]:\n        \"\"\"Create a SQLAlchemy :class:`sqlalchemy.schema.Table` class that chooses a\n        metadata automatically based on the ``bind_key``. The result is available as\n        :attr:`Table`.\n\n        This method is used for internal setup. Its signature may change at any time.\n\n        :meta private:\n\n        .. versionadded:: 3.0\n        \"\"\"\n\n        class Table(_Table):\n            def __new__(\n                cls, *args: t.Any, bind_key: str | None = None, **kwargs: t.Any\n            ) -> Table:\n                # If a metadata arg is passed, go directly to the base Table. Also do\n                # this for no args so the correct error is shown.\n                if not args or (len(args) >= 2 and isinstance(args[1], sa.MetaData)):\n                    return super().__new__(cls, *args, **kwargs)\n\n                metadata = self._make_metadata(bind_key)\n                return super().__new__(cls, *[args[0], metadata, *args[1:]], **kwargs)\n\n        return Table\n\n    def _make_declarative_base(\n        self,\n        model_class: _FSA_MCT,\n        disable_autonaming: bool = False,\n    ) -> type[_FSAModel]:\n        \"\"\"Create a SQLAlchemy declarative model class. The result is available as\n        :attr:`Model`.\n\n        To customize, subclass :class:`.Model` and pass it as ``model_class`` to\n        :class:`SQLAlchemy`. To customize at the metaclass level, pass an already\n        created declarative model class as ``model_class``.\n\n        This method is used for internal setup. Its signature may change at any time.\n\n        :meta private:\n\n        :param model_class: A model base class, or an already created declarative model\n        class.\n\n        :param disable_autonaming: Turns off automatic tablename generation in models.\n\n        .. versionchanged:: 3.1.0\n            Added support for passing SQLAlchemy 2.x base class as model class.\n            Added optional ``disable_autonaming`` parameter.\n\n        .. versionchanged:: 3.0\n            Renamed with a leading underscore, this method is internal.\n\n        .. versionchanged:: 2.3\n            ``model`` can be an already created declarative model class.\n        \"\"\"\n        model: type[_FSAModel]\n        declarative_bases = _get_2x_declarative_bases(model_class)\n        if len(declarative_bases) > 1:\n            # raise error if more than one declarative base is found\n            raise ValueError(\n                \"Only one declarative base can be passed to SQLAlchemy.\"\n                f\" Got: {model_class.__bases__}\"\n            )\n        elif len(declarative_bases) == 1:\n            body = dict(model_class.__dict__)\n            body[\"__fsa__\"] = self\n            mixin_classes = [BindMixin, NameMixin, Model]\n            if disable_autonaming:\n                mixin_classes.remove(NameMixin)\n            model = types.new_class(\n                \"FlaskSQLAlchemyBase\",\n                (*mixin_classes, *model_class.__bases__),\n                {\"metaclass\": type(declarative_bases[0])},\n                lambda ns: ns.update(body),\n            )\n        elif not isinstance(model_class, sa_orm.DeclarativeMeta):\n            metadata = self._make_metadata(None)\n            metaclass = DefaultMetaNoName if disable_autonaming else DefaultMeta\n            model = sa_orm.declarative_base(\n                metadata=metadata, cls=model_class, name=\"Model\", metaclass=metaclass\n            )\n        else:\n            model = model_class  # type: ignore[assignment]\n\n        if None not in self.metadatas:\n            # Use the model's metadata as the default metadata.\n            model.metadata.info[\"bind_key\"] = None\n            self.metadatas[None] = model.metadata\n        else:\n            # Use the passed in default metadata as the model's metadata.\n            model.metadata = self.metadatas[None]\n\n        model.query_class = self.Query\n        model.query = _QueryProperty()  # type: ignore[assignment]\n        model.__fsa__ = self\n        return model\n\n    def _apply_driver_defaults(self, options: dict[str, t.Any], app: Flask) -> None:\n        \"\"\"Apply driver-specific configuration to an engine.\n\n        SQLite in-memory databases use ``StaticPool`` and disable ``check_same_thread``.\n        File paths are relative to the app's :attr:`~flask.Flask.instance_path`,\n        which is created if it doesn't exist.\n\n        MySQL sets ``charset=\"utf8mb4\"``, and ``pool_timeout`` defaults to 2 hours.\n\n        This method is used for internal setup. Its signature may change at any time.\n\n        :meta private:\n\n        :param options: Arguments passed to the engine.\n        :param app: The application that the engine configuration belongs to.\n\n        .. versionchanged:: 3.0\n            SQLite paths are relative to ``app.instance_path``. It does not use\n            ``NullPool`` if ``pool_size`` is 0. Driver-level URIs are supported.\n\n        .. versionchanged:: 3.0\n            MySQL sets ``charset=\"utf8mb4\". It does not set ``pool_size`` to 10. It\n            does not set ``pool_recycle`` if not using a queue pool.\n\n        .. versionchanged:: 3.0\n            Renamed from ``apply_driver_hacks``, this method is internal. It does not\n            return anything.\n\n        .. versionchanged:: 2.5\n            Returns ``(sa_url, options)``.\n        \"\"\"\n        url = sa.engine.make_url(options[\"url\"])\n\n        if url.drivername in {\"sqlite\", \"sqlite+pysqlite\"}:\n            if url.database is None or url.database in {\"\", \":memory:\"}:\n                options[\"poolclass\"] = sa.pool.StaticPool\n\n                if \"connect_args\" not in options:\n                    options[\"connect_args\"] = {}\n\n                options[\"connect_args\"][\"check_same_thread\"] = False\n            else:\n                # the url might look like sqlite:///file:path?uri=true\n                is_uri = url.query.get(\"uri\", False)\n\n                if is_uri:\n                    db_str = url.database[5:]\n                else:\n                    db_str = url.database\n\n                if not os.path.isabs(db_str):\n                    os.makedirs(app.instance_path, exist_ok=True)\n                    db_str = os.path.join(app.instance_path, db_str)\n\n                    if is_uri:\n                        db_str = f\"file:{db_str}\"\n\n                    options[\"url\"] = url.set(database=db_str)\n        elif url.drivername.startswith(\"mysql\"):\n            # set queue defaults only when using queue pool\n            if (\n                \"pool_class\" not in options\n                or options[\"pool_class\"] is sa.pool.QueuePool\n            ):\n                options.setdefault(\"pool_recycle\", 7200)\n\n            if \"charset\" not in url.query:\n                options[\"url\"] = url.update_query_dict({\"charset\": \"utf8mb4\"})\n\n    def _make_engine(\n        self, bind_key: str | None, options: dict[str, t.Any], app: Flask\n    ) -> sa.engine.Engine:\n        \"\"\"Create the :class:`sqlalchemy.engine.Engine` for the given bind key and app.\n\n        To customize, use :data:`.SQLALCHEMY_ENGINE_OPTIONS` or\n        :data:`.SQLALCHEMY_BINDS` config. Pass ``engine_options`` to :class:`SQLAlchemy`\n        to set defaults for all engines.\n\n        This method is used for internal setup. Its signature may change at any time.\n\n        :meta private:\n\n        :param bind_key: The name of the engine being created.\n        :param options: Arguments passed to the engine.\n        :param app: The application that the engine configuration belongs to.\n\n        .. versionchanged:: 3.0\n            Renamed from ``create_engine``, this method is internal.\n        \"\"\"\n        return sa.engine_from_config(options, prefix=\"\")\n\n    @property\n    def metadata(self) -> sa.MetaData:\n        \"\"\"The default metadata used by :attr:`Model` and :attr:`Table` if no bind key\n        is set.\n        \"\"\"\n        return self.metadatas[None]\n\n    @property\n    def engines(self) -> t.Mapping[str | None, sa.engine.Engine]:\n        \"\"\"Map of bind keys to :class:`sqlalchemy.engine.Engine` instances for current\n        application. The ``None`` key refers to the default engine, and is available as\n        :attr:`engine`.\n\n        To customize, set the :data:`.SQLALCHEMY_BINDS` config, and set defaults by\n        passing the ``engine_options`` parameter to the extension.\n\n        This requires that a Flask application context is active.\n\n        .. versionadded:: 3.0\n        \"\"\"\n        app = current_app._get_current_object()  # type: ignore[attr-defined]\n\n        if app not in self._app_engines:\n            raise RuntimeError(\n                \"The current Flask app is not registered with this 'SQLAlchemy'\"\n                \" instance. Did you forget to call 'init_app', or did you create\"\n                \" multiple 'SQLAlchemy' instances?\"\n            )\n\n        return self._app_engines[app]\n\n    @property\n    def engine(self) -> sa.engine.Engine:\n        \"\"\"The default :class:`~sqlalchemy.engine.Engine` for the current application,\n        used by :attr:`session` if the :attr:`Model` or :attr:`Table` being queried does\n        not set a bind key.\n\n        To customize, set the :data:`.SQLALCHEMY_ENGINE_OPTIONS` config, and set\n        defaults by passing the ``engine_options`` parameter to the extension.\n\n        This requires that a Flask application context is active.\n        \"\"\"\n        return self.engines[None]\n\n    def get_engine(\n        self, bind_key: str | None = None, **kwargs: t.Any\n    ) -> sa.engine.Engine:\n        \"\"\"Get the engine for the given bind key for the current application.\n        This requires that a Flask application context is active.\n\n        :param bind_key: The name of the engine.\n\n        .. deprecated:: 3.0\n            Will be removed in Flask-SQLAlchemy 3.2. Use ``engines[key]`` instead.\n\n        .. versionchanged:: 3.0\n            Renamed the ``bind`` parameter to ``bind_key``. Removed the ``app``\n            parameter.\n        \"\"\"\n        warnings.warn(\n            \"'get_engine' is deprecated and will be removed in Flask-SQLAlchemy\"\n            \" 3.2. Use 'engine' or 'engines[key]' instead. If you're using\"\n            \" Flask-Migrate or Alembic, you'll need to update your 'env.py' file.\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n\n        if \"bind\" in kwargs:\n            bind_key = kwargs.pop(\"bind\")\n\n        return self.engines[bind_key]\n\n    def get_or_404(\n        self,\n        entity: type[_O],\n        ident: t.Any,\n        *,\n        description: str | None = None,\n        **kwargs: t.Any,\n    ) -> _O:\n        \"\"\"Like :meth:`session.get() <sqlalchemy.orm.Session.get>` but aborts with a\n        ``404 Not Found`` error instead of returning ``None``.\n\n        :param entity: The model class to query.\n        :param ident: The primary key to query.\n        :param description: A custom message to show on the error page.\n        :param kwargs: Extra arguments passed to ``session.get()``.\n\n        .. versionchanged:: 3.1\n            Pass extra keyword arguments to ``session.get()``.\n\n        .. versionadded:: 3.0\n        \"\"\"\n        value = self.session.get(entity, ident, **kwargs)\n\n        if value is None:\n            abort(404, description=description)\n\n        return value\n\n    def first_or_404(\n        self, statement: sa.sql.Select[t.Any], *, description: str | None = None\n    ) -> t.Any:\n        \"\"\"Like :meth:`Result.scalar() <sqlalchemy.engine.Result.scalar>`, but aborts\n        with a ``404 Not Found`` error instead of returning ``None``.\n\n        :param statement: The ``select`` statement to execute.\n        :param description: A custom message to show on the error page.\n\n        .. versionadded:: 3.0\n        \"\"\"\n        value = self.session.execute(statement).scalar()\n\n        if value is None:\n            abort(404, description=description)\n\n        return value\n\n    def one_or_404(\n        self, statement: sa.sql.Select[t.Any], *, description: str | None = None\n    ) -> t.Any:\n        \"\"\"Like :meth:`Result.scalar_one() <sqlalchemy.engine.Result.scalar_one>`,\n        but aborts with a ``404 Not Found`` error instead of raising ``NoResultFound``\n        or ``MultipleResultsFound``.\n\n        :param statement: The ``select`` statement to execute.\n        :param description: A custom message to show on the error page.\n\n        .. versionadded:: 3.0\n        \"\"\"\n        try:\n            return self.session.execute(statement).scalar_one()\n        except (sa_exc.NoResultFound, sa_exc.MultipleResultsFound):\n            abort(404, description=description)\n\n    def paginate(\n        self,\n        select: sa.sql.Select[t.Any],\n        *,\n        page: int | None = None,\n        per_page: int | None = None,\n        max_per_page: int | None = None,\n        error_out: bool = True,\n        count: bool = True,\n    ) -> Pagination:\n        \"\"\"Apply an offset and limit to a select statment based on the current page and\n        number of items per page, returning a :class:`.Pagination` object.\n\n        The statement should select a model class, like ``select(User)``. This applies\n        ``unique()`` and ``scalars()`` modifiers to the result, so compound selects will\n        not return the expected results.\n\n        :param select: The ``select`` statement to paginate.\n        :param page: The current page, used to calculate the offset. Defaults to the\n            ``page`` query arg during a request, or 1 otherwise.\n        :param per_page: The maximum number of items on a page, used to calculate the\n            offset and limit. Defaults to the ``per_page`` query arg during a request,\n            or 20 otherwise.\n        :param max_per_page: The maximum allowed value for ``per_page``, to limit a\n            user-provided value. Use ``None`` for no limit. Defaults to 100.\n        :param error_out: Abort with a ``404 Not Found`` error if no items are returned\n            and ``page`` is not 1, or if ``page`` or ``per_page`` is less than 1, or if\n            either are not ints.\n        :param count: Calculate the total number of values by issuing an extra count\n            query. For very complex queries this may be inaccurate or slow, so it can be\n            disabled and set manually if necessary.\n\n        .. versionchanged:: 3.0\n            The ``count`` query is more efficient.\n\n        .. versionadded:: 3.0\n        \"\"\"\n        return SelectPagination(\n            select=select,\n            session=self.session(),\n            page=page,\n            per_page=per_page,\n            max_per_page=max_per_page,\n            error_out=error_out,\n            count=count,\n        )\n\n    def _call_for_binds(\n        self, bind_key: str | None | list[str | None], op_name: str\n    ) -> None:\n        \"\"\"Call a method on each metadata.\n\n        :meta private:\n\n        :param bind_key: A bind key or list of keys. Defaults to all binds.\n        :param op_name: The name of the method to call.\n\n        .. versionchanged:: 3.0\n            Renamed from ``_execute_for_all_tables``.\n        \"\"\"\n        if bind_key == \"__all__\":\n            keys: list[str | None] = list(self.metadatas)\n        elif bind_key is None or isinstance(bind_key, str):\n            keys = [bind_key]\n        else:\n            keys = bind_key\n\n        for key in keys:\n            try:\n                engine = self.engines[key]\n            except KeyError:\n                message = f\"Bind key '{key}' is not in 'SQLALCHEMY_BINDS' config.\"\n\n                if key is None:\n                    message = f\"'SQLALCHEMY_DATABASE_URI' config is not set. {message}\"\n\n                raise sa_exc.UnboundExecutionError(message) from None\n\n            metadata = self.metadatas[key]\n            getattr(metadata, op_name)(bind=engine)\n\n    def create_all(self, bind_key: str | None | list[str | None] = \"__all__\") -> None:\n        \"\"\"Create tables that do not exist in the database by calling\n        ``metadata.create_all()`` for all or some bind keys. This does not\n        update existing tables, use a migration library for that.\n\n        This requires that a Flask application context is active.\n\n        :param bind_key: A bind key or list of keys to create the tables for. Defaults\n            to all binds.\n\n        .. versionchanged:: 3.0\n            Renamed the ``bind`` parameter to ``bind_key``. Removed the ``app``\n            parameter.\n\n        .. versionchanged:: 0.12\n            Added the ``bind`` and ``app`` parameters.\n        \"\"\"\n        self._call_for_binds(bind_key, \"create_all\")\n\n    def drop_all(self, bind_key: str | None | list[str | None] = \"__all__\") -> None:\n        \"\"\"Drop tables by calling ``metadata.drop_all()`` for all or some bind keys.\n\n        This requires that a Flask application context is active.\n\n        :param bind_key: A bind key or list of keys to drop the tables from. Defaults to\n            all binds.\n\n        .. versionchanged:: 3.0\n            Renamed the ``bind`` parameter to ``bind_key``. Removed the ``app``\n            parameter.\n\n        .. versionchanged:: 0.12\n            Added the ``bind`` and ``app`` parameters.\n        \"\"\"\n        self._call_for_binds(bind_key, \"drop_all\")\n\n    def reflect(self, bind_key: str | None | list[str | None] = \"__all__\") -> None:\n        \"\"\"Load table definitions from the database by calling ``metadata.reflect()``\n        for all or some bind keys.\n\n        This requires that a Flask application context is active.\n\n        :param bind_key: A bind key or list of keys to reflect the tables from. Defaults\n            to all binds.\n\n        .. versionchanged:: 3.0\n            Renamed the ``bind`` parameter to ``bind_key``. Removed the ``app``\n            parameter.\n\n        .. versionchanged:: 0.12\n            Added the ``bind`` and ``app`` parameters.\n        \"\"\"\n        self._call_for_binds(bind_key, \"reflect\")\n\n    def _set_rel_query(self, kwargs: dict[str, t.Any]) -> None:\n        \"\"\"Apply the extension's :attr:`Query` class as the default for relationships\n        and backrefs.\n\n        :meta private:\n        \"\"\"\n        kwargs.setdefault(\"query_class\", self.Query)\n\n        if \"backref\" in kwargs:\n            backref = kwargs[\"backref\"]\n\n            if isinstance(backref, str):\n                backref = (backref, {})\n\n            backref[1].setdefault(\"query_class\", self.Query)\n\n    def relationship(\n        self, *args: t.Any, **kwargs: t.Any\n    ) -> sa_orm.RelationshipProperty[t.Any]:\n        \"\"\"A :func:`sqlalchemy.orm.relationship` that applies this extension's\n        :attr:`Query` class for dynamic relationships and backrefs.\n\n        .. versionchanged:: 3.0\n            The :attr:`Query` class is set on ``backref``.\n        \"\"\"\n        self._set_rel_query(kwargs)\n        return sa_orm.relationship(*args, **kwargs)\n\n    def dynamic_loader(\n        self, argument: t.Any, **kwargs: t.Any\n    ) -> sa_orm.RelationshipProperty[t.Any]:\n        \"\"\"A :func:`sqlalchemy.orm.dynamic_loader` that applies this extension's\n        :attr:`Query` class for relationships and backrefs.\n\n        .. versionchanged:: 3.0\n            The :attr:`Query` class is set on ``backref``.\n        \"\"\"\n        self._set_rel_query(kwargs)\n        return sa_orm.dynamic_loader(argument, **kwargs)\n\n    def _relation(\n        self, *args: t.Any, **kwargs: t.Any\n    ) -> sa_orm.RelationshipProperty[t.Any]:\n        \"\"\"A :func:`sqlalchemy.orm.relationship` that applies this extension's\n        :attr:`Query` class for dynamic relationships and backrefs.\n\n        SQLAlchemy 2.0 removes this name, use ``relationship`` instead.\n\n        :meta private:\n\n        .. versionchanged:: 3.0\n            The :attr:`Query` class is set on ``backref``.\n        \"\"\"\n        self._set_rel_query(kwargs)\n        f = sa_orm.relationship\n        return f(*args, **kwargs)\n\n    def __getattr__(self, name: str) -> t.Any:\n        if name == \"relation\":\n            return self._relation\n\n        if name == \"event\":\n            return sa_event\n\n        if name.startswith(\"_\"):\n            raise AttributeError(name)\n\n        for mod in (sa, sa_orm):\n            if hasattr(mod, name):\n                return getattr(mod, name)\n\n        raise AttributeError(name)\n", "metadata": {"source": "github.com/pallets-eco/flask-sqlalchemy", "file_path": "src/flask_sqlalchemy/extension.py", "language": "python", "complexity": "complex", "star_count": 4297, "quality_score": 0.71, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": true, "has_documentation": true, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 30, "class_count": 35, "line_count": 1011}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "database_integration", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create database model with ORM relationships and queries with SQLAlchemy/Django ORM patterns\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from __future__ import annotations\n\nimport re\nimport typing as t\n\nimport sqlalchemy as sa\nimport sqlalchemy.orm as sa_orm\n\nfrom .query import Query\n\nif t.TYPE_CHECKING:\n    from .extension import SQLAlchemy\n\n\nclass _QueryProperty:\n    \"\"\"A class property that creates a query object for a model.\n\n    :meta private:\n    \"\"\"\n\n    def __get__(self, obj: Model | None, cls: type[Model]) -> Query:\n        return cls.query_class(cls, session=cls.__fsa__.session())\n\n\nclass Model:\n    \"\"\"The base class of the :attr:`.SQLAlchemy.Model` declarative model class.\n\n    To define models, subclass :attr:`db.Model <.SQLAlchemy.Model>`, not this. To\n    customize ``db.Model``, subclass this and pass it as ``model_class`` to\n    :class:`.SQLAlchemy`. To customize ``db.Model`` at the metaclass level, pass an\n    already created declarative model class as ``model_class``.\n    \"\"\"\n\n    __fsa__: t.ClassVar[SQLAlchemy]\n    \"\"\"Internal reference to the extension object.\n\n    :meta private:\n    \"\"\"\n\n    query_class: t.ClassVar[type[Query]] = Query\n    \"\"\"Query class used by :attr:`query`. Defaults to :attr:`.SQLAlchemy.Query`, which\n    defaults to :class:`.Query`.\n    \"\"\"\n\n    query: t.ClassVar[Query] = _QueryProperty()  # type: ignore[assignment]\n    \"\"\"A SQLAlchemy query for a model. Equivalent to ``db.session.query(Model)``. Can be\n    customized per-model by overriding :attr:`query_class`.\n\n    .. warning::\n        The query interface is considered legacy in SQLAlchemy. Prefer using\n        ``session.execute(select())`` instead.\n    \"\"\"\n\n    def __repr__(self) -> str:\n        state = sa.inspect(self)\n        assert state is not None\n\n        if state.transient:\n            pk = f\"(transient {id(self)})\"\n        elif state.pending:\n            pk = f\"(pending {id(self)})\"\n        else:\n            pk = \", \".join(map(str, state.identity))\n\n        return f\"<{type(self).__name__} {pk}>\"\n\n\nclass BindMetaMixin(type):\n    \"\"\"Metaclass mixin that sets a model's ``metadata`` based on its ``__bind_key__``.\n\n    If the model sets ``metadata`` or ``__table__`` directly, ``__bind_key__`` is\n    ignored. If the ``metadata`` is the same as the parent model, it will not be set\n    directly on the child model.\n    \"\"\"\n\n    __fsa__: SQLAlchemy\n    metadata: sa.MetaData\n\n    def __init__(\n        cls, name: str, bases: tuple[type, ...], d: dict[str, t.Any], **kwargs: t.Any\n    ) -> None:\n        if not (\"metadata\" in cls.__dict__ or \"__table__\" in cls.__dict__):\n            bind_key = getattr(cls, \"__bind_key__\", None)\n            parent_metadata = getattr(cls, \"metadata\", None)\n            metadata = cls.__fsa__._make_metadata(bind_key)\n\n            if metadata is not parent_metadata:\n                cls.metadata = metadata\n\n        super().__init__(name, bases, d, **kwargs)\n\n\nclass BindMixin:\n    \"\"\"DeclarativeBase mixin to set a model's ``metadata`` based on ``__bind_key__``.\n\n    If no ``__bind_key__`` is specified, the model will use the default metadata\n    provided by ``DeclarativeBase`` or ``DeclarativeBaseNoMeta``.\n    If the model doesn't set ``metadata`` or ``__table__`` directly\n    and does set ``__bind_key__``, the model will use the metadata\n    for the specified bind key.\n    If the ``metadata`` is the same as the parent model, it will not be set\n    directly on the child model.\n\n    .. versionchanged:: 3.1.0\n    \"\"\"\n\n    __fsa__: SQLAlchemy\n    metadata: sa.MetaData\n\n    @classmethod\n    def __init_subclass__(cls: type[BindMixin], **kwargs: dict[str, t.Any]) -> None:\n        if not (\"metadata\" in cls.__dict__ or \"__table__\" in cls.__dict__) and hasattr(\n            cls, \"__bind_key__\"\n        ):\n            bind_key = getattr(cls, \"__bind_key__\", None)\n            parent_metadata = getattr(cls, \"metadata\", None)\n            metadata = cls.__fsa__._make_metadata(bind_key)\n\n            if metadata is not parent_metadata:\n                cls.metadata = metadata\n\n        super().__init_subclass__(**kwargs)\n\n\nclass NameMetaMixin(type):\n    \"\"\"Metaclass mixin that sets a model's ``__tablename__`` by converting the\n    ``CamelCase`` class name to ``snake_case``. A name is set for non-abstract models\n    that do not otherwise define ``__tablename__``. If a model does not define a primary\n    key, it will not generate a name or ``__table__``, for single-table inheritance.\n    \"\"\"\n\n    metadata: sa.MetaData\n    __tablename__: str\n    __table__: sa.Table\n\n    def __init__(\n        cls, name: str, bases: tuple[type, ...], d: dict[str, t.Any], **kwargs: t.Any\n    ) -> None:\n        if should_set_tablename(cls):\n            cls.__tablename__ = camel_to_snake_case(cls.__name__)\n\n        super().__init__(name, bases, d, **kwargs)\n\n        # __table_cls__ has run. If no table was created, use the parent table.\n        if (\n            \"__tablename__\" not in cls.__dict__\n            and \"__table__\" in cls.__dict__\n            and cls.__dict__[\"__table__\"] is None\n        ):\n            del cls.__table__\n\n    def __table_cls__(cls, *args: t.Any, **kwargs: t.Any) -> sa.Table | None:\n        \"\"\"This is called by SQLAlchemy during mapper setup. It determines the final\n        table object that the model will use.\n\n        If no primary key is found, that indicates single-table inheritance, so no table\n        will be created and ``__tablename__`` will be unset.\n        \"\"\"\n        schema = kwargs.get(\"schema\")\n\n        if schema is None:\n            key = args[0]\n        else:\n            key = f\"{schema}.{args[0]}\"\n\n        # Check if a table with this name already exists. Allows reflected tables to be\n        # applied to models by name.\n        if key in cls.metadata.tables:\n            return sa.Table(*args, **kwargs)\n\n        # If a primary key is found, create a table for joined-table inheritance.\n        for arg in args:\n            if (isinstance(arg, sa.Column) and arg.primary_key) or isinstance(\n                arg, sa.PrimaryKeyConstraint\n            ):\n                return sa.Table(*args, **kwargs)\n\n        # If no base classes define a table, return one that's missing a primary key\n        # so SQLAlchemy shows the correct error.\n        for base in cls.__mro__[1:-1]:\n            if \"__table__\" in base.__dict__:\n                break\n        else:\n            return sa.Table(*args, **kwargs)\n\n        # Single-table inheritance, use the parent table name. __init__ will unset\n        # __table__ based on this.\n        if \"__tablename__\" in cls.__dict__:\n            del cls.__tablename__\n\n        return None\n\n\nclass NameMixin:\n    \"\"\"DeclarativeBase mixin that sets a model's ``__tablename__`` by converting the\n    ``CamelCase`` class name to ``snake_case``. A name is set for non-abstract models\n    that do not otherwise define ``__tablename__``. If a model does not define a primary\n    key, it will not generate a name or ``__table__``, for single-table inheritance.\n\n    .. versionchanged:: 3.1.0\n    \"\"\"\n\n    metadata: sa.MetaData\n    __tablename__: str\n    __table__: sa.Table\n\n    @classmethod\n    def __init_subclass__(cls: type[NameMixin], **kwargs: dict[str, t.Any]) -> None:\n        if should_set_tablename(cls):\n            cls.__tablename__ = camel_to_snake_case(cls.__name__)\n\n        super().__init_subclass__(**kwargs)\n\n        # __table_cls__ has run. If no table was created, use the parent table.\n        if (\n            \"__tablename__\" not in cls.__dict__\n            and \"__table__\" in cls.__dict__\n            and cls.__dict__[\"__table__\"] is None\n        ):\n            del cls.__table__\n\n    @classmethod\n    def __table_cls__(cls, *args: t.Any, **kwargs: t.Any) -> sa.Table | None:\n        \"\"\"This is called by SQLAlchemy during mapper setup. It determines the final\n        table object that the model will use.\n\n        If no primary key is found, that indicates single-table inheritance, so no table\n        will be created and ``__tablename__`` will be unset.\n        \"\"\"\n        schema = kwargs.get(\"schema\")\n\n        if schema is None:\n            key = args[0]\n        else:\n            key = f\"{schema}.{args[0]}\"\n\n        # Check if a table with this name already exists. Allows reflected tables to be\n        # applied to models by name.\n        if key in cls.metadata.tables:\n            return sa.Table(*args, **kwargs)\n\n        # If a primary key is found, create a table for joined-table inheritance.\n        for arg in args:\n            if (isinstance(arg, sa.Column) and arg.primary_key) or isinstance(\n                arg, sa.PrimaryKeyConstraint\n            ):\n                return sa.Table(*args, **kwargs)\n\n        # If no base classes define a table, return one that's missing a primary key\n        # so SQLAlchemy shows the correct error.\n        for base in cls.__mro__[1:-1]:\n            if \"__table__\" in base.__dict__:\n                break\n        else:\n            return sa.Table(*args, **kwargs)\n\n        # Single-table inheritance, use the parent table name. __init__ will unset\n        # __table__ based on this.\n        if \"__tablename__\" in cls.__dict__:\n            del cls.__tablename__\n\n        return None\n\n\ndef should_set_tablename(cls: type) -> bool:\n    \"\"\"Determine whether ``__tablename__`` should be generated for a model.\n\n    -   If no class in the MRO sets a name, one should be generated.\n    -   If a declared attr is found, it should be used instead.\n    -   If a name is found, it should be used if the class is a mixin, otherwise one\n        should be generated.\n    -   Abstract models should not have one generated.\n\n    Later, ``__table_cls__`` will determine if the model looks like single or\n    joined-table inheritance. If no primary key is found, the name will be unset.\n    \"\"\"\n    if (\n        cls.__dict__.get(\"__abstract__\", False)\n        or (\n            not issubclass(cls, (sa_orm.DeclarativeBase, sa_orm.DeclarativeBaseNoMeta))\n            and not any(isinstance(b, sa_orm.DeclarativeMeta) for b in cls.__mro__[1:])\n        )\n        or any(\n            (b is sa_orm.DeclarativeBase or b is sa_orm.DeclarativeBaseNoMeta)\n            for b in cls.__bases__\n        )\n    ):\n        return False\n\n    for base in cls.__mro__:\n        if \"__tablename__\" not in base.__dict__:\n            continue\n\n        if isinstance(base.__dict__[\"__tablename__\"], sa_orm.declared_attr):\n            return False\n\n        return not (\n            base is cls\n            or base.__dict__.get(\"__abstract__\", False)\n            or not (\n                # SQLAlchemy 1.x\n                isinstance(base, sa_orm.DeclarativeMeta)\n                # 2.x: DeclarativeBas uses this as metaclass\n                or isinstance(base, sa_orm.decl_api.DeclarativeAttributeIntercept)\n                # 2.x: DeclarativeBaseNoMeta doesn't use a metaclass\n                or issubclass(base, sa_orm.DeclarativeBaseNoMeta)\n            )\n        )\n\n    return True\n\n\ndef camel_to_snake_case(name: str) -> str:\n    \"\"\"Convert a ``CamelCase`` name to ``snake_case``.\"\"\"\n    name = re.sub(r\"((?<=[a-z0-9])[A-Z]|(?!^)[A-Z](?=[a-z]))\", r\"_\\1\", name)\n    return name.lower().lstrip(\"_\")\n\n\nclass DefaultMeta(BindMetaMixin, NameMetaMixin, sa_orm.DeclarativeMeta):\n    \"\"\"SQLAlchemy declarative metaclass that provides ``__bind_key__`` and\n    ``__tablename__`` support.\n    \"\"\"\n\n\nclass DefaultMetaNoName(BindMetaMixin, sa_orm.DeclarativeMeta):\n    \"\"\"SQLAlchemy declarative metaclass that provides ``__bind_key__`` and\n    ``__tablename__`` support.\n    \"\"\"\n", "metadata": {"source": "github.com/pallets-eco/flask-sqlalchemy", "file_path": "src/flask_sqlalchemy/model.py", "language": "python", "complexity": "complex", "star_count": 4297, "quality_score": 0.78, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": true, "has_type_hints": true, "has_testing": true, "has_logging": false, "function_count": 10, "class_count": 23, "line_count": 329}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "database_integration", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create database model with ORM relationships and queries\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from __future__ import annotations\n\nimport typing as t\nfrom math import ceil\n\nimport sqlalchemy as sa\nimport sqlalchemy.orm as sa_orm\nfrom flask import abort\nfrom flask import request\n\n\nclass Pagination:\n    \"\"\"Apply an offset and limit to the query based on the current page and number of\n    items per page.\n\n    Don't create pagination objects manually. They are created by\n    :meth:`.SQLAlchemy.paginate` and :meth:`.Query.paginate`.\n\n    This is a base class, a subclass must implement :meth:`_query_items` and\n    :meth:`_query_count`. Those methods will use arguments passed as ``kwargs`` to\n    perform the queries.\n\n    :param page: The current page, used to calculate the offset. Defaults to the\n        ``page`` query arg during a request, or 1 otherwise.\n    :param per_page: The maximum number of items on a page, used to calculate the\n        offset and limit. Defaults to the ``per_page`` query arg during a request,\n        or 20 otherwise.\n    :param max_per_page: The maximum allowed value for ``per_page``, to limit a\n        user-provided value. Use ``None`` for no limit. Defaults to 100.\n    :param error_out: Abort with a ``404 Not Found`` error if no items are returned\n        and ``page`` is not 1, or if ``page`` or ``per_page`` is less than 1, or if\n        either are not ints.\n    :param count: Calculate the total number of values by issuing an extra count\n        query. For very complex queries this may be inaccurate or slow, so it can be\n        disabled and set manually if necessary.\n    :param kwargs: Information about the query to paginate. Different subclasses will\n        require different arguments.\n\n    .. versionchanged:: 3.0\n        Iterating over a pagination object iterates over its items.\n\n    .. versionchanged:: 3.0\n        Creating instances manually is not a public API.\n    \"\"\"\n\n    def __init__(\n        self,\n        page: int | None = None,\n        per_page: int | None = None,\n        max_per_page: int | None = 100,\n        error_out: bool = True,\n        count: bool = True,\n        **kwargs: t.Any,\n    ) -> None:\n        self._query_args = kwargs\n        page, per_page = self._prepare_page_args(\n            page=page,\n            per_page=per_page,\n            max_per_page=max_per_page,\n            error_out=error_out,\n        )\n\n        self.page: int = page\n        \"\"\"The current page.\"\"\"\n\n        self.per_page: int = per_page\n        \"\"\"The maximum number of items on a page.\"\"\"\n\n        self.max_per_page: int | None = max_per_page\n        \"\"\"The maximum allowed value for ``per_page``.\"\"\"\n\n        items = self._query_items()\n\n        if not items and page != 1 and error_out:\n            abort(404)\n\n        self.items: list[t.Any] = items\n        \"\"\"The items on the current page. Iterating over the pagination object is\n        equivalent to iterating over the items.\n        \"\"\"\n\n        if count:\n            total = self._query_count()\n        else:\n            total = None\n\n        self.total: int | None = total\n        \"\"\"The total number of items across all pages.\"\"\"\n\n    @staticmethod\n    def _prepare_page_args(\n        *,\n        page: int | None = None,\n        per_page: int | None = None,\n        max_per_page: int | None = None,\n        error_out: bool = True,\n    ) -> tuple[int, int]:\n        if request:\n            if page is None:\n                try:\n                    page = int(request.args.get(\"page\", 1))\n                except (TypeError, ValueError):\n                    if error_out:\n                        abort(404)\n\n                    page = 1\n\n            if per_page is None:\n                try:\n                    per_page = int(request.args.get(\"per_page\", 20))\n                except (TypeError, ValueError):\n                    if error_out:\n                        abort(404)\n\n                    per_page = 20\n        else:\n            if page is None:\n                page = 1\n\n            if per_page is None:\n                per_page = 20\n\n        if max_per_page is not None:\n            per_page = min(per_page, max_per_page)\n\n        if page < 1:\n            if error_out:\n                abort(404)\n            else:\n                page = 1\n\n        if per_page < 1:\n            if error_out:\n                abort(404)\n            else:\n                per_page = 20\n\n        return page, per_page\n\n    @property\n    def _query_offset(self) -> int:\n        \"\"\"The index of the first item to query, passed to ``offset()``.\n\n        :meta private:\n\n        .. versionadded:: 3.0\n        \"\"\"\n        return (self.page - 1) * self.per_page\n\n    def _query_items(self) -> list[t.Any]:\n        \"\"\"Execute the query to get the items on the current page.\n\n        Uses init arguments stored in :attr:`_query_args`.\n\n        :meta private:\n\n        .. versionadded:: 3.0\n        \"\"\"\n        raise NotImplementedError\n\n    def _query_count(self) -> int:\n        \"\"\"Execute the query to get the total number of items.\n\n        Uses init arguments stored in :attr:`_query_args`.\n\n        :meta private:\n\n        .. versionadded:: 3.0\n        \"\"\"\n        raise NotImplementedError\n\n    @property\n    def first(self) -> int:\n        \"\"\"The number of the first item on the page, starting from 1, or 0 if there are\n        no items.\n\n        .. versionadded:: 3.0\n        \"\"\"\n        if len(self.items) == 0:\n            return 0\n\n        return (self.page - 1) * self.per_page + 1\n\n    @property\n    def last(self) -> int:\n        \"\"\"The number of the last item on the page, starting from 1, inclusive, or 0 if\n        there are no items.\n\n        .. versionadded:: 3.0\n        \"\"\"\n        first = self.first\n        return max(first, first + len(self.items) - 1)\n\n    @property\n    def pages(self) -> int:\n        \"\"\"The total number of pages.\"\"\"\n        if self.total == 0 or self.total is None:\n            return 0\n\n        return ceil(self.total / self.per_page)\n\n    @property\n    def has_prev(self) -> bool:\n        \"\"\"``True`` if this is not the first page.\"\"\"\n        return self.page > 1\n\n    @property\n    def prev_num(self) -> int | None:\n        \"\"\"The previous page number, or ``None`` if this is the first page.\"\"\"\n        if not self.has_prev:\n            return None\n\n        return self.page - 1\n\n    def prev(self, *, error_out: bool = False) -> Pagination:\n        \"\"\"Query the :class:`Pagination` object for the previous page.\n\n        :param error_out: Abort with a ``404 Not Found`` error if no items are returned\n            and ``page`` is not 1, or if ``page`` or ``per_page`` is less than 1, or if\n            either are not ints.\n        \"\"\"\n        p = type(self)(\n            page=self.page - 1,\n            per_page=self.per_page,\n            error_out=error_out,\n            count=False,\n            **self._query_args,\n        )\n        p.total = self.total\n        return p\n\n    @property\n    def has_next(self) -> bool:\n        \"\"\"``True`` if this is not the last page.\"\"\"\n        return self.page < self.pages\n\n    @property\n    def next_num(self) -> int | None:\n        \"\"\"The next page number, or ``None`` if this is the last page.\"\"\"\n        if not self.has_next:\n            return None\n\n        return self.page + 1\n\n    def next(self, *, error_out: bool = False) -> Pagination:\n        \"\"\"Query the :class:`Pagination` object for the next page.\n\n        :param error_out: Abort with a ``404 Not Found`` error if no items are returned\n            and ``page`` is not 1, or if ``page`` or ``per_page`` is less than 1, or if\n            either are not ints.\n        \"\"\"\n        p = type(self)(\n            page=self.page + 1,\n            per_page=self.per_page,\n            max_per_page=self.max_per_page,\n            error_out=error_out,\n            count=False,\n            **self._query_args,\n        )\n        p.total = self.total\n        return p\n\n    def iter_pages(\n        self,\n        *,\n        left_edge: int = 2,\n        left_current: int = 2,\n        right_current: int = 4,\n        right_edge: int = 2,\n    ) -> t.Iterator[int | None]:\n        \"\"\"Yield page numbers for a pagination widget. Skipped pages between the edges\n        and middle are represented by a ``None``.\n\n        For example, if there are 20 pages and the current page is 7, the following\n        values are yielded.\n\n        .. code-block:: python\n\n            1, 2, None, 5, 6, 7, 8, 9, 10, 11, None, 19, 20\n\n        :param left_edge: How many pages to show from the first page.\n        :param left_current: How many pages to show left of the current page.\n        :param right_current: How many pages to show right of the current page.\n        :param right_edge: How many pages to show from the last page.\n\n        .. versionchanged:: 3.0\n            Improved efficiency of calculating what to yield.\n\n        .. versionchanged:: 3.0\n            ``right_current`` boundary is inclusive.\n\n        .. versionchanged:: 3.0\n            All parameters are keyword-only.\n        \"\"\"\n        pages_end = self.pages + 1\n\n        if pages_end == 1:\n            return\n\n        left_end = min(1 + left_edge, pages_end)\n        yield from range(1, left_end)\n\n        if left_end == pages_end:\n            return\n\n        mid_start = max(left_end, self.page - left_current)\n        mid_end = min(self.page + right_current + 1, pages_end)\n\n        if mid_start - left_end > 0:\n            yield None\n\n        yield from range(mid_start, mid_end)\n\n        if mid_end == pages_end:\n            return\n\n        right_start = max(mid_end, pages_end - right_edge)\n\n        if right_start - mid_end > 0:\n            yield None\n\n        yield from range(right_start, pages_end)\n\n    def __iter__(self) -> t.Iterator[t.Any]:\n        yield from self.items\n\n\nclass SelectPagination(Pagination):\n    \"\"\"Returned by :meth:`.SQLAlchemy.paginate`. Takes ``select`` and ``session``\n    arguments in addition to the :class:`Pagination` arguments.\n\n    .. versionadded:: 3.0\n    \"\"\"\n\n    def _query_items(self) -> list[t.Any]:\n        select = self._query_args[\"select\"]\n        select = select.limit(self.per_page).offset(self._query_offset)\n        session = self._query_args[\"session\"]\n        return list(session.execute(select).unique().scalars())\n\n    def _query_count(self) -> int:\n        select = self._query_args[\"select\"]\n        sub = select.options(sa_orm.lazyload(\"*\")).order_by(None).subquery()\n        session = self._query_args[\"session\"]\n        out = session.execute(sa.select(sa.func.count()).select_from(sub)).scalar()\n        return out  # type: ignore[no-any-return]\n\n\nclass QueryPagination(Pagination):\n    \"\"\"Returned by :meth:`.Query.paginate`. Takes a ``query`` argument in addition to\n    the :class:`Pagination` arguments.\n\n    .. versionadded:: 3.0\n    \"\"\"\n\n    def _query_items(self) -> list[t.Any]:\n        query = self._query_args[\"query\"]\n        out = query.limit(self.per_page).offset(self._query_offset).all()\n        return out  # type: ignore[no-any-return]\n\n    def _query_count(self) -> int:\n        # Query.count automatically disables eager loads\n        out = self._query_args[\"query\"].order_by(None).count()\n        return out  # type: ignore[no-any-return]\n", "metadata": {"source": "github.com/pallets-eco/flask-sqlalchemy", "file_path": "src/flask_sqlalchemy/pagination.py", "language": "python", "complexity": "complex", "star_count": 4297, "quality_score": 0.78, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": true, "has_documentation": true, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 20, "class_count": 4, "line_count": 365}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "database_integration", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create database model with ORM relationships and queries with SQLAlchemy/Django ORM patterns\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from __future__ import annotations\n\nimport typing as t\n\nimport sqlalchemy.exc as sa_exc\nimport sqlalchemy.orm as sa_orm\nfrom flask import abort\n\nfrom .pagination import Pagination\nfrom .pagination import QueryPagination\n\n\nclass Query(sa_orm.Query):  # type: ignore[type-arg]\n    \"\"\"SQLAlchemy :class:`~sqlalchemy.orm.query.Query` subclass with some extra methods\n    useful for querying in a web application.\n\n    This is the default query class for :attr:`.Model.query`.\n\n    .. versionchanged:: 3.0\n        Renamed to ``Query`` from ``BaseQuery``.\n    \"\"\"\n\n    def get_or_404(self, ident: t.Any, description: str | None = None) -> t.Any:\n        \"\"\"Like :meth:`~sqlalchemy.orm.Query.get` but aborts with a ``404 Not Found``\n        error instead of returning ``None``.\n\n        :param ident: The primary key to query.\n        :param description: A custom message to show on the error page.\n        \"\"\"\n        rv = self.get(ident)\n\n        if rv is None:\n            abort(404, description=description)\n\n        return rv\n\n    def first_or_404(self, description: str | None = None) -> t.Any:\n        \"\"\"Like :meth:`~sqlalchemy.orm.Query.first` but aborts with a ``404 Not Found``\n        error instead of returning ``None``.\n\n        :param description: A custom message to show on the error page.\n        \"\"\"\n        rv = self.first()\n\n        if rv is None:\n            abort(404, description=description)\n\n        return rv\n\n    def one_or_404(self, description: str | None = None) -> t.Any:\n        \"\"\"Like :meth:`~sqlalchemy.orm.Query.one` but aborts with a ``404 Not Found``\n        error instead of raising ``NoResultFound`` or ``MultipleResultsFound``.\n\n        :param description: A custom message to show on the error page.\n\n        .. versionadded:: 3.0\n        \"\"\"\n        try:\n            return self.one()\n        except (sa_exc.NoResultFound, sa_exc.MultipleResultsFound):\n            abort(404, description=description)\n\n    def paginate(\n        self,\n        *,\n        page: int | None = None,\n        per_page: int | None = None,\n        max_per_page: int | None = None,\n        error_out: bool = True,\n        count: bool = True,\n    ) -> Pagination:\n        \"\"\"Apply an offset and limit to the query based on the current page and number\n        of items per page, returning a :class:`.Pagination` object.\n\n        :param page: The current page, used to calculate the offset. Defaults to the\n            ``page`` query arg during a request, or 1 otherwise.\n        :param per_page: The maximum number of items on a page, used to calculate the\n            offset and limit. Defaults to the ``per_page`` query arg during a request,\n            or 20 otherwise.\n        :param max_per_page: The maximum allowed value for ``per_page``, to limit a\n            user-provided value. Use ``None`` for no limit. Defaults to 100.\n        :param error_out: Abort with a ``404 Not Found`` error if no items are returned\n            and ``page`` is not 1, or if ``page`` or ``per_page`` is less than 1, or if\n            either are not ints.\n        :param count: Calculate the total number of values by issuing an extra count\n            query. For very complex queries this may be inaccurate or slow, so it can be\n            disabled and set manually if necessary.\n\n        .. versionchanged:: 3.0\n            All parameters are keyword-only.\n\n        .. versionchanged:: 3.0\n            The ``count`` query is more efficient.\n\n        .. versionchanged:: 3.0\n            ``max_per_page`` defaults to 100.\n        \"\"\"\n        return QueryPagination(\n            query=self,\n            page=page,\n            per_page=per_page,\n            max_per_page=max_per_page,\n            error_out=error_out,\n            count=count,\n        )\n", "metadata": {"source": "github.com/pallets-eco/flask-sqlalchemy", "file_path": "src/flask_sqlalchemy/query.py", "language": "python", "complexity": "medium", "star_count": 4297, "quality_score": 0.78, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": true, "has_documentation": true, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 4, "class_count": 3, "line_count": 106}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "database_integration", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create database model with ORM relationships and queries\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from __future__ import annotations\n\nimport dataclasses\nimport inspect\nimport typing as t\nfrom time import perf_counter\n\nimport sqlalchemy as sa\nimport sqlalchemy.event as sa_event\nfrom flask import current_app\nfrom flask import g\nfrom flask import has_app_context\n\n\ndef get_recorded_queries() -> list[_QueryInfo]:\n    \"\"\"Get the list of recorded query information for the current session. Queries are\n    recorded if the config :data:`.SQLALCHEMY_RECORD_QUERIES` is enabled.\n\n    Each query info object has the following attributes:\n\n    ``statement``\n        The string of SQL generated by SQLAlchemy with parameter placeholders.\n    ``parameters``\n        The parameters sent with the SQL statement.\n    ``start_time`` / ``end_time``\n        Timing info about when the query started execution and when the results where\n        returned. Accuracy and value depends on the operating system.\n    ``duration``\n        The time the query took in seconds.\n    ``location``\n        A string description of where in your application code the query was executed.\n        This may not be possible to calculate, and the format is not stable.\n\n    .. versionchanged:: 3.0\n        Renamed from ``get_debug_queries``.\n\n    .. versionchanged:: 3.0\n        The info object is a dataclass instead of a tuple.\n\n    .. versionchanged:: 3.0\n        The info object attribute ``context`` is renamed to ``location``.\n\n    .. versionchanged:: 3.0\n        Not enabled automatically in debug or testing mode.\n    \"\"\"\n    return g.get(\"_sqlalchemy_queries\", [])  # type: ignore[no-any-return]\n\n\n@dataclasses.dataclass\nclass _QueryInfo:\n    \"\"\"Information about an executed query. Returned by :func:`get_recorded_queries`.\n\n    .. versionchanged:: 3.0\n        Renamed from ``_DebugQueryTuple``.\n\n    .. versionchanged:: 3.0\n        Changed to a dataclass instead of a tuple.\n\n    .. versionchanged:: 3.0\n        ``context`` is renamed to ``location``.\n    \"\"\"\n\n    statement: str | None\n    parameters: t.Any\n    start_time: float\n    end_time: float\n    location: str\n\n    @property\n    def duration(self) -> float:\n        return self.end_time - self.start_time\n\n\ndef _listen(engine: sa.engine.Engine) -> None:\n    sa_event.listen(engine, \"before_cursor_execute\", _record_start, named=True)\n    sa_event.listen(engine, \"after_cursor_execute\", _record_end, named=True)\n\n\ndef _record_start(context: sa.engine.ExecutionContext, **kwargs: t.Any) -> None:\n    if not has_app_context():\n        return\n\n    context._fsa_start_time = perf_counter()  # type: ignore[attr-defined]\n\n\ndef _record_end(context: sa.engine.ExecutionContext, **kwargs: t.Any) -> None:\n    if not has_app_context():\n        return\n\n    if \"_sqlalchemy_queries\" not in g:\n        g._sqlalchemy_queries = []\n\n    import_top = current_app.import_name.partition(\".\")[0]\n    import_dot = f\"{import_top}.\"\n    frame = inspect.currentframe()\n\n    while frame:\n        name = frame.f_globals.get(\"__name__\")\n\n        if name and (name == import_top or name.startswith(import_dot)):\n            code = frame.f_code\n            location = f\"{code.co_filename}:{frame.f_lineno} ({code.co_name})\"\n            break\n\n        frame = frame.f_back\n    else:\n        location = \"<unknown>\"\n\n    g._sqlalchemy_queries.append(\n        _QueryInfo(\n            statement=context.statement,\n            parameters=context.parameters,\n            start_time=context._fsa_start_time,  # type: ignore[attr-defined]\n            end_time=perf_counter(),\n            location=location,\n        )\n    )\n", "metadata": {"source": "github.com/pallets-eco/flask-sqlalchemy", "file_path": "src/flask_sqlalchemy/record_queries.py", "language": "python", "complexity": "medium", "star_count": 4297, "quality_score": 0.78, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": true, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 5, "class_count": 3, "line_count": 118}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "database_integration", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create database model with ORM relationships and queries with asyncio coroutines\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import getpass\nimport json\nimport logging\nimport typing\nimport uuid\n\nimport aiopg\nfrom sqlalchemy.engine.cursor import CursorResultMetaData\nfrom sqlalchemy.engine.interfaces import Dialect, ExecutionContext\nfrom sqlalchemy.engine.row import Row\nfrom sqlalchemy.sql import ClauseElement\nfrom sqlalchemy.sql.ddl import DDLElement\n\nfrom databases.backends.common.records import Record, Row, create_column_maps\nfrom databases.backends.compilers.psycopg import PGCompiler_psycopg\nfrom databases.backends.dialects.psycopg import PGDialect_psycopg\nfrom databases.core import LOG_EXTRA, DatabaseURL\nfrom databases.interfaces import (\n    ConnectionBackend,\n    DatabaseBackend,\n    Record as RecordInterface,\n    TransactionBackend,\n)\n\nlogger = logging.getLogger(\"databases\")\n\n\nclass AiopgBackend(DatabaseBackend):\n    def __init__(\n        self, database_url: typing.Union[DatabaseURL, str], **options: typing.Any\n    ) -> None:\n        self._database_url = DatabaseURL(database_url)\n        self._options = options\n        self._dialect = self._get_dialect()\n        self._pool: typing.Union[aiopg.Pool, None] = None\n\n    def _get_dialect(self) -> Dialect:\n        dialect = PGDialect_psycopg(\n            json_serializer=json.dumps, json_deserializer=lambda x: x\n        )\n        dialect.statement_compiler = PGCompiler_psycopg\n        dialect.implicit_returning = True\n        dialect.supports_native_enum = True\n        dialect.supports_smallserial = True  # 9.2+\n        dialect._backslash_escapes = False\n        dialect.supports_sane_multi_rowcount = True  # psycopg 2.0.9+\n        dialect._has_native_hstore = True\n        dialect.supports_native_decimal = True\n\n        return dialect\n\n    def _get_connection_kwargs(self) -> dict:\n        url_options = self._database_url.options\n\n        kwargs = {}\n        min_size = url_options.get(\"min_size\")\n        max_size = url_options.get(\"max_size\")\n        ssl = url_options.get(\"ssl\")\n\n        if min_size is not None:\n            kwargs[\"minsize\"] = int(min_size)\n        if max_size is not None:\n            kwargs[\"maxsize\"] = int(max_size)\n        if ssl is not None:\n            kwargs[\"ssl\"] = {\"true\": True, \"false\": False}[ssl.lower()]\n\n        for key, value in self._options.items():\n            # Coerce 'min_size' and 'max_size' for consistency.\n            if key == \"min_size\":\n                key = \"minsize\"\n            elif key == \"max_size\":\n                key = \"maxsize\"\n            kwargs[key] = value\n\n        return kwargs\n\n    async def connect(self) -> None:\n        assert self._pool is None, \"DatabaseBackend is already running\"\n        kwargs = self._get_connection_kwargs()\n        self._pool = await aiopg.create_pool(\n            host=self._database_url.hostname,\n            port=self._database_url.port,\n            user=self._database_url.username or getpass.getuser(),\n            password=self._database_url.password,\n            database=self._database_url.database,\n            **kwargs,\n        )\n\n    async def disconnect(self) -> None:\n        assert self._pool is not None, \"DatabaseBackend is not running\"\n        self._pool.close()\n        await self._pool.wait_closed()\n        self._pool = None\n\n    def connection(self) -> \"AiopgConnection\":\n        return AiopgConnection(self, self._dialect)\n\n\nclass CompilationContext:\n    def __init__(self, context: ExecutionContext):\n        self.context = context\n\n\nclass AiopgConnection(ConnectionBackend):\n    def __init__(self, database: AiopgBackend, dialect: Dialect):\n        self._database = database\n        self._dialect = dialect\n        self._connection: typing.Optional[aiopg.Connection] = None\n\n    async def acquire(self) -> None:\n        assert self._connection is None, \"Connection is already acquired\"\n        assert self._database._pool is not None, \"DatabaseBackend is not running\"\n        self._connection = await self._database._pool.acquire()\n\n    async def release(self) -> None:\n        assert self._connection is not None, \"Connection is not acquired\"\n        assert self._database._pool is not None, \"DatabaseBackend is not running\"\n        await self._database._pool.release(self._connection)\n        self._connection = None\n\n    async def fetch_all(self, query: ClauseElement) -> typing.List[RecordInterface]:\n        assert self._connection is not None, \"Connection is not acquired\"\n        query_str, args, result_columns, context = self._compile(query)\n        column_maps = create_column_maps(result_columns)\n        dialect = self._dialect\n\n        cursor = await self._connection.cursor()\n        try:\n            await cursor.execute(query_str, args)\n            rows = await cursor.fetchall()\n            metadata = CursorResultMetaData(context, cursor.description)\n            rows = [\n                Row(\n                    metadata,\n                    metadata._processors,\n                    metadata._keymap,\n                    row,\n                )\n                for row in rows\n            ]\n            return [Record(row, result_columns, dialect, column_maps) for row in rows]\n        finally:\n            cursor.close()\n\n    async def fetch_one(self, query: ClauseElement) -> typing.Optional[RecordInterface]:\n        assert self._connection is not None, \"Connection is not acquired\"\n        query_str, args, result_columns, context = self._compile(query)\n        column_maps = create_column_maps(result_columns)\n        dialect = self._dialect\n        cursor = await self._connection.cursor()\n        try:\n            await cursor.execute(query_str, args)\n            row = await cursor.fetchone()\n            if row is None:\n                return None\n            metadata = CursorResultMetaData(context, cursor.description)\n            row = Row(\n                metadata,\n                metadata._processors,\n                metadata._keymap,\n                row,\n            )\n            return Record(row, result_columns, dialect, column_maps)\n        finally:\n            cursor.close()\n\n    async def execute(self, query: ClauseElement) -> typing.Any:\n        assert self._connection is not None, \"Connection is not acquired\"\n        query_str, args, _, _ = self._compile(query)\n        cursor = await self._connection.cursor()\n        try:\n            await cursor.execute(query_str, args)\n            return cursor.lastrowid\n        finally:\n            cursor.close()\n\n    async def execute_many(self, queries: typing.List[ClauseElement]) -> None:\n        assert self._connection is not None, \"Connection is not acquired\"\n        cursor = await self._connection.cursor()\n        try:\n            for single_query in queries:\n                single_query, args, _, _ = self._compile(single_query)\n                await cursor.execute(single_query, args)\n        finally:\n            cursor.close()\n\n    async def iterate(\n        self, query: ClauseElement\n    ) -> typing.AsyncGenerator[typing.Any, None]:\n        assert self._connection is not None, \"Connection is not acquired\"\n        query_str, args, result_columns, context = self._compile(query)\n        column_maps = create_column_maps(result_columns)\n        dialect = self._dialect\n        cursor = await self._connection.cursor()\n        try:\n            await cursor.execute(query_str, args)\n            metadata = CursorResultMetaData(context, cursor.description)\n            async for row in cursor:\n                record = Row(\n                    metadata,\n                    metadata._processors,\n                    metadata._keymap,\n                    row,\n                )\n                yield Record(record, result_columns, dialect, column_maps)\n        finally:\n            cursor.close()\n\n    def transaction(self) -> TransactionBackend:\n        return AiopgTransaction(self)\n\n    def _compile(self, query: ClauseElement) -> typing.Tuple[str, list, tuple]:\n        compiled = query.compile(\n            dialect=self._dialect, compile_kwargs={\"render_postcompile\": True}\n        )\n        execution_context = self._dialect.execution_ctx_cls()\n        execution_context.dialect = self._dialect\n\n        if not isinstance(query, DDLElement):\n            compiled_params = sorted(compiled.params.items())\n\n            args = compiled.construct_params()\n            for key, val in args.items():\n                if key in compiled._bind_processors:\n                    args[key] = compiled._bind_processors[key](val)\n\n            execution_context.result_column_struct = (\n                compiled._result_columns,\n                compiled._ordered_columns,\n                compiled._textual_ordered_columns,\n                compiled._ad_hoc_textual,\n                compiled._loose_column_name_matching,\n            )\n\n            mapping = {\n                key: \"$\" + str(i) for i, (key, _) in enumerate(compiled_params, start=1)\n            }\n            compiled_query = compiled.string % mapping\n            result_map = compiled._result_columns\n\n        else:\n            args = {}\n            result_map = None\n            compiled_query = compiled.string\n\n        query_message = compiled_query.replace(\" \\n\", \" \").replace(\"\\n\", \" \")\n        logger.debug(\n            \"Query: %s Args: %s\", query_message, repr(tuple(args)), extra=LOG_EXTRA\n        )\n        return compiled.string, args, result_map, CompilationContext(execution_context)\n\n    @property\n    def raw_connection(self) -> aiopg.connection.Connection:\n        assert self._connection is not None, \"Connection is not acquired\"\n        return self._connection\n\n\nclass AiopgTransaction(TransactionBackend):\n    def __init__(self, connection: AiopgConnection):\n        self._connection = connection\n        self._is_root = False\n        self._savepoint_name = \"\"\n\n    async def start(\n        self, is_root: bool, extra_options: typing.Dict[typing.Any, typing.Any]\n    ) -> None:\n        assert self._connection._connection is not None, \"Connection is not acquired\"\n        self._is_root = is_root\n        cursor = await self._connection._connection.cursor()\n        if self._is_root:\n            await cursor.execute(\"BEGIN\")\n        else:\n            id = str(uuid.uuid4()).replace(\"-\", \"_\")\n            self._savepoint_name = f\"STARLETTE_SAVEPOINT_{id}\"\n            try:\n                await cursor.execute(f\"SAVEPOINT {self._savepoint_name}\")\n            finally:\n                cursor.close()\n\n    async def commit(self) -> None:\n        assert self._connection._connection is not None, \"Connection is not acquired\"\n        cursor = await self._connection._connection.cursor()\n        if self._is_root:\n            await cursor.execute(\"COMMIT\")\n        else:\n            try:\n                await cursor.execute(f\"RELEASE SAVEPOINT {self._savepoint_name}\")\n            finally:\n                cursor.close()\n\n    async def rollback(self) -> None:\n        assert self._connection._connection is not None, \"Connection is not acquired\"\n        cursor = await self._connection._connection.cursor()\n        if self._is_root:\n            await cursor.execute(\"ROLLBACK\")\n        else:\n            try:\n                await cursor.execute(f\"ROLLBACK TO SAVEPOINT {self._savepoint_name}\")\n            finally:\n                cursor.close()\n", "metadata": {"source": "github.com/encode/databases", "file_path": "databases/backends/aiopg.py", "language": "python", "complexity": "complex", "star_count": 3928, "quality_score": 0.87, "features": {"has_classes": true, "has_functions": true, "has_async": true, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": true, "has_logging": true, "function_count": 22, "class_count": 4, "line_count": 301}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "database_integration", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create database model with ORM relationships and queries with asyncio coroutines\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import getpass\nimport logging\nimport typing\nimport uuid\n\nimport asyncmy\nfrom sqlalchemy.dialects.mysql import pymysql\nfrom sqlalchemy.engine.cursor import CursorResultMetaData\nfrom sqlalchemy.engine.interfaces import Dialect, ExecutionContext\nfrom sqlalchemy.sql import ClauseElement\nfrom sqlalchemy.sql.ddl import DDLElement\n\nfrom databases.backends.common.records import Record, Row, create_column_maps\nfrom databases.core import LOG_EXTRA, DatabaseURL\nfrom databases.interfaces import (\n    ConnectionBackend,\n    DatabaseBackend,\n    Record as RecordInterface,\n    TransactionBackend,\n)\n\nlogger = logging.getLogger(\"databases\")\n\n\nclass AsyncMyBackend(DatabaseBackend):\n    def __init__(\n        self, database_url: typing.Union[DatabaseURL, str], **options: typing.Any\n    ) -> None:\n        self._database_url = DatabaseURL(database_url)\n        self._options = options\n        self._dialect = pymysql.dialect(paramstyle=\"pyformat\")\n        self._dialect.supports_native_decimal = True\n        self._pool = None\n\n    def _get_connection_kwargs(self) -> dict:\n        url_options = self._database_url.options\n\n        kwargs = {}\n        min_size = url_options.get(\"min_size\")\n        max_size = url_options.get(\"max_size\")\n        pool_recycle = url_options.get(\"pool_recycle\")\n        ssl = url_options.get(\"ssl\")\n        unix_socket = url_options.get(\"unix_socket\")\n\n        if min_size is not None:\n            kwargs[\"minsize\"] = int(min_size)\n        if max_size is not None:\n            kwargs[\"maxsize\"] = int(max_size)\n        if pool_recycle is not None:\n            kwargs[\"pool_recycle\"] = int(pool_recycle)\n        if ssl is not None:\n            kwargs[\"ssl\"] = {\"true\": True, \"false\": False}[ssl.lower()]\n        if unix_socket is not None:\n            kwargs[\"unix_socket\"] = unix_socket\n\n        for key, value in self._options.items():\n            # Coerce 'min_size' and 'max_size' for consistency.\n            if key == \"min_size\":\n                key = \"minsize\"\n            elif key == \"max_size\":\n                key = \"maxsize\"\n            kwargs[key] = value\n\n        return kwargs\n\n    async def connect(self) -> None:\n        assert self._pool is None, \"DatabaseBackend is already running\"\n        kwargs = self._get_connection_kwargs()\n        self._pool = await asyncmy.create_pool(\n            host=self._database_url.hostname,\n            port=self._database_url.port or 3306,\n            user=self._database_url.username or getpass.getuser(),\n            password=self._database_url.password,\n            db=self._database_url.database,\n            autocommit=True,\n            **kwargs,\n        )\n\n    async def disconnect(self) -> None:\n        assert self._pool is not None, \"DatabaseBackend is not running\"\n        self._pool.close()\n        await self._pool.wait_closed()\n        self._pool = None\n\n    def connection(self) -> \"AsyncMyConnection\":\n        return AsyncMyConnection(self, self._dialect)\n\n\nclass CompilationContext:\n    def __init__(self, context: ExecutionContext):\n        self.context = context\n\n\nclass AsyncMyConnection(ConnectionBackend):\n    def __init__(self, database: AsyncMyBackend, dialect: Dialect):\n        self._database = database\n        self._dialect = dialect\n        self._connection: typing.Optional[asyncmy.Connection] = None\n\n    async def acquire(self) -> None:\n        assert self._connection is None, \"Connection is already acquired\"\n        assert self._database._pool is not None, \"DatabaseBackend is not running\"\n        self._connection = await self._database._pool.acquire()\n\n    async def release(self) -> None:\n        assert self._connection is not None, \"Connection is not acquired\"\n        assert self._database._pool is not None, \"DatabaseBackend is not running\"\n        await self._database._pool.release(self._connection)\n        self._connection = None\n\n    async def fetch_all(self, query: ClauseElement) -> typing.List[RecordInterface]:\n        assert self._connection is not None, \"Connection is not acquired\"\n        query_str, args, result_columns, context = self._compile(query)\n        column_maps = create_column_maps(result_columns)\n        dialect = self._dialect\n\n        async with self._connection.cursor() as cursor:\n            try:\n                await cursor.execute(query_str, args)\n                rows = await cursor.fetchall()\n                metadata = CursorResultMetaData(context, cursor.description)\n                rows = [\n                    Row(\n                        metadata,\n                        metadata._processors,\n                        metadata._keymap,\n                        row,\n                    )\n                    for row in rows\n                ]\n                return [\n                    Record(row, result_columns, dialect, column_maps) for row in rows\n                ]\n            finally:\n                await cursor.close()\n\n    async def fetch_one(self, query: ClauseElement) -> typing.Optional[RecordInterface]:\n        assert self._connection is not None, \"Connection is not acquired\"\n        query_str, args, result_columns, context = self._compile(query)\n        column_maps = create_column_maps(result_columns)\n        dialect = self._dialect\n        async with self._connection.cursor() as cursor:\n            try:\n                await cursor.execute(query_str, args)\n                row = await cursor.fetchone()\n                if row is None:\n                    return None\n                metadata = CursorResultMetaData(context, cursor.description)\n                row = Row(\n                    metadata,\n                    metadata._processors,\n                    metadata._keymap,\n                    row,\n                )\n                return Record(row, result_columns, dialect, column_maps)\n            finally:\n                await cursor.close()\n\n    async def execute(self, query: ClauseElement) -> typing.Any:\n        assert self._connection is not None, \"Connection is not acquired\"\n        query_str, args, _, _ = self._compile(query)\n        async with self._connection.cursor() as cursor:\n            try:\n                await cursor.execute(query_str, args)\n                if cursor.lastrowid == 0:\n                    return cursor.rowcount\n                return cursor.lastrowid\n            finally:\n                await cursor.close()\n\n    async def execute_many(self, queries: typing.List[ClauseElement]) -> None:\n        assert self._connection is not None, \"Connection is not acquired\"\n        async with self._connection.cursor() as cursor:\n            try:\n                for single_query in queries:\n                    single_query, args, _, _ = self._compile(single_query)\n                    await cursor.execute(single_query, args)\n            finally:\n                await cursor.close()\n\n    async def iterate(\n        self, query: ClauseElement\n    ) -> typing.AsyncGenerator[typing.Any, None]:\n        assert self._connection is not None, \"Connection is not acquired\"\n        query_str, args, result_columns, context = self._compile(query)\n        column_maps = create_column_maps(result_columns)\n        dialect = self._dialect\n        async with self._connection.cursor() as cursor:\n            try:\n                await cursor.execute(query_str, args)\n                metadata = CursorResultMetaData(context, cursor.description)\n                async for row in cursor:\n                    record = Row(\n                        metadata,\n                        metadata._processors,\n                        metadata._keymap,\n                        row,\n                    )\n                    yield Record(record, result_columns, dialect, column_maps)\n            finally:\n                await cursor.close()\n\n    def transaction(self) -> TransactionBackend:\n        return AsyncMyTransaction(self)\n\n    def _compile(self, query: ClauseElement) -> typing.Tuple[str, list, tuple]:\n        compiled = query.compile(\n            dialect=self._dialect, compile_kwargs={\"render_postcompile\": True}\n        )\n        execution_context = self._dialect.execution_ctx_cls()\n        execution_context.dialect = self._dialect\n\n        if not isinstance(query, DDLElement):\n            compiled_params = sorted(compiled.params.items())\n\n            args = compiled.construct_params()\n            for key, val in args.items():\n                if key in compiled._bind_processors:\n                    args[key] = compiled._bind_processors[key](val)\n\n            execution_context.result_column_struct = (\n                compiled._result_columns,\n                compiled._ordered_columns,\n                compiled._textual_ordered_columns,\n                compiled._ad_hoc_textual,\n                compiled._loose_column_name_matching,\n            )\n\n            mapping = {\n                key: \"$\" + str(i) for i, (key, _) in enumerate(compiled_params, start=1)\n            }\n            compiled_query = compiled.string % mapping\n            result_map = compiled._result_columns\n\n        else:\n            args = {}\n            result_map = None\n            compiled_query = compiled.string\n\n        query_message = compiled_query.replace(\" \\n\", \" \").replace(\"\\n\", \" \")\n        logger.debug(\n            \"Query: %s Args: %s\", query_message, repr(tuple(args)), extra=LOG_EXTRA\n        )\n        return compiled.string, args, result_map, CompilationContext(execution_context)\n\n    @property\n    def raw_connection(self) -> asyncmy.connection.Connection:\n        assert self._connection is not None, \"Connection is not acquired\"\n        return self._connection\n\n\nclass AsyncMyTransaction(TransactionBackend):\n    def __init__(self, connection: AsyncMyConnection):\n        self._connection = connection\n        self._is_root = False\n        self._savepoint_name = \"\"\n\n    async def start(\n        self, is_root: bool, extra_options: typing.Dict[typing.Any, typing.Any]\n    ) -> None:\n        assert self._connection._connection is not None, \"Connection is not acquired\"\n        self._is_root = is_root\n        if self._is_root:\n            await self._connection._connection.begin()\n        else:\n            id = str(uuid.uuid4()).replace(\"-\", \"_\")\n            self._savepoint_name = f\"STARLETTE_SAVEPOINT_{id}\"\n            async with self._connection._connection.cursor() as cursor:\n                try:\n                    await cursor.execute(f\"SAVEPOINT {self._savepoint_name}\")\n                finally:\n                    await cursor.close()\n\n    async def commit(self) -> None:\n        assert self._connection._connection is not None, \"Connection is not acquired\"\n        if self._is_root:\n            await self._connection._connection.commit()\n        else:\n            async with self._connection._connection.cursor() as cursor:\n                try:\n                    await cursor.execute(f\"RELEASE SAVEPOINT {self._savepoint_name}\")\n                finally:\n                    await cursor.close()\n\n    async def rollback(self) -> None:\n        assert self._connection._connection is not None, \"Connection is not acquired\"\n        if self._is_root:\n            await self._connection._connection.rollback()\n        else:\n            async with self._connection._connection.cursor() as cursor:\n                try:\n                    await cursor.execute(\n                        f\"ROLLBACK TO SAVEPOINT {self._savepoint_name}\"\n                    )\n                finally:\n                    await cursor.close()\n", "metadata": {"source": "github.com/encode/databases", "file_path": "databases/backends/asyncmy.py", "language": "python", "complexity": "complex", "star_count": 3928, "quality_score": 0.87, "features": {"has_classes": true, "has_functions": true, "has_async": true, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": true, "has_logging": true, "function_count": 21, "class_count": 4, "line_count": 297}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "database_integration", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create database model with ORM relationships and queries\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import enum\nimport typing\nfrom datetime import date, datetime, time\n\nfrom sqlalchemy.engine.interfaces import Dialect\nfrom sqlalchemy.engine.row import Row as SQLRow\nfrom sqlalchemy.sql.compiler import _CompileLabel\nfrom sqlalchemy.sql.schema import Column\nfrom sqlalchemy.sql.sqltypes import JSON\nfrom sqlalchemy.types import TypeEngine\n\nfrom databases.interfaces import Record as RecordInterface\n\nDIALECT_EXCLUDE = {\"postgresql\"}\n\n\nclass Record(RecordInterface):\n    __slots__ = (\n        \"_row\",\n        \"_result_columns\",\n        \"_dialect\",\n        \"_column_map\",\n        \"_column_map_int\",\n        \"_column_map_full\",\n    )\n\n    def __init__(\n        self,\n        row: typing.Any,\n        result_columns: tuple,\n        dialect: Dialect,\n        column_maps: typing.Tuple[\n            typing.Mapping[typing.Any, typing.Tuple[int, TypeEngine]],\n            typing.Mapping[int, typing.Tuple[int, TypeEngine]],\n            typing.Mapping[str, typing.Tuple[int, TypeEngine]],\n        ],\n    ) -> None:\n        self._row = row\n        self._result_columns = result_columns\n        self._dialect = dialect\n        self._column_map, self._column_map_int, self._column_map_full = column_maps\n\n    @property\n    def _mapping(self) -> typing.Mapping:\n        return self._row\n\n    def keys(self) -> typing.KeysView:\n        return self._mapping.keys()\n\n    def values(self) -> typing.ValuesView:\n        return self._mapping.values()\n\n    def __getitem__(self, key: typing.Any) -> typing.Any:\n        if len(self._column_map) == 0:\n            return self._row[key]\n        elif isinstance(key, Column):\n            idx, datatype = self._column_map_full[str(key)]\n        elif isinstance(key, int):\n            idx, datatype = self._column_map_int[key]\n        else:\n            idx, datatype = self._column_map[key]\n\n        raw = self._row[idx]\n        processor = datatype._cached_result_processor(self._dialect, None)\n\n        if self._dialect.name in DIALECT_EXCLUDE:\n            if processor is not None and isinstance(raw, (int, str, float)):\n                return processor(raw)\n\n        return raw\n\n    def __iter__(self) -> typing.Iterator:\n        return iter(self._row.keys())\n\n    def __len__(self) -> int:\n        return len(self._row)\n\n    def __getattr__(self, name: str) -> typing.Any:\n        try:\n            return self.__getitem__(name)\n        except KeyError as e:\n            raise AttributeError(e.args[0]) from e\n\n\nclass Row(SQLRow):\n    def __getitem__(self, key: typing.Any) -> typing.Any:\n        \"\"\"\n        An instance of a Row in SQLAlchemy allows the access\n        to the Row._fields as tuple and the Row._mapping for\n        the values.\n        \"\"\"\n        if isinstance(key, int):\n            return super().__getitem__(key)\n\n        idx = self._key_to_index[key][0]\n        return super().__getitem__(idx)\n\n    def keys(self):\n        return self._mapping.keys()\n\n    def values(self):\n        return self._mapping.values()\n\n\ndef create_column_maps(\n    result_columns: typing.Any,\n) -> typing.Tuple[\n    typing.Mapping[typing.Any, typing.Tuple[int, TypeEngine]],\n    typing.Mapping[int, typing.Tuple[int, TypeEngine]],\n    typing.Mapping[str, typing.Tuple[int, TypeEngine]],\n]:\n    \"\"\"\n    Generate column -> datatype mappings from the column definitions.\n\n    These mappings are used throughout PostgresConnection methods\n    to initialize Record-s. The underlying DB driver does not do type\n    conversion for us so we have wrap the returned asyncpg.Record-s.\n\n    :return: Three mappings from different ways to address a column to \\\n                corresponding column indexes and datatypes: \\\n                1. by column identifier; \\\n                2. by column index; \\\n                3. by column name in Column sqlalchemy objects.\n    \"\"\"\n    column_map, column_map_int, column_map_full = {}, {}, {}\n    for idx, (column_name, _, column, datatype) in enumerate(result_columns):\n        column_map[column_name] = (idx, datatype)\n        column_map_int[idx] = (idx, datatype)\n\n        # Added in SQLA 2.0 and _CompileLabels do not have _annotations\n        # When this happens, the mapping is on the second position\n        if isinstance(column[0], _CompileLabel):\n            column_map_full[str(column[2])] = (idx, datatype)\n        else:\n            column_map_full[str(column[0])] = (idx, datatype)\n    return column_map, column_map_int, column_map_full\n", "metadata": {"source": "github.com/encode/databases", "file_path": "databases/backends/common/records.py", "language": "python", "complexity": "complex", "star_count": 3928, "quality_score": 0.78, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": true, "has_documentation": true, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 12, "class_count": 2, "line_count": 137}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "database_integration", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create database model with ORM relationships and queries\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from sqlalchemy.dialects.postgresql.psycopg import PGCompiler_psycopg\n\n\nclass APGCompiler_psycopg2(PGCompiler_psycopg):\n    def construct_params(self, *args, **kwargs):\n        pd = super().construct_params(*args, **kwargs)\n\n        for column in self.prefetch:\n            pd[column.key] = self._exec_default(column.default)\n\n        return pd\n\n    def _exec_default(self, default):\n        if default.is_callable:\n            return default.arg(self.dialect)\n        else:\n            return default.arg\n", "metadata": {"source": "github.com/encode/databases", "file_path": "databases/backends/compilers/psycopg.py", "language": "python", "complexity": "simple", "star_count": 3928, "quality_score": 0.72, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": false, "has_testing": false, "has_logging": false, "function_count": 2, "class_count": 1, "line_count": 18}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "database_integration", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create database model with ORM relationships and queries with asyncio coroutines\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import getpass\nimport logging\nimport typing\nimport uuid\n\nimport aiomysql\nfrom sqlalchemy.dialects.mysql import pymysql\nfrom sqlalchemy.engine.cursor import CursorResultMetaData\nfrom sqlalchemy.engine.interfaces import Dialect, ExecutionContext\nfrom sqlalchemy.sql import ClauseElement\nfrom sqlalchemy.sql.ddl import DDLElement\n\nfrom databases.backends.common.records import Record, Row, create_column_maps\nfrom databases.core import LOG_EXTRA, DatabaseURL\nfrom databases.interfaces import (\n    ConnectionBackend,\n    DatabaseBackend,\n    Record as RecordInterface,\n    TransactionBackend,\n)\n\nlogger = logging.getLogger(\"databases\")\n\n\nclass MySQLBackend(DatabaseBackend):\n    def __init__(\n        self, database_url: typing.Union[DatabaseURL, str], **options: typing.Any\n    ) -> None:\n        self._database_url = DatabaseURL(database_url)\n        self._options = options\n        self._dialect = pymysql.dialect(paramstyle=\"pyformat\")\n        self._dialect.supports_native_decimal = True\n        self._pool = None\n\n    def _get_connection_kwargs(self) -> dict:\n        url_options = self._database_url.options\n\n        kwargs = {}\n        min_size = url_options.get(\"min_size\")\n        max_size = url_options.get(\"max_size\")\n        pool_recycle = url_options.get(\"pool_recycle\")\n        ssl = url_options.get(\"ssl\")\n        unix_socket = url_options.get(\"unix_socket\")\n\n        if min_size is not None:\n            kwargs[\"minsize\"] = int(min_size)\n        if max_size is not None:\n            kwargs[\"maxsize\"] = int(max_size)\n        if pool_recycle is not None:\n            kwargs[\"pool_recycle\"] = int(pool_recycle)\n        if ssl is not None:\n            kwargs[\"ssl\"] = {\"true\": True, \"false\": False}[ssl.lower()]\n        if unix_socket is not None:\n            kwargs[\"unix_socket\"] = unix_socket\n\n        for key, value in self._options.items():\n            # Coerce 'min_size' and 'max_size' for consistency.\n            if key == \"min_size\":\n                key = \"minsize\"\n            elif key == \"max_size\":\n                key = \"maxsize\"\n            kwargs[key] = value\n\n        return kwargs\n\n    async def connect(self) -> None:\n        assert self._pool is None, \"DatabaseBackend is already running\"\n        kwargs = self._get_connection_kwargs()\n        self._pool = await aiomysql.create_pool(\n            host=self._database_url.hostname,\n            port=self._database_url.port or 3306,\n            user=self._database_url.username or getpass.getuser(),\n            password=self._database_url.password,\n            db=self._database_url.database,\n            autocommit=True,\n            **kwargs,\n        )\n\n    async def disconnect(self) -> None:\n        assert self._pool is not None, \"DatabaseBackend is not running\"\n        self._pool.close()\n        await self._pool.wait_closed()\n        self._pool = None\n\n    def connection(self) -> \"MySQLConnection\":\n        return MySQLConnection(self, self._dialect)\n\n\nclass CompilationContext:\n    def __init__(self, context: ExecutionContext):\n        self.context = context\n\n\nclass MySQLConnection(ConnectionBackend):\n    def __init__(self, database: MySQLBackend, dialect: Dialect):\n        self._database = database\n        self._dialect = dialect\n        self._connection: typing.Optional[aiomysql.Connection] = None\n\n    async def acquire(self) -> None:\n        assert self._connection is None, \"Connection is already acquired\"\n        assert self._database._pool is not None, \"DatabaseBackend is not running\"\n        self._connection = await self._database._pool.acquire()\n\n    async def release(self) -> None:\n        assert self._connection is not None, \"Connection is not acquired\"\n        assert self._database._pool is not None, \"DatabaseBackend is not running\"\n        await self._database._pool.release(self._connection)\n        self._connection = None\n\n    async def fetch_all(self, query: ClauseElement) -> typing.List[RecordInterface]:\n        assert self._connection is not None, \"Connection is not acquired\"\n        query_str, args, result_columns, context = self._compile(query)\n        column_maps = create_column_maps(result_columns)\n        dialect = self._dialect\n        cursor = await self._connection.cursor()\n        try:\n            await cursor.execute(query_str, args)\n            rows = await cursor.fetchall()\n            metadata = CursorResultMetaData(context, cursor.description)\n            rows = [\n                Row(\n                    metadata,\n                    metadata._processors,\n                    metadata._keymap,\n                    row,\n                )\n                for row in rows\n            ]\n            return [Record(row, result_columns, dialect, column_maps) for row in rows]\n        finally:\n            await cursor.close()\n\n    async def fetch_one(self, query: ClauseElement) -> typing.Optional[RecordInterface]:\n        assert self._connection is not None, \"Connection is not acquired\"\n        query_str, args, result_columns, context = self._compile(query)\n        column_maps = create_column_maps(result_columns)\n        dialect = self._dialect\n        cursor = await self._connection.cursor()\n        try:\n            await cursor.execute(query_str, args)\n            row = await cursor.fetchone()\n            if row is None:\n                return None\n            metadata = CursorResultMetaData(context, cursor.description)\n            row = Row(\n                metadata,\n                metadata._processors,\n                metadata._keymap,\n                row,\n            )\n            return Record(row, result_columns, dialect, column_maps)\n        finally:\n            await cursor.close()\n\n    async def execute(self, query: ClauseElement) -> typing.Any:\n        assert self._connection is not None, \"Connection is not acquired\"\n        query_str, args, _, _ = self._compile(query)\n        cursor = await self._connection.cursor()\n        try:\n            await cursor.execute(query_str, args)\n            if cursor.lastrowid == 0:\n                return cursor.rowcount\n            return cursor.lastrowid\n        finally:\n            await cursor.close()\n\n    async def execute_many(self, queries: typing.List[ClauseElement]) -> None:\n        assert self._connection is not None, \"Connection is not acquired\"\n        cursor = await self._connection.cursor()\n        try:\n            for single_query in queries:\n                single_query, args, _, _ = self._compile(single_query)\n                await cursor.execute(single_query, args)\n        finally:\n            await cursor.close()\n\n    async def iterate(\n        self, query: ClauseElement\n    ) -> typing.AsyncGenerator[typing.Any, None]:\n        assert self._connection is not None, \"Connection is not acquired\"\n        query_str, args, result_columns, context = self._compile(query)\n        column_maps = create_column_maps(result_columns)\n        dialect = self._dialect\n        cursor = await self._connection.cursor()\n        try:\n            await cursor.execute(query_str, args)\n            metadata = CursorResultMetaData(context, cursor.description)\n            async for row in cursor:\n                record = Row(\n                    metadata,\n                    metadata._processors,\n                    metadata._keymap,\n                    row,\n                )\n                yield Record(record, result_columns, dialect, column_maps)\n        finally:\n            await cursor.close()\n\n    def transaction(self) -> TransactionBackend:\n        return MySQLTransaction(self)\n\n    def _compile(self, query: ClauseElement) -> typing.Tuple[str, list, tuple]:\n        compiled = query.compile(\n            dialect=self._dialect, compile_kwargs={\"render_postcompile\": True}\n        )\n        execution_context = self._dialect.execution_ctx_cls()\n        execution_context.dialect = self._dialect\n\n        if not isinstance(query, DDLElement):\n            compiled_params = sorted(compiled.params.items())\n\n            args = compiled.construct_params()\n            for key, val in args.items():\n                if key in compiled._bind_processors:\n                    args[key] = compiled._bind_processors[key](val)\n\n            execution_context.result_column_struct = (\n                compiled._result_columns,\n                compiled._ordered_columns,\n                compiled._textual_ordered_columns,\n                compiled._ad_hoc_textual,\n                compiled._loose_column_name_matching,\n            )\n\n            mapping = {\n                key: \"$\" + str(i) for i, (key, _) in enumerate(compiled_params, start=1)\n            }\n            compiled_query = compiled.string % mapping\n            result_map = compiled._result_columns\n\n        else:\n            args = {}\n            result_map = None\n            compiled_query = compiled.string\n\n        query_message = compiled_query.replace(\" \\n\", \" \").replace(\"\\n\", \" \")\n        logger.debug(\n            \"Query: %s Args: %s\", query_message, repr(tuple(args)), extra=LOG_EXTRA\n        )\n        return compiled.string, args, result_map, CompilationContext(execution_context)\n\n    @property\n    def raw_connection(self) -> aiomysql.connection.Connection:\n        assert self._connection is not None, \"Connection is not acquired\"\n        return self._connection\n\n\nclass MySQLTransaction(TransactionBackend):\n    def __init__(self, connection: MySQLConnection):\n        self._connection = connection\n        self._is_root = False\n        self._savepoint_name = \"\"\n\n    async def start(\n        self, is_root: bool, extra_options: typing.Dict[typing.Any, typing.Any]\n    ) -> None:\n        assert self._connection._connection is not None, \"Connection is not acquired\"\n        self._is_root = is_root\n        if self._is_root:\n            await self._connection._connection.begin()\n        else:\n            id = str(uuid.uuid4()).replace(\"-\", \"_\")\n            self._savepoint_name = f\"STARLETTE_SAVEPOINT_{id}\"\n            cursor = await self._connection._connection.cursor()\n            try:\n                await cursor.execute(f\"SAVEPOINT {self._savepoint_name}\")\n            finally:\n                await cursor.close()\n\n    async def commit(self) -> None:\n        assert self._connection._connection is not None, \"Connection is not acquired\"\n        if self._is_root:\n            await self._connection._connection.commit()\n        else:\n            cursor = await self._connection._connection.cursor()\n            try:\n                await cursor.execute(f\"RELEASE SAVEPOINT {self._savepoint_name}\")\n            finally:\n                await cursor.close()\n\n    async def rollback(self) -> None:\n        assert self._connection._connection is not None, \"Connection is not acquired\"\n        if self._is_root:\n            await self._connection._connection.rollback()\n        else:\n            cursor = await self._connection._connection.cursor()\n            try:\n                await cursor.execute(f\"ROLLBACK TO SAVEPOINT {self._savepoint_name}\")\n            finally:\n                await cursor.close()\n", "metadata": {"source": "github.com/encode/databases", "file_path": "databases/backends/mysql.py", "language": "python", "complexity": "complex", "star_count": 3928, "quality_score": 0.87, "features": {"has_classes": true, "has_functions": true, "has_async": true, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": true, "has_logging": true, "function_count": 21, "class_count": 4, "line_count": 292}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "database_integration", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create database model with ORM relationships and queries with asyncio coroutines\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import logging\nimport typing\n\nimport asyncpg\nfrom sqlalchemy.engine.interfaces import Dialect\nfrom sqlalchemy.sql import ClauseElement\nfrom sqlalchemy.sql.ddl import DDLElement\n\nfrom databases.backends.common.records import Record, create_column_maps\nfrom databases.backends.dialects.psycopg import dialect as psycopg_dialect\nfrom databases.core import LOG_EXTRA, DatabaseURL\nfrom databases.interfaces import (\n    ConnectionBackend,\n    DatabaseBackend,\n    Record as RecordInterface,\n    TransactionBackend,\n)\n\nlogger = logging.getLogger(\"databases\")\n\n\nclass PostgresBackend(DatabaseBackend):\n    def __init__(\n        self, database_url: typing.Union[DatabaseURL, str], **options: typing.Any\n    ) -> None:\n        self._database_url = DatabaseURL(database_url)\n        self._options = options\n        self._dialect = self._get_dialect()\n        self._pool = None\n\n    def _get_dialect(self) -> Dialect:\n        dialect = psycopg_dialect(paramstyle=\"pyformat\")\n\n        dialect.implicit_returning = True\n        dialect.supports_native_enum = True\n        dialect.supports_smallserial = True  # 9.2+\n        dialect._backslash_escapes = False\n        dialect.supports_sane_multi_rowcount = True  # psycopg 2.0.9+\n        dialect._has_native_hstore = True\n        dialect.supports_native_decimal = True\n\n        return dialect\n\n    def _get_connection_kwargs(self) -> dict:\n        url_options = self._database_url.options\n\n        kwargs: typing.Dict[str, typing.Any] = {}\n        min_size = url_options.get(\"min_size\")\n        max_size = url_options.get(\"max_size\")\n        ssl = url_options.get(\"ssl\")\n\n        if min_size is not None:\n            kwargs[\"min_size\"] = int(min_size)\n        if max_size is not None:\n            kwargs[\"max_size\"] = int(max_size)\n        if ssl is not None:\n            ssl = ssl.lower()\n            kwargs[\"ssl\"] = {\"true\": True, \"false\": False}.get(ssl, ssl)\n\n        kwargs.update(self._options)\n\n        return kwargs\n\n    async def connect(self) -> None:\n        assert self._pool is None, \"DatabaseBackend is already running\"\n        kwargs = dict(\n            host=self._database_url.hostname,\n            port=self._database_url.port,\n            user=self._database_url.username,\n            password=self._database_url.password,\n            database=self._database_url.database,\n        )\n        kwargs.update(self._get_connection_kwargs())\n        self._pool = await asyncpg.create_pool(**kwargs)\n\n    async def disconnect(self) -> None:\n        assert self._pool is not None, \"DatabaseBackend is not running\"\n        await self._pool.close()\n        self._pool = None\n\n    def connection(self) -> \"PostgresConnection\":\n        return PostgresConnection(self, self._dialect)\n\n\nclass PostgresConnection(ConnectionBackend):\n    def __init__(self, database: PostgresBackend, dialect: Dialect):\n        self._database = database\n        self._dialect = dialect\n        self._connection: typing.Optional[asyncpg.connection.Connection] = None\n\n    async def acquire(self) -> None:\n        assert self._connection is None, \"Connection is already acquired\"\n        assert self._database._pool is not None, \"DatabaseBackend is not running\"\n        self._connection = await self._database._pool.acquire()\n\n    async def release(self) -> None:\n        assert self._connection is not None, \"Connection is not acquired\"\n        assert self._database._pool is not None, \"DatabaseBackend is not running\"\n        self._connection = await self._database._pool.release(self._connection)\n        self._connection = None\n\n    async def fetch_all(self, query: ClauseElement) -> typing.List[RecordInterface]:\n        assert self._connection is not None, \"Connection is not acquired\"\n        query_str, args, result_columns = self._compile(query)\n        rows = await self._connection.fetch(query_str, *args)\n        dialect = self._dialect\n        column_maps = create_column_maps(result_columns)\n        return [Record(row, result_columns, dialect, column_maps) for row in rows]\n\n    async def fetch_one(self, query: ClauseElement) -> typing.Optional[RecordInterface]:\n        assert self._connection is not None, \"Connection is not acquired\"\n        query_str, args, result_columns = self._compile(query)\n        row = await self._connection.fetchrow(query_str, *args)\n        if row is None:\n            return None\n        return Record(\n            row,\n            result_columns,\n            self._dialect,\n            create_column_maps(result_columns),\n        )\n\n    async def fetch_val(\n        self, query: ClauseElement, column: typing.Any = 0\n    ) -> typing.Any:\n        # we are not calling self._connection.fetchval here because\n        # it does not convert all the types, e.g. JSON stays string\n        # instead of an object\n        # see also:\n        # https://github.com/encode/databases/pull/131\n        # https://github.com/encode/databases/pull/132\n        # https://github.com/encode/databases/pull/246\n        row = await self.fetch_one(query)\n        if row is None:\n            return None\n        return row[column]\n\n    async def execute(self, query: ClauseElement) -> typing.Any:\n        assert self._connection is not None, \"Connection is not acquired\"\n        query_str, args, _ = self._compile(query)\n        return await self._connection.fetchval(query_str, *args)\n\n    async def execute_many(self, queries: typing.List[ClauseElement]) -> None:\n        assert self._connection is not None, \"Connection is not acquired\"\n        # asyncpg uses prepared statements under the hood, so we just\n        # loop through multiple executes here, which should all end up\n        # using the same prepared statement.\n        for single_query in queries:\n            single_query, args, _ = self._compile(single_query)\n            await self._connection.execute(single_query, *args)\n\n    async def iterate(\n        self, query: ClauseElement\n    ) -> typing.AsyncGenerator[typing.Any, None]:\n        assert self._connection is not None, \"Connection is not acquired\"\n        query_str, args, result_columns = self._compile(query)\n        column_maps = create_column_maps(result_columns)\n        async for row in self._connection.cursor(query_str, *args):\n            yield Record(row, result_columns, self._dialect, column_maps)\n\n    def transaction(self) -> TransactionBackend:\n        return PostgresTransaction(connection=self)\n\n    def _compile(self, query: ClauseElement) -> typing.Tuple[str, list, tuple]:\n        compiled = query.compile(\n            dialect=self._dialect, compile_kwargs={\"render_postcompile\": True}\n        )\n\n        if not isinstance(query, DDLElement):\n            compiled_params = sorted(compiled.params.items())\n\n            mapping = {\n                key: \"$\" + str(i) for i, (key, _) in enumerate(compiled_params, start=1)\n            }\n            compiled_query = compiled.string % mapping\n\n            processors = compiled._bind_processors\n            args = [\n                processors[key](val) if key in processors else val\n                for key, val in compiled_params\n            ]\n            result_map = compiled._result_columns\n        else:\n            compiled_query = compiled.string\n            args = []\n            result_map = None\n\n        query_message = compiled_query.replace(\" \\n\", \" \").replace(\"\\n\", \" \")\n        logger.debug(\n            \"Query: %s Args: %s\", query_message, repr(tuple(args)), extra=LOG_EXTRA\n        )\n        return compiled_query, args, result_map\n\n    @property\n    def raw_connection(self) -> asyncpg.connection.Connection:\n        assert self._connection is not None, \"Connection is not acquired\"\n        return self._connection\n\n\nclass PostgresTransaction(TransactionBackend):\n    def __init__(self, connection: PostgresConnection):\n        self._connection = connection\n        self._transaction: typing.Optional[asyncpg.transaction.Transaction] = None\n\n    async def start(\n        self, is_root: bool, extra_options: typing.Dict[typing.Any, typing.Any]\n    ) -> None:\n        assert self._connection._connection is not None, \"Connection is not acquired\"\n        self._transaction = self._connection._connection.transaction(**extra_options)\n        await self._transaction.start()\n\n    async def commit(self) -> None:\n        assert self._transaction is not None\n        await self._transaction.commit()\n\n    async def rollback(self) -> None:\n        assert self._transaction is not None\n        await self._transaction.rollback()\n", "metadata": {"source": "github.com/encode/databases", "file_path": "databases/backends/postgres.py", "language": "python", "complexity": "complex", "star_count": 3928, "quality_score": 0.8099999999999999, "features": {"has_classes": true, "has_functions": true, "has_async": true, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": true, "has_logging": true, "function_count": 22, "class_count": 3, "line_count": 219}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "database_integration", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create database model with ORM relationships and queries with asyncio coroutines\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import logging\nimport sqlite3\nimport typing\nimport uuid\nfrom urllib.parse import urlencode\n\nimport aiosqlite\nfrom sqlalchemy.dialects.sqlite import pysqlite\nfrom sqlalchemy.engine.cursor import CursorResultMetaData\nfrom sqlalchemy.engine.interfaces import Dialect, ExecutionContext\nfrom sqlalchemy.sql import ClauseElement\nfrom sqlalchemy.sql.ddl import DDLElement\n\nfrom databases.backends.common.records import Record, Row, create_column_maps\nfrom databases.core import LOG_EXTRA, DatabaseURL\nfrom databases.interfaces import ConnectionBackend, DatabaseBackend, TransactionBackend\n\nlogger = logging.getLogger(\"databases\")\n\n\nclass SQLiteBackend(DatabaseBackend):\n    def __init__(\n        self, database_url: typing.Union[DatabaseURL, str], **options: typing.Any\n    ) -> None:\n        self._database_url = DatabaseURL(database_url)\n        self._options = options\n        self._dialect = pysqlite.dialect(paramstyle=\"qmark\")\n        # aiosqlite does not support decimals\n        self._dialect.supports_native_decimal = False\n        self._pool = SQLitePool(self._database_url, **self._options)\n\n    async def connect(self) -> None:\n        ...\n\n    async def disconnect(self) -> None:\n        # if it extsis, remove reference to connection to cached in-memory database on disconnect\n        if self._pool._memref:\n            self._pool._memref = None\n        # assert self._pool is not None, \"DatabaseBackend is not running\"\n        # self._pool.close()\n        # await self._pool.wait_closed()\n        # self._pool = None\n\n    def connection(self) -> \"SQLiteConnection\":\n        return SQLiteConnection(self._pool, self._dialect)\n\n\nclass SQLitePool:\n    def __init__(self, url: DatabaseURL, **options: typing.Any) -> None:\n        self._database = url.database\n        self._memref = None\n        # add query params to database connection string\n        if url.options:\n            self._database += \"?\" + urlencode(url.options)\n        self._options = options\n\n        if url.options and \"cache\" in url.options:\n            # reference to a connection to the cached in-memory database must be held to keep it from being deleted\n            self._memref = sqlite3.connect(self._database, **self._options)\n\n    async def acquire(self) -> aiosqlite.Connection:\n        connection = aiosqlite.connect(\n            database=self._database, isolation_level=None, **self._options\n        )\n        await connection.__aenter__()\n        return connection\n\n    async def release(self, connection: aiosqlite.Connection) -> None:\n        await connection.__aexit__(None, None, None)\n\n\nclass CompilationContext:\n    def __init__(self, context: ExecutionContext):\n        self.context = context\n\n\nclass SQLiteConnection(ConnectionBackend):\n    def __init__(self, pool: SQLitePool, dialect: Dialect):\n        self._pool = pool\n        self._dialect = dialect\n        self._connection: typing.Optional[aiosqlite.Connection] = None\n\n    async def acquire(self) -> None:\n        assert self._connection is None, \"Connection is already acquired\"\n        self._connection = await self._pool.acquire()\n\n    async def release(self) -> None:\n        assert self._connection is not None, \"Connection is not acquired\"\n        await self._pool.release(self._connection)\n        self._connection = None\n\n    async def fetch_all(self, query: ClauseElement) -> typing.List[Record]:\n        assert self._connection is not None, \"Connection is not acquired\"\n        query_str, args, result_columns, context = self._compile(query)\n        column_maps = create_column_maps(result_columns)\n        dialect = self._dialect\n\n        async with self._connection.execute(query_str, args) as cursor:\n            rows = await cursor.fetchall()\n            metadata = CursorResultMetaData(context, cursor.description)\n            rows = [\n                Row(\n                    metadata,\n                    metadata._processors,\n                    metadata._keymap,\n                    row,\n                )\n                for row in rows\n            ]\n            return [Record(row, result_columns, dialect, column_maps) for row in rows]\n\n    async def fetch_one(self, query: ClauseElement) -> typing.Optional[Record]:\n        assert self._connection is not None, \"Connection is not acquired\"\n        query_str, args, result_columns, context = self._compile(query)\n        column_maps = create_column_maps(result_columns)\n        dialect = self._dialect\n\n        async with self._connection.execute(query_str, args) as cursor:\n            row = await cursor.fetchone()\n            if row is None:\n                return None\n            metadata = CursorResultMetaData(context, cursor.description)\n            row = Row(\n                metadata,\n                metadata._processors,\n                metadata._keymap,\n                row,\n            )\n            return Record(row, result_columns, dialect, column_maps)\n\n    async def execute(self, query: ClauseElement) -> typing.Any:\n        assert self._connection is not None, \"Connection is not acquired\"\n        query_str, args, result_columns, context = self._compile(query)\n        async with self._connection.cursor() as cursor:\n            await cursor.execute(query_str, args)\n            if cursor.lastrowid == 0:\n                return cursor.rowcount\n            return cursor.lastrowid\n\n    async def execute_many(self, queries: typing.List[ClauseElement]) -> None:\n        assert self._connection is not None, \"Connection is not acquired\"\n        for single_query in queries:\n            await self.execute(single_query)\n\n    async def iterate(\n        self, query: ClauseElement\n    ) -> typing.AsyncGenerator[typing.Any, None]:\n        assert self._connection is not None, \"Connection is not acquired\"\n        query_str, args, result_columns, context = self._compile(query)\n        column_maps = create_column_maps(result_columns)\n        dialect = self._dialect\n\n        async with self._connection.execute(query_str, args) as cursor:\n            metadata = CursorResultMetaData(context, cursor.description)\n            async for row in cursor:\n                record = Row(\n                    metadata,\n                    metadata._processors,\n                    metadata._keymap,\n                    row,\n                )\n                yield Record(record, result_columns, dialect, column_maps)\n\n    def transaction(self) -> TransactionBackend:\n        return SQLiteTransaction(self)\n\n    def _compile(self, query: ClauseElement) -> typing.Tuple[str, list, tuple]:\n        compiled = query.compile(\n            dialect=self._dialect, compile_kwargs={\"render_postcompile\": True}\n        )\n        execution_context = self._dialect.execution_ctx_cls()\n        execution_context.dialect = self._dialect\n\n        args = []\n        result_map = None\n\n        if not isinstance(query, DDLElement):\n            compiled_params = sorted(compiled.params.items())\n\n            params = compiled.construct_params()\n            for key in compiled.positiontup:\n                raw_val = params[key]\n                if key in compiled._bind_processors:\n                    val = compiled._bind_processors[key](raw_val)\n                else:\n                    val = raw_val\n                args.append(val)\n\n            execution_context.result_column_struct = (\n                compiled._result_columns,\n                compiled._ordered_columns,\n                compiled._textual_ordered_columns,\n                compiled._ad_hoc_textual,\n                compiled._loose_column_name_matching,\n            )\n\n            mapping = {\n                key: \"$\" + str(i) for i, (key, _) in enumerate(compiled_params, start=1)\n            }\n            compiled_query = compiled.string % mapping\n            result_map = compiled._result_columns\n\n        else:\n            compiled_query = compiled.string\n\n        query_message = compiled_query.replace(\" \\n\", \" \").replace(\"\\n\", \" \")\n        logger.debug(\n            \"Query: %s Args: %s\", query_message, repr(tuple(args)), extra=LOG_EXTRA\n        )\n        return compiled.string, args, result_map, CompilationContext(execution_context)\n\n    @property\n    def raw_connection(self) -> aiosqlite.core.Connection:\n        assert self._connection is not None, \"Connection is not acquired\"\n        return self._connection\n\n\nclass SQLiteTransaction(TransactionBackend):\n    def __init__(self, connection: SQLiteConnection):\n        self._connection = connection\n        self._is_root = False\n        self._savepoint_name = \"\"\n\n    async def start(\n        self, is_root: bool, extra_options: typing.Dict[typing.Any, typing.Any]\n    ) -> None:\n        assert self._connection._connection is not None, \"Connection is not acquired\"\n        self._is_root = is_root\n        if self._is_root:\n            async with self._connection._connection.execute(\"BEGIN\") as cursor:\n                await cursor.close()\n        else:\n            id = str(uuid.uuid4()).replace(\"-\", \"_\")\n            self._savepoint_name = f\"STARLETTE_SAVEPOINT_{id}\"\n            async with self._connection._connection.execute(\n                f\"SAVEPOINT {self._savepoint_name}\"\n            ) as cursor:\n                await cursor.close()\n\n    async def commit(self) -> None:\n        assert self._connection._connection is not None, \"Connection is not acquired\"\n        if self._is_root:\n            async with self._connection._connection.execute(\"COMMIT\") as cursor:\n                await cursor.close()\n        else:\n            async with self._connection._connection.execute(\n                f\"RELEASE SAVEPOINT {self._savepoint_name}\"\n            ) as cursor:\n                await cursor.close()\n\n    async def rollback(self) -> None:\n        assert self._connection._connection is not None, \"Connection is not acquired\"\n        if self._is_root:\n            async with self._connection._connection.execute(\"ROLLBACK\") as cursor:\n                await cursor.close()\n        else:\n            async with self._connection._connection.execute(\n                f\"ROLLBACK TO SAVEPOINT {self._savepoint_name}\"\n            ) as cursor:\n                await cursor.close()\n", "metadata": {"source": "github.com/encode/databases", "file_path": "databases/backends/sqlite.py", "language": "python", "complexity": "complex", "star_count": 3928, "quality_score": 0.8099999999999999, "features": {"has_classes": true, "has_functions": true, "has_async": true, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": true, "has_logging": true, "function_count": 23, "class_count": 5, "line_count": 261}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "database_integration", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create database model with ORM relationships and queries with asyncio coroutines\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import asyncio\nimport contextlib\nimport functools\nimport logging\nimport typing\nimport weakref\nfrom contextvars import ContextVar\nfrom types import TracebackType\nfrom urllib.parse import SplitResult, parse_qsl, unquote, urlsplit\n\nfrom sqlalchemy import text\nfrom sqlalchemy.sql import ClauseElement\n\nfrom databases.importer import import_from_string\nfrom databases.interfaces import DatabaseBackend, Record, TransactionBackend\n\ntry:  # pragma: no cover\n    import click\n\n    # Extra log info for optional coloured terminal outputs.\n    LOG_EXTRA = {\n        \"color_message\": \"Query: \" + click.style(\"%s\", bold=True) + \" Args: %s\"\n    }\n    CONNECT_EXTRA = {\n        \"color_message\": \"Connected to database \" + click.style(\"%s\", bold=True)\n    }\n    DISCONNECT_EXTRA = {\n        \"color_message\": \"Disconnected from database \" + click.style(\"%s\", bold=True)\n    }\nexcept ImportError:  # pragma: no cover\n    LOG_EXTRA = {}\n    CONNECT_EXTRA = {}\n    DISCONNECT_EXTRA = {}\n\n\nlogger = logging.getLogger(\"databases\")\n\n\n_ACTIVE_TRANSACTIONS: ContextVar[\n    typing.Optional[\"weakref.WeakKeyDictionary['Transaction', 'TransactionBackend']\"]\n] = ContextVar(\"databases:active_transactions\", default=None)\n\n\nclass Database:\n    SUPPORTED_BACKENDS = {\n        \"postgresql\": \"databases.backends.postgres:PostgresBackend\",\n        \"postgresql+aiopg\": \"databases.backends.aiopg:AiopgBackend\",\n        \"postgres\": \"databases.backends.postgres:PostgresBackend\",\n        \"mysql\": \"databases.backends.mysql:MySQLBackend\",\n        \"mysql+asyncmy\": \"databases.backends.asyncmy:AsyncMyBackend\",\n        \"sqlite\": \"databases.backends.sqlite:SQLiteBackend\",\n    }\n\n    _connection_map: \"weakref.WeakKeyDictionary[asyncio.Task, 'Connection']\"\n\n    def __init__(\n        self,\n        url: typing.Union[str, \"DatabaseURL\"],\n        *,\n        force_rollback: bool = False,\n        **options: typing.Any,\n    ):\n        self.url = DatabaseURL(url)\n        self.options = options\n        self.is_connected = False\n        self._connection_map = weakref.WeakKeyDictionary()\n\n        self._force_rollback = force_rollback\n\n        backend_str = self._get_backend()\n        backend_cls = import_from_string(backend_str)\n        assert issubclass(backend_cls, DatabaseBackend)\n        self._backend = backend_cls(self.url, **self.options)\n\n        # When `force_rollback=True` is used, we use a single global\n        # connection, within a transaction that always rolls back.\n        self._global_connection: typing.Optional[Connection] = None\n        self._global_transaction: typing.Optional[Transaction] = None\n\n    @property\n    def _current_task(self) -> asyncio.Task:\n        task = asyncio.current_task()\n        if not task:\n            raise RuntimeError(\"No currently active asyncio.Task found\")\n        return task\n\n    @property\n    def _connection(self) -> typing.Optional[\"Connection\"]:\n        return self._connection_map.get(self._current_task)\n\n    @_connection.setter\n    def _connection(\n        self, connection: typing.Optional[\"Connection\"]\n    ) -> typing.Optional[\"Connection\"]:\n        task = self._current_task\n\n        if connection is None:\n            self._connection_map.pop(task, None)\n        else:\n            self._connection_map[task] = connection\n\n        return self._connection\n\n    async def connect(self) -> None:\n        \"\"\"\n        Establish the connection pool.\n        \"\"\"\n        if self.is_connected:\n            logger.debug(\"Already connected, skipping connection\")\n            return None\n\n        await self._backend.connect()\n        logger.info(\n            \"Connected to database %s\", self.url.obscure_password, extra=CONNECT_EXTRA\n        )\n        self.is_connected = True\n\n        if self._force_rollback:\n            assert self._global_connection is None\n            assert self._global_transaction is None\n\n            self._global_connection = Connection(self, self._backend)\n            self._global_transaction = self._global_connection.transaction(\n                force_rollback=True\n            )\n\n            await self._global_transaction.__aenter__()\n\n    async def disconnect(self) -> None:\n        \"\"\"\n        Close all connections in the connection pool.\n        \"\"\"\n        if not self.is_connected:\n            logger.debug(\"Already disconnected, skipping disconnection\")\n            return None\n\n        if self._force_rollback:\n            assert self._global_connection is not None\n            assert self._global_transaction is not None\n\n            await self._global_transaction.__aexit__()\n\n            self._global_transaction = None\n            self._global_connection = None\n        else:\n            self._connection = None\n\n        await self._backend.disconnect()\n        logger.info(\n            \"Disconnected from database %s\",\n            self.url.obscure_password,\n            extra=DISCONNECT_EXTRA,\n        )\n        self.is_connected = False\n\n    async def __aenter__(self) -> \"Database\":\n        await self.connect()\n        return self\n\n    async def __aexit__(\n        self,\n        exc_type: typing.Optional[typing.Type[BaseException]] = None,\n        exc_value: typing.Optional[BaseException] = None,\n        traceback: typing.Optional[TracebackType] = None,\n    ) -> None:\n        await self.disconnect()\n\n    async def fetch_all(\n        self,\n        query: typing.Union[ClauseElement, str],\n        values: typing.Optional[dict] = None,\n    ) -> typing.List[Record]:\n        async with self.connection() as connection:\n            return await connection.fetch_all(query, values)\n\n    async def fetch_one(\n        self,\n        query: typing.Union[ClauseElement, str],\n        values: typing.Optional[dict] = None,\n    ) -> typing.Optional[Record]:\n        async with self.connection() as connection:\n            return await connection.fetch_one(query, values)\n\n    async def fetch_val(\n        self,\n        query: typing.Union[ClauseElement, str],\n        values: typing.Optional[dict] = None,\n        column: typing.Any = 0,\n    ) -> typing.Any:\n        async with self.connection() as connection:\n            return await connection.fetch_val(query, values, column=column)\n\n    async def execute(\n        self,\n        query: typing.Union[ClauseElement, str],\n        values: typing.Optional[dict] = None,\n    ) -> typing.Any:\n        async with self.connection() as connection:\n            return await connection.execute(query, values)\n\n    async def execute_many(\n        self, query: typing.Union[ClauseElement, str], values: list\n    ) -> None:\n        async with self.connection() as connection:\n            return await connection.execute_many(query, values)\n\n    async def iterate(\n        self,\n        query: typing.Union[ClauseElement, str],\n        values: typing.Optional[dict] = None,\n    ) -> typing.AsyncGenerator[typing.Mapping, None]:\n        async with self.connection() as connection:\n            async for record in connection.iterate(query, values):\n                yield record\n\n    def connection(self) -> \"Connection\":\n        if self._global_connection is not None:\n            return self._global_connection\n\n        if not self._connection:\n            self._connection = Connection(self, self._backend)\n\n        return self._connection\n\n    def transaction(\n        self, *, force_rollback: bool = False, **kwargs: typing.Any\n    ) -> \"Transaction\":\n        return Transaction(self.connection, force_rollback=force_rollback, **kwargs)\n\n    @contextlib.contextmanager\n    def force_rollback(self) -> typing.Iterator[None]:\n        initial = self._force_rollback\n        self._force_rollback = True\n        try:\n            yield\n        finally:\n            self._force_rollback = initial\n\n    def _get_backend(self) -> str:\n        return self.SUPPORTED_BACKENDS.get(\n            self.url.scheme, self.SUPPORTED_BACKENDS[self.url.dialect]\n        )\n\n\nclass Connection:\n    def __init__(self, database: Database, backend: DatabaseBackend) -> None:\n        self._database = database\n        self._backend = backend\n\n        self._connection_lock = asyncio.Lock()\n        self._connection = self._backend.connection()\n        self._connection_counter = 0\n\n        self._transaction_lock = asyncio.Lock()\n        self._transaction_stack: typing.List[Transaction] = []\n\n        self._query_lock = asyncio.Lock()\n\n    async def __aenter__(self) -> \"Connection\":\n        async with self._connection_lock:\n            self._connection_counter += 1\n            try:\n                if self._connection_counter == 1:\n                    await self._connection.acquire()\n            except BaseException as e:\n                self._connection_counter -= 1\n                raise e\n        return self\n\n    async def __aexit__(\n        self,\n        exc_type: typing.Optional[typing.Type[BaseException]] = None,\n        exc_value: typing.Optional[BaseException] = None,\n        traceback: typing.Optional[TracebackType] = None,\n    ) -> None:\n        async with self._connection_lock:\n            assert self._connection is not None\n            self._connection_counter -= 1\n            if self._connection_counter == 0:\n                await self._connection.release()\n                self._database._connection = None\n\n    async def fetch_all(\n        self,\n        query: typing.Union[ClauseElement, str],\n        values: typing.Optional[dict] = None,\n    ) -> typing.List[Record]:\n        built_query = self._build_query(query, values)\n        async with self._query_lock:\n            return await self._connection.fetch_all(built_query)\n\n    async def fetch_one(\n        self,\n        query: typing.Union[ClauseElement, str],\n        values: typing.Optional[dict] = None,\n    ) -> typing.Optional[Record]:\n        built_query = self._build_query(query, values)\n        async with self._query_lock:\n            return await self._connection.fetch_one(built_query)\n\n    async def fetch_val(\n        self,\n        query: typing.Union[ClauseElement, str],\n        values: typing.Optional[dict] = None,\n        column: typing.Any = 0,\n    ) -> typing.Any:\n        built_query = self._build_query(query, values)\n        async with self._query_lock:\n            return await self._connection.fetch_val(built_query, column)\n\n    async def execute(\n        self,\n        query: typing.Union[ClauseElement, str],\n        values: typing.Optional[dict] = None,\n    ) -> typing.Any:\n        built_query = self._build_query(query, values)\n        async with self._query_lock:\n            return await self._connection.execute(built_query)\n\n    async def execute_many(\n        self, query: typing.Union[ClauseElement, str], values: list\n    ) -> None:\n        queries = [self._build_query(query, values_set) for values_set in values]\n        async with self._query_lock:\n            await self._connection.execute_many(queries)\n\n    async def iterate(\n        self,\n        query: typing.Union[ClauseElement, str],\n        values: typing.Optional[dict] = None,\n    ) -> typing.AsyncGenerator[typing.Any, None]:\n        built_query = self._build_query(query, values)\n        async with self.transaction():\n            async with self._query_lock:\n                async for record in self._connection.iterate(built_query):\n                    yield record\n\n    def transaction(\n        self, *, force_rollback: bool = False, **kwargs: typing.Any\n    ) -> \"Transaction\":\n        def connection_callable() -> Connection:\n            return self\n\n        return Transaction(connection_callable, force_rollback, **kwargs)\n\n    @property\n    def raw_connection(self) -> typing.Any:\n        return self._connection.raw_connection\n\n    @staticmethod\n    def _build_query(\n        query: typing.Union[ClauseElement, str], values: typing.Optional[dict] = None\n    ) -> ClauseElement:\n        if isinstance(query, str):\n            query = text(query)\n\n            return query.bindparams(**values) if values is not None else query\n        elif values:\n            return query.values(**values)  # type: ignore\n\n        return query\n\n\n_CallableType = typing.TypeVar(\"_CallableType\", bound=typing.Callable)\n\n\nclass Transaction:\n    def __init__(\n        self,\n        connection_callable: typing.Callable[[], Connection],\n        force_rollback: bool,\n        **kwargs: typing.Any,\n    ) -> None:\n        self._connection_callable = connection_callable\n        self._force_rollback = force_rollback\n        self._extra_options = kwargs\n\n    @property\n    def _connection(self) -> \"Connection\":\n        # Returns the same connection if called multiple times\n        return self._connection_callable()\n\n    @property\n    def _transaction(self) -> typing.Optional[\"TransactionBackend\"]:\n        transactions = _ACTIVE_TRANSACTIONS.get()\n        if transactions is None:\n            return None\n\n        return transactions.get(self, None)\n\n    @_transaction.setter\n    def _transaction(\n        self, transaction: typing.Optional[\"TransactionBackend\"]\n    ) -> typing.Optional[\"TransactionBackend\"]:\n        transactions = _ACTIVE_TRANSACTIONS.get()\n        if transactions is None:\n            transactions = weakref.WeakKeyDictionary()\n        else:\n            transactions = transactions.copy()\n\n        if transaction is None:\n            transactions.pop(self, None)\n        else:\n            transactions[self] = transaction\n\n        _ACTIVE_TRANSACTIONS.set(transactions)\n        return transactions.get(self, None)\n\n    async def __aenter__(self) -> \"Transaction\":\n        \"\"\"\n        Called when entering `async with database.transaction()`\n        \"\"\"\n        await self.start()\n        return self\n\n    async def __aexit__(\n        self,\n        exc_type: typing.Optional[typing.Type[BaseException]] = None,\n        exc_value: typing.Optional[BaseException] = None,\n        traceback: typing.Optional[TracebackType] = None,\n    ) -> None:\n        \"\"\"\n        Called when exiting `async with database.transaction()`\n        \"\"\"\n        if exc_type is not None or self._force_rollback:\n            await self.rollback()\n        else:\n            await self.commit()\n\n    def __await__(self) -> typing.Generator[None, None, \"Transaction\"]:\n        \"\"\"\n        Called if using the low-level `transaction = await database.transaction()`\n        \"\"\"\n        return self.start().__await__()\n\n    def __call__(self, func: _CallableType) -> _CallableType:\n        \"\"\"\n        Called if using `@database.transaction()` as a decorator.\n        \"\"\"\n\n        @functools.wraps(func)\n        async def wrapper(*args: typing.Any, **kwargs: typing.Any) -> typing.Any:\n            async with self:\n                return await func(*args, **kwargs)\n\n        return wrapper  # type: ignore\n\n    async def start(self) -> \"Transaction\":\n        self._transaction = self._connection._connection.transaction()\n\n        async with self._connection._transaction_lock:\n            is_root = not self._connection._transaction_stack\n            await self._connection.__aenter__()\n            await self._transaction.start(\n                is_root=is_root, extra_options=self._extra_options\n            )\n            self._connection._transaction_stack.append(self)\n        return self\n\n    async def commit(self) -> None:\n        async with self._connection._transaction_lock:\n            assert self._connection._transaction_stack[-1] is self\n            self._connection._transaction_stack.pop()\n            assert self._transaction is not None\n            await self._transaction.commit()\n            await self._connection.__aexit__()\n            self._transaction = None\n\n    async def rollback(self) -> None:\n        async with self._connection._transaction_lock:\n            assert self._connection._transaction_stack[-1] is self\n            self._connection._transaction_stack.pop()\n            assert self._transaction is not None\n            await self._transaction.rollback()\n            await self._connection.__aexit__()\n            self._transaction = None\n\n\nclass _EmptyNetloc(str):\n    def __bool__(self) -> bool:\n        return True\n\n\nclass DatabaseURL:\n    def __init__(self, url: typing.Union[str, \"DatabaseURL\"]):\n        if isinstance(url, DatabaseURL):\n            self._url: str = url._url\n        elif isinstance(url, str):\n            self._url = url\n        else:\n            raise TypeError(\n                f\"Invalid type for DatabaseURL. Expected str or DatabaseURL, got {type(url)}\"\n            )\n\n    @property\n    def components(self) -> SplitResult:\n        if not hasattr(self, \"_components\"):\n            self._components = urlsplit(self._url)\n        return self._components\n\n    @property\n    def scheme(self) -> str:\n        return self.components.scheme\n\n    @property\n    def dialect(self) -> str:\n        return self.components.scheme.split(\"+\")[0]\n\n    @property\n    def driver(self) -> str:\n        if \"+\" not in self.components.scheme:\n            return \"\"\n        return self.components.scheme.split(\"+\", 1)[1]\n\n    @property\n    def userinfo(self) -> typing.Optional[bytes]:\n        if self.components.username:\n            info = self.components.username\n            if self.components.password:\n                info += \":\" + self.components.password\n            return info.encode(\"utf-8\")\n        return None\n\n    @property\n    def username(self) -> typing.Optional[str]:\n        if self.components.username is None:\n            return None\n        return unquote(self.components.username)\n\n    @property\n    def password(self) -> typing.Optional[str]:\n        if self.components.password is None:\n            return None\n        return unquote(self.components.password)\n\n    @property\n    def hostname(self) -> typing.Optional[str]:\n        return (\n            self.components.hostname\n            or self.options.get(\"host\")\n            or self.options.get(\"unix_sock\")\n        )\n\n    @property\n    def port(self) -> typing.Optional[int]:\n        return self.components.port\n\n    @property\n    def netloc(self) -> typing.Optional[str]:\n        return self.components.netloc\n\n    @property\n    def database(self) -> str:\n        path = self.components.path\n        if path.startswith(\"/\"):\n            path = path[1:]\n        return unquote(path)\n\n    @property\n    def options(self) -> dict:\n        if not hasattr(self, \"_options\"):\n            self._options = dict(parse_qsl(self.components.query))\n        return self._options\n\n    def replace(self, **kwargs: typing.Any) -> \"DatabaseURL\":\n        if (\n            \"username\" in kwargs\n            or \"password\" in kwargs\n            or \"hostname\" in kwargs\n            or \"port\" in kwargs\n        ):\n            hostname = kwargs.pop(\"hostname\", self.hostname)\n            port = kwargs.pop(\"port\", self.port)\n            username = kwargs.pop(\"username\", self.components.username)\n            password = kwargs.pop(\"password\", self.components.password)\n\n            netloc = hostname\n            if port is not None:\n                netloc += f\":{port}\"\n            if username is not None:\n                userpass = username\n                if password is not None:\n                    userpass += f\":{password}\"\n                netloc = f\"{userpass}@{netloc}\"\n\n            kwargs[\"netloc\"] = netloc\n\n        if \"database\" in kwargs:\n            kwargs[\"path\"] = \"/\" + kwargs.pop(\"database\")\n\n        if \"dialect\" in kwargs or \"driver\" in kwargs:\n            dialect = kwargs.pop(\"dialect\", self.dialect)\n            driver = kwargs.pop(\"driver\", self.driver)\n            kwargs[\"scheme\"] = f\"{dialect}+{driver}\" if driver else dialect\n\n        if not kwargs.get(\"netloc\", self.netloc):\n            # Using an empty string that evaluates as True means we end up\n            # with URLs like `sqlite:///database` instead of `sqlite:/database`\n            kwargs[\"netloc\"] = _EmptyNetloc()\n\n        components = self.components._replace(**kwargs)\n        return self.__class__(components.geturl())\n\n    @property\n    def obscure_password(self) -> str:\n        if self.password:\n            return self.replace(password=\"********\")._url\n        return self._url\n\n    def __str__(self) -> str:\n        return self._url\n\n    def __repr__(self) -> str:\n        return f\"{self.__class__.__name__}({repr(self.obscure_password)})\"\n\n    def __eq__(self, other: typing.Any) -> bool:\n        return str(self) == str(other)\n", "metadata": {"source": "github.com/encode/databases", "file_path": "databases/core.py", "language": "python", "complexity": "complex", "star_count": 3928, "quality_score": 0.8999999999999999, "features": {"has_classes": true, "has_functions": true, "has_async": true, "has_decorators": true, "has_imports": true, "has_error_handling": true, "has_documentation": true, "has_type_hints": true, "has_testing": true, "has_logging": true, "function_count": 62, "class_count": 5, "line_count": 618}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "database_integration", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create database model with ORM relationships and queries with asyncio coroutines\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import typing\nfrom collections.abc import Sequence\n\nfrom sqlalchemy.sql import ClauseElement\n\n\nclass DatabaseBackend:\n    async def connect(self) -> None:\n        raise NotImplementedError()  # pragma: no cover\n\n    async def disconnect(self) -> None:\n        raise NotImplementedError()  # pragma: no cover\n\n    def connection(self) -> \"ConnectionBackend\":\n        raise NotImplementedError()  # pragma: no cover\n\n\nclass ConnectionBackend:\n    async def acquire(self) -> None:\n        raise NotImplementedError()  # pragma: no cover\n\n    async def release(self) -> None:\n        raise NotImplementedError()  # pragma: no cover\n\n    async def fetch_all(self, query: ClauseElement) -> typing.List[\"Record\"]:\n        raise NotImplementedError()  # pragma: no cover\n\n    async def fetch_one(self, query: ClauseElement) -> typing.Optional[\"Record\"]:\n        raise NotImplementedError()  # pragma: no cover\n\n    async def fetch_val(\n        self, query: ClauseElement, column: typing.Any = 0\n    ) -> typing.Any:\n        row = await self.fetch_one(query)\n        return None if row is None else row[column]\n\n    async def execute(self, query: ClauseElement) -> typing.Any:\n        raise NotImplementedError()  # pragma: no cover\n\n    async def execute_many(self, queries: typing.List[ClauseElement]) -> None:\n        raise NotImplementedError()  # pragma: no cover\n\n    async def iterate(\n        self, query: ClauseElement\n    ) -> typing.AsyncGenerator[typing.Mapping, None]:\n        raise NotImplementedError()  # pragma: no cover\n        # mypy needs async iterators to contain a `yield`\n        # https://github.com/python/mypy/issues/5385#issuecomment-407281656\n        yield True  # pragma: no cover\n\n    def transaction(self) -> \"TransactionBackend\":\n        raise NotImplementedError()  # pragma: no cover\n\n    @property\n    def raw_connection(self) -> typing.Any:\n        raise NotImplementedError()  # pragma: no cover\n\n\nclass TransactionBackend:\n    async def start(\n        self, is_root: bool, extra_options: typing.Dict[typing.Any, typing.Any]\n    ) -> None:\n        raise NotImplementedError()  # pragma: no cover\n\n    async def commit(self) -> None:\n        raise NotImplementedError()  # pragma: no cover\n\n    async def rollback(self) -> None:\n        raise NotImplementedError()  # pragma: no cover\n\n\nclass Record(Sequence):\n    @property\n    def _mapping(self) -> typing.Mapping:\n        raise NotImplementedError()  # pragma: no cover\n\n    def __getitem__(self, key: typing.Any) -> typing.Any:\n        raise NotImplementedError()  # pragma: no cover\n", "metadata": {"source": "github.com/encode/databases", "file_path": "databases/interfaces.py", "language": "python", "complexity": "complex", "star_count": 3928, "quality_score": 0.72, "features": {"has_classes": true, "has_functions": true, "has_async": true, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 18, "class_count": 4, "line_count": 79}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "database_integration", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create database model with ORM relationships and queries\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport os\nimport re\n\nfrom setuptools import setup\n\n\ndef get_version(package):\n    \"\"\"\n    Return package version as listed in `__version__` in `init.py`.\n    \"\"\"\n    with open(os.path.join(package, \"__init__.py\")) as f:\n        return re.search(\"__version__ = ['\\\"]([^'\\\"]+)['\\\"]\", f.read()).group(1)\n\n\ndef get_long_description():\n    \"\"\"\n    Return the README.\n    \"\"\"\n    with open(\"README.md\", encoding=\"utf8\") as f:\n        return f.read()\n\n\ndef get_packages(package):\n    \"\"\"\n    Return root package and all sub-packages.\n    \"\"\"\n    return [\n        dirpath\n        for dirpath, dirnames, filenames in os.walk(package)\n        if os.path.exists(os.path.join(dirpath, \"__init__.py\"))\n    ]\n\n\nsetup(\n    name=\"databases\",\n    version=get_version(\"databases\"),\n    python_requires=\">=3.8\",\n    url=\"https://github.com/encode/databases\",\n    license=\"BSD\",\n    description=\"Async database support for Python.\",\n    long_description=get_long_description(),\n    long_description_content_type=\"text/markdown\",\n    author=\"Tom Christie\",\n    author_email=\"tom@tomchristie.com\",\n    packages=get_packages(\"databases\"),\n    package_data={\"databases\": [\"py.typed\"]},\n    install_requires=[\"sqlalchemy>=2.0.7\"],\n    extras_require={\n        \"postgresql\": [\"asyncpg\"],\n        \"asyncpg\": [\"asyncpg\"],\n        \"aiopg\": [\"aiopg\"],\n        \"mysql\": [\"aiomysql\"],\n        \"aiomysql\": [\"aiomysql\"],\n        \"asyncmy\": [\"asyncmy\"],\n        \"sqlite\": [\"aiosqlite\"],\n        \"aiosqlite\": [\"aiosqlite\"],\n    },\n    classifiers=[\n        \"Development Status :: 3 - Alpha\",\n        \"Environment :: Web Environment\",\n        \"Intended Audience :: Developers\",\n        \"License :: OSI Approved :: BSD License\",\n        \"Operating System :: OS Independent\",\n        \"Topic :: Internet :: WWW/HTTP\",\n        \"Programming Language :: Python :: 3\",\n        \"Programming Language :: Python :: 3.8\",\n        \"Programming Language :: Python :: 3.9\",\n        \"Programming Language :: Python :: 3.10\",\n        \"Programming Language :: Python :: 3.11\",\n        \"Programming Language :: Python :: 3.12\",\n        \"Programming Language :: Python :: 3 :: Only\",\n    ],\n    zip_safe=False,\n)\n", "metadata": {"source": "github.com/encode/databases", "file_path": "setup.py", "language": "python", "complexity": "medium", "star_count": 3928, "quality_score": 0.72, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": true, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 3, "class_count": 0, "line_count": 78}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "database_integration", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create database model with ORM relationships and queries using Flask/FastAPI decorators\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import pytest\nimport sqlalchemy\nfrom starlette.applications import Starlette\nfrom starlette.responses import JSONResponse\nfrom starlette.testclient import TestClient\n\nfrom databases import Database, DatabaseURL\nfrom tests.test_databases import DATABASE_URLS\n\nmetadata = sqlalchemy.MetaData()\n\nnotes = sqlalchemy.Table(\n    \"notes\",\n    metadata,\n    sqlalchemy.Column(\"id\", sqlalchemy.Integer, primary_key=True),\n    sqlalchemy.Column(\"text\", sqlalchemy.String(length=100)),\n    sqlalchemy.Column(\"completed\", sqlalchemy.Boolean),\n)\n\n\n@pytest.fixture(autouse=True, scope=\"module\")\ndef create_test_database():\n    # Create test databases\n    for url in DATABASE_URLS:\n        database_url = DatabaseURL(url)\n        if database_url.scheme in [\"mysql\", \"mysql+aiomysql\", \"mysql+asyncmy\"]:\n            url = str(database_url.replace(driver=\"pymysql\"))\n        elif database_url.scheme in [\n            \"postgresql+aiopg\",\n            \"sqlite+aiosqlite\",\n            \"postgresql+asyncpg\",\n        ]:\n            url = str(database_url.replace(driver=None))\n        engine = sqlalchemy.create_engine(url)\n        metadata.create_all(engine)\n\n    # Run the test suite\n    yield\n\n    for url in DATABASE_URLS:\n        database_url = DatabaseURL(url)\n        if database_url.scheme in [\"mysql\", \"mysql+aiomysql\", \"mysql+asyncmy\"]:\n            url = str(database_url.replace(driver=\"pymysql\"))\n        elif database_url.scheme in [\n            \"postgresql+aiopg\",\n            \"sqlite+aiosqlite\",\n            \"postgresql+asyncpg\",\n        ]:\n            url = str(database_url.replace(driver=None))\n        engine = sqlalchemy.create_engine(url)\n        metadata.drop_all(engine)\n\n\ndef get_app(database_url):\n    database = Database(database_url, force_rollback=True)\n    app = Starlette()\n\n    @app.on_event(\"startup\")\n    async def startup():\n        await database.connect()\n\n    @app.on_event(\"shutdown\")\n    async def shutdown():\n        await database.disconnect()\n\n    @app.route(\"/notes\", methods=[\"GET\"])\n    async def list_notes(request):\n        query = notes.select()\n        results = await database.fetch_all(query)\n        content = [\n            {\"text\": result[\"text\"], \"completed\": result[\"completed\"]}\n            for result in results\n        ]\n        return JSONResponse(content)\n\n    @app.route(\"/notes\", methods=[\"POST\"])\n    async def add_note(request):\n        data = await request.json()\n        query = notes.insert().values(text=data[\"text\"], completed=data[\"completed\"])\n        await database.execute(query)\n        return JSONResponse({\"text\": data[\"text\"], \"completed\": data[\"completed\"]})\n\n    return app\n\n\n@pytest.mark.parametrize(\"database_url\", DATABASE_URLS)\ndef test_integration(database_url):\n    app = get_app(database_url)\n\n    with TestClient(app) as client:\n        response = client.post(\"/notes\", json={\"text\": \"example\", \"completed\": True})\n        assert response.status_code == 200\n        assert response.json() == {\"text\": \"example\", \"completed\": True}\n\n        response = client.get(\"/notes\")\n        assert response.status_code == 200\n        assert response.json() == [{\"text\": \"example\", \"completed\": True}]\n\n    with TestClient(app) as client:\n        # Ensure sessions are isolated\n        response = client.get(\"/notes\")\n        assert response.status_code == 200\n        assert response.json() == []\n", "metadata": {"source": "github.com/encode/databases", "file_path": "tests/test_integration.py", "language": "python", "complexity": "medium", "star_count": 3928, "quality_score": 0.75, "features": {"has_classes": false, "has_functions": true, "has_async": true, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": true, "has_logging": false, "function_count": 7, "class_count": 0, "line_count": 104}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "database_integration", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create database model with ORM relationships and queries\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import inspect\nimport os\nfrom importlib import import_module\n\nfrom django.core.exceptions import ImproperlyConfigured\nfrom django.utils.functional import cached_property\nfrom django.utils.module_loading import import_string, module_has_submodule\n\nAPPS_MODULE_NAME = \"apps\"\nMODELS_MODULE_NAME = \"models\"\n\n\nclass AppConfig:\n    \"\"\"Class representing a Django application and its configuration.\"\"\"\n\n    def __init__(self, app_name, app_module):\n        # Full Python path to the application e.g. 'django.contrib.admin'.\n        self.name = app_name\n\n        # Root module for the application e.g. <module 'django.contrib.admin'\n        # from 'django/contrib/admin/__init__.py'>.\n        self.module = app_module\n\n        # Reference to the Apps registry that holds this AppConfig. Set by the\n        # registry when it registers the AppConfig instance.\n        self.apps = None\n\n        # The following attributes could be defined at the class level in a\n        # subclass, hence the test-and-set pattern.\n\n        # Last component of the Python path to the application e.g. 'admin'.\n        # This value must be unique across a Django project.\n        if not hasattr(self, \"label\"):\n            self.label = app_name.rpartition(\".\")[2]\n        if not self.label.isidentifier():\n            raise ImproperlyConfigured(\n                \"The app label '%s' is not a valid Python identifier.\" % self.label\n            )\n\n        # Human-readable name for the application e.g. \"Admin\".\n        if not hasattr(self, \"verbose_name\"):\n            self.verbose_name = self.label.title()\n\n        # Filesystem path to the application directory e.g.\n        # '/path/to/django/contrib/admin'.\n        if not hasattr(self, \"path\"):\n            self.path = self._path_from_module(app_module)\n\n        # Module containing models e.g. <module 'django.contrib.admin.models'\n        # from 'django/contrib/admin/models.py'>. Set by import_models().\n        # None if the application doesn't have a models module.\n        self.models_module = None\n\n        # Mapping of lowercase model names to model classes. Initially set to\n        # None to prevent accidental access before import_models() runs.\n        self.models = None\n\n    def __repr__(self):\n        return \"<%s: %s>\" % (self.__class__.__name__, self.label)\n\n    @cached_property\n    def default_auto_field(self):\n        from django.conf import settings\n\n        return settings.DEFAULT_AUTO_FIELD\n\n    @property\n    def _is_default_auto_field_overridden(self):\n        return self.__class__.default_auto_field is not AppConfig.default_auto_field\n\n    def _path_from_module(self, module):\n        \"\"\"Attempt to determine app's filesystem path from its module.\"\"\"\n        # See #21874 for extended discussion of the behavior of this method in\n        # various cases.\n        # Convert to list because __path__ may not support indexing.\n        paths = list(getattr(module, \"__path__\", []))\n        if len(paths) != 1:\n            filename = getattr(module, \"__file__\", None)\n            if filename is not None:\n                paths = [os.path.dirname(filename)]\n            else:\n                # For unknown reasons, sometimes the list returned by __path__\n                # contains duplicates that must be removed (#25246).\n                paths = list(set(paths))\n        if len(paths) > 1:\n            raise ImproperlyConfigured(\n                \"The app module %r has multiple filesystem locations (%r); \"\n                \"you must configure this app with an AppConfig subclass \"\n                \"with a 'path' class attribute.\" % (module, paths)\n            )\n        elif not paths:\n            raise ImproperlyConfigured(\n                \"The app module %r has no filesystem location, \"\n                \"you must configure this app with an AppConfig subclass \"\n                \"with a 'path' class attribute.\" % module\n            )\n        return paths[0]\n\n    @classmethod\n    def create(cls, entry):\n        \"\"\"\n        Factory that creates an app config from an entry in INSTALLED_APPS.\n        \"\"\"\n        # create() eventually returns app_config_class(app_name, app_module).\n        app_config_class = None\n        app_name = None\n        app_module = None\n\n        # If import_module succeeds, entry points to the app module.\n        try:\n            app_module = import_module(entry)\n        except Exception:\n            pass\n        else:\n            # If app_module has an apps submodule that defines a single\n            # AppConfig subclass, use it automatically.\n            # To prevent this, an AppConfig subclass can declare a class\n            # variable default = False.\n            # If the apps module defines more than one AppConfig subclass,\n            # the default one can declare default = True.\n            if module_has_submodule(app_module, APPS_MODULE_NAME):\n                mod_path = \"%s.%s\" % (entry, APPS_MODULE_NAME)\n                mod = import_module(mod_path)\n                # Check if there's exactly one AppConfig candidate,\n                # excluding those that explicitly define default = False.\n                app_configs = [\n                    (name, candidate)\n                    for name, candidate in inspect.getmembers(mod, inspect.isclass)\n                    if (\n                        issubclass(candidate, cls)\n                        and candidate is not cls\n                        and getattr(candidate, \"default\", True)\n                    )\n                ]\n                if len(app_configs) == 1:\n                    app_config_class = app_configs[0][1]\n                else:\n                    # Check if there's exactly one AppConfig subclass,\n                    # among those that explicitly define default = True.\n                    app_configs = [\n                        (name, candidate)\n                        for name, candidate in app_configs\n                        if getattr(candidate, \"default\", False)\n                    ]\n                    if len(app_configs) > 1:\n                        candidates = [repr(name) for name, _ in app_configs]\n                        raise RuntimeError(\n                            \"%r declares more than one default AppConfig: \"\n                            \"%s.\" % (mod_path, \", \".join(candidates))\n                        )\n                    elif len(app_configs) == 1:\n                        app_config_class = app_configs[0][1]\n\n            # Use the default app config class if we didn't find anything.\n            if app_config_class is None:\n                app_config_class = cls\n                app_name = entry\n\n        # If import_string succeeds, entry is an app config class.\n        if app_config_class is None:\n            try:\n                app_config_class = import_string(entry)\n            except Exception:\n                pass\n        # If both import_module and import_string failed, it means that entry\n        # doesn't have a valid value.\n        if app_module is None and app_config_class is None:\n            # If the last component of entry starts with an uppercase letter,\n            # then it was likely intended to be an app config class; if not,\n            # an app module. Provide a nice error message in both cases.\n            mod_path, _, cls_name = entry.rpartition(\".\")\n            if mod_path and cls_name[0].isupper():\n                # We could simply re-trigger the string import exception, but\n                # we're going the extra mile and providing a better error\n                # message for typos in INSTALLED_APPS.\n                # This may raise ImportError, which is the best exception\n                # possible if the module at mod_path cannot be imported.\n                mod = import_module(mod_path)\n                candidates = [\n                    repr(name)\n                    for name, candidate in inspect.getmembers(mod, inspect.isclass)\n                    if issubclass(candidate, cls) and candidate is not cls\n                ]\n                msg = \"Module '%s' does not contain a '%s' class.\" % (\n                    mod_path,\n                    cls_name,\n                )\n                if candidates:\n                    msg += \" Choices are: %s.\" % \", \".join(candidates)\n                raise ImportError(msg)\n            else:\n                # Re-trigger the module import exception.\n                import_module(entry)\n\n        # Check for obvious errors. (This check prevents duck typing, but\n        # it could be removed if it became a problem in practice.)\n        if not issubclass(app_config_class, AppConfig):\n            raise ImproperlyConfigured(\"'%s' isn't a subclass of AppConfig.\" % entry)\n\n        # Obtain app name here rather than in AppClass.__init__ to keep\n        # all error checking for entries in INSTALLED_APPS in one place.\n        if app_name is None:\n            try:\n                app_name = app_config_class.name\n            except AttributeError:\n                raise ImproperlyConfigured(\"'%s' must supply a name attribute.\" % entry)\n\n        # Ensure app_name points to a valid module.\n        try:\n            app_module = import_module(app_name)\n        except ImportError:\n            raise ImproperlyConfigured(\n                \"Cannot import '%s'. Check that '%s.%s.name' is correct.\"\n                % (\n                    app_name,\n                    app_config_class.__module__,\n                    app_config_class.__qualname__,\n                )\n            )\n\n        # Entry is a path to an app config class.\n        return app_config_class(app_name, app_module)\n\n    def get_model(self, model_name, require_ready=True):\n        \"\"\"\n        Return the model with the given case-insensitive model_name.\n\n        Raise LookupError if no model exists with this name.\n        \"\"\"\n        if require_ready:\n            self.apps.check_models_ready()\n        else:\n            self.apps.check_apps_ready()\n        try:\n            return self.models[model_name.lower()]\n        except KeyError:\n            raise LookupError(\n                \"App '%s' doesn't have a '%s' model.\" % (self.label, model_name)\n            )\n\n    def get_models(self, include_auto_created=False, include_swapped=False):\n        \"\"\"\n        Return an iterable of models.\n\n        By default, the following models aren't included:\n\n        - auto-created models for many-to-many relations without\n          an explicit intermediate table,\n        - models that have been swapped out.\n\n        Set the corresponding keyword argument to True to include such models.\n        Keyword arguments aren't documented; they're a private API.\n        \"\"\"\n        self.apps.check_models_ready()\n        for model in self.models.values():\n            if model._meta.auto_created and not include_auto_created:\n                continue\n            if model._meta.swapped and not include_swapped:\n                continue\n            yield model\n\n    def import_models(self):\n        # Dictionary of models for this app, primarily maintained in the\n        # 'all_models' attribute of the Apps this AppConfig is attached to.\n        self.models = self.apps.all_models[self.label]\n\n        if module_has_submodule(self.module, MODELS_MODULE_NAME):\n            models_module_name = \"%s.%s\" % (self.name, MODELS_MODULE_NAME)\n            self.models_module = import_module(models_module_name)\n\n    def ready(self):\n        \"\"\"\n        Override this method in subclasses to run code when Django starts.\n        \"\"\"\n", "metadata": {"source": "github.com/django/django", "file_path": "django/apps/config.py", "language": "python", "complexity": "complex", "star_count": 84349, "quality_score": 1.0, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": true, "has_documentation": true, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 10, "class_count": 17, "line_count": 275}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "database_integration", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create database model with ORM relationships and queries with SQLAlchemy/Django ORM patterns\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import functools\nimport sys\nimport threading\nimport warnings\nfrom collections import Counter, defaultdict\nfrom functools import partial\n\nfrom django.core.exceptions import AppRegistryNotReady, ImproperlyConfigured\n\nfrom .config import AppConfig\n\n\nclass Apps:\n    \"\"\"\n    A registry that stores the configuration of installed applications.\n\n    It also keeps track of models, e.g. to provide reverse relations.\n    \"\"\"\n\n    def __init__(self, installed_apps=()):\n        # installed_apps is set to None when creating the main registry\n        # because it cannot be populated at that point. Other registries must\n        # provide a list of installed apps and are populated immediately.\n        if installed_apps is None and hasattr(sys.modules[__name__], \"apps\"):\n            raise RuntimeError(\"You must supply an installed_apps argument.\")\n\n        # Mapping of app labels => model names => model classes. Every time a\n        # model is imported, ModelBase.__new__ calls apps.register_model which\n        # creates an entry in all_models. All imported models are registered,\n        # regardless of whether they're defined in an installed application\n        # and whether the registry has been populated. Since it isn't possible\n        # to reimport a module safely (it could reexecute initialization code)\n        # all_models is never overridden or reset.\n        self.all_models = defaultdict(dict)\n\n        # Mapping of labels to AppConfig instances for installed apps.\n        self.app_configs = {}\n\n        # Stack of app_configs. Used to store the current state in\n        # set_available_apps and set_installed_apps.\n        self.stored_app_configs = []\n\n        # Whether the registry is populated.\n        self.apps_ready = self.models_ready = self.ready = False\n        # For the autoreloader.\n        self.ready_event = threading.Event()\n\n        # Lock for thread-safe population.\n        self._lock = threading.RLock()\n        self.loading = False\n\n        # Maps (\"app_label\", \"modelname\") tuples to lists of functions to be\n        # called when the corresponding model is ready. Used by this class's\n        # `lazy_model_operation()` and `do_pending_operations()` methods.\n        self._pending_operations = defaultdict(list)\n\n        # Populate apps and models, unless it's the main registry.\n        if installed_apps is not None:\n            self.populate(installed_apps)\n\n    def populate(self, installed_apps=None):\n        \"\"\"\n        Load application configurations and models.\n\n        Import each application module and then each model module.\n\n        It is thread-safe and idempotent, but not reentrant.\n        \"\"\"\n        if self.ready:\n            return\n\n        # populate() might be called by two threads in parallel on servers\n        # that create threads before initializing the WSGI callable.\n        with self._lock:\n            if self.ready:\n                return\n\n            # An RLock prevents other threads from entering this section. The\n            # compare and set operation below is atomic.\n            if self.loading:\n                # Prevent reentrant calls to avoid running AppConfig.ready()\n                # methods twice.\n                raise RuntimeError(\"populate() isn't reentrant\")\n            self.loading = True\n\n            # Phase 1: initialize app configs and import app modules.\n            for entry in installed_apps:\n                if isinstance(entry, AppConfig):\n                    app_config = entry\n                else:\n                    app_config = AppConfig.create(entry)\n                if app_config.label in self.app_configs:\n                    raise ImproperlyConfigured(\n                        \"Application labels aren't unique, \"\n                        \"duplicates: %s\" % app_config.label\n                    )\n\n                self.app_configs[app_config.label] = app_config\n                app_config.apps = self\n\n            # Check for duplicate app names.\n            counts = Counter(\n                app_config.name for app_config in self.app_configs.values()\n            )\n            duplicates = [name for name, count in counts.most_common() if count > 1]\n            if duplicates:\n                raise ImproperlyConfigured(\n                    \"Application names aren't unique, \"\n                    \"duplicates: %s\" % \", \".join(duplicates)\n                )\n\n            self.apps_ready = True\n\n            # Phase 2: import models modules.\n            for app_config in self.app_configs.values():\n                app_config.import_models()\n\n            self.clear_cache()\n\n            self.models_ready = True\n\n            # Phase 3: run ready() methods of app configs.\n            for app_config in self.get_app_configs():\n                app_config.ready()\n\n            self.ready = True\n            self.ready_event.set()\n\n    def check_apps_ready(self):\n        \"\"\"Raise an exception if all apps haven't been imported yet.\"\"\"\n        if not self.apps_ready:\n            from django.conf import settings\n\n            # If \"not ready\" is due to unconfigured settings, accessing\n            # INSTALLED_APPS raises a more helpful ImproperlyConfigured\n            # exception.\n            settings.INSTALLED_APPS\n            raise AppRegistryNotReady(\"Apps aren't loaded yet.\")\n\n    def check_models_ready(self):\n        \"\"\"Raise an exception if all models haven't been imported yet.\"\"\"\n        if not self.models_ready:\n            raise AppRegistryNotReady(\"Models aren't loaded yet.\")\n\n    def get_app_configs(self):\n        \"\"\"Import applications and return an iterable of app configs.\"\"\"\n        self.check_apps_ready()\n        return self.app_configs.values()\n\n    def get_app_config(self, app_label):\n        \"\"\"\n        Import applications and returns an app config for the given label.\n\n        Raise LookupError if no application exists with this label.\n        \"\"\"\n        self.check_apps_ready()\n        try:\n            return self.app_configs[app_label]\n        except KeyError:\n            message = \"No installed app with label '%s'.\" % app_label\n            for app_config in self.get_app_configs():\n                if app_config.name == app_label:\n                    message += \" Did you mean '%s'?\" % app_config.label\n                    break\n            raise LookupError(message)\n\n    # This method is performance-critical at least for Django's test suite.\n    @functools.cache\n    def get_models(self, include_auto_created=False, include_swapped=False):\n        \"\"\"\n        Return a list of all installed models.\n\n        By default, the following models aren't included:\n\n        - auto-created models for many-to-many relations without\n          an explicit intermediate table,\n        - models that have been swapped out.\n\n        Set the corresponding keyword argument to True to include such models.\n        \"\"\"\n        self.check_models_ready()\n\n        result = []\n        for app_config in self.app_configs.values():\n            result.extend(app_config.get_models(include_auto_created, include_swapped))\n        return result\n\n    def get_model(self, app_label, model_name=None, require_ready=True):\n        \"\"\"\n        Return the model matching the given app_label and model_name.\n\n        As a shortcut, app_label may be in the form <app_label>.<model_name>.\n\n        model_name is case-insensitive.\n\n        Raise LookupError if no application exists with this label, or no\n        model exists with this name in the application. Raise ValueError if\n        called with a single argument that doesn't contain exactly one dot.\n        \"\"\"\n        if require_ready:\n            self.check_models_ready()\n        else:\n            self.check_apps_ready()\n\n        if model_name is None:\n            app_label, model_name = app_label.split(\".\")\n\n        app_config = self.get_app_config(app_label)\n\n        if not require_ready and app_config.models is None:\n            app_config.import_models()\n\n        return app_config.get_model(model_name, require_ready=require_ready)\n\n    def register_model(self, app_label, model):\n        # Since this method is called when models are imported, it cannot\n        # perform imports because of the risk of import loops. It mustn't\n        # call get_app_config().\n        model_name = model._meta.model_name\n        app_models = self.all_models[app_label]\n        if model_name in app_models:\n            if (\n                model.__name__ == app_models[model_name].__name__\n                and model.__module__ == app_models[model_name].__module__\n            ):\n                warnings.warn(\n                    \"Model '%s.%s' was already registered. Reloading models is not \"\n                    \"advised as it can lead to inconsistencies, most notably with \"\n                    \"related models.\" % (app_label, model_name),\n                    RuntimeWarning,\n                    stacklevel=2,\n                )\n            else:\n                raise RuntimeError(\n                    \"Conflicting '%s' models in application '%s': %s and %s.\"\n                    % (model_name, app_label, app_models[model_name], model)\n                )\n        app_models[model_name] = model\n        self.do_pending_operations(model)\n        self.clear_cache()\n\n    def is_installed(self, app_name):\n        \"\"\"\n        Check whether an application with this name exists in the registry.\n\n        app_name is the full name of the app e.g. 'django.contrib.admin'.\n        \"\"\"\n        self.check_apps_ready()\n        return any(ac.name == app_name for ac in self.app_configs.values())\n\n    def get_containing_app_config(self, object_name):\n        \"\"\"\n        Look for an app config containing a given object.\n\n        object_name is the dotted Python path to the object.\n\n        Return the app config for the inner application in case of nesting.\n        Return None if the object isn't in any registered app config.\n        \"\"\"\n        self.check_apps_ready()\n        candidates = []\n        for app_config in self.app_configs.values():\n            if object_name.startswith(app_config.name):\n                subpath = object_name.removeprefix(app_config.name)\n                if subpath == \"\" or subpath[0] == \".\":\n                    candidates.append(app_config)\n        if candidates:\n            return sorted(candidates, key=lambda ac: -len(ac.name))[0]\n\n    def get_registered_model(self, app_label, model_name):\n        \"\"\"\n        Similar to get_model(), but doesn't require that an app exists with\n        the given app_label.\n\n        It's safe to call this method at import time, even while the registry\n        is being populated.\n        \"\"\"\n        model = self.all_models[app_label].get(model_name.lower())\n        if model is None:\n            raise LookupError(\"Model '%s.%s' not registered.\" % (app_label, model_name))\n        return model\n\n    @functools.cache\n    def get_swappable_settings_name(self, to_string):\n        \"\"\"\n        For a given model string (e.g. \"auth.User\"), return the name of the\n        corresponding settings name if it refers to a swappable model. If the\n        referred model is not swappable, return None.\n\n        This method is decorated with @functools.cache because it's performance\n        critical when it comes to migrations. Since the swappable settings\n        don't change after Django has loaded the settings, there is no reason\n        to get the respective settings attribute over and over again.\n        \"\"\"\n        to_string = to_string.lower()\n        for model in self.get_models(include_swapped=True):\n            swapped = model._meta.swapped\n            # Is this model swapped out for the model given by to_string?\n            if swapped and swapped.lower() == to_string:\n                return model._meta.swappable\n            # Is this model swappable and the one given by to_string?\n            if model._meta.swappable and model._meta.label_lower == to_string:\n                return model._meta.swappable\n        return None\n\n    def set_available_apps(self, available):\n        \"\"\"\n        Restrict the set of installed apps used by get_app_config[s].\n\n        available must be an iterable of application names.\n\n        set_available_apps() must be balanced with unset_available_apps().\n\n        Primarily used for performance optimization in TransactionTestCase.\n\n        This method is safe in the sense that it doesn't trigger any imports.\n        \"\"\"\n        available = set(available)\n        installed = {app_config.name for app_config in self.get_app_configs()}\n        if not available.issubset(installed):\n            raise ValueError(\n                \"Available apps isn't a subset of installed apps, extra apps: %s\"\n                % \", \".join(available - installed)\n            )\n\n        self.stored_app_configs.append(self.app_configs)\n        self.app_configs = {\n            label: app_config\n            for label, app_config in self.app_configs.items()\n            if app_config.name in available\n        }\n        self.clear_cache()\n\n    def unset_available_apps(self):\n        \"\"\"Cancel a previous call to set_available_apps().\"\"\"\n        self.app_configs = self.stored_app_configs.pop()\n        self.clear_cache()\n\n    def set_installed_apps(self, installed):\n        \"\"\"\n        Enable a different set of installed apps for get_app_config[s].\n\n        installed must be an iterable in the same format as INSTALLED_APPS.\n\n        set_installed_apps() must be balanced with unset_installed_apps(),\n        even if it exits with an exception.\n\n        Primarily used as a receiver of the setting_changed signal in tests.\n\n        This method may trigger new imports, which may add new models to the\n        registry of all imported models. They will stay in the registry even\n        after unset_installed_apps(). Since it isn't possible to replay\n        imports safely (e.g. that could lead to registering listeners twice),\n        models are registered when they're imported and never removed.\n        \"\"\"\n        if not self.ready:\n            raise AppRegistryNotReady(\"App registry isn't ready yet.\")\n        self.stored_app_configs.append(self.app_configs)\n        self.app_configs = {}\n        self.apps_ready = self.models_ready = self.loading = self.ready = False\n        self.clear_cache()\n        self.populate(installed)\n\n    def unset_installed_apps(self):\n        \"\"\"Cancel a previous call to set_installed_apps().\"\"\"\n        self.app_configs = self.stored_app_configs.pop()\n        self.apps_ready = self.models_ready = self.ready = True\n        self.clear_cache()\n\n    def clear_cache(self):\n        \"\"\"\n        Clear all internal caches, for methods that alter the app registry.\n\n        This is mostly used in tests.\n        \"\"\"\n        self.get_swappable_settings_name.cache_clear()\n        # Call expire cache on each model. This will purge\n        # the relation tree and the fields cache.\n        self.get_models.cache_clear()\n        if self.ready:\n            # Circumvent self.get_models() to prevent that the cache is\n            # refilled. This particularly prevents that an empty value is\n            # cached while cloning.\n            for app_config in self.app_configs.values():\n                for model in app_config.get_models(include_auto_created=True):\n                    model._meta._expire_cache()\n\n    def lazy_model_operation(self, function, *model_keys):\n        \"\"\"\n        Take a function and a number of (\"app_label\", \"modelname\") tuples, and\n        when all the corresponding models have been imported and registered,\n        call the function with the model classes as its arguments.\n\n        The function passed to this method must accept exactly n models as\n        arguments, where n=len(model_keys).\n        \"\"\"\n        # Base case: no arguments, just execute the function.\n        if not model_keys:\n            function()\n        # Recursive case: take the head of model_keys, wait for the\n        # corresponding model class to be imported and registered, then apply\n        # that argument to the supplied function. Pass the resulting partial\n        # to lazy_model_operation() along with the remaining model args and\n        # repeat until all models are loaded and all arguments are applied.\n        else:\n            next_model, *more_models = model_keys\n\n            # This will be executed after the class corresponding to next_model\n            # has been imported and registered. The `func` attribute provides\n            # duck-type compatibility with partials.\n            def apply_next_model(model):\n                next_function = partial(apply_next_model.func, model)\n                self.lazy_model_operation(next_function, *more_models)\n\n            apply_next_model.func = function\n\n            # If the model has already been imported and registered, partially\n            # apply it to the function now. If not, add it to the list of\n            # pending operations for the model, where it will be executed with\n            # the model class as its sole argument once the model is ready.\n            try:\n                model_class = self.get_registered_model(*next_model)\n            except LookupError:\n                self._pending_operations[next_model].append(apply_next_model)\n            else:\n                apply_next_model(model_class)\n\n    def do_pending_operations(self, model):\n        \"\"\"\n        Take a newly-prepared model and pass it to each function waiting for\n        it. This is called at the very end of Apps.register_model().\n        \"\"\"\n        key = model._meta.app_label, model._meta.model_name\n        for function in self._pending_operations.pop(key, []):\n            function(model)\n\n\napps = Apps(installed_apps=None)\n", "metadata": {"source": "github.com/django/django", "file_path": "django/apps/registry.py", "language": "python", "complexity": "complex", "star_count": 84349, "quality_score": 1.0, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": true, "has_documentation": true, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 21, "class_count": 5, "line_count": 439}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "database_integration", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create database model with ORM relationships and queries\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"\nSettings and configuration for Django.\n\nRead values from the module specified by the DJANGO_SETTINGS_MODULE environment\nvariable, and then from django.conf.global_settings; see the global_settings.py\nfor a list of all possible variables.\n\"\"\"\n\nimport importlib\nimport os\nimport time\nimport traceback\nimport warnings\nfrom pathlib import Path\n\nimport django\nfrom django.conf import global_settings\nfrom django.core.exceptions import ImproperlyConfigured\nfrom django.utils.functional import LazyObject, empty\n\nENVIRONMENT_VARIABLE = \"DJANGO_SETTINGS_MODULE\"\nDEFAULT_STORAGE_ALIAS = \"default\"\nSTATICFILES_STORAGE_ALIAS = \"staticfiles\"\n\n\nclass SettingsReference(str):\n    \"\"\"\n    String subclass which references a current settings value. It's treated as\n    the value in memory but serializes to a settings.NAME attribute reference.\n    \"\"\"\n\n    def __new__(self, value, setting_name):\n        return str.__new__(self, value)\n\n    def __init__(self, value, setting_name):\n        self.setting_name = setting_name\n\n\nclass LazySettings(LazyObject):\n    \"\"\"\n    A lazy proxy for either global Django settings or a custom settings object.\n    The user can manually configure settings prior to using them. Otherwise,\n    Django uses the settings module pointed to by DJANGO_SETTINGS_MODULE.\n    \"\"\"\n\n    def _setup(self, name=None):\n        \"\"\"\n        Load the settings module pointed to by the environment variable. This\n        is used the first time settings are needed, if the user hasn't\n        configured settings manually.\n        \"\"\"\n        settings_module = os.environ.get(ENVIRONMENT_VARIABLE)\n        if not settings_module:\n            desc = (\"setting %s\" % name) if name else \"settings\"\n            raise ImproperlyConfigured(\n                \"Requested %s, but settings are not configured. \"\n                \"You must either define the environment variable %s \"\n                \"or call settings.configure() before accessing settings.\"\n                % (desc, ENVIRONMENT_VARIABLE)\n            )\n\n        self._wrapped = Settings(settings_module)\n\n    def __repr__(self):\n        # Hardcode the class name as otherwise it yields 'Settings'.\n        if self._wrapped is empty:\n            return \"<LazySettings [Unevaluated]>\"\n        return '<LazySettings \"%(settings_module)s\">' % {\n            \"settings_module\": self._wrapped.SETTINGS_MODULE,\n        }\n\n    def __getattr__(self, name):\n        \"\"\"Return the value of a setting and cache it in self.__dict__.\"\"\"\n        if (_wrapped := self._wrapped) is empty:\n            self._setup(name)\n            _wrapped = self._wrapped\n        val = getattr(_wrapped, name)\n\n        # Special case some settings which require further modification.\n        # This is done here for performance reasons so the modified value is\n        # cached.\n        if name in {\"MEDIA_URL\", \"STATIC_URL\"} and val is not None:\n            val = self._add_script_prefix(val)\n        elif name == \"SECRET_KEY\" and not val:\n            raise ImproperlyConfigured(\"The SECRET_KEY setting must not be empty.\")\n\n        self.__dict__[name] = val\n        return val\n\n    def __setattr__(self, name, value):\n        \"\"\"\n        Set the value of setting. Clear all cached values if _wrapped changes\n        (@override_settings does this) or clear single values when set.\n        \"\"\"\n        if name == \"_wrapped\":\n            self.__dict__.clear()\n        else:\n            self.__dict__.pop(name, None)\n        super().__setattr__(name, value)\n\n    def __delattr__(self, name):\n        \"\"\"Delete a setting and clear it from cache if needed.\"\"\"\n        super().__delattr__(name)\n        self.__dict__.pop(name, None)\n\n    def configure(self, default_settings=global_settings, **options):\n        \"\"\"\n        Called to manually configure the settings. The 'default_settings'\n        parameter sets where to retrieve any unspecified values from (its\n        argument must support attribute access (__getattr__)).\n        \"\"\"\n        if self._wrapped is not empty:\n            raise RuntimeError(\"Settings already configured.\")\n        holder = UserSettingsHolder(default_settings)\n        for name, value in options.items():\n            if not name.isupper():\n                raise TypeError(\"Setting %r must be uppercase.\" % name)\n            setattr(holder, name, value)\n        self._wrapped = holder\n\n    @staticmethod\n    def _add_script_prefix(value):\n        \"\"\"\n        Add SCRIPT_NAME prefix to relative paths.\n\n        Useful when the app is being served at a subpath and manually prefixing\n        subpath to STATIC_URL and MEDIA_URL in settings is inconvenient.\n        \"\"\"\n        # Don't apply prefix to absolute paths and URLs.\n        if value.startswith((\"http://\", \"https://\", \"/\")):\n            return value\n        from django.urls import get_script_prefix\n\n        return \"%s%s\" % (get_script_prefix(), value)\n\n    @property\n    def configured(self):\n        \"\"\"Return True if the settings have already been configured.\"\"\"\n        return self._wrapped is not empty\n\n    def _show_deprecation_warning(self, message, category):\n        stack = traceback.extract_stack()\n        # Show a warning if the setting is used outside of Django.\n        # Stack index: -1 this line, -2 the property, -3 the\n        # LazyObject __getattribute__(), -4 the caller.\n        filename, _, _, _ = stack[-4]\n        if not filename.startswith(os.path.dirname(django.__file__)):\n            warnings.warn(message, category, stacklevel=2)\n\n\nclass Settings:\n    def __init__(self, settings_module):\n        # update this dict from global settings (but only for ALL_CAPS\n        # settings)\n        for setting in dir(global_settings):\n            if setting.isupper():\n                setattr(self, setting, getattr(global_settings, setting))\n\n        # store the settings module in case someone later cares\n        self.SETTINGS_MODULE = settings_module\n\n        mod = importlib.import_module(self.SETTINGS_MODULE)\n\n        tuple_settings = (\n            \"ALLOWED_HOSTS\",\n            \"INSTALLED_APPS\",\n            \"TEMPLATE_DIRS\",\n            \"LOCALE_PATHS\",\n            \"SECRET_KEY_FALLBACKS\",\n        )\n        self._explicit_settings = set()\n        for setting in dir(mod):\n            if setting.isupper():\n                setting_value = getattr(mod, setting)\n\n                if setting in tuple_settings and not isinstance(\n                    setting_value, (list, tuple)\n                ):\n                    raise ImproperlyConfigured(\n                        \"The %s setting must be a list or a tuple.\" % setting\n                    )\n                setattr(self, setting, setting_value)\n                self._explicit_settings.add(setting)\n\n        if hasattr(time, \"tzset\") and self.TIME_ZONE:\n            # When we can, attempt to validate the timezone. If we can't find\n            # this file, no check happens and it's harmless.\n            zoneinfo_root = Path(\"/usr/share/zoneinfo\")\n            zone_info_file = zoneinfo_root.joinpath(*self.TIME_ZONE.split(\"/\"))\n            if zoneinfo_root.exists() and not zone_info_file.exists():\n                raise ValueError(\"Incorrect timezone setting: %s\" % self.TIME_ZONE)\n            # Move the time zone info into os.environ. See ticket #2315 for why\n            # we don't do this unconditionally (breaks Windows).\n            os.environ[\"TZ\"] = self.TIME_ZONE\n            time.tzset()\n\n    def is_overridden(self, setting):\n        return setting in self._explicit_settings\n\n    def __repr__(self):\n        return '<%(cls)s \"%(settings_module)s\">' % {\n            \"cls\": self.__class__.__name__,\n            \"settings_module\": self.SETTINGS_MODULE,\n        }\n\n\nclass UserSettingsHolder:\n    \"\"\"Holder for user configured settings.\"\"\"\n\n    # SETTINGS_MODULE doesn't make much sense in the manually configured\n    # (standalone) case.\n    SETTINGS_MODULE = None\n\n    def __init__(self, default_settings):\n        \"\"\"\n        Requests for configuration variables not in this class are satisfied\n        from the module specified in default_settings (if possible).\n        \"\"\"\n        self.__dict__[\"_deleted\"] = set()\n        self.default_settings = default_settings\n\n    def __getattr__(self, name):\n        if not name.isupper() or name in self._deleted:\n            raise AttributeError\n        return getattr(self.default_settings, name)\n\n    def __setattr__(self, name, value):\n        self._deleted.discard(name)\n        super().__setattr__(name, value)\n\n    def __delattr__(self, name):\n        self._deleted.add(name)\n        if hasattr(self, name):\n            super().__delattr__(name)\n\n    def __dir__(self):\n        return sorted(\n            s\n            for s in [*self.__dict__, *dir(self.default_settings)]\n            if s not in self._deleted\n        )\n\n    def is_overridden(self, setting):\n        deleted = setting in self._deleted\n        set_locally = setting in self.__dict__\n        set_on_default = getattr(\n            self.default_settings, \"is_overridden\", lambda s: False\n        )(setting)\n        return deleted or set_locally or set_on_default\n\n    def __repr__(self):\n        return \"<%(cls)s>\" % {\n            \"cls\": self.__class__.__name__,\n        }\n\n\nsettings = LazySettings()\n", "metadata": {"source": "github.com/django/django", "file_path": "django/conf/__init__.py", "language": "python", "complexity": "complex", "star_count": 84349, "quality_score": 1.0, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": true, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 21, "class_count": 7, "line_count": 258}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "database_integration", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create database model with ORM relationships and queries with SQLAlchemy/Django ORM patterns\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"\nDefault Django settings. Override these with settings in the module pointed to\nby the DJANGO_SETTINGS_MODULE environment variable.\n\"\"\"\n\n\n# This is defined here as a do-nothing function because we can't import\n# django.utils.translation -- that module depends on the settings.\ndef gettext_noop(s):\n    return s\n\n\n####################\n# CORE             #\n####################\n\nDEBUG = False\n\n# Whether the framework should propagate raw exceptions rather than catching\n# them. This is useful under some testing situations and should never be used\n# on a live site.\nDEBUG_PROPAGATE_EXCEPTIONS = False\n\n# People who get code error notifications. In the format\n# [\"email@example.com\", '\"Full Name\" <anotheremail@example.com>']\nADMINS = []\n\n# List of IP addresses, as strings, that:\n#   * See debug comments, when DEBUG is true\n#   * Receive x-headers\nINTERNAL_IPS = []\n\n# Hosts/domain names that are valid for this site.\n# \"*\" matches anything, \".example.com\" matches example.com and all subdomains\nALLOWED_HOSTS = []\n\n# Local time zone for this installation. All choices can be found here:\n# https://en.wikipedia.org/wiki/List_of_tz_zones_by_name (although not all\n# systems may support all possibilities). When USE_TZ is True, this is\n# interpreted as the default user time zone.\nTIME_ZONE = \"America/Chicago\"\n\n# If you set this to True, Django will use timezone-aware datetimes.\nUSE_TZ = True\n\n# Language code for this installation. Valid choices can be found here:\n# https://www.iana.org/assignments/language-subtag-registry/\n# If LANGUAGE_CODE is not listed in LANGUAGES (below), the project must\n# provide the necessary translations and locale definitions.\nLANGUAGE_CODE = \"en-us\"\n\n# Languages we provide translations for, out of the box.\nLANGUAGES = [\n    (\"af\", gettext_noop(\"Afrikaans\")),\n    (\"ar\", gettext_noop(\"Arabic\")),\n    (\"ar-dz\", gettext_noop(\"Algerian Arabic\")),\n    (\"ast\", gettext_noop(\"Asturian\")),\n    (\"az\", gettext_noop(\"Azerbaijani\")),\n    (\"bg\", gettext_noop(\"Bulgarian\")),\n    (\"be\", gettext_noop(\"Belarusian\")),\n    (\"bn\", gettext_noop(\"Bengali\")),\n    (\"br\", gettext_noop(\"Breton\")),\n    (\"bs\", gettext_noop(\"Bosnian\")),\n    (\"ca\", gettext_noop(\"Catalan\")),\n    (\"ckb\", gettext_noop(\"Central Kurdish (Sorani)\")),\n    (\"cs\", gettext_noop(\"Czech\")),\n    (\"cy\", gettext_noop(\"Welsh\")),\n    (\"da\", gettext_noop(\"Danish\")),\n    (\"de\", gettext_noop(\"German\")),\n    (\"dsb\", gettext_noop(\"Lower Sorbian\")),\n    (\"el\", gettext_noop(\"Greek\")),\n    (\"en\", gettext_noop(\"English\")),\n    (\"en-au\", gettext_noop(\"Australian English\")),\n    (\"en-gb\", gettext_noop(\"British English\")),\n    (\"eo\", gettext_noop(\"Esperanto\")),\n    (\"es\", gettext_noop(\"Spanish\")),\n    (\"es-ar\", gettext_noop(\"Argentinian Spanish\")),\n    (\"es-co\", gettext_noop(\"Colombian Spanish\")),\n    (\"es-mx\", gettext_noop(\"Mexican Spanish\")),\n    (\"es-ni\", gettext_noop(\"Nicaraguan Spanish\")),\n    (\"es-ve\", gettext_noop(\"Venezuelan Spanish\")),\n    (\"et\", gettext_noop(\"Estonian\")),\n    (\"eu\", gettext_noop(\"Basque\")),\n    (\"fa\", gettext_noop(\"Persian\")),\n    (\"fi\", gettext_noop(\"Finnish\")),\n    (\"fr\", gettext_noop(\"French\")),\n    (\"fy\", gettext_noop(\"Frisian\")),\n    (\"ga\", gettext_noop(\"Irish\")),\n    (\"gd\", gettext_noop(\"Scottish Gaelic\")),\n    (\"gl\", gettext_noop(\"Galician\")),\n    (\"he\", gettext_noop(\"Hebrew\")),\n    (\"hi\", gettext_noop(\"Hindi\")),\n    (\"hr\", gettext_noop(\"Croatian\")),\n    (\"hsb\", gettext_noop(\"Upper Sorbian\")),\n    (\"hu\", gettext_noop(\"Hungarian\")),\n    (\"hy\", gettext_noop(\"Armenian\")),\n    (\"ia\", gettext_noop(\"Interlingua\")),\n    (\"id\", gettext_noop(\"Indonesian\")),\n    (\"ig\", gettext_noop(\"Igbo\")),\n    (\"io\", gettext_noop(\"Ido\")),\n    (\"is\", gettext_noop(\"Icelandic\")),\n    (\"it\", gettext_noop(\"Italian\")),\n    (\"ja\", gettext_noop(\"Japanese\")),\n    (\"ka\", gettext_noop(\"Georgian\")),\n    (\"kab\", gettext_noop(\"Kabyle\")),\n    (\"kk\", gettext_noop(\"Kazakh\")),\n    (\"km\", gettext_noop(\"Khmer\")),\n    (\"kn\", gettext_noop(\"Kannada\")),\n    (\"ko\", gettext_noop(\"Korean\")),\n    (\"ky\", gettext_noop(\"Kyrgyz\")),\n    (\"lb\", gettext_noop(\"Luxembourgish\")),\n    (\"lt\", gettext_noop(\"Lithuanian\")),\n    (\"lv\", gettext_noop(\"Latvian\")),\n    (\"mk\", gettext_noop(\"Macedonian\")),\n    (\"ml\", gettext_noop(\"Malayalam\")),\n    (\"mn\", gettext_noop(\"Mongolian\")),\n    (\"mr\", gettext_noop(\"Marathi\")),\n    (\"ms\", gettext_noop(\"Malay\")),\n    (\"my\", gettext_noop(\"Burmese\")),\n    (\"nb\", gettext_noop(\"Norwegian Bokmål\")),\n    (\"ne\", gettext_noop(\"Nepali\")),\n    (\"nl\", gettext_noop(\"Dutch\")),\n    (\"nn\", gettext_noop(\"Norwegian Nynorsk\")),\n    (\"os\", gettext_noop(\"Ossetic\")),\n    (\"pa\", gettext_noop(\"Punjabi\")),\n    (\"pl\", gettext_noop(\"Polish\")),\n    (\"pt\", gettext_noop(\"Portuguese\")),\n    (\"pt-br\", gettext_noop(\"Brazilian Portuguese\")),\n    (\"ro\", gettext_noop(\"Romanian\")),\n    (\"ru\", gettext_noop(\"Russian\")),\n    (\"sk\", gettext_noop(\"Slovak\")),\n    (\"sl\", gettext_noop(\"Slovenian\")),\n    (\"sq\", gettext_noop(\"Albanian\")),\n    (\"sr\", gettext_noop(\"Serbian\")),\n    (\"sr-latn\", gettext_noop(\"Serbian Latin\")),\n    (\"sv\", gettext_noop(\"Swedish\")),\n    (\"sw\", gettext_noop(\"Swahili\")),\n    (\"ta\", gettext_noop(\"Tamil\")),\n    (\"te\", gettext_noop(\"Telugu\")),\n    (\"tg\", gettext_noop(\"Tajik\")),\n    (\"th\", gettext_noop(\"Thai\")),\n    (\"tk\", gettext_noop(\"Turkmen\")),\n    (\"tr\", gettext_noop(\"Turkish\")),\n    (\"tt\", gettext_noop(\"Tatar\")),\n    (\"udm\", gettext_noop(\"Udmurt\")),\n    (\"ug\", gettext_noop(\"Uyghur\")),\n    (\"uk\", gettext_noop(\"Ukrainian\")),\n    (\"ur\", gettext_noop(\"Urdu\")),\n    (\"uz\", gettext_noop(\"Uzbek\")),\n    (\"vi\", gettext_noop(\"Vietnamese\")),\n    (\"zh-hans\", gettext_noop(\"Simplified Chinese\")),\n    (\"zh-hant\", gettext_noop(\"Traditional Chinese\")),\n]\n\n# Languages using BiDi (right-to-left) layout\nLANGUAGES_BIDI = [\"he\", \"ar\", \"ar-dz\", \"ckb\", \"fa\", \"ug\", \"ur\"]\n\n# If you set this to False, Django will make some optimizations so as not\n# to load the internationalization machinery.\nUSE_I18N = True\nLOCALE_PATHS = []\n\n# Settings for language cookie\nLANGUAGE_COOKIE_NAME = \"django_language\"\nLANGUAGE_COOKIE_AGE = None\nLANGUAGE_COOKIE_DOMAIN = None\nLANGUAGE_COOKIE_PATH = \"/\"\nLANGUAGE_COOKIE_SECURE = False\nLANGUAGE_COOKIE_HTTPONLY = False\nLANGUAGE_COOKIE_SAMESITE = None\n\n# Not-necessarily-technical managers of the site. They get broken link\n# notifications and other various emails.\nMANAGERS = ADMINS\n\n# Default charset to use for all HttpResponse objects, if a MIME type isn't\n# manually specified. It's used to construct the Content-Type header.\nDEFAULT_CHARSET = \"utf-8\"\n\n# Email address that error messages come from.\nSERVER_EMAIL = \"root@localhost\"\n\n# Database connection info. If left empty, will default to the dummy backend.\nDATABASES = {}\n\n# Classes used to implement DB routing behavior.\nDATABASE_ROUTERS = []\n\n# The email backend to use. For possible shortcuts see django.core.mail.\n# The default is to use the SMTP backend.\n# Third-party backends can be specified by providing a Python path\n# to a module that defines an EmailBackend class.\nEMAIL_BACKEND = \"django.core.mail.backends.smtp.EmailBackend\"\n\n# Host for sending email.\nEMAIL_HOST = \"localhost\"\n\n# Port for sending email.\nEMAIL_PORT = 25\n\n# Whether to send SMTP 'Date' header in the local time zone or in UTC.\nEMAIL_USE_LOCALTIME = False\n\n# Optional SMTP authentication information for EMAIL_HOST.\nEMAIL_HOST_USER = \"\"\nEMAIL_HOST_PASSWORD = \"\"\nEMAIL_USE_TLS = False\nEMAIL_USE_SSL = False\nEMAIL_SSL_CERTFILE = None\nEMAIL_SSL_KEYFILE = None\nEMAIL_TIMEOUT = None\n\n# List of strings representing installed apps.\nINSTALLED_APPS = []\n\nTEMPLATES = []\n\n# Default form rendering class.\nFORM_RENDERER = \"django.forms.renderers.DjangoTemplates\"\n\n# Default email address to use for various automated correspondence from\n# the site managers.\nDEFAULT_FROM_EMAIL = \"webmaster@localhost\"\n\n# Subject-line prefix for email messages send with django.core.mail.mail_admins\n# or ...mail_managers. Make sure to include the trailing space.\nEMAIL_SUBJECT_PREFIX = \"[Django] \"\n\n# Whether to append trailing slashes to URLs.\nAPPEND_SLASH = True\n\n# Whether to prepend the \"www.\" subdomain to URLs that don't have it.\nPREPEND_WWW = False\n\n# Override the server-derived value of SCRIPT_NAME\nFORCE_SCRIPT_NAME = None\n\n# List of compiled regular expression objects representing User-Agent strings\n# that are not allowed to visit any page, systemwide. Use this for bad\n# robots/crawlers. Here are a few examples:\n#     import re\n#     DISALLOWED_USER_AGENTS = [\n#         re.compile(r'^NaverBot.*'),\n#         re.compile(r'^EmailSiphon.*'),\n#         re.compile(r'^SiteSucker.*'),\n#         re.compile(r'^sohu-search'),\n#     ]\nDISALLOWED_USER_AGENTS = []\n\nABSOLUTE_URL_OVERRIDES = {}\n\n# List of compiled regular expression objects representing URLs that need not\n# be reported by BrokenLinkEmailsMiddleware. Here are a few examples:\n#    import re\n#    IGNORABLE_404_URLS = [\n#        re.compile(r'^/apple-touch-icon.*\\.png$'),\n#        re.compile(r'^/favicon.ico$'),\n#        re.compile(r'^/robots.txt$'),\n#        re.compile(r'^/phpmyadmin/'),\n#        re.compile(r'\\.(cgi|php|pl)$'),\n#    ]\nIGNORABLE_404_URLS = []\n\n# A secret key for this particular Django installation. Used in secret-key\n# hashing algorithms. Set this in your settings, or Django will complain\n# loudly.\nSECRET_KEY = \"\"\n\n# List of secret keys used to verify the validity of signatures. This allows\n# secret key rotation.\nSECRET_KEY_FALLBACKS = []\n\nSTORAGES = {\n    \"default\": {\n        \"BACKEND\": \"django.core.files.storage.FileSystemStorage\",\n    },\n    \"staticfiles\": {\n        \"BACKEND\": \"django.contrib.staticfiles.storage.StaticFilesStorage\",\n    },\n}\n\n# Absolute filesystem path to the directory that will hold user-uploaded files.\n# Example: \"/var/www/example.com/media/\"\nMEDIA_ROOT = \"\"\n\n# URL that handles the media served from MEDIA_ROOT.\n# Examples: \"http://example.com/media/\", \"http://media.example.com/\"\nMEDIA_URL = \"\"\n\n# Absolute path to the directory static files should be collected to.\n# Example: \"/var/www/example.com/static/\"\nSTATIC_ROOT = None\n\n# URL that handles the static files served from STATIC_ROOT.\n# Example: \"http://example.com/static/\", \"http://static.example.com/\"\nSTATIC_URL = None\n\n# List of upload handler classes to be applied in order.\nFILE_UPLOAD_HANDLERS = [\n    \"django.core.files.uploadhandler.MemoryFileUploadHandler\",\n    \"django.core.files.uploadhandler.TemporaryFileUploadHandler\",\n]\n\n# Maximum size, in bytes, of a request before it will be streamed to the\n# file system instead of into memory.\nFILE_UPLOAD_MAX_MEMORY_SIZE = 2621440  # i.e. 2.5 MB\n\n# Maximum size in bytes of request data (excluding file uploads) that will be\n# read before a SuspiciousOperation (RequestDataTooBig) is raised.\nDATA_UPLOAD_MAX_MEMORY_SIZE = 2621440  # i.e. 2.5 MB\n\n# Maximum number of GET/POST parameters that will be read before a\n# SuspiciousOperation (TooManyFieldsSent) is raised.\nDATA_UPLOAD_MAX_NUMBER_FIELDS = 1000\n\n# Maximum number of files encoded in a multipart upload that will be read\n# before a SuspiciousOperation (TooManyFilesSent) is raised.\nDATA_UPLOAD_MAX_NUMBER_FILES = 100\n\n# Directory in which upload streamed files will be temporarily saved. A value\n# of `None` will make Django use the operating system's default temporary\n# directory (i.e. \"/tmp\" on *nix systems).\nFILE_UPLOAD_TEMP_DIR = None\n\n# The numeric mode to set newly-uploaded files to. The value should be a mode\n# you'd pass directly to os.chmod; see\n# https://docs.python.org/library/os.html#files-and-directories.\nFILE_UPLOAD_PERMISSIONS = 0o644\n\n# The numeric mode to assign to newly-created directories, when uploading\n# files. The value should be a mode as you'd pass to os.chmod; see\n# https://docs.python.org/library/os.html#files-and-directories.\nFILE_UPLOAD_DIRECTORY_PERMISSIONS = None\n\n# Python module path where user will place custom format definition.\n# The directory where this setting is pointing should contain subdirectories\n# named as the locales, containing a formats.py file\n# (i.e. \"myproject.locale\" for myproject/locale/en/formats.py etc. use)\nFORMAT_MODULE_PATH = None\n\n# Default formatting for date objects. See all available format strings here:\n# https://docs.djangoproject.com/en/dev/ref/templates/builtins/#date\nDATE_FORMAT = \"N j, Y\"\n\n# Default formatting for datetime objects. See all available format strings\n# here: https://docs.djangoproject.com/en/dev/ref/templates/builtins/#date\nDATETIME_FORMAT = \"N j, Y, P\"\n\n# Default formatting for time objects. See all available format strings here:\n# https://docs.djangoproject.com/en/dev/ref/templates/builtins/#date\nTIME_FORMAT = \"P\"\n\n# Default formatting for date objects when only the year and month are\n# relevant. See all available format strings here:\n# https://docs.djangoproject.com/en/dev/ref/templates/builtins/#date\nYEAR_MONTH_FORMAT = \"F Y\"\n\n# Default formatting for date objects when only the month and day are relevant.\n# See all available format strings here:\n# https://docs.djangoproject.com/en/dev/ref/templates/builtins/#date\nMONTH_DAY_FORMAT = \"F j\"\n\n# Default short formatting for date objects. See all available format strings\n# here: https://docs.djangoproject.com/en/dev/ref/templates/builtins/#date\nSHORT_DATE_FORMAT = \"m/d/Y\"\n\n# Default short formatting for datetime objects.\n# See all available format strings here:\n# https://docs.djangoproject.com/en/dev/ref/templates/builtins/#date\nSHORT_DATETIME_FORMAT = \"m/d/Y P\"\n\n# Default formats to be used when parsing dates from input boxes, in order\n# See all available format string here:\n# https://docs.python.org/library/datetime.html#strftime-behavior\n# * Note that these format strings are different from the ones to display dates\nDATE_INPUT_FORMATS = [\n    \"%Y-%m-%d\",  # '2006-10-25'\n    \"%m/%d/%Y\",  # '10/25/2006'\n    \"%m/%d/%y\",  # '10/25/06'\n    \"%b %d %Y\",  # 'Oct 25 2006'\n    \"%b %d, %Y\",  # 'Oct 25, 2006'\n    \"%d %b %Y\",  # '25 Oct 2006'\n    \"%d %b, %Y\",  # '25 Oct, 2006'\n    \"%B %d %Y\",  # 'October 25 2006'\n    \"%B %d, %Y\",  # 'October 25, 2006'\n    \"%d %B %Y\",  # '25 October 2006'\n    \"%d %B, %Y\",  # '25 October, 2006'\n]\n\n# Default formats to be used when parsing times from input boxes, in order\n# See all available format string here:\n# https://docs.python.org/library/datetime.html#strftime-behavior\n# * Note that these format strings are different from the ones to display dates\nTIME_INPUT_FORMATS = [\n    \"%H:%M:%S\",  # '14:30:59'\n    \"%H:%M:%S.%f\",  # '14:30:59.000200'\n    \"%H:%M\",  # '14:30'\n]\n\n# Default formats to be used when parsing dates and times from input boxes,\n# in order\n# See all available format string here:\n# https://docs.python.org/library/datetime.html#strftime-behavior\n# * Note that these format strings are different from the ones to display dates\nDATETIME_INPUT_FORMATS = [\n    \"%Y-%m-%d %H:%M:%S\",  # '2006-10-25 14:30:59'\n    \"%Y-%m-%d %H:%M:%S.%f\",  # '2006-10-25 14:30:59.000200'\n    \"%Y-%m-%d %H:%M\",  # '2006-10-25 14:30'\n    \"%m/%d/%Y %H:%M:%S\",  # '10/25/2006 14:30:59'\n    \"%m/%d/%Y %H:%M:%S.%f\",  # '10/25/2006 14:30:59.000200'\n    \"%m/%d/%Y %H:%M\",  # '10/25/2006 14:30'\n    \"%m/%d/%y %H:%M:%S\",  # '10/25/06 14:30:59'\n    \"%m/%d/%y %H:%M:%S.%f\",  # '10/25/06 14:30:59.000200'\n    \"%m/%d/%y %H:%M\",  # '10/25/06 14:30'\n]\n\n# First day of week, to be used on calendars\n# 0 means Sunday, 1 means Monday...\nFIRST_DAY_OF_WEEK = 0\n\n# Decimal separator symbol\nDECIMAL_SEPARATOR = \".\"\n\n# Boolean that sets whether to add thousand separator when formatting numbers\nUSE_THOUSAND_SEPARATOR = False\n\n# Number of digits that will be together, when splitting them by\n# THOUSAND_SEPARATOR. 0 means no grouping, 3 means splitting by thousands...\nNUMBER_GROUPING = 0\n\n# Thousand separator symbol\nTHOUSAND_SEPARATOR = \",\"\n\n# The tablespaces to use for each model when not specified otherwise.\nDEFAULT_TABLESPACE = \"\"\nDEFAULT_INDEX_TABLESPACE = \"\"\n\n# Default primary key field type.\nDEFAULT_AUTO_FIELD = \"django.db.models.AutoField\"\n\n# Default X-Frame-Options header value\nX_FRAME_OPTIONS = \"DENY\"\n\nUSE_X_FORWARDED_HOST = False\nUSE_X_FORWARDED_PORT = False\n\n# The Python dotted path to the WSGI application that Django's internal server\n# (runserver) will use. If `None`, the return value of\n# 'django.core.wsgi.get_wsgi_application' is used, thus preserving the same\n# behavior as previous versions of Django. Otherwise this should point to an\n# actual WSGI application object.\nWSGI_APPLICATION = None\n\n# If your Django app is behind a proxy that sets a header to specify secure\n# connections, AND that proxy ensures that user-submitted headers with the\n# same name are ignored (so that people can't spoof it), set this value to\n# a tuple of (header_name, header_value). For any requests that come in with\n# that header/value, request.is_secure() will return True.\n# WARNING! Only set this if you fully understand what you're doing. Otherwise,\n# you may be opening yourself up to a security risk.\nSECURE_PROXY_SSL_HEADER = None\n\n##############\n# MIDDLEWARE #\n##############\n\n# List of middleware to use. Order is important; in the request phase, these\n# middleware will be applied in the order given, and in the response\n# phase the middleware will be applied in reverse order.\nMIDDLEWARE = []\n\n############\n# SESSIONS #\n############\n\n# Cache to store session data if using the cache session backend.\nSESSION_CACHE_ALIAS = \"default\"\n# Cookie name. This can be whatever you want.\nSESSION_COOKIE_NAME = \"sessionid\"\n# Age of cookie, in seconds (default: 2 weeks).\nSESSION_COOKIE_AGE = 60 * 60 * 24 * 7 * 2\n# A string like \"example.com\", or None for standard domain cookie.\nSESSION_COOKIE_DOMAIN = None\n# Whether the session cookie should be secure (https:// only).\nSESSION_COOKIE_SECURE = False\n# The path of the session cookie.\nSESSION_COOKIE_PATH = \"/\"\n# Whether to use the HttpOnly flag.\nSESSION_COOKIE_HTTPONLY = True\n# Whether to set the flag restricting cookie leaks on cross-site requests.\n# This can be 'Lax', 'Strict', 'None', or False to disable the flag.\nSESSION_COOKIE_SAMESITE = \"Lax\"\n# Whether to save the session data on every request.\nSESSION_SAVE_EVERY_REQUEST = False\n# Whether a user's session cookie expires when the web browser is closed.\nSESSION_EXPIRE_AT_BROWSER_CLOSE = False\n# The module to store session data\nSESSION_ENGINE = \"django.contrib.sessions.backends.db\"\n# Directory to store session files if using the file session module. If None,\n# the backend will use a sensible default.\nSESSION_FILE_PATH = None\n# class to serialize session data\nSESSION_SERIALIZER = \"django.contrib.sessions.serializers.JSONSerializer\"\n\n#########\n# CACHE #\n#########\n\n# The cache backends to use.\nCACHES = {\n    \"default\": {\n        \"BACKEND\": \"django.core.cache.backends.locmem.LocMemCache\",\n    }\n}\nCACHE_MIDDLEWARE_KEY_PREFIX = \"\"\nCACHE_MIDDLEWARE_SECONDS = 600\nCACHE_MIDDLEWARE_ALIAS = \"default\"\n\n##################\n# AUTHENTICATION #\n##################\n\nAUTH_USER_MODEL = \"auth.User\"\n\nAUTHENTICATION_BACKENDS = [\"django.contrib.auth.backends.ModelBackend\"]\n\nLOGIN_URL = \"/accounts/login/\"\n\nLOGIN_REDIRECT_URL = \"/accounts/profile/\"\n\nLOGOUT_REDIRECT_URL = None\n\n# The number of seconds a password reset link is valid for (default: 3 days).\nPASSWORD_RESET_TIMEOUT = 60 * 60 * 24 * 3\n\n# The first hasher in this list is the preferred algorithm. Any password using\n# different algorithms will be converted automatically upon login.\nPASSWORD_HASHERS = [\n    \"django.contrib.auth.hashers.PBKDF2PasswordHasher\",\n    \"django.contrib.auth.hashers.PBKDF2SHA1PasswordHasher\",\n    \"django.contrib.auth.hashers.Argon2PasswordHasher\",\n    \"django.contrib.auth.hashers.BCryptSHA256PasswordHasher\",\n    \"django.contrib.auth.hashers.ScryptPasswordHasher\",\n]\n\nAUTH_PASSWORD_VALIDATORS = []\n\n###########\n# SIGNING #\n###########\n\nSIGNING_BACKEND = \"django.core.signing.TimestampSigner\"\n\n########\n# CSRF #\n########\n\n# Dotted path to callable to be used as view when a request is\n# rejected by the CSRF middleware.\nCSRF_FAILURE_VIEW = \"django.views.csrf.csrf_failure\"\n\n# Settings for CSRF cookie.\nCSRF_COOKIE_NAME = \"csrftoken\"\nCSRF_COOKIE_AGE = 60 * 60 * 24 * 7 * 52\nCSRF_COOKIE_DOMAIN = None\nCSRF_COOKIE_PATH = \"/\"\nCSRF_COOKIE_SECURE = False\nCSRF_COOKIE_HTTPONLY = False\nCSRF_COOKIE_SAMESITE = \"Lax\"\nCSRF_HEADER_NAME = \"HTTP_X_CSRFTOKEN\"\nCSRF_TRUSTED_ORIGINS = []\nCSRF_USE_SESSIONS = False\n\n############\n# MESSAGES #\n############\n\n# Class to use as messages backend\nMESSAGE_STORAGE = \"django.contrib.messages.storage.fallback.FallbackStorage\"\n\n# Default values of MESSAGE_LEVEL and MESSAGE_TAGS are defined within\n# django.contrib.messages to avoid imports in this settings file.\n\n###########\n# LOGGING #\n###########\n\n# The callable to use to configure logging\nLOGGING_CONFIG = \"logging.config.dictConfig\"\n\n# Custom logging configuration.\nLOGGING = {}\n\n# Default exception reporter class used in case none has been\n# specifically assigned to the HttpRequest instance.\nDEFAULT_EXCEPTION_REPORTER = \"django.views.debug.ExceptionReporter\"\n\n# Default exception reporter filter class used in case none has been\n# specifically assigned to the HttpRequest instance.\nDEFAULT_EXCEPTION_REPORTER_FILTER = \"django.views.debug.SafeExceptionReporterFilter\"\n\n###########\n# TESTING #\n###########\n\n# The name of the class to use to run the test suite\nTEST_RUNNER = \"django.test.runner.DiscoverRunner\"\n\n# Apps that don't need to be serialized at test database creation time\n# (only apps with migrations are to start with)\nTEST_NON_SERIALIZED_APPS = []\n\n############\n# FIXTURES #\n############\n\n# The list of directories to search for fixtures\nFIXTURE_DIRS = []\n\n###############\n# STATICFILES #\n###############\n\n# A list of locations of additional static files\nSTATICFILES_DIRS = []\n\n# List of finder classes that know how to find static files in\n# various locations.\nSTATICFILES_FINDERS = [\n    \"django.contrib.staticfiles.finders.FileSystemFinder\",\n    \"django.contrib.staticfiles.finders.AppDirectoriesFinder\",\n    # 'django.contrib.staticfiles.finders.DefaultStorageFinder',\n]\n\n##############\n# MIGRATIONS #\n##############\n\n# Migration module overrides for apps, by app label.\nMIGRATION_MODULES = {}\n\n#################\n# SYSTEM CHECKS #\n#################\n\n# List of all issues generated by system checks that should be silenced. Light\n# issues like warnings, infos or debugs will not generate a message. Silencing\n# serious issues like errors and criticals does not result in hiding the\n# message, but Django will not stop you from e.g. running server.\nSILENCED_SYSTEM_CHECKS = []\n\n#######################\n# SECURITY MIDDLEWARE #\n#######################\nSECURE_CONTENT_TYPE_NOSNIFF = True\nSECURE_CROSS_ORIGIN_OPENER_POLICY = \"same-origin\"\nSECURE_HSTS_INCLUDE_SUBDOMAINS = False\nSECURE_HSTS_PRELOAD = False\nSECURE_HSTS_SECONDS = 0\nSECURE_REDIRECT_EXEMPT = []\nSECURE_REFERRER_POLICY = \"same-origin\"\nSECURE_SSL_HOST = None\nSECURE_SSL_REDIRECT = False\n\n##################\n# CSP MIDDLEWARE #\n##################\nSECURE_CSP = {}\nSECURE_CSP_REPORT_ONLY = {}\n\n# RemovedInDjango70Warning: A transitional setting helpful in early adoption of\n# HTTPS as the default protocol in urlize and urlizetrunc when no protocol is\n# provided. Set to True to assume HTTPS during the Django 6.x release cycle.\nURLIZE_ASSUME_HTTPS = False\n", "metadata": {"source": "github.com/django/django", "file_path": "django/conf/global_settings.py", "language": "python", "complexity": "complex", "star_count": 84349, "quality_score": 1.0, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": true, "has_type_hints": true, "has_testing": true, "has_logging": true, "function_count": 1, "class_count": 4, "line_count": 675}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "database_integration", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create database model with ORM relationships and queries\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from django.contrib.admin.decorators import action, display, register\nfrom django.contrib.admin.filters import (\n    AllValuesFieldListFilter,\n    BooleanFieldListFilter,\n    ChoicesFieldListFilter,\n    DateFieldListFilter,\n    EmptyFieldListFilter,\n    FieldListFilter,\n    ListFilter,\n    RelatedFieldListFilter,\n    RelatedOnlyFieldListFilter,\n    SimpleListFilter,\n)\nfrom django.contrib.admin.options import (\n    HORIZONTAL,\n    VERTICAL,\n    ModelAdmin,\n    ShowFacets,\n    StackedInline,\n    TabularInline,\n)\nfrom django.contrib.admin.sites import AdminSite, site\nfrom django.utils.module_loading import autodiscover_modules\n\n__all__ = [\n    \"action\",\n    \"display\",\n    \"register\",\n    \"ModelAdmin\",\n    \"HORIZONTAL\",\n    \"VERTICAL\",\n    \"StackedInline\",\n    \"TabularInline\",\n    \"AdminSite\",\n    \"site\",\n    \"ListFilter\",\n    \"SimpleListFilter\",\n    \"FieldListFilter\",\n    \"BooleanFieldListFilter\",\n    \"RelatedFieldListFilter\",\n    \"ChoicesFieldListFilter\",\n    \"DateFieldListFilter\",\n    \"AllValuesFieldListFilter\",\n    \"EmptyFieldListFilter\",\n    \"RelatedOnlyFieldListFilter\",\n    \"ShowFacets\",\n    \"autodiscover\",\n]\n\n\ndef autodiscover():\n    autodiscover_modules(\"admin\", register_to=site)\n", "metadata": {"source": "github.com/django/django", "file_path": "django/contrib/admin/__init__.py", "language": "python", "complexity": "simple", "star_count": 84349, "quality_score": 0.89, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": false, "has_testing": false, "has_logging": false, "function_count": 1, "class_count": 0, "line_count": 53}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "database_integration", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create database model with ORM relationships and queries\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"\nBuilt-in, globally-available admin actions.\n\"\"\"\n\nfrom django.contrib import messages\nfrom django.contrib.admin import helpers\nfrom django.contrib.admin.decorators import action\nfrom django.contrib.admin.utils import model_ngettext\nfrom django.core.exceptions import PermissionDenied\nfrom django.template.response import TemplateResponse\nfrom django.utils.translation import gettext as _\nfrom django.utils.translation import gettext_lazy\n\n\n@action(\n    permissions=[\"delete\"],\n    description=gettext_lazy(\"Delete selected %(verbose_name_plural)s\"),\n)\ndef delete_selected(modeladmin, request, queryset):\n    \"\"\"\n    Default action which deletes the selected objects.\n\n    This action first displays a confirmation page which shows all the\n    deletable objects, or, if the user has no permission one of the related\n    childs (foreignkeys), a \"permission denied\" message.\n\n    Next, it deletes all selected objects and redirects back to the change\n    list.\n    \"\"\"\n    opts = modeladmin.model._meta\n    app_label = opts.app_label\n\n    # Populate deletable_objects, a data structure of all related objects that\n    # will also be deleted.\n    (\n        deletable_objects,\n        model_count,\n        perms_needed,\n        protected,\n    ) = modeladmin.get_deleted_objects(queryset, request)\n\n    # The user has already confirmed the deletion.\n    # Do the deletion and return None to display the change list view again.\n    if request.POST.get(\"post\") and not protected:\n        if perms_needed:\n            raise PermissionDenied\n        n = len(queryset)\n        if n:\n            modeladmin.log_deletions(request, queryset)\n            modeladmin.delete_queryset(request, queryset)\n            modeladmin.message_user(\n                request,\n                _(\"Successfully deleted %(count)d %(items)s.\")\n                % {\"count\": n, \"items\": model_ngettext(modeladmin.opts, n)},\n                messages.SUCCESS,\n            )\n        # Return None to display the change list page again.\n        return None\n\n    objects_name = model_ngettext(queryset)\n\n    if perms_needed or protected:\n        title = _(\"Cannot delete %(name)s\") % {\"name\": objects_name}\n    else:\n        title = _(\"Delete multiple objects\")\n\n    context = {\n        **modeladmin.admin_site.each_context(request),\n        \"title\": title,\n        \"subtitle\": None,\n        \"objects_name\": str(objects_name),\n        \"deletable_objects\": [deletable_objects],\n        \"model_count\": dict(model_count).items(),\n        \"queryset\": queryset,\n        \"perms_lacking\": perms_needed,\n        \"protected\": protected,\n        \"opts\": opts,\n        \"action_checkbox_name\": helpers.ACTION_CHECKBOX_NAME,\n        \"media\": modeladmin.media,\n    }\n\n    request.current_app = modeladmin.admin_site.name\n\n    # Display the confirmation page\n    return TemplateResponse(\n        request,\n        modeladmin.delete_selected_confirmation_template\n        or [\n            \"admin/%s/%s/delete_selected_confirmation.html\"\n            % (app_label, opts.model_name),\n            \"admin/%s/delete_selected_confirmation.html\" % app_label,\n            \"admin/delete_selected_confirmation.html\",\n        ],\n        context,\n    )\n", "metadata": {"source": "github.com/django/django", "file_path": "django/contrib/admin/actions.py", "language": "python", "complexity": "medium", "star_count": 84349, "quality_score": 0.92, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": true, "has_type_hints": true, "has_testing": false, "has_logging": true, "function_count": 1, "class_count": 0, "line_count": 96}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "database_integration", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create database model with ORM relationships and queries\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from django.apps import AppConfig\nfrom django.contrib.admin.checks import check_admin_app, check_dependencies\nfrom django.core import checks\nfrom django.utils.translation import gettext_lazy as _\n\n\nclass SimpleAdminConfig(AppConfig):\n    \"\"\"Simple AppConfig which does not do automatic discovery.\"\"\"\n\n    default_auto_field = \"django.db.models.AutoField\"\n    default_site = \"django.contrib.admin.sites.AdminSite\"\n    name = \"django.contrib.admin\"\n    verbose_name = _(\"Administration\")\n\n    def ready(self):\n        checks.register(check_dependencies, checks.Tags.admin)\n        checks.register(check_admin_app, checks.Tags.admin)\n\n\nclass AdminConfig(SimpleAdminConfig):\n    \"\"\"The default AppConfig for admin which does autodiscovery.\"\"\"\n\n    default = True\n\n    def ready(self):\n        super().ready()\n        self.module.autodiscover()\n", "metadata": {"source": "github.com/django/django", "file_path": "django/contrib/admin/apps.py", "language": "python", "complexity": "simple", "star_count": 84349, "quality_score": 0.9500000000000001, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": true, "has_type_hints": false, "has_testing": false, "has_logging": false, "function_count": 2, "class_count": 2, "line_count": 28}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "database_integration", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create database model with ORM relationships and queries with SQLAlchemy/Django ORM patterns\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "def action(function=None, *, permissions=None, description=None):\n    \"\"\"\n    Conveniently add attributes to an action function::\n\n        @admin.action(\n            permissions=['publish'],\n            description='Mark selected stories as published',\n        )\n        def make_published(self, request, queryset):\n            queryset.update(status='p')\n\n    This is equivalent to setting some attributes (with the original, longer\n    names) on the function directly::\n\n        def make_published(self, request, queryset):\n            queryset.update(status='p')\n        make_published.allowed_permissions = ['publish']\n        make_published.short_description = 'Mark selected stories as published'\n    \"\"\"\n\n    def decorator(func):\n        if permissions is not None:\n            func.allowed_permissions = permissions\n        if description is not None:\n            func.short_description = description\n        return func\n\n    if function is None:\n        return decorator\n    else:\n        return decorator(function)\n\n\ndef display(\n    function=None, *, boolean=None, ordering=None, description=None, empty_value=None\n):\n    \"\"\"\n    Conveniently add attributes to a display function::\n\n        @admin.display(\n            boolean=True,\n            ordering='-publish_date',\n            description='Is Published?',\n        )\n        def is_published(self, obj):\n            return obj.publish_date is not None\n\n    This is equivalent to setting some attributes (with the original, longer\n    names) on the function directly::\n\n        def is_published(self, obj):\n            return obj.publish_date is not None\n        is_published.boolean = True\n        is_published.admin_order_field = '-publish_date'\n        is_published.short_description = 'Is Published?'\n    \"\"\"\n\n    def decorator(func):\n        if boolean is not None and empty_value is not None:\n            raise ValueError(\n                \"The boolean and empty_value arguments to the @display \"\n                \"decorator are mutually exclusive.\"\n            )\n        if boolean is not None:\n            func.boolean = boolean\n        if ordering is not None:\n            func.admin_order_field = ordering\n        if description is not None:\n            func.short_description = description\n        if empty_value is not None:\n            func.empty_value_display = empty_value\n        return func\n\n    if function is None:\n        return decorator\n    else:\n        return decorator(function)\n\n\ndef register(*models, site=None):\n    \"\"\"\n    Register the given model(s) classes and wrapped ModelAdmin class with\n    admin site:\n\n    @register(Author)\n    class AuthorAdmin(admin.ModelAdmin):\n        pass\n\n    The `site` kwarg is an admin site to use instead of the default admin site.\n    \"\"\"\n    from django.contrib.admin import ModelAdmin\n    from django.contrib.admin.sites import AdminSite\n    from django.contrib.admin.sites import site as default_site\n\n    def _model_admin_wrapper(admin_class):\n        if not models:\n            raise ValueError(\"At least one model must be passed to register.\")\n\n        admin_site = site or default_site\n\n        if not isinstance(admin_site, AdminSite):\n            raise ValueError(\"site must subclass AdminSite\")\n\n        if not issubclass(admin_class, ModelAdmin):\n            raise ValueError(\"Wrapped class must subclass ModelAdmin.\")\n\n        admin_site.register(models, admin_class=admin_class)\n\n        return admin_class\n\n    return _model_admin_wrapper\n", "metadata": {"source": "github.com/django/django", "file_path": "django/contrib/admin/decorators.py", "language": "python", "complexity": "complex", "star_count": 84349, "quality_score": 0.9500000000000001, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": true, "has_type_hints": false, "has_testing": false, "has_logging": false, "function_count": 10, "class_count": 4, "line_count": 112}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "database_integration", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create database model with ORM relationships and queries with SQLAlchemy/Django ORM patterns\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from django.core.exceptions import SuspiciousOperation\n\n\nclass DisallowedModelAdminLookup(SuspiciousOperation):\n    \"\"\"Invalid filter was passed to admin view via URL querystring\"\"\"\n\n    pass\n\n\nclass DisallowedModelAdminToField(SuspiciousOperation):\n    \"\"\"Invalid to_field was passed to admin view via URL query string\"\"\"\n\n    pass\n\n\nclass AlreadyRegistered(Exception):\n    \"\"\"The model is already registered.\"\"\"\n\n    pass\n\n\nclass NotRegistered(Exception):\n    \"\"\"The model is not registered.\"\"\"\n\n    pass\n", "metadata": {"source": "github.com/django/django", "file_path": "django/contrib/admin/exceptions.py", "language": "python", "complexity": "medium", "star_count": 84349, "quality_score": 0.92, "features": {"has_classes": true, "has_functions": false, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": true, "has_type_hints": false, "has_testing": false, "has_logging": false, "function_count": 0, "class_count": 4, "line_count": 26}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "database_integration", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create database model with ORM relationships and queries with SQLAlchemy/Django ORM patterns\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"\nThis encapsulates the logic for displaying filters in the Django admin.\nFilters are specified in models with the \"list_filter\" option.\n\nEach filter subclass knows how to display a filter for a field that passes a\ncertain test -- e.g. being a DateField or ForeignKey.\n\"\"\"\n\nimport datetime\n\nfrom django.contrib.admin.exceptions import NotRegistered\nfrom django.contrib.admin.options import IncorrectLookupParameters\nfrom django.contrib.admin.utils import (\n    build_q_object_from_lookup_parameters,\n    get_last_value_from_parameters,\n    get_model_from_relation,\n    prepare_lookup_value,\n    reverse_field_path,\n)\nfrom django.core.exceptions import ImproperlyConfigured, ValidationError\nfrom django.db import models\nfrom django.utils import timezone\nfrom django.utils.translation import gettext_lazy as _\n\n\nclass ListFilter:\n    title = None  # Human-readable title to appear in the right sidebar.\n    template = \"admin/filter.html\"\n\n    def __init__(self, request, params, model, model_admin):\n        self.request = request\n        # This dictionary will eventually contain the request's query string\n        # parameters actually used by this filter.\n        self.used_parameters = {}\n        if self.title is None:\n            raise ImproperlyConfigured(\n                \"The list filter '%s' does not specify a 'title'.\"\n                % self.__class__.__name__\n            )\n\n    def has_output(self):\n        \"\"\"\n        Return True if some choices would be output for this filter.\n        \"\"\"\n        raise NotImplementedError(\n            \"subclasses of ListFilter must provide a has_output() method\"\n        )\n\n    def choices(self, changelist):\n        \"\"\"\n        Return choices ready to be output in the template.\n\n        `changelist` is the ChangeList to be displayed.\n        \"\"\"\n        raise NotImplementedError(\n            \"subclasses of ListFilter must provide a choices() method\"\n        )\n\n    def queryset(self, request, queryset):\n        \"\"\"\n        Return the filtered queryset.\n        \"\"\"\n        raise NotImplementedError(\n            \"subclasses of ListFilter must provide a queryset() method\"\n        )\n\n    def expected_parameters(self):\n        \"\"\"\n        Return the list of parameter names that are expected from the\n        request's query string and that will be used by this filter.\n        \"\"\"\n        raise NotImplementedError(\n            \"subclasses of ListFilter must provide an expected_parameters() method\"\n        )\n\n\nclass FacetsMixin:\n    def get_facet_counts(self, pk_attname, filtered_qs):\n        raise NotImplementedError(\n            \"subclasses of FacetsMixin must provide a get_facet_counts() method.\"\n        )\n\n    def get_facet_queryset(self, changelist):\n        filtered_qs = changelist.get_queryset(\n            self.request, exclude_parameters=self.expected_parameters()\n        )\n        return filtered_qs.aggregate(\n            **self.get_facet_counts(changelist.pk_attname, filtered_qs)\n        )\n\n\nclass SimpleListFilter(FacetsMixin, ListFilter):\n    # The parameter that should be used in the query string for that filter.\n    parameter_name = None\n\n    def __init__(self, request, params, model, model_admin):\n        super().__init__(request, params, model, model_admin)\n        if self.parameter_name is None:\n            raise ImproperlyConfigured(\n                \"The list filter '%s' does not specify a 'parameter_name'.\"\n                % self.__class__.__name__\n            )\n        if self.parameter_name in params:\n            value = params.pop(self.parameter_name)\n            self.used_parameters[self.parameter_name] = value[-1]\n        lookup_choices = self.lookups(request, model_admin)\n        if lookup_choices is None:\n            lookup_choices = ()\n        self.lookup_choices = list(lookup_choices)\n\n    def has_output(self):\n        return len(self.lookup_choices) > 0\n\n    def value(self):\n        \"\"\"\n        Return the value (in string format) provided in the request's\n        query string for this filter, if any, or None if the value wasn't\n        provided.\n        \"\"\"\n        return self.used_parameters.get(self.parameter_name)\n\n    def lookups(self, request, model_admin):\n        \"\"\"\n        Must be overridden to return a list of tuples (value, verbose value)\n        \"\"\"\n        raise NotImplementedError(\n            \"The SimpleListFilter.lookups() method must be overridden to \"\n            \"return a list of tuples (value, verbose value).\"\n        )\n\n    def expected_parameters(self):\n        return [self.parameter_name]\n\n    def get_facet_counts(self, pk_attname, filtered_qs):\n        original_value = self.used_parameters.get(self.parameter_name)\n        counts = {}\n        for i, choice in enumerate(self.lookup_choices):\n            self.used_parameters[self.parameter_name] = choice[0]\n            lookup_qs = self.queryset(self.request, filtered_qs)\n            if lookup_qs is not None:\n                counts[f\"{i}__c\"] = models.Count(\n                    pk_attname,\n                    filter=models.Q(pk__in=lookup_qs),\n                )\n        self.used_parameters[self.parameter_name] = original_value\n        return counts\n\n    def choices(self, changelist):\n        add_facets = changelist.add_facets\n        facet_counts = self.get_facet_queryset(changelist) if add_facets else None\n        yield {\n            \"selected\": self.value() is None,\n            \"query_string\": changelist.get_query_string(remove=[self.parameter_name]),\n            \"display\": _(\"All\"),\n        }\n        for i, (lookup, title) in enumerate(self.lookup_choices):\n            if add_facets:\n                if (count := facet_counts.get(f\"{i}__c\", -1)) != -1:\n                    title = f\"{title} ({count})\"\n                else:\n                    title = f\"{title} (-)\"\n            yield {\n                \"selected\": self.value() == str(lookup),\n                \"query_string\": changelist.get_query_string(\n                    {self.parameter_name: lookup}\n                ),\n                \"display\": title,\n            }\n\n\nclass FieldListFilter(FacetsMixin, ListFilter):\n    _field_list_filters = []\n    _take_priority_index = 0\n    list_separator = \",\"\n\n    def __init__(self, field, request, params, model, model_admin, field_path):\n        self.field = field\n        self.field_path = field_path\n        self.title = getattr(field, \"verbose_name\", field_path)\n        super().__init__(request, params, model, model_admin)\n        for p in self.expected_parameters():\n            if p in params:\n                value = params.pop(p)\n                self.used_parameters[p] = prepare_lookup_value(\n                    p, value, self.list_separator\n                )\n\n    def has_output(self):\n        return True\n\n    def queryset(self, request, queryset):\n        try:\n            q_object = build_q_object_from_lookup_parameters(self.used_parameters)\n            return queryset.filter(q_object)\n        except (ValueError, ValidationError) as e:\n            # Fields may raise a ValueError or ValidationError when converting\n            # the parameters to the correct type.\n            raise IncorrectLookupParameters(e)\n\n    @classmethod\n    def register(cls, test, list_filter_class, take_priority=False):\n        if take_priority:\n            # This is to allow overriding the default filters for certain types\n            # of fields with some custom filters. The first found in the list\n            # is used in priority.\n            cls._field_list_filters.insert(\n                cls._take_priority_index, (test, list_filter_class)\n            )\n            cls._take_priority_index += 1\n        else:\n            cls._field_list_filters.append((test, list_filter_class))\n\n    @classmethod\n    def create(cls, field, request, params, model, model_admin, field_path):\n        for test, list_filter_class in cls._field_list_filters:\n            if test(field):\n                return list_filter_class(\n                    field, request, params, model, model_admin, field_path=field_path\n                )\n\n\nclass RelatedFieldListFilter(FieldListFilter):\n    def __init__(self, field, request, params, model, model_admin, field_path):\n        other_model = get_model_from_relation(field)\n        self.lookup_kwarg = \"%s__%s__exact\" % (field_path, field.target_field.name)\n        self.lookup_kwarg_isnull = \"%s__isnull\" % field_path\n        self.lookup_val = params.get(self.lookup_kwarg)\n        self.lookup_val_isnull = get_last_value_from_parameters(\n            params, self.lookup_kwarg_isnull\n        )\n        super().__init__(field, request, params, model, model_admin, field_path)\n        self.lookup_choices = self.field_choices(field, request, model_admin)\n        if hasattr(field, \"verbose_name\"):\n            self.lookup_title = field.verbose_name\n        else:\n            self.lookup_title = other_model._meta.verbose_name\n        self.title = self.lookup_title\n        self.empty_value_display = model_admin.get_empty_value_display()\n\n    @property\n    def include_empty_choice(self):\n        \"\"\"\n        Return True if a \"(None)\" choice should be included, which filters\n        out everything except empty relationships.\n        \"\"\"\n        return self.field.null or (self.field.is_relation and self.field.many_to_many)\n\n    def has_output(self):\n        if self.include_empty_choice:\n            extra = 1\n        else:\n            extra = 0\n        return len(self.lookup_choices) + extra > 1\n\n    def expected_parameters(self):\n        return [self.lookup_kwarg, self.lookup_kwarg_isnull]\n\n    def field_admin_ordering(self, field, request, model_admin):\n        \"\"\"\n        Return the model admin's ordering for related field, if provided.\n        \"\"\"\n        try:\n            related_admin = model_admin.admin_site.get_model_admin(\n                field.remote_field.model\n            )\n        except NotRegistered:\n            return ()\n        else:\n            return related_admin.get_ordering(request)\n\n    def field_choices(self, field, request, model_admin):\n        ordering = self.field_admin_ordering(field, request, model_admin)\n        return field.get_choices(include_blank=False, ordering=ordering)\n\n    def get_facet_counts(self, pk_attname, filtered_qs):\n        counts = {\n            f\"{pk_val}__c\": models.Count(\n                pk_attname, filter=models.Q(**{self.lookup_kwarg: pk_val})\n            )\n            for pk_val, _ in self.lookup_choices\n        }\n        if self.include_empty_choice:\n            counts[\"__c\"] = models.Count(\n                pk_attname, filter=models.Q(**{self.lookup_kwarg_isnull: True})\n            )\n        return counts\n\n    def choices(self, changelist):\n        add_facets = changelist.add_facets\n        facet_counts = self.get_facet_queryset(changelist) if add_facets else None\n        yield {\n            \"selected\": self.lookup_val is None and not self.lookup_val_isnull,\n            \"query_string\": changelist.get_query_string(\n                remove=[self.lookup_kwarg, self.lookup_kwarg_isnull]\n            ),\n            \"display\": _(\"All\"),\n        }\n        count = None\n        for pk_val, val in self.lookup_choices:\n            if add_facets:\n                count = facet_counts[f\"{pk_val}__c\"]\n                val = f\"{val} ({count})\"\n            yield {\n                \"selected\": self.lookup_val is not None\n                and str(pk_val) in self.lookup_val,\n                \"query_string\": changelist.get_query_string(\n                    {self.lookup_kwarg: pk_val}, [self.lookup_kwarg_isnull]\n                ),\n                \"display\": val,\n            }\n        empty_title = self.empty_value_display\n        if self.include_empty_choice:\n            if add_facets:\n                count = facet_counts[\"__c\"]\n                empty_title = f\"{empty_title} ({count})\"\n            yield {\n                \"selected\": bool(self.lookup_val_isnull),\n                \"query_string\": changelist.get_query_string(\n                    {self.lookup_kwarg_isnull: \"True\"}, [self.lookup_kwarg]\n                ),\n                \"display\": empty_title,\n            }\n\n\nFieldListFilter.register(lambda f: f.remote_field, RelatedFieldListFilter)\n\n\nclass BooleanFieldListFilter(FieldListFilter):\n    def __init__(self, field, request, params, model, model_admin, field_path):\n        self.lookup_kwarg = \"%s__exact\" % field_path\n        self.lookup_kwarg2 = \"%s__isnull\" % field_path\n        self.lookup_val = get_last_value_from_parameters(params, self.lookup_kwarg)\n        self.lookup_val2 = get_last_value_from_parameters(params, self.lookup_kwarg2)\n        super().__init__(field, request, params, model, model_admin, field_path)\n        if (\n            self.used_parameters\n            and self.lookup_kwarg in self.used_parameters\n            and self.used_parameters[self.lookup_kwarg] in (\"1\", \"0\")\n        ):\n            self.used_parameters[self.lookup_kwarg] = bool(\n                int(self.used_parameters[self.lookup_kwarg])\n            )\n\n    def expected_parameters(self):\n        return [self.lookup_kwarg, self.lookup_kwarg2]\n\n    def get_facet_counts(self, pk_attname, filtered_qs):\n        return {\n            \"true__c\": models.Count(\n                pk_attname, filter=models.Q(**{self.field_path: True})\n            ),\n            \"false__c\": models.Count(\n                pk_attname, filter=models.Q(**{self.field_path: False})\n            ),\n            \"null__c\": models.Count(\n                pk_attname, filter=models.Q(**{self.lookup_kwarg2: True})\n            ),\n        }\n\n    def choices(self, changelist):\n        field_choices = dict(self.field.flatchoices)\n        add_facets = changelist.add_facets\n        facet_counts = self.get_facet_queryset(changelist) if add_facets else None\n        for lookup, title, count_field in (\n            (None, _(\"All\"), None),\n            (\"1\", field_choices.get(True, _(\"Yes\")), \"true__c\"),\n            (\"0\", field_choices.get(False, _(\"No\")), \"false__c\"),\n        ):\n            if add_facets:\n                if count_field is not None:\n                    count = facet_counts[count_field]\n                    title = f\"{title} ({count})\"\n            yield {\n                \"selected\": self.lookup_val == lookup and not self.lookup_val2,\n                \"query_string\": changelist.get_query_string(\n                    {self.lookup_kwarg: lookup}, [self.lookup_kwarg2]\n                ),\n                \"display\": title,\n            }\n        if self.field.null:\n            display = field_choices.get(None, _(\"Unknown\"))\n            if add_facets:\n                count = facet_counts[\"null__c\"]\n                display = f\"{display} ({count})\"\n            yield {\n                \"selected\": self.lookup_val2 == \"True\",\n                \"query_string\": changelist.get_query_string(\n                    {self.lookup_kwarg2: \"True\"}, [self.lookup_kwarg]\n                ),\n                \"display\": display,\n            }\n\n\nFieldListFilter.register(\n    lambda f: isinstance(f, models.BooleanField), BooleanFieldListFilter\n)\n\n\nclass ChoicesFieldListFilter(FieldListFilter):\n    def __init__(self, field, request, params, model, model_admin, field_path):\n        self.lookup_kwarg = \"%s__exact\" % field_path\n        self.lookup_kwarg_isnull = \"%s__isnull\" % field_path\n        self.lookup_val = params.get(self.lookup_kwarg)\n        self.lookup_val_isnull = get_last_value_from_parameters(\n            params, self.lookup_kwarg_isnull\n        )\n        super().__init__(field, request, params, model, model_admin, field_path)\n\n    def expected_parameters(self):\n        return [self.lookup_kwarg, self.lookup_kwarg_isnull]\n\n    def get_facet_counts(self, pk_attname, filtered_qs):\n        return {\n            f\"{i}__c\": models.Count(\n                pk_attname,\n                filter=models.Q(\n                    (self.lookup_kwarg, value)\n                    if value is not None\n                    else (self.lookup_kwarg_isnull, True)\n                ),\n            )\n            for i, (value, _) in enumerate(self.field.flatchoices)\n        }\n\n    def choices(self, changelist):\n        add_facets = changelist.add_facets\n        facet_counts = self.get_facet_queryset(changelist) if add_facets else None\n        yield {\n            \"selected\": self.lookup_val is None,\n            \"query_string\": changelist.get_query_string(\n                remove=[self.lookup_kwarg, self.lookup_kwarg_isnull]\n            ),\n            \"display\": _(\"All\"),\n        }\n        none_title = \"\"\n        for i, (lookup, title) in enumerate(self.field.flatchoices):\n            if add_facets:\n                count = facet_counts[f\"{i}__c\"]\n                title = f\"{title} ({count})\"\n            if lookup is None:\n                none_title = title\n                continue\n            yield {\n                \"selected\": self.lookup_val is not None\n                and str(lookup) in self.lookup_val,\n                \"query_string\": changelist.get_query_string(\n                    {self.lookup_kwarg: lookup}, [self.lookup_kwarg_isnull]\n                ),\n                \"display\": title,\n            }\n        if none_title:\n            yield {\n                \"selected\": bool(self.lookup_val_isnull),\n                \"query_string\": changelist.get_query_string(\n                    {self.lookup_kwarg_isnull: \"True\"}, [self.lookup_kwarg]\n                ),\n                \"display\": none_title,\n            }\n\n\nFieldListFilter.register(lambda f: bool(f.choices), ChoicesFieldListFilter)\n\n\nclass DateFieldListFilter(FieldListFilter):\n    def __init__(self, field, request, params, model, model_admin, field_path):\n        self.field_generic = \"%s__\" % field_path\n        self.date_params = {\n            k: v[-1] for k, v in params.items() if k.startswith(self.field_generic)\n        }\n\n        now = timezone.now()\n        # When time zone support is enabled, convert \"now\" to the user's time\n        # zone so Django's definition of \"Today\" matches what the user expects.\n        if timezone.is_aware(now):\n            now = timezone.localtime(now)\n\n        if isinstance(field, models.DateTimeField):\n            today = now.replace(hour=0, minute=0, second=0, microsecond=0)\n        else:  # field is a models.DateField\n            today = now.date()\n        tomorrow = today + datetime.timedelta(days=1)\n        if today.month == 12:\n            next_month = today.replace(year=today.year + 1, month=1, day=1)\n        else:\n            next_month = today.replace(month=today.month + 1, day=1)\n        next_year = today.replace(year=today.year + 1, month=1, day=1)\n\n        self.lookup_kwarg_since = \"%s__gte\" % field_path\n        self.lookup_kwarg_until = \"%s__lt\" % field_path\n        self.links = (\n            (_(\"Any date\"), {}),\n            (\n                _(\"Today\"),\n                {\n                    self.lookup_kwarg_since: today,\n                    self.lookup_kwarg_until: tomorrow,\n                },\n            ),\n            (\n                _(\"Past 7 days\"),\n                {\n                    self.lookup_kwarg_since: today - datetime.timedelta(days=7),\n                    self.lookup_kwarg_until: tomorrow,\n                },\n            ),\n            (\n                _(\"This month\"),\n                {\n                    self.lookup_kwarg_since: today.replace(day=1),\n                    self.lookup_kwarg_until: next_month,\n                },\n            ),\n            (\n                _(\"This year\"),\n                {\n                    self.lookup_kwarg_since: today.replace(month=1, day=1),\n                    self.lookup_kwarg_until: next_year,\n                },\n            ),\n        )\n        if field.null:\n            self.lookup_kwarg_isnull = \"%s__isnull\" % field_path\n            self.links += (\n                (_(\"No date\"), {self.field_generic + \"isnull\": True}),\n                (_(\"Has date\"), {self.field_generic + \"isnull\": False}),\n            )\n        super().__init__(field, request, params, model, model_admin, field_path)\n\n    def expected_parameters(self):\n        params = [self.lookup_kwarg_since, self.lookup_kwarg_until]\n        if self.field.null:\n            params.append(self.lookup_kwarg_isnull)\n        return params\n\n    def get_facet_counts(self, pk_attname, filtered_qs):\n        return {\n            f\"{i}__c\": models.Count(pk_attname, filter=models.Q(**param_dict))\n            for i, (_, param_dict) in enumerate(self.links)\n        }\n\n    def choices(self, changelist):\n        add_facets = changelist.add_facets\n        facet_counts = self.get_facet_queryset(changelist) if add_facets else None\n        for i, (title, param_dict) in enumerate(self.links):\n            param_dict_str = {key: str(value) for key, value in param_dict.items()}\n            if add_facets:\n                count = facet_counts[f\"{i}__c\"]\n                title = f\"{title} ({count})\"\n            yield {\n                \"selected\": self.date_params == param_dict_str,\n                \"query_string\": changelist.get_query_string(\n                    param_dict_str, [self.field_generic]\n                ),\n                \"display\": title,\n            }\n\n\nFieldListFilter.register(lambda f: isinstance(f, models.DateField), DateFieldListFilter)\n\n\n# This should be registered last, because it's a last resort. For example,\n# if a field is eligible to use the BooleanFieldListFilter, that'd be much\n# more appropriate, and the AllValuesFieldListFilter won't get used for it.\nclass AllValuesFieldListFilter(FieldListFilter):\n    def __init__(self, field, request, params, model, model_admin, field_path):\n        self.lookup_kwarg = field_path\n        self.lookup_kwarg_isnull = \"%s__isnull\" % field_path\n        self.lookup_val = params.get(self.lookup_kwarg)\n        self.lookup_val_isnull = get_last_value_from_parameters(\n            params, self.lookup_kwarg_isnull\n        )\n        self.empty_value_display = model_admin.get_empty_value_display()\n        parent_model, reverse_path = reverse_field_path(model, field_path)\n        # Obey parent ModelAdmin queryset when deciding which options to show\n        if model == parent_model:\n            queryset = model_admin.get_queryset(request)\n        else:\n            queryset = parent_model._default_manager.all()\n        self.lookup_choices = (\n            queryset.distinct().order_by(field.name).values_list(field.name, flat=True)\n        )\n        super().__init__(field, request, params, model, model_admin, field_path)\n\n    def expected_parameters(self):\n        return [self.lookup_kwarg, self.lookup_kwarg_isnull]\n\n    def get_facet_counts(self, pk_attname, filtered_qs):\n        return {\n            f\"{i}__c\": models.Count(\n                pk_attname,\n                filter=models.Q(\n                    (self.lookup_kwarg, value)\n                    if value is not None\n                    else (self.lookup_kwarg_isnull, True)\n                ),\n            )\n            for i, value in enumerate(self.lookup_choices)\n        }\n\n    def choices(self, changelist):\n        add_facets = changelist.add_facets\n        facet_counts = self.get_facet_queryset(changelist) if add_facets else None\n        yield {\n            \"selected\": self.lookup_val is None and self.lookup_val_isnull is None,\n            \"query_string\": changelist.get_query_string(\n                remove=[self.lookup_kwarg, self.lookup_kwarg_isnull]\n            ),\n            \"display\": _(\"All\"),\n        }\n        include_none = False\n        count = None\n        empty_title = self.empty_value_display\n        for i, val in enumerate(self.lookup_choices):\n            if add_facets:\n                count = facet_counts[f\"{i}__c\"]\n            if val is None:\n                include_none = True\n                empty_title = f\"{empty_title} ({count})\" if add_facets else empty_title\n                continue\n            val = str(val)\n            yield {\n                \"selected\": self.lookup_val is not None and val in self.lookup_val,\n                \"query_string\": changelist.get_query_string(\n                    {self.lookup_kwarg: val}, [self.lookup_kwarg_isnull]\n                ),\n                \"display\": f\"{val} ({count})\" if add_facets else val,\n            }\n        if include_none:\n            yield {\n                \"selected\": bool(self.lookup_val_isnull),\n                \"query_string\": changelist.get_query_string(\n                    {self.lookup_kwarg_isnull: \"True\"}, [self.lookup_kwarg]\n                ),\n                \"display\": empty_title,\n            }\n\n\nFieldListFilter.register(lambda f: True, AllValuesFieldListFilter)\n\n\nclass RelatedOnlyFieldListFilter(RelatedFieldListFilter):\n    def field_choices(self, field, request, model_admin):\n        pk_qs = (\n            model_admin.get_queryset(request)\n            .distinct()\n            .values_list(\"%s__pk\" % self.field_path, flat=True)\n        )\n        ordering = self.field_admin_ordering(field, request, model_admin)\n        return field.get_choices(\n            include_blank=False, limit_choices_to={\"pk__in\": pk_qs}, ordering=ordering\n        )\n\n\nclass EmptyFieldListFilter(FieldListFilter):\n    def __init__(self, field, request, params, model, model_admin, field_path):\n        if not field.empty_strings_allowed and not field.null:\n            raise ImproperlyConfigured(\n                \"The list filter '%s' cannot be used with field '%s' which \"\n                \"doesn't allow empty strings and nulls.\"\n                % (\n                    self.__class__.__name__,\n                    field.name,\n                )\n            )\n        self.lookup_kwarg = \"%s__isempty\" % field_path\n        self.lookup_val = get_last_value_from_parameters(params, self.lookup_kwarg)\n        super().__init__(field, request, params, model, model_admin, field_path)\n\n    def get_lookup_condition(self):\n        lookup_conditions = []\n        if self.field.empty_strings_allowed:\n            lookup_conditions.append((self.field_path, \"\"))\n        if self.field.null:\n            lookup_conditions.append((f\"{self.field_path}__isnull\", True))\n        return models.Q.create(lookup_conditions, connector=models.Q.OR)\n\n    def queryset(self, request, queryset):\n        if self.lookup_kwarg not in self.used_parameters:\n            return queryset\n        if self.lookup_val not in (\"0\", \"1\"):\n            raise IncorrectLookupParameters\n\n        lookup_condition = self.get_lookup_condition()\n        if self.lookup_val == \"1\":\n            return queryset.filter(lookup_condition)\n        return queryset.exclude(lookup_condition)\n\n    def expected_parameters(self):\n        return [self.lookup_kwarg]\n\n    def get_facet_counts(self, pk_attname, filtered_qs):\n        lookup_condition = self.get_lookup_condition()\n        return {\n            \"empty__c\": models.Count(pk_attname, filter=lookup_condition),\n            \"not_empty__c\": models.Count(pk_attname, filter=~lookup_condition),\n        }\n\n    def choices(self, changelist):\n        add_facets = changelist.add_facets\n        facet_counts = self.get_facet_queryset(changelist) if add_facets else None\n        for lookup, title, count_field in (\n            (None, _(\"All\"), None),\n            (\"1\", _(\"Empty\"), \"empty__c\"),\n            (\"0\", _(\"Not empty\"), \"not_empty__c\"),\n        ):\n            if add_facets:\n                if count_field is not None:\n                    count = facet_counts[count_field]\n                    title = f\"{title} ({count})\"\n            yield {\n                \"selected\": self.lookup_val == lookup,\n                \"query_string\": changelist.get_query_string(\n                    {self.lookup_kwarg: lookup}\n                ),\n                \"display\": title,\n            }\n", "metadata": {"source": "github.com/django/django", "file_path": "django/contrib/admin/filters.py", "language": "python", "complexity": "complex", "star_count": 84349, "quality_score": 0.98, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": true, "has_documentation": true, "has_type_hints": true, "has_testing": false, "has_logging": true, "function_count": 50, "class_count": 13, "line_count": 717}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "database_integration", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create database model with ORM relationships and queries\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import json\n\nfrom django import forms\nfrom django.contrib.admin.utils import (\n    display_for_field,\n    flatten_fieldsets,\n    help_text_for_field,\n    label_for_field,\n    lookup_field,\n    quote,\n)\nfrom django.core.exceptions import ObjectDoesNotExist\nfrom django.db.models.fields.related import (\n    ForeignObjectRel,\n    ManyToManyRel,\n    OneToOneField,\n)\nfrom django.forms.utils import flatatt\nfrom django.template.defaultfilters import capfirst, linebreaksbr\nfrom django.urls import NoReverseMatch, reverse\nfrom django.utils.functional import cached_property\nfrom django.utils.html import conditional_escape, format_html\nfrom django.utils.safestring import mark_safe\nfrom django.utils.translation import gettext\nfrom django.utils.translation import gettext_lazy as _\n\nACTION_CHECKBOX_NAME = \"_selected_action\"\n\n\nclass ActionForm(forms.Form):\n    action = forms.ChoiceField(label=_(\"Action:\"))\n    select_across = forms.BooleanField(\n        label=\"\",\n        required=False,\n        initial=0,\n        widget=forms.HiddenInput({\"class\": \"select-across\"}),\n    )\n\n\nclass AdminForm:\n    def __init__(\n        self,\n        form,\n        fieldsets,\n        prepopulated_fields,\n        readonly_fields=None,\n        model_admin=None,\n    ):\n        self.form, self.fieldsets = form, fieldsets\n        self.prepopulated_fields = [\n            {\"field\": form[field_name], \"dependencies\": [form[f] for f in dependencies]}\n            for field_name, dependencies in prepopulated_fields.items()\n        ]\n        self.model_admin = model_admin\n        if readonly_fields is None:\n            readonly_fields = ()\n        self.readonly_fields = readonly_fields\n\n    def __repr__(self):\n        return (\n            f\"<{self.__class__.__qualname__}: \"\n            f\"form={self.form.__class__.__qualname__} \"\n            f\"fieldsets={self.fieldsets!r}>\"\n        )\n\n    def __iter__(self):\n        for name, options in self.fieldsets:\n            yield Fieldset(\n                self.form,\n                name,\n                readonly_fields=self.readonly_fields,\n                model_admin=self.model_admin,\n                **options,\n            )\n\n    @property\n    def errors(self):\n        return self.form.errors\n\n    @property\n    def non_field_errors(self):\n        return self.form.non_field_errors\n\n    @property\n    def fields(self):\n        return self.form.fields\n\n    @property\n    def is_bound(self):\n        return self.form.is_bound\n\n    @property\n    def media(self):\n        media = self.form.media\n        for fs in self:\n            media += fs.media\n        return media\n\n\nclass Fieldset:\n    def __init__(\n        self,\n        form,\n        name=None,\n        readonly_fields=(),\n        fields=(),\n        classes=(),\n        description=None,\n        model_admin=None,\n    ):\n        self.form = form\n        self.name, self.fields = name, fields\n        self.classes = \" \".join(classes)\n        self.description = description\n        self.model_admin = model_admin\n        self.readonly_fields = readonly_fields\n\n    @property\n    def media(self):\n        return forms.Media()\n\n    @cached_property\n    def is_collapsible(self):\n        if any(field in self.fields for field in self.form.errors):\n            return False\n        return \"collapse\" in self.classes\n\n    def __iter__(self):\n        for field in self.fields:\n            yield Fieldline(\n                self.form, field, self.readonly_fields, model_admin=self.model_admin\n            )\n\n\nclass Fieldline:\n    def __init__(self, form, field, readonly_fields=None, model_admin=None):\n        self.form = form  # A django.forms.Form instance\n        if not hasattr(field, \"__iter__\") or isinstance(field, str):\n            self.fields = [field]\n        else:\n            self.fields = field\n        self.has_visible_field = not all(\n            field in self.form.fields and self.form.fields[field].widget.is_hidden\n            for field in self.fields\n        )\n        self.model_admin = model_admin\n        if readonly_fields is None:\n            readonly_fields = ()\n        self.readonly_fields = readonly_fields\n\n    def __iter__(self):\n        for i, field in enumerate(self.fields):\n            if field in self.readonly_fields:\n                yield AdminReadonlyField(\n                    self.form, field, is_first=(i == 0), model_admin=self.model_admin\n                )\n            else:\n                yield AdminField(self.form, field, is_first=(i == 0))\n\n    def errors(self):\n        return mark_safe(\n            \"\\n\".join(\n                self.form[f].errors.as_ul()\n                for f in self.fields\n                if f not in self.readonly_fields\n            ).strip(\"\\n\")\n        )\n\n\nclass AdminField:\n    def __init__(self, form, field, is_first):\n        self.field = form[field]  # A django.forms.BoundField instance\n        self.is_first = is_first  # Whether this field is first on the line\n        self.is_checkbox = isinstance(self.field.field.widget, forms.CheckboxInput)\n        self.is_readonly = False\n\n    def label_tag(self):\n        classes = []\n        contents = conditional_escape(self.field.label)\n        if self.is_checkbox:\n            classes.append(\"vCheckboxLabel\")\n\n        if self.field.field.required:\n            classes.append(\"required\")\n        if not self.is_first:\n            classes.append(\"inline\")\n        attrs = {\"class\": \" \".join(classes)} if classes else {}\n        # checkboxes should not have a label suffix as the checkbox appears\n        # to the left of the label.\n        return self.field.label_tag(\n            contents=mark_safe(contents),\n            attrs=attrs,\n            label_suffix=\"\" if self.is_checkbox else None,\n        )\n\n    def errors(self):\n        return mark_safe(self.field.errors.as_ul())\n\n\nclass AdminReadonlyField:\n    def __init__(self, form, field, is_first, model_admin=None):\n        # Make self.field look a little bit like a field. This means that\n        # {{ field.name }} must be a useful class name to identify the field.\n        # For convenience, store other field-related data here too.\n        if callable(field):\n            class_name = field.__name__ if field.__name__ != \"<lambda>\" else \"\"\n        else:\n            class_name = field\n\n        if form._meta.labels and class_name in form._meta.labels:\n            label = form._meta.labels[class_name]\n        else:\n            label = label_for_field(field, form._meta.model, model_admin, form=form)\n\n        if form._meta.help_texts and class_name in form._meta.help_texts:\n            help_text = form._meta.help_texts[class_name]\n        else:\n            help_text = help_text_for_field(class_name, form._meta.model)\n\n        if field in form.fields:\n            is_hidden = form.fields[field].widget.is_hidden\n        else:\n            is_hidden = False\n\n        self.field = {\n            \"name\": class_name,\n            \"label\": label,\n            \"help_text\": help_text,\n            \"field\": field,\n            \"is_hidden\": is_hidden,\n        }\n        self.form = form\n        self.model_admin = model_admin\n        self.is_first = is_first\n        self.is_checkbox = False\n        self.is_readonly = True\n        self.empty_value_display = model_admin.get_empty_value_display()\n\n    def label_tag(self):\n        attrs = {}\n        if not self.is_first:\n            attrs[\"class\"] = \"inline\"\n        label = self.field[\"label\"]\n        return format_html(\n            \"<label{}>{}{}</label>\",\n            flatatt(attrs),\n            capfirst(label),\n            self.form.label_suffix,\n        )\n\n    def get_admin_url(self, remote_field, remote_obj):\n        url_name = \"admin:%s_%s_change\" % (\n            remote_field.model._meta.app_label,\n            remote_field.model._meta.model_name,\n        )\n        try:\n            url = reverse(\n                url_name,\n                args=[quote(remote_obj.pk)],\n                current_app=self.model_admin.admin_site.name,\n            )\n            return format_html('<a href=\"{}\">{}</a>', url, remote_obj)\n        except NoReverseMatch:\n            return str(remote_obj)\n\n    def contents(self):\n        from django.contrib.admin.templatetags.admin_list import _boolean_icon\n\n        field, obj, model_admin = (\n            self.field[\"field\"],\n            self.form.instance,\n            self.model_admin,\n        )\n        try:\n            f, attr, value = lookup_field(field, obj, model_admin)\n        except (AttributeError, ValueError, ObjectDoesNotExist):\n            result_repr = self.empty_value_display\n        else:\n            if f is None:\n                if getattr(attr, \"boolean\", False):\n                    result_repr = _boolean_icon(value)\n                else:\n                    if hasattr(value, \"__html__\"):\n                        result_repr = value\n                    else:\n                        result_repr = linebreaksbr(value)\n            else:\n                if isinstance(f.remote_field, ManyToManyRel) and value is not None:\n                    result_repr = \", \".join(map(str, value.all()))\n                elif (\n                    isinstance(f.remote_field, (ForeignObjectRel, OneToOneField))\n                    and value is not None\n                ):\n                    result_repr = self.get_admin_url(f.remote_field, value)\n                else:\n                    result_repr = display_for_field(value, f, self.empty_value_display)\n                result_repr = linebreaksbr(result_repr)\n        return conditional_escape(result_repr)\n\n\nclass InlineAdminFormSet:\n    \"\"\"\n    A wrapper around an inline formset for use in the admin system.\n    \"\"\"\n\n    def __init__(\n        self,\n        inline,\n        formset,\n        fieldsets,\n        prepopulated_fields=None,\n        readonly_fields=None,\n        model_admin=None,\n        has_add_permission=True,\n        has_change_permission=True,\n        has_delete_permission=True,\n        has_view_permission=True,\n    ):\n        self.opts = inline\n        self.formset = formset\n        self.fieldsets = fieldsets\n        self.model_admin = model_admin\n        if readonly_fields is None:\n            readonly_fields = ()\n        self.readonly_fields = readonly_fields\n        if prepopulated_fields is None:\n            prepopulated_fields = {}\n        self.prepopulated_fields = prepopulated_fields\n        self.classes = \" \".join(inline.classes) if inline.classes else \"\"\n        self.has_add_permission = has_add_permission\n        self.has_change_permission = has_change_permission\n        self.has_delete_permission = has_delete_permission\n        self.has_view_permission = has_view_permission\n\n    def __iter__(self):\n        if self.has_change_permission:\n            readonly_fields_for_editing = self.readonly_fields\n        else:\n            readonly_fields_for_editing = self.readonly_fields + flatten_fieldsets(\n                self.fieldsets\n            )\n\n        for form, original in zip(\n            self.formset.initial_forms, self.formset.get_queryset()\n        ):\n            view_on_site_url = self.opts.get_view_on_site_url(original)\n            yield InlineAdminForm(\n                self.formset,\n                form,\n                self.fieldsets,\n                self.prepopulated_fields,\n                original,\n                readonly_fields_for_editing,\n                model_admin=self.opts,\n                view_on_site_url=view_on_site_url,\n            )\n        for form in self.formset.extra_forms:\n            yield InlineAdminForm(\n                self.formset,\n                form,\n                self.fieldsets,\n                self.prepopulated_fields,\n                None,\n                self.readonly_fields,\n                model_admin=self.opts,\n            )\n        if self.has_add_permission:\n            yield InlineAdminForm(\n                self.formset,\n                self.formset.empty_form,\n                self.fieldsets,\n                self.prepopulated_fields,\n                None,\n                self.readonly_fields,\n                model_admin=self.opts,\n            )\n\n    def fields(self):\n        fk = getattr(self.formset, \"fk\", None)\n        empty_form = self.formset.empty_form\n        meta_labels = empty_form._meta.labels or {}\n        meta_help_texts = empty_form._meta.help_texts or {}\n        for i, field_name in enumerate(flatten_fieldsets(self.fieldsets)):\n            if fk and fk.name == field_name:\n                continue\n            if not self.has_change_permission or field_name in self.readonly_fields:\n                form_field = empty_form.fields.get(field_name)\n                widget_is_hidden = False\n                if form_field is not None:\n                    widget_is_hidden = form_field.widget.is_hidden\n                yield {\n                    \"name\": field_name,\n                    \"label\": meta_labels.get(field_name)\n                    or label_for_field(\n                        field_name,\n                        self.opts.model,\n                        self.opts,\n                        form=empty_form,\n                    ),\n                    \"widget\": {\"is_hidden\": widget_is_hidden},\n                    \"required\": False,\n                    \"help_text\": meta_help_texts.get(field_name)\n                    or help_text_for_field(field_name, self.opts.model),\n                }\n            else:\n                form_field = empty_form.fields[field_name]\n                label = form_field.label\n                if label is None:\n                    label = label_for_field(\n                        field_name, self.opts.model, self.opts, form=empty_form\n                    )\n                yield {\n                    \"name\": field_name,\n                    \"label\": label,\n                    \"widget\": form_field.widget,\n                    \"required\": form_field.required,\n                    \"help_text\": form_field.help_text,\n                }\n\n    def inline_formset_data(self):\n        verbose_name = self.opts.verbose_name\n        return json.dumps(\n            {\n                \"name\": \"#%s\" % self.formset.prefix,\n                \"options\": {\n                    \"prefix\": self.formset.prefix,\n                    \"addText\": gettext(\"Add another %(verbose_name)s\")\n                    % {\n                        \"verbose_name\": capfirst(verbose_name),\n                    },\n                    \"deleteText\": gettext(\"Remove\"),\n                },\n            }\n        )\n\n    @property\n    def forms(self):\n        return self.formset.forms\n\n    @cached_property\n    def is_collapsible(self):\n        if any(self.formset.errors):\n            return False\n        return \"collapse\" in self.classes\n\n    def non_form_errors(self):\n        return self.formset.non_form_errors()\n\n    @property\n    def is_bound(self):\n        return self.formset.is_bound\n\n    @property\n    def total_form_count(self):\n        return self.formset.total_form_count\n\n    @property\n    def media(self):\n        media = self.opts.media + self.formset.media\n        for fs in self:\n            media += fs.media\n        return media\n\n\nclass InlineAdminForm(AdminForm):\n    \"\"\"\n    A wrapper around an inline form for use in the admin system.\n    \"\"\"\n\n    def __init__(\n        self,\n        formset,\n        form,\n        fieldsets,\n        prepopulated_fields,\n        original,\n        readonly_fields=None,\n        model_admin=None,\n        view_on_site_url=None,\n    ):\n        self.formset = formset\n        self.model_admin = model_admin\n        self.original = original\n        self.show_url = original and view_on_site_url is not None\n        self.absolute_url = view_on_site_url\n        super().__init__(\n            form, fieldsets, prepopulated_fields, readonly_fields, model_admin\n        )\n\n    def __iter__(self):\n        for name, options in self.fieldsets:\n            yield InlineFieldset(\n                self.formset,\n                self.form,\n                name,\n                self.readonly_fields,\n                model_admin=self.model_admin,\n                **options,\n            )\n\n    def needs_explicit_pk_field(self):\n        return (\n            # Auto fields are editable, so check for auto or non-editable pk.\n            self.form._meta.model._meta.auto_field\n            or not self.form._meta.model._meta.pk.editable\n            # The pk can be editable, but excluded from the inline.\n            or (\n                self.form._meta.exclude\n                and self.form._meta.model._meta.pk.name in self.form._meta.exclude\n            )\n            or\n            # Also search any parents for an auto field. (The pk info is\n            # propagated to child models so that does not need to be checked\n            # in parents.)\n            any(\n                parent._meta.auto_field or not parent._meta.model._meta.pk.editable\n                for parent in self.form._meta.model._meta.all_parents\n            )\n        )\n\n    def pk_field(self):\n        return AdminField(self.form, self.formset._pk_field.name, False)\n\n    def fk_field(self):\n        fk = getattr(self.formset, \"fk\", None)\n        if fk:\n            return AdminField(self.form, fk.name, False)\n        else:\n            return \"\"\n\n    def deletion_field(self):\n        from django.forms.formsets import DELETION_FIELD_NAME\n\n        return AdminField(self.form, DELETION_FIELD_NAME, False)\n\n\nclass InlineFieldset(Fieldset):\n    def __init__(self, formset, *args, **kwargs):\n        self.formset = formset\n        super().__init__(*args, **kwargs)\n\n    def __iter__(self):\n        fk = getattr(self.formset, \"fk\", None)\n        for field in self.fields:\n            if not fk or fk.name != field:\n                yield Fieldline(\n                    self.form, field, self.readonly_fields, model_admin=self.model_admin\n                )\n\n\nclass AdminErrorList(forms.utils.ErrorList):\n    \"\"\"Store errors for the form/formsets in an add/change view.\"\"\"\n\n    def __init__(self, form, inline_formsets):\n        super().__init__()\n\n        if form.is_bound:\n            self.extend(form.errors.values())\n            for inline_formset in inline_formsets:\n                self.extend(inline_formset.non_form_errors())\n                for errors_in_inline_form in inline_formset.errors:\n                    self.extend(errors_in_inline_form.values())\n", "metadata": {"source": "github.com/django/django", "file_path": "django/contrib/admin/helpers.py", "language": "python", "complexity": "complex", "star_count": 84349, "quality_score": 0.98, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": true, "has_documentation": true, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 41, "class_count": 11, "line_count": 563}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "database_integration", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create database model with ORM relationships and queries with SQLAlchemy/Django ORM patterns\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import django.contrib.admin.models\nfrom django.conf import settings\nfrom django.db import migrations, models\n\n\nclass Migration(migrations.Migration):\n    dependencies = [\n        migrations.swappable_dependency(settings.AUTH_USER_MODEL),\n        (\"contenttypes\", \"__first__\"),\n    ]\n\n    operations = [\n        migrations.CreateModel(\n            name=\"LogEntry\",\n            fields=[\n                (\n                    \"id\",\n                    models.AutoField(\n                        verbose_name=\"ID\",\n                        serialize=False,\n                        auto_created=True,\n                        primary_key=True,\n                    ),\n                ),\n                (\n                    \"action_time\",\n                    models.DateTimeField(auto_now=True, verbose_name=\"action time\"),\n                ),\n                (\n                    \"object_id\",\n                    models.TextField(null=True, verbose_name=\"object id\", blank=True),\n                ),\n                (\n                    \"object_repr\",\n                    models.CharField(max_length=200, verbose_name=\"object repr\"),\n                ),\n                (\n                    \"action_flag\",\n                    models.PositiveSmallIntegerField(verbose_name=\"action flag\"),\n                ),\n                (\n                    \"change_message\",\n                    models.TextField(verbose_name=\"change message\", blank=True),\n                ),\n                (\n                    \"content_type\",\n                    models.ForeignKey(\n                        on_delete=models.SET_NULL,\n                        blank=True,\n                        null=True,\n                        to=\"contenttypes.ContentType\",\n                        verbose_name=\"content type\",\n                    ),\n                ),\n                (\n                    \"user\",\n                    models.ForeignKey(\n                        to=settings.AUTH_USER_MODEL,\n                        on_delete=models.CASCADE,\n                        verbose_name=\"user\",\n                    ),\n                ),\n            ],\n            options={\n                \"ordering\": [\"-action_time\"],\n                \"db_table\": \"django_admin_log\",\n                \"verbose_name\": \"log entry\",\n                \"verbose_name_plural\": \"log entries\",\n            },\n            bases=(models.Model,),\n            managers=[\n                (\"objects\", django.contrib.admin.models.LogEntryManager()),\n            ],\n        ),\n    ]\n", "metadata": {"source": "github.com/django/django", "file_path": "django/contrib/admin/migrations/0001_initial.py", "language": "python", "complexity": "medium", "star_count": 84349, "quality_score": 0.92, "features": {"has_classes": true, "has_functions": false, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": false, "has_logging": true, "function_count": 0, "class_count": 1, "line_count": 76}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "database_integration", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create database model with ORM relationships and queries\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from django.db import migrations, models\nfrom django.utils import timezone\n\n\nclass Migration(migrations.Migration):\n    dependencies = [\n        (\"admin\", \"0001_initial\"),\n    ]\n\n    # No database changes; removes auto_add and adds default/editable.\n    operations = [\n        migrations.AlterField(\n            model_name=\"logentry\",\n            name=\"action_time\",\n            field=models.DateTimeField(\n                verbose_name=\"action time\",\n                default=timezone.now,\n                editable=False,\n            ),\n        ),\n    ]\n", "metadata": {"source": "github.com/django/django", "file_path": "django/contrib/admin/migrations/0002_logentry_remove_auto_add.py", "language": "python", "complexity": "simple", "star_count": 84349, "quality_score": 0.89, "features": {"has_classes": true, "has_functions": false, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": false, "has_testing": false, "has_logging": true, "function_count": 0, "class_count": 1, "line_count": 22}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "database_integration", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create database model with ORM relationships and queries\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from django.db import migrations, models\n\n\nclass Migration(migrations.Migration):\n    dependencies = [\n        (\"admin\", \"0002_logentry_remove_auto_add\"),\n    ]\n\n    # No database changes; adds choices to action_flag.\n    operations = [\n        migrations.AlterField(\n            model_name=\"logentry\",\n            name=\"action_flag\",\n            field=models.PositiveSmallIntegerField(\n                choices=[(1, \"Addition\"), (2, \"Change\"), (3, \"Deletion\")],\n                verbose_name=\"action flag\",\n            ),\n        ),\n    ]\n", "metadata": {"source": "github.com/django/django", "file_path": "django/contrib/admin/migrations/0003_logentry_add_action_flag_choices.py", "language": "python", "complexity": "simple", "star_count": 84349, "quality_score": 0.89, "features": {"has_classes": true, "has_functions": false, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": false, "has_testing": false, "has_logging": true, "function_count": 0, "class_count": 1, "line_count": 20}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "database_integration", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create database model with ORM relationships and queries with SQLAlchemy/Django ORM patterns\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import json\n\nfrom django.conf import settings\nfrom django.contrib.admin.utils import quote\nfrom django.contrib.contenttypes.models import ContentType\nfrom django.db import models\nfrom django.urls import NoReverseMatch, reverse\nfrom django.utils import timezone\nfrom django.utils.text import get_text_list\nfrom django.utils.translation import gettext\nfrom django.utils.translation import gettext_lazy as _\n\nADDITION = 1\nCHANGE = 2\nDELETION = 3\n\nACTION_FLAG_CHOICES = [\n    (ADDITION, _(\"Addition\")),\n    (CHANGE, _(\"Change\")),\n    (DELETION, _(\"Deletion\")),\n]\n\n\nclass LogEntryManager(models.Manager):\n    use_in_migrations = True\n\n    def log_actions(\n        self, user_id, queryset, action_flag, change_message=\"\", *, single_object=False\n    ):\n        if isinstance(change_message, list):\n            change_message = json.dumps(change_message)\n\n        log_entry_list = [\n            self.model(\n                user_id=user_id,\n                content_type_id=ContentType.objects.get_for_model(\n                    obj, for_concrete_model=False\n                ).id,\n                object_id=obj.pk,\n                object_repr=str(obj)[:200],\n                action_flag=action_flag,\n                change_message=change_message,\n            )\n            for obj in queryset\n        ]\n\n        if len(log_entry_list) == 1:\n            instance = log_entry_list[0]\n            instance.save()\n            if single_object:\n                return instance\n            return [instance]\n\n        return self.model.objects.bulk_create(log_entry_list)\n\n\nclass LogEntry(models.Model):\n    action_time = models.DateTimeField(\n        _(\"action time\"),\n        default=timezone.now,\n        editable=False,\n    )\n    user = models.ForeignKey(\n        settings.AUTH_USER_MODEL,\n        models.CASCADE,\n        verbose_name=_(\"user\"),\n    )\n    content_type = models.ForeignKey(\n        ContentType,\n        models.SET_NULL,\n        verbose_name=_(\"content type\"),\n        blank=True,\n        null=True,\n    )\n    object_id = models.TextField(_(\"object id\"), blank=True, null=True)\n    # Translators: 'repr' means representation\n    # (https://docs.python.org/library/functions.html#repr)\n    object_repr = models.CharField(_(\"object repr\"), max_length=200)\n    action_flag = models.PositiveSmallIntegerField(\n        _(\"action flag\"), choices=ACTION_FLAG_CHOICES\n    )\n    # change_message is either a string or a JSON structure\n    change_message = models.TextField(_(\"change message\"), blank=True)\n\n    objects = LogEntryManager()\n\n    class Meta:\n        verbose_name = _(\"log entry\")\n        verbose_name_plural = _(\"log entries\")\n        db_table = \"django_admin_log\"\n        ordering = [\"-action_time\"]\n\n    def __repr__(self):\n        return str(self.action_time)\n\n    def __str__(self):\n        if self.is_addition():\n            return gettext(\"Added “%(object)s”.\") % {\"object\": self.object_repr}\n        elif self.is_change():\n            return gettext(\"Changed “%(object)s” — %(changes)s\") % {\n                \"object\": self.object_repr,\n                \"changes\": self.get_change_message(),\n            }\n        elif self.is_deletion():\n            return gettext(\"Deleted “%(object)s.”\") % {\"object\": self.object_repr}\n\n        return gettext(\"LogEntry Object\")\n\n    def is_addition(self):\n        return self.action_flag == ADDITION\n\n    def is_change(self):\n        return self.action_flag == CHANGE\n\n    def is_deletion(self):\n        return self.action_flag == DELETION\n\n    def get_change_message(self):\n        \"\"\"\n        If self.change_message is a JSON structure, interpret it as a change\n        string, properly translated.\n        \"\"\"\n        if self.change_message and self.change_message[0] == \"[\":\n            try:\n                change_message = json.loads(self.change_message)\n            except json.JSONDecodeError:\n                return self.change_message\n            messages = []\n            for sub_message in change_message:\n                if \"added\" in sub_message:\n                    if sub_message[\"added\"]:\n                        sub_message[\"added\"][\"name\"] = gettext(\n                            sub_message[\"added\"][\"name\"]\n                        )\n                        messages.append(\n                            gettext(\"Added {name} “{object}”.\").format(\n                                **sub_message[\"added\"]\n                            )\n                        )\n                    else:\n                        messages.append(gettext(\"Added.\"))\n\n                elif \"changed\" in sub_message:\n                    sub_message[\"changed\"][\"fields\"] = get_text_list(\n                        [\n                            gettext(field_name)\n                            for field_name in sub_message[\"changed\"][\"fields\"]\n                        ],\n                        gettext(\"and\"),\n                    )\n                    if \"name\" in sub_message[\"changed\"]:\n                        sub_message[\"changed\"][\"name\"] = gettext(\n                            sub_message[\"changed\"][\"name\"]\n                        )\n                        messages.append(\n                            gettext(\"Changed {fields} for {name} “{object}”.\").format(\n                                **sub_message[\"changed\"]\n                            )\n                        )\n                    else:\n                        messages.append(\n                            gettext(\"Changed {fields}.\").format(\n                                **sub_message[\"changed\"]\n                            )\n                        )\n\n                elif \"deleted\" in sub_message:\n                    sub_message[\"deleted\"][\"name\"] = gettext(\n                        sub_message[\"deleted\"][\"name\"]\n                    )\n                    messages.append(\n                        gettext(\"Deleted {name} “{object}”.\").format(\n                            **sub_message[\"deleted\"]\n                        )\n                    )\n\n            change_message = \" \".join(msg[0].upper() + msg[1:] for msg in messages)\n            return change_message or gettext(\"No fields changed.\")\n        else:\n            return self.change_message\n\n    def get_edited_object(self):\n        \"\"\"Return the edited object represented by this log entry.\"\"\"\n        return self.content_type.get_object_for_this_type(pk=self.object_id)\n\n    def get_admin_url(self):\n        \"\"\"\n        Return the admin URL to edit the object represented by this log entry.\n        \"\"\"\n        if self.content_type and self.object_id:\n            url_name = \"admin:%s_%s_change\" % (\n                self.content_type.app_label,\n                self.content_type.model,\n            )\n            try:\n                return reverse(url_name, args=(quote(self.object_id),))\n            except NoReverseMatch:\n                pass\n        return None\n", "metadata": {"source": "github.com/django/django", "file_path": "django/contrib/admin/models.py", "language": "python", "complexity": "complex", "star_count": 84349, "quality_score": 1.0, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": true, "has_documentation": true, "has_type_hints": true, "has_testing": false, "has_logging": true, "function_count": 9, "class_count": 3, "line_count": 200}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "database_integration", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create database model with ORM relationships and queries\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "#!/usr/bin/env python3\n\nimport glob\nimport itertools\nimport os\nfrom subprocess import Popen\n\n\ndef run_command(command: list[str], log_file: str):\n    with open(log_file, \"wb\") as f:\n        return Popen(command, stdout=f, stderr=f)\n\n\ndef get_files(input_files: list[str]) -> list[str]:\n    \"\"\"\n    this function expands the globs specified in the input file then\n    filters for paths that are actually files\n    \"\"\"\n    glob_expanded_files = [glob.glob(file, recursive=True) for file in input_files]\n    flattened_glob_matches = list(itertools.chain.from_iterable(glob_expanded_files))\n    filtered_glob_matches = [file for file in flattened_glob_matches if os.path.isfile(file)]\n    return filtered_glob_matches\n\n\ndef main():\n    \"\"\"\n    First we get the arguments passed to the upload artifacts action via the env vars,\n    then we build the command for uploading coverage, run it in a thread, and build the\n    command for uploading test results and run it in a thread. We wait for both commands\n    to finish running then print their logs sequentially.\n\n    When we run the commands we're piping their stdout and stderr to a file so we can print the\n    contents of the files sequentially. We don't want the output of each command to be interleaved.\n\n    --plugin noop is passed to both commands, we don't need the CLI plugins because we're already\n    generating the coverage files ourselves.\n    \"\"\"\n    input_token = os.getenv(\"INPUT_TOKEN\")\n    input_commit_sha = os.getenv(\"INPUT_COMMIT_SHA\")\n    input_type = os.getenv(\"INPUT_TYPE\")\n    input_files = os.getenv(\"INPUT_FILES\", \"\").split(\",\")\n    input_test_result_files = os.getenv(\"INPUT_TEST_RESULT_FILES\", \"\").split(\",\")\n\n    codecov_base_cmd = [\"./codecov\", \"--verbose\"]\n\n    upload_flags = [\n        \"-t\",\n        input_token,\n        \"--plugin\",\n        \"noop\",\n        \"--flag\",\n        input_type,\n    ]\n\n    if input_commit_sha:\n        upload_flags += [\"--commit-sha\", input_commit_sha]\n\n    upload_coverage_cmd = [*codecov_base_cmd, \"upload-process\", *upload_flags]\n\n    coverage_files = get_files(input_files)\n    for file in coverage_files:\n        upload_coverage_cmd += [\"--file\", file]\n\n    upload_coverage_log_file = \"coverage-upload.log\"\n\n    test_result_files = get_files(input_test_result_files)\n\n    upload_test_results_cmd = [\n        *codecov_base_cmd,\n        \"do-upload\",\n        \"--report-type\",\n        \"test_results\",\n        *upload_flags,\n    ]\n    for file in test_result_files:\n        upload_test_results_cmd += [\"--file\", file]\n\n    upload_test_results_log_file = \"upload-test-results.log\"\n\n    # so that the logs are not interleaved when printed\n    jobs = [\n        run_command(upload_test_results_cmd, upload_test_results_log_file),\n        run_command(upload_coverage_cmd, upload_coverage_log_file),\n    ]\n    tail_args = (\"tail\", \"-f\", \"--sleep-interval\", \"3\")\n    for job in jobs:\n        tail_args += (\"--pid\", str(job.pid))\n    tail_args += (\n        upload_coverage_log_file,\n        upload_test_results_log_file,\n    )\n\n    # wait, while showing un-interleaved logs\n    jobs.append(Popen(tail_args))\n\n    return_codes = []\n\n    for job in jobs:\n        job.wait()\n\n    if any(return_codes):\n        raise Exception(\"Error uploading to codecov\")\n\n\nif __name__ == \"__main__\":\n    main()\n", "metadata": {"source": "github.com/getsentry/sentry", "file_path": ".github/actions/artifacts/do_upload.py", "language": "python", "complexity": "medium", "star_count": 41527, "quality_score": 0.9500000000000001, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": true, "has_type_hints": true, "has_testing": true, "has_logging": true, "function_count": 3, "class_count": 0, "line_count": 107}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "database_integration", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create database model with ORM relationships and queries\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"\nGenerate a mapping from file extensions to language for languages that are part of platforms supported by Sentry.\n\"\"\"\n\nimport requests\nimport yaml\n\nresponse = requests.get(\n    \"https://raw.githubusercontent.com/github-linguist/linguist/master/lib/linguist/languages.yml\"\n)\nlanguages_dict = yaml.safe_load(response.content)\n\n# referenced from platforms.tsx and _platforms.json\n# node.js contains JavaScript, Python, C, C++, CoffeeScript\n# .NET contains C#, F#, visual basic\n# apple: swift, objective-c\n# android: java, kotlin, dart\n# flutter: dart\n\n# pulled from the sentry_projectplatform table: groovy, haskell\nlanguages = [\n    \"go\",\n    \"javascript\",\n    \"typescript\",\n    \"java\",\n    \"python\",\n    \"php\",\n    \"c\",\n    \"c++\",\n    \"coffeescript\",\n    \"c#\",\n    \"f#\",\n    \"visual basic\",\n    \"swift\",\n    \"objective-c\",\n    \"objective-c++\",\n    \"ruby\",\n    \"kotlin\",\n    \"dart\",\n    \"elixir\",\n    \"rust\",\n    \"scala\",\n    \"perl\",\n    \"groovy\",\n    \"haskell\",\n    # community supported sdks\n    \"clojure\",\n    \"coldfusion\",\n    \"crystal\",\n    \"lua\",\n    \"ocaml\",\n    \"hcl\",\n    \"powershell\",\n]\n\nEXTENSION_LANGUAGE_MAP: dict[str, str] = {}\n\nyaml_languages = languages_dict.keys()\n\n\ndef add_lang_to_map(language, map):\n    v = languages_dict[language]\n\n    if v[\"type\"] != \"programming\":\n        return map\n    if \"extensions\" not in v:\n        return map\n\n    extensions = v[\"extensions\"]\n    for ext in extensions:\n        if ext[1:].lower() in extensions:\n            raise Exception\n        map[ext[1:].lower()] = language.lower()\n\n    return map\n\n\nfor yaml_lang in yaml_languages:\n    lowercase_yaml_lang = yaml_lang.lower()\n    if lowercase_yaml_lang in languages:\n        EXTENSION_LANGUAGE_MAP = add_lang_to_map(yaml_lang, EXTENSION_LANGUAGE_MAP)\n    elif \"group\" in languages_dict[yaml_lang]:\n        if languages_dict[yaml_lang][\"group\"].lower() in languages:\n            EXTENSION_LANGUAGE_MAP = add_lang_to_map(yaml_lang, EXTENSION_LANGUAGE_MAP)\n    else:\n        # check if substring exists\n        for lang in languages:\n            if lang in lowercase_yaml_lang and len(lang) > 2:\n                EXTENSION_LANGUAGE_MAP = add_lang_to_map(yaml_lang, EXTENSION_LANGUAGE_MAP)\n", "metadata": {"source": "github.com/getsentry/sentry", "file_path": "bin/extension_language_map.py", "language": "python", "complexity": "medium", "star_count": 41527, "quality_score": 0.92, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": true, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 1, "class_count": 0, "line_count": 90}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "database_integration", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create database model with ORM relationships and queries\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "#!/usr/bin/env python3\nfrom __future__ import annotations\n\nimport os\nimport sys\nfrom os.path import dirname, join, realpath\n\nimport yaml\n\nPREFIX = \"Product Area: \"\nLABELS_YAML = \".github/labels.yml\"\n\n\ndef main():\n    # Run from project root.\n    os.chdir(realpath(join(dirname(sys.argv[0]), \"..\")))\n\n    product_owners = yaml.safe_load(open(sys.argv[1]))\n    labels = open(LABELS_YAML)\n    areas = [\n        # For `sentry` repo we don't want SDK or Docs areas, that stuff should\n        # be transferred to other repos.\n        x\n        for x in product_owners[\"by_area\"]\n        if not x.startswith(\"SDK\") and not x == \"Docs\"\n    ]\n\n    fastforward = False\n    head: list[str] = []\n    area_lines = [\"- name: 'Product Area: Unknown'\\n\", \"  color: '8D5494'\\n\"]\n    tail: list[str] = []\n    current = head\n\n    # Best to look the other way, Buck. This is just waaaay easier than trying to\n    # use ruamel.yaml to preserve comments and other formatting. What this does is\n    # loop through line by line, collecting everything before the Product Area\n    # labels in `head` and everything after in `tail`. It \"fast-forwards\" past the\n    # existing Product Area labels. Then we generate output for the new Product\n    # Area labels, then emit the three parts (head, area_lines, tail) in order.\n\n    for line in labels:\n        if line == \"\\n\":\n            fastforward = False\n        elif fastforward:\n            continue\n        elif line.startswith(\"- name: 'Product Area: \"):\n            fastforward = True\n            current = tail\n            continue\n        current.append(line)\n\n    for area in areas:\n        # These are specifically placed at the front and end of the list of product areas.\n        # They will always exist, so ignore what is coming in from security-as-code as a workaround.\n        if area != \"Other\" and area != \"Unknown\":\n            if \"'\" in area:\n                area_lines.append(f'- name: \"Product Area: {area}\"\\n')\n            else:\n                area_lines.append(f\"- name: 'Product Area: {area}'\\n\")\n            area_lines.append(\"  color: '8D5494'\\n\")\n\n    area_lines += [\"- name: 'Product Area: Other'\\n\", \"  color: '8D5494'\\n\"]\n\n    with open(\".github/labels.yml\", \"w+\") as fp:\n        fp.writelines(head)\n        fp.writelines(area_lines)\n        fp.writelines(tail)\n\n    # Now for issue templates. Same game. Hang in there.\n\n    BEGINNING = \"        # begin product areas - autogenerated by bin/react-to-product-owners-yml-changes.py\\n\"\n    END = \"        # end product areas\\n\"\n\n    for template in (\"bug\", \"feature\", \"support\"):\n        filepath = f\".github/ISSUE_TEMPLATE/{template}.yml\"\n        head = []\n        area_lines = [BEGINNING, \"        - 'Unknown'\\n\"]\n        tail = []\n        current = head\n        for line in open(filepath):\n            if line == END:\n                fastforward = False\n            elif line == BEGINNING:\n                fastforward = True\n                current = tail\n            elif not fastforward:\n                current.append(line)\n\n        for area in areas:\n            if area != \"Other\" and area != \"Unknown\":\n                if \"'\" in area:\n                    area_lines.append(f'        - \"{area}\"\\n')\n                else:\n                    area_lines.append(f\"        - '{area}'\\n\")\n\n        area_lines += [\"        - 'Other'\\n\", END]\n\n        with open(filepath, \"w+\") as fp:\n            fp.writelines(head)\n            fp.writelines(area_lines)\n            fp.writelines(tail)\n\n    return 0\n\n\nif __name__ == \"__main__\":\n    raise SystemExit(main())\n", "metadata": {"source": "github.com/getsentry/sentry", "file_path": "bin/react-to-product-owners-yml-changes.py", "language": "python", "complexity": "medium", "star_count": 41527, "quality_score": 0.89, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 1, "class_count": 0, "line_count": 108}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "database_integration", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create database model with ORM relationships and queries\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# pylint: skip-file\n# flake8: noqa\n\nimport base64\nimport datetime\nimport functools\nimport itertools\nimport json\nimport pprint\nimport random\nimport string\nimport struct\n\nimport click\nfrom arroyo.backends.kafka import KafkaPayload, KafkaProducer\nfrom arroyo.types import Topic\n\nfrom sentry.sentry_metrics.use_case_id_registry import UseCaseID\n\n\ndef make_counter_payload(use_case, org_id, rand_str, sampling_weight=None):\n    return {\n        \"name\": f\"c:{use_case}/{use_case}@none\",\n        \"tags\": {\n            \"environment\": \"production\",\n            \"session.status\": \"init\",\n            f\"metric_e2e_{use_case}_counter_k_{rand_str}\": f\"metric_e2e_{use_case}_counter_v_{rand_str}\",\n        },\n        \"timestamp\": int(datetime.datetime.now(tz=datetime.UTC).timestamp()),\n        \"type\": \"c\",\n        \"value\": 1,\n        \"org_id\": org_id,\n        \"retention_days\": 90,\n        \"project_id\": 3,\n        **({\"sampling_weight\": sampling_weight} if sampling_weight else {}),\n    }\n\n\ndef make_dist_payload(use_case, org_id, rand_str, value_len, b64_encode, sampling_weight=None):\n    nums = [random.random() for _ in range(value_len)]\n    return {\n        \"name\": f\"d:{use_case}/duration@second\",\n        \"tags\": {\n            \"environment\": \"production\",\n            \"session.status\": \"healthy\",\n            f\"metric_e2e_{use_case}_dist_k_{rand_str}\": f\"metric_e2e_{use_case}_dist_v_{rand_str}\",\n        },\n        \"timestamp\": int(datetime.datetime.now(tz=datetime.UTC).timestamp()),\n        \"type\": \"d\",\n        \"value\": (\n            {\n                \"format\": \"base64\",\n                \"data\": base64.b64encode(struct.pack(f\"<{len(nums)}d\", *nums))\n                .replace(b\"=\", b\"\")\n                .decode(\"ascii\"),\n            }\n            if b64_encode\n            else {\n                \"format\": \"zstd\",\n                \"data\": \"KLUv/QBYrQAAcAAA8D8AQAAAAAAAAAhAAgBgRgCw\",\n            }\n        ),\n        \"org_id\": org_id,\n        \"retention_days\": 90,\n        \"project_id\": 3,\n        **({\"sampling_weight\": sampling_weight} if sampling_weight else {}),\n    }\n\n\ndef make_set_payload(use_case, org_id, rand_str, value_len, b64_encode, sampling_weight=None):\n    INT_WIDTH = 4\n    nums = [random.randint(0, 2048) for _ in range(value_len)]\n    return {\n        \"name\": f\"s:{use_case}/error@none\",\n        \"tags\": {\n            \"environment\": \"production\",\n            \"session.status\": \"errored\",\n            f\"metric_e2e_{use_case}_set_k_{rand_str}\": f\"metric_e2e_{use_case}_set_v_{rand_str}\",\n        },\n        \"timestamp\": int(datetime.datetime.now(tz=datetime.UTC).timestamp()),\n        \"type\": \"s\",\n        \"value\": (\n            {\n                \"format\": \"base64\",\n                \"data\": base64.b64encode(\n                    b\"\".join([num.to_bytes(INT_WIDTH, byteorder=\"little\") for num in nums])\n                )\n                .replace(b\"=\", b\"\")\n                .decode(\"ascii\"),\n            }\n            if b64_encode\n            else {\n                \"format\": \"array\",\n                \"data\": nums,\n            }\n        ),\n        \"org_id\": org_id,\n        \"retention_days\": 90,\n        \"project_id\": 3,\n        **({\"sampling_weight\": sampling_weight} if sampling_weight else {}),\n    }\n\n\ndef make_gauge_payload(use_case, org_id, rand_str, sampling_weight):\n    return {\n        \"name\": f\"s:{use_case}/error@none\",\n        \"tags\": {\n            \"environment\": \"production\",\n            \"session.status\": \"errored\",\n            f\"metric_e2e_{use_case}_gauge_k_{rand_str}\": f\"metric_e2e_{use_case}_gauge_v_{rand_str}\",\n        },\n        \"timestamp\": int(datetime.datetime.now(tz=datetime.UTC).timestamp()),\n        \"type\": \"g\",\n        \"value\": {\n            \"min\": 1,\n            \"max\": 1,\n            \"sum\": 1,\n            \"count\": 1,\n            \"last\": 1,\n        },\n        \"org_id\": org_id,\n        \"retention_days\": 90,\n        \"project_id\": 3,\n        **({\"sampling_weight\": sampling_weight} if sampling_weight else {}),\n    }\n\n\ndef make_psql(rand_str, is_generic):\n    return f\"\"\"\n        SELECT string,\n        organization_id,\n        {\"use_case_id,\" if is_generic else \"\"}\n        date_added,\n        last_seen\n        FROM {\"sentry_perfstringindexer\" if is_generic else \"sentry_stringindexer\"}\n        WHERE string ~ 'metric_e2e_.*{rand_str}';\n    \"\"\"\n\n\ndef make_csql(rand_str, is_generic):\n    return \"UNION ALL\".join(\n        [\n            f\"\"\"\n    SELECT use_case_id,\n        org_id,\n        project_id,\n        metric_id,\n        timestamp,\n        tags.key,\n        tags.raw_value\n    FROM {table_name}\n    WHERE arrayExists(v -> match(v, 'metric_e2e_.*{rand_str}'), tags.raw_value)\n    \"\"\"\n            for table_name in (\n                [\n                    \"generic_metric_counters_raw_local\",\n                    \"generic_metric_distributions_raw_local\",\n                    \"generic_metric_sets_raw_local\",\n                    \"generic_metric_gauges_raw_local\",\n                ]\n                if is_generic\n                else [\n                    \"metrics_counters_v2_local\",\n                    \"metrics_distributions_v2_local\",\n                    \"metrics_sets_v2_local\",\n                ]\n            )\n        ]\n    )\n\n\ndef produce_msgs(messages, is_generic, host, dryrun, quiet):\n    conf = {\"bootstrap.servers\": host}\n\n    producer = KafkaProducer(conf)\n    for i, message in enumerate(messages):\n        print(f\"{i + 1} / {len(messages)}\")\n        if not quiet:\n            pprint.pprint(message)\n        if not dryrun:\n            producer.produce(\n                Topic(name=(\"ingest-performance-metrics\" if is_generic else \"ingest-metrics\")),\n                KafkaPayload(key=None, value=json.dumps(message).encode(\"utf-8\"), headers=[]),\n            )\n            print(\"Done\")\n        print()\n\n    producer.close()\n\n\n@click.command()\n@click.option(\n    \"--metric-types\", default=\"cdsg\", show_default=True, help=\"The types of metrics to send\"\n)\n@click.option(\n    \"--use-cases\",\n    multiple=True,\n    default=[\n        use_case_id.value for use_case_id in UseCaseID if use_case_id is not UseCaseID.SESSIONS\n    ],\n    show_default=True,\n    help=\"The use case IDs.\",\n)\n@click.option(\"--rand-str\", default=None, help=\"The random string prefix for each key value pairs.\")\n@click.option(\n    \"--host\", default=\"127.0.0.1:9092\", show_default=True, help=\"The host and port for kafka.\"\n)\n@click.option(\n    \"--dryrun\",\n    \"-d\",\n    is_flag=True,\n    default=False,\n    show_default=True,\n    help=\"Generate the messages without sending them.\",\n)\n@click.option(\n    \"--quiet\",\n    \"-q\",\n    is_flag=True,\n    default=False,\n    show_default=True,\n    help=\"Disable printing the messages.\",\n)\n@click.option(\n    \"--start-org-id\",\n    default=1,\n    show_default=True,\n    help=\"Specify which org id(s) to start from.\",\n)\n@click.option(\n    \"--end-org-id\",\n    default=1,\n    show_default=True,\n    help=\"Specify which org id(s) to end with.\",\n)\n@click.option(\n    \"--num-bad-msg\",\n    default=0,\n    show_default=True,\n    help=\"Number of additional badly formatted metric messages to send.\",\n)\n@click.option(\n    \"--value-len\",\n    default=8,\n    show_default=True,\n    help=\"Number of elements for metrics (sets and distributions).\",\n)\n@click.option(\n    \"--b64-encode\",\n    default=True,\n    show_default=True,\n    help=\"Encode sets and distribution metrics values in base64\",\n)\n@click.option(\n    \"--sampling-weight\",\n    type=int,\n    default=None,\n    show_default=True,\n    help=\"Sampling weight for the metrics\",\n)\ndef main(\n    metric_types,\n    use_cases,\n    rand_str,\n    host,\n    dryrun,\n    quiet,\n    start_org_id,\n    end_org_id,\n    num_bad_msg,\n    value_len,\n    b64_encode,\n    sampling_weight,\n):\n    if UseCaseID.SESSIONS.value in use_cases and len(use_cases) > 1:\n        click.secho(\n            \"ERROR: UseCaseID.SESSIONS is in use_cases and there are more than 1 use cases\",\n            blink=True,\n            bold=True,\n        )\n        exit(1)\n\n    is_generic = UseCaseID.SESSIONS.value not in use_cases\n    metric_types = \"\".join(set(metric_types))\n    rand_str = rand_str or \"\".join(random.choices(string.ascii_uppercase + string.digits, k=8))\n    payload_generators = {\n        \"c\": functools.partial(\n            make_counter_payload, rand_str=rand_str, sampling_weight=sampling_weight\n        ),\n        \"d\": functools.partial(\n            make_dist_payload,\n            rand_str=rand_str,\n            value_len=value_len,\n            b64_encode=b64_encode,\n            sampling_weight=sampling_weight,\n        ),\n        \"s\": functools.partial(\n            make_set_payload,\n            rand_str=rand_str,\n            value_len=value_len,\n            b64_encode=b64_encode,\n            sampling_weight=sampling_weight,\n        ),\n        \"g\": functools.partial(\n            make_gauge_payload, rand_str=rand_str, sampling_weight=sampling_weight\n        ),\n    }\n\n    messages = list(\n        itertools.chain.from_iterable(\n            (\n                payload_generators[metric_type](use_case=use_case, org_id=org_id)\n                for metric_type in metric_types\n            )\n            for use_case in use_cases\n            for org_id in range(start_org_id, end_org_id + 1)\n        )\n    )\n    messages.extend([{\"BAD_VALUE\": rand_str, \"idx\": i} for i in range(num_bad_msg)])\n\n    random.shuffle(messages)\n    produce_msgs(messages, is_generic, host, dryrun, quiet)\n\n    strs_per_use_case = 3\n    print(\n        f\"Use the following SQL to verify postgres, \"\n        f\"there should be {strs_per_use_case} strings for each use cases, \"\n        f\"{strs_per_use_case * len(use_cases) * (end_org_id - start_org_id + 1)} in total.\"\n    )\n    print(make_psql(rand_str, is_generic))\n\n    if is_generic:\n        print(\n            f\"Use the following SQL to verify clickhouse, \"\n            f\"there should be {len(metric_types)} metrics for each use cases, \"\n            f\"{len(metric_types) * len(use_cases) * (end_org_id - start_org_id + 1)} in total.\"\n        )\n        print(make_csql(rand_str, is_generic))\n\n\nif __name__ == \"__main__\":\n    main()\n", "metadata": {"source": "github.com/getsentry/sentry", "file_path": "bin/send_metrics.py", "language": "python", "complexity": "complex", "star_count": 41527, "quality_score": 0.92, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": true, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 8, "class_count": 0, "line_count": 343}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "database_integration", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create database model with ORM relationships and queries\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from __future__ import annotations\n\nimport importlib.metadata\nimport json\nimport os\nimport shlex\nimport shutil\nimport subprocess\n\nfrom devenv import constants\nfrom devenv.lib import colima, config, fs, limactl, proc, venv\n\n\n# TODO: need to replace this with a nicer process executor in devenv.lib\ndef run_procs(\n    repo: str,\n    reporoot: str,\n    venv_path: str,\n    _procs: tuple[tuple[str, tuple[str, ...], dict[str, str]], ...],\n    verbose: bool = False,\n) -> bool:\n    procs: list[tuple[str, tuple[str, ...], subprocess.Popen[bytes]]] = []\n\n    stdout = subprocess.PIPE if not verbose else None\n    stderr = subprocess.STDOUT if not verbose else None\n\n    for name, cmd, extra_env in _procs:\n        print(f\"⏳ {name}\")\n        if constants.DEBUG:\n            proc.xtrace(cmd)\n        env = {\n            **constants.user_environ,\n            **proc.base_env,\n            \"VIRTUAL_ENV\": venv_path,\n            \"PATH\": f\"{venv_path}/bin:{reporoot}/.devenv/bin:{proc.base_path}\",\n        }\n        if extra_env:\n            env = {**env, **extra_env}\n        procs.append(\n            (\n                name,\n                cmd,\n                subprocess.Popen(\n                    cmd,\n                    stdout=stdout,\n                    stderr=stderr,\n                    env=env,\n                    cwd=reporoot,\n                ),\n            )\n        )\n\n    all_good = True\n    for name, final_cmd, p in procs:\n        out, _ = p.communicate()\n        if p.returncode != 0:\n            all_good = False\n            out_str = f\"Output:\\n{out.decode()}\" if not verbose else \"\"\n            print(\n                f\"\"\"\n❌ {name}\n\nfailed command (code {p.returncode}):\n    {shlex.join(final_cmd)}\n\n{out_str}\n\n\"\"\"\n            )\n        else:\n            print(f\"✅ {name}\")\n\n    return all_good\n\n\n# Temporary, see https://github.com/getsentry/sentry/pull/78881\ndef check_minimum_version(minimum_version: str) -> bool:\n    version = importlib.metadata.version(\"sentry-devenv\")\n\n    parsed_version = tuple(map(int, version.split(\".\")))\n    parsed_minimum_version = tuple(map(int, minimum_version.split(\".\")))\n\n    return parsed_version >= parsed_minimum_version\n\n\ndef installed_pnpm(version: str, binroot: str) -> bool:\n    if shutil.which(\"pnpm\", path=binroot) != f\"{binroot}/pnpm\" or not os.path.exists(\n        f\"{binroot}/node-env/bin/pnpm\"\n    ):\n        return False\n\n    stdout = proc.run((f\"{binroot}/pnpm\", \"--version\"), stdout=True)\n    installed_version = stdout.strip()\n    return version == installed_version\n\n\ndef install_pnpm(version: str, reporoot: str) -> None:\n    binroot = fs.ensure_binroot(reporoot)\n\n    if installed_pnpm(version, binroot):\n        return\n\n    print(f\"installing pnpm {version}...\")\n\n    # {binroot}/npm is a devenv-managed shim, so\n    # this install -g ends up putting pnpm into\n    # .devenv/bin/node-env/bin/pnpm which is pointed\n    # to by the {binroot}/pnpm shim\n    proc.run((f\"{binroot}/npm\", \"install\", \"-g\", f\"pnpm@{version}\"), stdout=True)\n\n    fs.write_script(\n        f\"{binroot}/pnpm\",\n        \"\"\"#!/bin/sh\nexport PATH={binroot}/node-env/bin:\"${{PATH}}\"\nexec {binroot}/node-env/bin/pnpm \"$@\"\n\"\"\",\n        shell_escape={\"binroot\": binroot},\n    )\n\n\ndef main(context: dict[str, str]) -> int:\n    minimum_version = \"1.14.2\"\n    if not check_minimum_version(minimum_version):\n        raise SystemExit(\n            f\"\"\"\nHi! To reduce potential breakage we've defined a minimum\ndevenv version ({minimum_version}) to run sync.\n\nPlease run the following to update your global devenv:\n\ndevenv update\n\nThen, use it to run sync this one time.\n\n{constants.root}/bin/devenv sync\n\"\"\"\n        )\n\n    repo = context[\"repo\"]\n    reporoot = context[\"reporoot\"]\n    repo_config = config.get_config(f\"{reporoot}/devenv/config.ini\")\n\n    # TODO: context[\"verbose\"]\n    verbose = os.environ.get(\"SENTRY_DEVENV_VERBOSE\") is not None\n\n    FRONTEND_ONLY = os.environ.get(\"SENTRY_DEVENV_FRONTEND_ONLY\") is not None\n    SKIP_FRONTEND = os.environ.get(\"SENTRY_DEVENV_SKIP_FRONTEND\") is not None\n\n    USE_OLD_DEVSERVICES = os.environ.get(\"USE_OLD_DEVSERVICES\") == \"1\"\n\n    if constants.DARWIN and os.path.exists(f\"{constants.root}/bin/colima\"):\n        binroot = f\"{reporoot}/.devenv/bin\"\n        colima.uninstall(binroot)\n        limactl.uninstall(binroot)\n\n    from devenv.lib import node\n\n    node.install(\n        repo_config[\"node\"][\"version\"],\n        repo_config[\"node\"][constants.SYSTEM_MACHINE],\n        repo_config[\"node\"][f\"{constants.SYSTEM_MACHINE}_sha256\"],\n        reporoot,\n    )\n\n    with open(f\"{reporoot}/package.json\") as f:\n        package_json = json.load(f)\n        pnpm = package_json[\"packageManager\"]\n        pnpm_version = pnpm.split(\"@\")[-1]\n\n    # TODO: move pnpm install into devenv\n    install_pnpm(pnpm_version, reporoot)\n\n    # no more imports from devenv past this point! if the venv is recreated\n    # then we won't have access to devenv libs until it gets reinstalled\n\n    # venv's still needed for frontend because repo-local devenv and pre-commit\n    # exist inside it\n    venv_dir, python_version, requirements, editable_paths, bins = venv.get(reporoot, repo)\n    url, sha256 = config.get_python(reporoot, python_version)\n    print(f\"ensuring {repo} venv at {venv_dir}...\")\n    venv.ensure(venv_dir, python_version, url, sha256)\n\n    if not run_procs(\n        repo,\n        reporoot,\n        venv_dir,\n        (\n            # TODO: devenv should provide a job runner (jobs run in parallel, tasks run sequentially)\n            (\n                \"python dependencies (1/3)\",\n                (\n                    # upgrading pip first\n                    \"pip\",\n                    \"install\",\n                    \"--constraint\",\n                    \"requirements-dev-frozen.txt\",\n                    \"pip\",\n                ),\n                {},\n            ),\n        ),\n        verbose,\n    ):\n        return 1\n\n    if not SKIP_FRONTEND and not run_procs(\n        repo,\n        reporoot,\n        venv_dir,\n        (\n            (\n                # Spreading out the network load by installing js,\n                # then py in the next batch.\n                \"javascript dependencies (1/1)\",\n                (\n                    \"pnpm\",\n                    \"install\",\n                    \"--frozen-lockfile\",\n                    \"--reporter=append-only\",\n                ),\n                {\n                    \"NODE_ENV\": \"development\",\n                    # this ensures interactive prompts are answered by\n                    # the defaults (usually yes), useful for recreating\n                    # node_modules if configuration or node version changes\n                    \"CI\": \"true\",\n                },\n            ),\n        ),\n        verbose,\n    ):\n        return 1\n\n    if not run_procs(\n        repo,\n        reporoot,\n        venv_dir,\n        (\n            # could opt out of syncing python if FRONTEND_ONLY but only if repo-local devenv\n            # and pre-commit were moved to inside devenv and not the sentry venv\n            (\n                \"python dependencies (2/3)\",\n                (\n                    \"pip\",\n                    \"install\",\n                    \"--constraint\",\n                    \"requirements-dev-frozen.txt\",\n                    \"-r\",\n                    \"requirements-dev-frozen.txt\",\n                ),\n                {},\n            ),\n        ),\n        verbose,\n    ):\n        return 1\n\n    if not run_procs(\n        repo,\n        reporoot,\n        venv_dir,\n        (\n            (\n                \"python dependencies (3/3)\",\n                (\"python3\", \"-m\", \"tools.fast_editable\", \"--path\", \".\"),\n                {},\n            ),\n            (\"pre-commit dependencies\", (\"pre-commit\", \"install\", \"--install-hooks\", \"-f\"), {}),\n        ),\n        verbose,\n    ):\n        return 1\n\n    fs.ensure_symlink(\"../../config/hooks/post-merge\", f\"{reporoot}/.git/hooks/post-merge\")\n\n    sentry_conf = os.environ.get(\"SENTRY_CONF\", f\"{constants.home}/.sentry\")\n\n    if not os.path.exists(f\"{sentry_conf}/config.yml\") or not os.path.exists(\n        f\"{sentry_conf}/sentry.conf.py\"\n    ):\n        proc.run((f\"{venv_dir}/bin/sentry\", \"init\", \"--dev\"))\n\n    # Frontend engineers don't necessarily always have devservices running and\n    # can configure to skip them to save on local resources\n    if FRONTEND_ONLY:\n        print(\"Skipping python migrations since SENTRY_DEVENV_FRONTEND_ONLY is set.\")\n        return 0\n\n    if USE_OLD_DEVSERVICES:\n        # Ensure new devservices is not being used, otherwise ports will conflict\n        proc.run(\n            (f\"{venv_dir}/bin/devservices\", \"down\"),\n            pathprepend=f\"{reporoot}/.devenv/bin\",\n            exit=True,\n        )\n        # TODO: check healthchecks for redis and postgres to short circuit this\n        proc.run(\n            (\n                f\"{venv_dir}/bin/{repo}\",\n                \"devservices\",\n                \"up\",\n                \"redis\",\n                \"postgres\",\n            ),\n            pathprepend=f\"{reporoot}/.devenv/bin\",\n            exit=True,\n        )\n    else:\n        # Ensure old sentry devservices is not being used, otherwise ports will conflict\n        proc.run(\n            (\n                f\"{venv_dir}/bin/{repo}\",\n                \"devservices\",\n                \"down\",\n            ),\n            pathprepend=f\"{reporoot}/.devenv/bin\",\n            exit=True,\n        )\n        proc.run(\n            (f\"{venv_dir}/bin/devservices\", \"up\", \"--mode\", \"migrations\"),\n            pathprepend=f\"{reporoot}/.devenv/bin\",\n            exit=True,\n        )\n\n    if not run_procs(\n        repo,\n        reporoot,\n        venv_dir,\n        (\n            (\n                \"python migrations\",\n                (\"make\", \"apply-migrations\"),\n                {},\n            ),\n        ),\n        verbose,\n    ):\n        return 1\n\n    postgres_container = (\n        \"sentry_postgres\" if os.environ.get(\"USE_OLD_DEVSERVICES\") == \"1\" else \"sentry-postgres-1\"\n    )\n\n    # faster prerequisite check than starting up sentry and running createuser idempotently\n    stdout = proc.run(\n        (\n            \"docker\",\n            \"exec\",\n            postgres_container,\n            \"psql\",\n            \"sentry\",\n            \"postgres\",\n            \"-t\",\n            \"-c\",\n            \"select exists (select from auth_user where email = 'admin@sentry.io')\",\n        ),\n        stdout=True,\n    )\n    if stdout != \"t\":\n        proc.run(\n            (\n                f\"{venv_dir}/bin/sentry\",\n                \"createuser\",\n                \"--superuser\",\n                \"--email\",\n                \"admin@sentry.io\",\n                \"--password\",\n                \"admin\",\n                \"--no-input\",\n            )\n        )\n\n    return 0\n", "metadata": {"source": "github.com/getsentry/sentry", "file_path": "devenv/sync.py", "language": "python", "complexity": "complex", "star_count": 41527, "quality_score": 0.9500000000000001, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": true, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 5, "class_count": 0, "line_count": 374}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "database_integration", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create database model with ORM relationships and queries\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import functools\nimport os\n\nimport orjson\nfrom django.conf import settings\nfrom openapi_core.contrib.django import DjangoOpenAPIRequest, DjangoOpenAPIResponse\nfrom openapi_core.spec import Spec\nfrom openapi_core.validation.response.validators import V30ResponseDataValidator\n\nfrom sentry.testutils.cases import APITestCase\nfrom sentry.testutils.helpers.datetime import before_now\nfrom sentry.testutils.skips import requires_snuba\n\n\n@requires_snuba\nclass APIDocsTestCase(APITestCase):\n    @functools.cached_property\n    def cached_schema(self):\n        path = os.path.join(os.path.dirname(__file__), \"../tests/apidocs/openapi-derefed.json\")\n        with open(path, \"rb\") as json_file:\n            data = orjson.loads(json_file.read())\n            data[\"servers\"][0][\"url\"] = settings.SENTRY_OPTIONS[\"system.url-prefix\"]\n            del data[\"components\"]\n\n            return Spec.from_dict(data)\n\n    def validate_schema(self, request, response):\n        assert 200 <= response.status_code < 300, response.status_code\n\n        if isinstance(response.data, list):\n            assert len(response.data) > 0, \"Cannot validate an empty list\"\n\n        response[\"Content-Type\"] = \"application/json\"\n        V30ResponseDataValidator(self.cached_schema).validate(\n            DjangoOpenAPIRequest(request), DjangoOpenAPIResponse(response)\n        )\n\n    def create_event(self, name, **kwargs):\n        # Somewhat sane default data.\n        data = {\n            \"event_id\": (name * 32)[:32],\n            \"fingerprint\": [\"1\"],\n            \"sdk\": {\"version\": \"5.17.0\", \"name\": \"sentry.javascript.browser\"},\n            \"timestamp\": before_now(seconds=1).isoformat(),\n            \"user\": {\"id\": self.user.id, \"email\": self.user.email},\n            \"release\": name,\n        }\n        data.update(kwargs)\n\n        return self.store_event(data=data, project_id=self.project.id)\n", "metadata": {"source": "github.com/getsentry/sentry", "file_path": "fixtures/apidocs_test_case.py", "language": "python", "complexity": "simple", "star_count": 41527, "quality_score": 0.98, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": true, "has_logging": false, "function_count": 3, "class_count": 1, "line_count": 51}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "database_integration", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate configuration for Create database model with ORM relationships and queries\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "EXAMPLE_PRIVATE_KEY = \"\"\"-----BEGIN RSA PRIVATE KEY-----\nMIICWwIBAAKBgQC1cd9t8sA03awggLiX2gjZxyvOVUPJksLly1E662tttTeR3Wm9\neo6onNeI8HRD+O4wubUp4h4Chc7DtLDmFEPhUZ8Qkwztiifm99Xo3s0nUq4Pygp5\nAU09KXTEPbzHLh1dnXLcxVLmGDE4drh0NWmYsd/Zp7XNIZq2TRQQ3NTdVQIDAQAB\nAoGAFwMyS0eWiR30TssEnn3Q0Y4pSCoYRuCOR4bZ7pcdMPTi72UdnCKHJWt/Cqc0\nl8piq1tiVsWO+NLvvnKUXRoE4cAyrGrpf1F0uP5zYW71SQALc9wwsjDzuj7BZEuK\nfg35JSceLHWE1WtzPDX5Xg20YPnMrA/xe/RwuPjuBH0wSqECQQDizzmKdKCq0ejy\n3OxEto5knqpSEgRcOk0HDsdgjwkwiZJOj5ECV2FKpNHuu2thGy/aDJyLlmUso8j0\nOpvLAzOvAkEAzMwAgGexTxKm8hy3ilvVn9EvhSKjaIakqY4ONK9LZ4zMiDHI0H6C\nFXlwWX7CJM0YVFMubj8SB8rnIuvFDEBMOwJABHtRyMGbNyTktH/XD1iIIcbc2LhQ\na74fLYeGOws4hEQDpxfBJsmxO3dcSppbedS+slFTepKjNymZW/IYh/9tMwJAEL5E\n9DqGBn7x4y1x2//yESTbC7lvPqZzY+FXS/tg4NBkEGZxkoolPHg3NTnlyXhzGsHK\nM/04DicKipJYA85l7QJAJ3u67qZXecM/oWTtJToBDuyKGHfdY1564+RbyDEjJJRb\nvz4O/8FQQ1sGjdEBMMrRBCHEG8o3/XDTrB97t45TeA==\n-----END RSA PRIVATE KEY-----\"\"\"\n\nCOMPARE_COMMITS_EXAMPLE = {\n    \"values\": [\n        {\n            \"id\": \"e18e4e72de0d824edfbe0d73efe34cbd0d01d301\",\n            \"displayId\": \"e18e4e72de0\",\n            \"author\": {\n                \"name\": \"SentryU\",\n                \"displayName\": \"Sentry User\",\n                \"emailAddress\": \"sentryuser@getsentry.com\",\n                \"type\": \"NORMAL\",\n            },\n            \"message\": \"README.md edited online with Bitbucket\",\n            \"authorTimestamp\": 1576763816000,\n        }\n    ]\n}\n\nCOMMIT_CHANGELIST_EXAMPLE = {\n    \"values\": [\n        {\n            \"path\": {\n                \"components\": [\"a.txt\"],\n                \"parent\": \"\",\n                \"name\": \"a.txt\",\n                \"extension\": \"txt\",\n                \"toString\": \"a.txt\",\n            },\n            \"executable\": False,\n            \"percentUnchanged\": -1,\n            \"type\": \"MODIFY\",\n            \"nodeType\": \"FILE\",\n            \"srcExecutable\": False,\n            \"properties\": {\"gitChangeType\": \"MODIFY\"},\n        },\n        {\n            \"path\": {\n                \"components\": [\"b.txt\"],\n                \"parent\": \"\",\n                \"name\": \"b.txt\",\n                \"extension\": \"txt\",\n                \"toString\": \"b.txt\",\n            },\n            \"executable\": False,\n            \"percentUnchanged\": -1,\n            \"type\": \"ADD\",\n            \"nodeType\": \"FILE\",\n            \"srcExecutable\": False,\n            \"properties\": {\"gitChangeType\": \"ADD\"},\n        },\n        {\n            \"path\": {\n                \"components\": [\"c.txt\"],\n                \"parent\": \"\",\n                \"name\": \"c.txt\",\n                \"extension\": \"txt\",\n                \"toString\": \"c.txt\",\n            },\n            \"executable\": False,\n            \"percentUnchanged\": -1,\n            \"type\": \"DELETE\",\n            \"nodeType\": \"FILE\",\n            \"srcExecutable\": False,\n            \"properties\": {\"gitChangeType\": \"DELETE\"},\n        },\n        {\n            \"path\": {\n                \"components\": [\"e.txt\"],\n                \"parent\": \"\",\n                \"name\": \"d.txt\",\n                \"extension\": \"txt\",\n                \"toString\": \"d.txt\",\n            },\n            \"srcPath\": {\n                \"components\": [\"d.txt\"],\n                \"parent\": \"\",\n                \"name\": \"e.txt\",\n                \"extension\": \"txt\",\n                \"toString\": \"e.txt\",\n            },\n            \"executable\": False,\n            \"percentUnchanged\": -1,\n            \"type\": \"MOVE\",\n            \"nodeType\": \"FILE\",\n            \"srcExecutable\": False,\n            \"properties\": {\"gitChangeType\": \"MOVE\"},\n        },\n    ]\n}\n\nREPO = {\n    \"slug\": \"helloworld\",\n    \"id\": 72,\n    \"name\": \"helloworld\",\n    \"scmId\": \"git\",\n    \"state\": \"AVAILABLE\",\n    \"statusMessage\": \"Available\",\n    \"forkable\": True,\n    \"project\": {\n        \"key\": \"laurynsentry\",\n        \"id\": 75,\n        \"name\": \"laurynsentry\",\n        \"description\": \"\",\n        \"public\": False,\n        \"type\": \"Normal\",\n        \"links\": {\"self\": {\"href\": \"https://bitbucket.example.org/projects/laurynsentry\"}},\n    },\n    \"public\": False,\n    \"links\": {\n        \"clone\": {\"href\": \"name\"},\n        \"self\": {\n            \"href\": \"https://bitbucket.example.org/projects/laurynsentry/repos/helloworld/browse\"\n        },\n    },\n}\n\nCOMPARE_COMMITS_WITH_PAGES_1_2_EXAMPLE = {\n    \"values\": [\n        {\n            \"id\": \"d0352305beb41afb3a4ea79e3a97bf6a97520339\",\n            \"displayId\": \"d0352305beb\",\n            \"author\": {\n                \"name\": \"SentryU\",\n                \"displayName\": \"Sentry User\",\n                \"emailAddress\": \"sentryuser@getsentry.com\",\n                \"type\": \"NORMAL\",\n            },\n            \"message\": \"Fist commit\",\n            \"authorTimestamp\": 1576763816000,\n        }\n    ],\n    \"size\": 1,\n    \"isLastPage\": False,\n    \"start\": 0,\n    \"limit\": 1,\n    \"nextPageStart\": 1,\n}\n\nCOMPARE_COMMITS_WITH_PAGES_2_2_EXAMPLE = {\n    \"values\": [\n        {\n            \"id\": \"042bc8434e0c178d8745c7d9f90bddab9c927887\",\n            \"displayId\": \"042bc8434e0\",\n            \"author\": {\n                \"name\": \"SentryU\",\n                \"displayName\": \"Sentry User\",\n                \"emailAddress\": \"sentryuser@getsentry.com\",\n                \"type\": \"NORMAL\",\n            },\n            \"message\": \"Second commit\",\n            \"authorTimestamp\": 1576763816000,\n        }\n    ],\n    \"size\": 1,\n    \"isLastPage\": True,\n    \"start\": 1,\n    \"limit\": 1,\n    \"nextPageStart\": None,\n}\n\nCOMMIT_CHANGELIST_WITH_PAGES_FIRST_COMMIT_EXAMPLE = {\n    \"values\": [\n        {\n            \"path\": {\n                \"components\": [\"a.txt\"],\n                \"parent\": \"\",\n                \"name\": \"a.txt\",\n                \"extension\": \"txt\",\n                \"toString\": \"a.txt\",\n            },\n            \"executable\": False,\n            \"percentUnchanged\": -1,\n            \"type\": \"MODIFY\",\n            \"nodeType\": \"FILE\",\n            \"srcExecutable\": False,\n            \"properties\": {\"gitChangeType\": \"MODIFY\"},\n        },\n        {\n            \"path\": {\n                \"components\": [\"b.txt\"],\n                \"parent\": \"\",\n                \"name\": \"b.txt\",\n                \"extension\": \"txt\",\n                \"toString\": \"b.txt\",\n            },\n            \"executable\": False,\n            \"percentUnchanged\": -1,\n            \"type\": \"ADD\",\n            \"nodeType\": \"FILE\",\n            \"srcExecutable\": False,\n            \"properties\": {\"gitChangeType\": \"ADD\"},\n        },\n    ]\n}\n\nCOMMIT_CHANGELIST_WITH_PAGES_SECOND_COMMIT_EXAMPLE_1_2 = {\n    \"values\": [\n        {\n            \"path\": {\n                \"components\": [\"c.txt\"],\n                \"parent\": \"\",\n                \"name\": \"c.txt\",\n                \"extension\": \"txt\",\n                \"toString\": \"c.txt\",\n            },\n            \"executable\": False,\n            \"percentUnchanged\": -1,\n            \"type\": \"DELETE\",\n            \"nodeType\": \"FILE\",\n            \"srcExecutable\": False,\n            \"properties\": {\"gitChangeType\": \"DELETE\"},\n        }\n    ],\n    \"size\": 1,\n    \"isLastPage\": False,\n    \"start\": 0,\n    \"limit\": 1,\n    \"nextPageStart\": 1,\n}\n\nCOMMIT_CHANGELIST_WITH_PAGES_SECOND_COMMIT_EXAMPLE_2_2 = {\n    \"values\": [\n        {\n            \"path\": {\n                \"components\": [\"e.txt\"],\n                \"parent\": \"\",\n                \"name\": \"d.txt\",\n                \"extension\": \"txt\",\n                \"toString\": \"d.txt\",\n            },\n            \"srcPath\": {\n                \"components\": [\"d.txt\"],\n                \"parent\": \"\",\n                \"name\": \"e.txt\",\n                \"extension\": \"txt\",\n                \"toString\": \"e.txt\",\n            },\n            \"executable\": False,\n            \"percentUnchanged\": -1,\n            \"type\": \"MOVE\",\n            \"nodeType\": \"FILE\",\n            \"srcExecutable\": False,\n            \"properties\": {\"gitChangeType\": \"MOVE\"},\n        },\n    ],\n    \"size\": 1,\n    \"isLastPage\": True,\n    \"start\": 1,\n    \"limit\": 1,\n    \"nextPageStart\": None,\n}\n", "metadata": {"source": "github.com/getsentry/sentry", "file_path": "fixtures/bitbucket_server.py", "language": "python", "complexity": "complex", "star_count": 41527, "quality_score": 0.8300000000000001, "features": {"has_classes": false, "has_functions": false, "has_async": false, "has_decorators": true, "has_imports": false, "has_error_handling": false, "has_documentation": true, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 0, "class_count": 0, "line_count": 267}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "database_integration", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create database model with ORM relationships and queries\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from time import time\n\nfrom sentry.integrations.gitlab.integration import GitlabIntegration\nfrom sentry.integrations.models.integration import Integration\nfrom sentry.integrations.types import IntegrationProviderSlug\nfrom sentry.models.repository import Repository\nfrom sentry.silo.base import SiloMode\nfrom sentry.testutils.cases import APITestCase\nfrom sentry.testutils.helpers.integrations import get_installation_of_type\nfrom sentry.testutils.silo import assume_test_silo_mode\nfrom sentry.users.models.identity import Identity, IdentityProvider\n\nEXTERNAL_ID = \"example.gitlab.com:group-x\"\nWEBHOOK_SECRET = \"secret-token-value\"\nWEBHOOK_TOKEN = f\"{EXTERNAL_ID}:{WEBHOOK_SECRET}\"\n\n\nclass GitLabTestCase(APITestCase):\n    provider = IntegrationProviderSlug.GITLAB.value\n\n    def setUp(self):\n        self.login_as(self.user)\n        with assume_test_silo_mode(SiloMode.CONTROL):\n            self.integration = Integration.objects.create(\n                provider=self.provider,\n                name=\"Example Gitlab\",\n                external_id=EXTERNAL_ID,\n                metadata={\n                    \"instance\": \"example.gitlab.com\",\n                    \"base_url\": \"https://example.gitlab.com\",\n                    \"domain_name\": \"example.gitlab.com/group-x\",\n                    \"verify_ssl\": False,\n                    \"webhook_secret\": WEBHOOK_SECRET,\n                    \"group_id\": 1,\n                },\n            )\n            identity = Identity.objects.create(\n                idp=IdentityProvider.objects.create(type=self.provider, config={}),\n                user=self.user,\n                external_id=\"gitlab123\",\n                data={\n                    \"access_token\": \"123456789\",\n                    \"created_at\": time(),\n                    \"refresh_token\": \"0987654321\",\n                },\n            )\n            self.integration.add_organization(self.organization, self.user, identity.id)\n        self.installation = get_installation_of_type(\n            GitlabIntegration, self.integration, self.organization.id\n        )\n\n    @assume_test_silo_mode(SiloMode.REGION)\n    def create_gitlab_repo(\n        self,\n        name: str,\n        external_id: int = 15,\n        url: str | None = None,\n        organization_id: int | None = None,\n    ) -> Repository:\n        instance = self.integration.metadata[\"instance\"]\n        organization_id = organization_id or self.organization.id\n        return Repository.objects.create(\n            organization_id=organization_id,\n            name=name,\n            external_id=f\"{instance}:{external_id}\",\n            url=url,\n            config={\"project_id\": external_id, \"path\": \"example-repo\"},\n            provider=\"integrations:gitlab\",\n            integration_id=self.integration.id,\n        )\n\n\nMERGE_REQUEST_OPENED_EVENT = b\"\"\"{\n  \"object_kind\": \"merge_request\",\n  \"user\": {\n    \"name\": \"Administrator\",\n    \"username\": \"root\",\n    \"avatar_url\": \"http://www.gravatar.com/avatar/e64c7d89f26bd1972efa854d13d7dd61?s=80&d=identicon\"\n  },\n  \"project\": {\n    \"id\": 15,\n    \"name\": \"Sentry\",\n    \"description\": \"\",\n    \"web_url\": \"http://example.com/cool-group/sentry\",\n    \"avatar_url\": null,\n    \"git_ssh_url\": \"git@example.com:cool-group/sentry.git\",\n    \"git_http_url\": \"http://example.com/cool-group/sentry.git\",\n    \"namespace\": \"cool-group\",\n    \"visibility_level\": 0,\n    \"path_with_namespace\": \"cool-group/sentry\",\n    \"default_branch\": \"master\",\n    \"ci_config_path\": \"\",\n    \"homepage\": \"http://example.com/cool-group/sentry\",\n    \"url\": \"git@example.com:cool-group/sentry.git\",\n    \"ssh_url\": \"git@example.com:cool-group/sentry.git\",\n    \"http_url\": \"http://example.com/cool-group/sentry.git\"\n  },\n  \"object_attributes\": {\n    \"id\": 90,\n    \"target_branch\": \"master\",\n    \"source_branch\": \"ms-viewport\",\n    \"source_project_id\": 14,\n    \"author_id\": 51,\n    \"assignee_id\": 6,\n    \"title\": \"Create a new Viewport\",\n    \"created_at\": \"2017-09-20T08:31:45.944Z\",\n    \"updated_at\": \"2017-09-28T12:23:42.365Z\",\n    \"milestone_id\": null,\n    \"state\": \"opened\",\n    \"merge_status\": \"unchecked\",\n    \"target_project_id\": 14,\n    \"iid\": 1,\n    \"description\": \"Create a viewport for things Fixes BAR-9\",\n    \"updated_by_id\": 1,\n    \"merge_error\": null,\n    \"merge_params\": {\n      \"force_remove_source_branch\": \"0\"\n    },\n    \"merge_when_pipeline_succeeds\": false,\n    \"merge_user_id\": null,\n    \"merge_commit_sha\": null,\n    \"deleted_at\": null,\n    \"in_progress_merge_commit_sha\": null,\n    \"lock_version\": 5,\n    \"time_estimate\": 0,\n    \"last_edited_at\": \"2017-09-27T12:43:37.558Z\",\n    \"last_edited_by_id\": 1,\n    \"head_pipeline_id\": 61,\n    \"ref_fetched\": true,\n    \"merge_jid\": null,\n    \"source\": {\n      \"name\": \"Awesome Project\",\n      \"description\": \"\",\n      \"web_url\": \"http://example.com/awesome_space/awesome_project\",\n      \"avatar_url\": null,\n      \"git_ssh_url\": \"git@example.com:awesome_space/awesome_project.git\",\n      \"git_http_url\": \"http://example.com/awesome_space/awesome_project.git\",\n      \"namespace\": \"root\",\n      \"visibility_level\": 0,\n      \"path_with_namespace\": \"awesome_space/awesome_project\",\n      \"default_branch\": \"master\",\n      \"ci_config_path\": \"\",\n      \"homepage\": \"http://example.com/awesome_space/awesome_project\",\n      \"url\": \"http://example.com/awesome_space/awesome_project.git\",\n      \"ssh_url\": \"git@example.com:awesome_space/awesome_project.git\",\n      \"http_url\": \"http://example.com/awesome_space/awesome_project.git\"\n    },\n    \"target\": {\n      \"name\": \"Awesome Project\",\n      \"description\": \"Aut reprehenderit ut est.\",\n      \"web_url\": \"http://example.com/awesome_space/awesome_project\",\n      \"avatar_url\": null,\n      \"git_ssh_url\": \"git@example.com:awesome_space/awesome_project.git\",\n      \"git_http_url\": \"http://example.com/awesome_space/awesome_project.git\",\n      \"namespace\": \"Awesome Space\",\n      \"visibility_level\": 0,\n      \"path_with_namespace\": \"awesome_space/awesome_project\",\n      \"default_branch\": \"master\",\n      \"ci_config_path\": \"\",\n      \"homepage\": \"http://example.com/awesome_space/awesome_project\",\n      \"url\": \"http://example.com/awesome_space/awesome_project.git\",\n      \"ssh_url\": \"git@example.com:awesome_space/awesome_project.git\",\n      \"http_url\": \"http://example.com/awesome_space/awesome_project.git\"\n    },\n    \"last_commit\": {\n      \"id\": \"ba3e0d8ff79c80d5b0bbb4f3e2e343e0aaa662b7\",\n      \"message\": \"fixed readme\",\n      \"timestamp\": \"2017-09-26T16:12:57Z\",\n      \"url\": \"http://example.com/awesome_space/awesome_project/commits/da1560886d4f094c3e6c9ef40349f7d38b5d27d7\",\n      \"author\": {\n        \"name\": \"GitLab dev user\",\n        \"email\": \"gitlabdev@dv6700.(none)\"\n      }\n    },\n    \"work_in_progress\": false,\n    \"total_time_spent\": 0,\n    \"human_total_time_spent\": null,\n    \"human_time_estimate\": null,\n    \"action\": \"open\"\n  },\n  \"labels\": null,\n  \"repository\": {\n    \"name\": \"git-gpg-test\",\n    \"url\": \"git@example.com:awesome_space/awesome_project.git\",\n    \"description\": \"\",\n    \"homepage\": \"http://example.com/awesome_space/awesome_project\"\n  }\n}\"\"\"\n\n\nPUSH_EVENT = b\"\"\"\n{\n  \"object_kind\": \"push\",\n  \"before\": \"95790bf891e76fee5e1747ab589903a6a1f80f22\",\n  \"after\": \"da1560886d4f094c3e6c9ef40349f7d38b5d27d7\",\n  \"ref\": \"refs/heads/master\",\n  \"checkout_sha\": \"da1560886d4f094c3e6c9ef40349f7d38b5d27d7\",\n  \"user_id\": 4,\n  \"user_name\": \"John Smith\",\n  \"user_username\": \"jsmith\",\n  \"user_email\": \"john@example.com\",\n  \"user_avatar\": \"https://s.gravatar.com/avatar/d4c74594d841139328695756648b6bd6?s=8://s.gravatar.com/avatar/d4c74594d841139328695756648b6bd6?s=80\",\n  \"project_id\": 15,\n  \"project\":{\n    \"id\": 15,\n    \"name\":\"Sentry\",\n    \"description\":\"\",\n    \"web_url\":\"http://example.com/cool-group/sentry\",\n    \"avatar_url\":null,\n    \"git_ssh_url\":\"git@example.com:cool-group/sentry.git\",\n    \"git_http_url\":\"http://example.com/cool-group/sentry.git\",\n    \"namespace\":\"Cool Group\",\n    \"visibility_level\":0,\n    \"path_with_namespace\":\"cool-group/sentry\",\n    \"default_branch\":\"master\",\n    \"homepage\":\"http://example.com/cool-group/sentry\",\n    \"url\":\"git@example.com:cool-group/sentry.git\",\n    \"ssh_url\":\"git@example.com:cool-group/sentry.git\",\n    \"http_url\":\"http://example.com/cool-group/sentry.git\"\n  },\n  \"repository\":{\n    \"name\": \"Sentry\",\n    \"url\": \"git@example.com:cool-group/sentry.git\",\n    \"description\": \"\",\n    \"homepage\": \"http://example.com/cool-group/sentry\",\n    \"git_http_url\":\"http://example.com/cool-group/sentry.git\",\n    \"git_ssh_url\":\"git@example.com:cool-group/sentry.git\",\n    \"visibility_level\":0\n  },\n  \"commits\": [\n    {\n      \"id\": \"b6568db1bc1dcd7f8b4d5a946b0b91f9dacd7327\",\n      \"message\": \"Update Catalan translation to e38cb41.\",\n      \"timestamp\": \"2011-12-12T14:27:31+02:00\",\n      \"url\": \"http://example.com/cool-group/sentry/commit/b6568db1bc1dcd7f8b4d5a946b0b91f9dacd7327\",\n      \"author\": {\n        \"name\": \"Jordi\",\n        \"email\": \"jordi@example.org\"\n      },\n      \"added\": [\"CHANGELOG\"],\n      \"modified\": [\"app/controller/application.rb\"],\n      \"removed\": []\n    },\n    {\n      \"id\": \"da1560886d4f094c3e6c9ef40349f7d38b5d27d7\",\n      \"message\": \"fixed readme\",\n      \"timestamp\": \"2012-01-03T23:36:29+02:00\",\n      \"url\": \"http://example.com/cool-group/sentry/commit/da1560886d4f094c3e6c9ef40349f7d38b5d27d7\",\n      \"author\": {\n        \"name\": \"GitLab dev user\",\n        \"email\": \"gitlabdev@example.org\"\n      },\n      \"added\": [\"CHANGELOG\"],\n      \"modified\": [\"app/controller/application.rb\"],\n      \"removed\": []\n    }\n  ],\n  \"total_commits_count\": 2\n}\n\"\"\"\n\nPUSH_EVENT_IGNORED_COMMIT = b\"\"\"\n{\n  \"object_kind\": \"push\",\n  \"before\": \"95790bf891e76fee5e1747ab589903a6a1f80f22\",\n  \"after\": \"da1560886d4f094c3e6c9ef40349f7d38b5d27d7\",\n  \"ref\": \"refs/heads/master\",\n  \"checkout_sha\": \"da1560886d4f094c3e6c9ef40349f7d38b5d27d7\",\n  \"user_id\": 4,\n  \"user_name\": \"John Smith\",\n  \"user_username\": \"jsmith\",\n  \"user_email\": \"john@example.com\",\n  \"user_avatar\": \"https://s.gravatar.com/avatar/d4c74594d841139328695756648b6bd6?s=8://s.gravatar.com/avatar/d4c74594d841139328695756648b6bd6?s=80\",\n  \"project_id\": 15,\n  \"project\":{\n    \"id\": 15,\n    \"name\":\"Sentry\",\n    \"description\":\"\",\n    \"web_url\":\"http://example.com/cool-group/sentry\",\n    \"avatar_url\":null,\n    \"git_ssh_url\":\"git@example.com:cool-group/sentry.git\",\n    \"git_http_url\":\"http://example.com/cool-group/sentry.git\",\n    \"namespace\":\"cool-group\",\n    \"visibility_level\":0,\n    \"path_with_namespace\":\"cool-group/sentry\",\n    \"default_branch\":\"master\",\n    \"homepage\":\"http://example.com/cool-group/sentry\",\n    \"url\":\"git@example.com:cool-group/sentry.git\",\n    \"ssh_url\":\"git@example.com:cool-group/sentry.git\",\n    \"http_url\":\"http://example.com/cool-group/sentry.git\"\n  },\n  \"repository\":{\n    \"name\": \"Sentry\",\n    \"url\": \"git@example.com:cool-group/sentry.git\",\n    \"description\": \"\",\n    \"homepage\": \"http://example.com/cool-group/sentry\",\n    \"git_http_url\":\"http://example.com/cool-group/sentry.git\",\n    \"git_ssh_url\":\"git@example.com:cool-group/sentry.git\",\n    \"visibility_level\":0\n  },\n  \"commits\": [\n    {\n      \"id\": \"b6568db1bc1dcd7f8b4d5a946b0b91f9dacd7327\",\n      \"message\": \"Update things #skipsentry\",\n      \"timestamp\": \"2011-12-12T14:27:31+02:00\",\n      \"url\": \"http://example.com/cool-group/sentry/commit/b6568db1bc1dcd7f8b4d5a946b0b91f9dacd7327\",\n      \"author\": {\n        \"name\": \"Jordi\",\n        \"email\": \"jordi@example.org\"\n      },\n      \"added\": [\"CHANGELOG\"],\n      \"modified\": [\"app/controller/application.rb\"],\n      \"removed\": []\n    }\n  ],\n  \"total_commits_count\": 1\n}\n\"\"\"\n\n\nCOMPARE_RESPONSE = r\"\"\"\n{\n  \"commit\": {\n    \"id\": \"12d65c8dd2b2676fa3ac47d955accc085a37a9c1\",\n    \"short_id\": \"12d65c8dd2b\",\n    \"title\": \"JS fix\",\n    \"author_name\": \"Dmitriy\",\n    \"author_email\": \"dmitriy@example.com\",\n    \"created_at\": \"2014-02-27T10:27:00+02:00\"\n  },\n  \"commits\": [{\n    \"id\": \"12d65c8dd2b2676fa3ac47d955accc085a37a9c1\",\n    \"short_id\": \"12d65c8dd2b\",\n    \"title\": \"JS fix\",\n    \"author_name\": \"Dmitriy\",\n    \"author_email\": \"dmitriy@example.com\",\n    \"created_at\": \"2014-02-27T10:27:00+02:00\"\n  },\n  {\n    \"id\": \"8b090c1b79a14f2bd9e8a738f717824ff53aebad\",\n    \"short_id\": \"8b090c1b\",\n    \"title\": \"Fix dmitriy mistake\",\n    \"author_name\": \"Sally\",\n    \"author_email\": \"sally@example.com\",\n    \"created_at\": \"2014-02-27T10:27:00+02:00\"\n  }],\n  \"diffs\": [{\n    \"old_path\": \"files/js/application.js\",\n    \"new_path\": \"files/js/application.js\",\n    \"a_mode\": null,\n    \"b_mode\": \"100644\",\n    \"diff\": \"--- a/files/js/application.js\\n+++ b/files/js/application.js\\n@@ -24,8 +24,10 @@\\n //= require g.raphael-min\\n //= require g.bar-min\\n //= require branch-graph\\n-//= require highlightjs.min\\n-//= require ace/ace\\n //= require_tree .\\n //= require d3\\n //= require underscore\\n+\\n+function fix() { \\n+  alert(\\\"Fixed\\\")\\n+}\",\n    \"new_file\": false,\n    \"renamed_file\": false,\n    \"deleted_file\": false\n  }],\n  \"compare_timeout\": false,\n  \"compare_same_ref\": false\n}\n\"\"\"\n\n\nCOMMIT_LIST_RESPONSE = r\"\"\"\n[\n  {\n    \"id\": \"ed899a2f4b50b4370feeea94676502b42383c746\",\n    \"short_id\": \"ed899a2f4b5\",\n    \"title\": \"Replace sanitize with escape once\",\n    \"author_name\": \"Dmitriy\",\n    \"author_email\": \"dmitriy@example.com\",\n    \"authored_date\": \"2018-09-20T11:50:22+03:00\",\n    \"committer_name\": \"Administrator\",\n    \"committer_email\": \"admin@example.com\",\n    \"committed_date\": \"2018-09-20T11:50:22+03:00\",\n    \"created_at\": \"2018-09-20T11:50:22+03:00\",\n    \"message\": \"Replace sanitize with escape once\",\n    \"parent_ids\": [\n      \"6104942438c14ec7bd21c6cd5bd995272b3faff6\"\n    ]\n  },\n  {\n    \"id\": \"6104942438c14ec7bd21c6cd5bd995272b3faff6\",\n    \"short_id\": \"6104942438c\",\n    \"title\": \"Sanitize for network graph\",\n    \"author_name\": \"randx\",\n    \"author_email\": \"dmitriy@example.com\",\n    \"committer_name\": \"Dmitriy\",\n    \"committer_email\": \"dmitriy@example.com\",\n    \"created_at\": \"2018-09-20T09:06:12+03:00\",\n    \"message\": \"Sanitize for network graph\",\n    \"parent_ids\": [\n      \"ae1d9fb46aa2b07ee9836d49862ec4e2c46fbbba\"\n    ]\n  }\n]\n\"\"\"\n\nCOMMIT_DIFF_RESPONSE = r\"\"\"\n[\n    {\n        \"old_path\": \"files/js/application.js\",\n        \"new_path\": \"files/js/application.js\",\n        \"a_mode\": null,\n        \"b_mode\": \"100644\",\n        \"diff\": \"--- a/files/js/application.js\\n+++ b/files/js/application.js\\n@@ -24,8 +24,10 @@\\n //= require g.raphael-min\\n //= require g.bar-min\\n //= require branch-graph\\n-//= require highlightjs.min\\n-//= require ace/ace\\n //= require_tree .\\n //= require d3\\n //= require underscore\\n+\\n+function fix() { \\n+  alert(\\\"Fixed\\\")\\n+}\",\n        \"new_file\": false,\n        \"renamed_file\": false,\n        \"deleted_file\": false\n    },\n    {\n        \"a_mode\": \"100644\",\n        \"b_mode\": \"100644\",\n        \"deleted_file\": false,\n        \"diff\": \"@@ -1 +1,3 @@\\n OH HAI\\n+OH HAI\\n+OH HAI\\n\",\n        \"new_file\": false,\n        \"new_path\": \"README.txt\",\n        \"old_path\": \"README.txt\",\n        \"renamed_file\": false\n    }\n]\n\"\"\"\n\n# Example taken from https://docs.gitlab.com/ee/api/commits.html#get-a-single-commit\nGET_COMMIT_RESPONSE = \"\"\"\n{\n  \"id\": \"6104942438c14ec7bd21c6cd5bd995272b3faff6\",\n  \"short_id\": \"6104942438c\",\n  \"title\": \"Sanitize for network graph\",\n  \"author_name\": \"randx\",\n  \"author_email\": \"user@example.com\",\n  \"committer_name\": \"Dmitriy\",\n  \"committer_email\": \"user@example.com\",\n  \"created_at\": \"2021-09-20T09:06:12.300+03:00\",\n  \"message\": \"Sanitize for network graph\",\n  \"committed_date\": \"2021-09-20T09:06:12.300+03:00\",\n  \"authored_date\": \"2021-09-20T09:06:12.420+03:00\",\n  \"parent_ids\": [\n    \"ae1d9fb46aa2b07ee9836d49862ec4e2c46fbbba\"\n  ],\n  \"last_pipeline\" : {\n    \"id\": 8,\n    \"ref\": \"master\",\n    \"sha\": \"2dc6aa325a317eda67812f05600bdf0fcdc70ab0\",\n    \"status\": \"created\"\n  },\n  \"stats\": {\n    \"additions\": 15,\n    \"deletions\": 10,\n    \"total\": 25\n  },\n  \"status\": \"running\",\n  \"web_url\": \"https://gitlab.example.com/thedude/gitlab-foss/-/commit/6104942438c14ec7bd21c6cd5bd995272b3faff6\"\n}\"\"\"\n", "metadata": {"source": "github.com/getsentry/sentry", "file_path": "fixtures/gitlab.py", "language": "python", "complexity": "complex", "star_count": 41527, "quality_score": 1.0, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": true, "has_type_hints": true, "has_testing": true, "has_logging": true, "function_count": 2, "class_count": 1, "line_count": 454}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "database_integration", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate module integration for Create database model with ORM relationships and queries\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import os\n\nFIXTURE_DIRECTORY = os.path.join(os.path.dirname(os.path.abspath(__file__)))\n", "metadata": {"source": "github.com/getsentry/sentry", "file_path": "fixtures/integrations/__init__.py", "language": "python", "complexity": "simple", "star_count": 41527, "quality_score": 0.6300000000000001, "features": {"has_classes": false, "has_functions": false, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": false, "has_testing": false, "has_logging": false, "function_count": 0, "class_count": 0, "line_count": 4}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "database_integration", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create database model with ORM relationships and queries\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from __future__ import annotations\n\nimport os\nimport shutil\nfrom collections import defaultdict\nfrom typing import Any\n\nimport orjson\n\nfrom fixtures.integrations import FIXTURE_DIRECTORY\nfrom fixtures.integrations.stub_service import StubService\nfrom sentry.utils.numbers import base32_encode\n\n\nclass MockService(StubService):\n    \"\"\"\n    A mock is a service that replicates the functionality of a real software\n    system by implementing the same interface with simplified business logic.\n    For example, a mocked random dice_roll function might return `hash(time()) % 6`.\n    Like stubs, mocks can make tests simpler and more reliable.\n    \"\"\"\n\n    def __init__(self, mode=\"memory\"):\n        \"\"\"\n        Initialize the mock instance. Wipe the previous instance's data if it exists.\n        \"\"\"\n        super().__init__()\n        self.mode = mode\n        self._next_error_code = None\n        self._next_ids = defaultdict(int)\n\n        if self.mode == \"file\":\n            path = os.path.join(FIXTURE_DIRECTORY, self.service_name, \"data\")\n            if os.path.exists(path):\n                shutil.rmtree(path)\n            os.makedirs(path)\n        else:\n            self._memory: dict[str, dict[str, Any]] = defaultdict(dict)\n\n    def add_project(self, project):\n        \"\"\"\n        Create a new, empty project.\n\n        :param project: String name of project\n        :return: void\n        \"\"\"\n        self._next_ids.get(project)  # touch\n        if self.mode == \"file\":\n            self._get_project_path(project)\n\n    def remove_project(self, project):\n        \"\"\"\n        Totally wipe out a project.\n\n        :param project: String name of project\n        :return: void\n        \"\"\"\n        del self._next_ids[project]\n        if self.mode == \"file\":\n            path = self._get_project_path(project)\n            shutil.rmtree(path)\n\n    def break_next_api_call(self, error_code=500):\n        \"\"\"\n        Simulate an outage for a single API call.\n        \"\"\"\n        self._next_error_code = error_code\n\n    def _throw_if_broken(self, message_option=None):\n        \"\"\"\n        See break_next_api_call.\n        :param message_option: What should the message be if this raises?\n        :raises: Generic Exception\n        \"\"\"\n        if self._next_error_code:\n            self._next_error_code = None\n            message = message_option or f\"{self.service_name} is down\"\n            raise Exception(f\"{self._next_error_code}: {message}\")\n\n    def _get_project_names(self):\n        return self._next_ids.keys()\n\n    def _get_new_ticket_name(self, project):\n        counter = self._next_ids[project]\n        self._next_ids[project] = counter + 1\n\n        return f\"{project}-{base32_encode(counter)}\"\n\n    def _get_project_path(self, project):\n        path = os.path.join(FIXTURE_DIRECTORY, self.service_name, \"data\", project)\n\n        os.makedirs(path, exist_ok=True)\n        return path\n\n    def _set_data(self, project, name, data):\n        if self.mode == \"memory\":\n            if not self._memory[project]:\n                self._memory[project] = defaultdict()\n            self._memory[project][name] = data\n            return\n\n        path = os.path.join(self._get_project_path(project), f\"{name}.json\")\n        with open(path, \"wb\") as f:\n            f.write(orjson.dumps(data))\n\n    def _get_data(self, project, name):\n        if self.mode == \"memory\":\n            if not self._memory[project]:\n                self._memory[project] = defaultdict()\n            return self._memory[project].get(name)\n\n        path = os.path.join(self._get_project_path(project), f\"{name}.json\")\n        if not os.path.exists(path):\n            return None\n\n        with open(path, \"rb\") as f:\n            return orjson.loads(f.read())\n", "metadata": {"source": "github.com/getsentry/sentry", "file_path": "fixtures/integrations/mock_service.py", "language": "python", "complexity": "complex", "star_count": 41527, "quality_score": 0.9500000000000001, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": true, "has_type_hints": true, "has_testing": true, "has_logging": true, "function_count": 10, "class_count": 1, "line_count": 118}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "database_integration", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create database model with ORM relationships and queries\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from __future__ import annotations\n\nimport os\nfrom copy import deepcopy\nfrom typing import Any\n\nimport orjson\n\nfrom fixtures.integrations import FIXTURE_DIRECTORY\n\n\nclass StubService:\n    \"\"\"\n    A stub is a service that replicates the functionality of a real software\n    system by returning valid data without actually implementing any business\n    logic. For example, a stubbed random dice_roll function might always return\n    6. Stubs can make tests simpler and more reliable because they can replace\n    flaky or slow networks call or allow you to have wider coverage in end-to-\n    end tests.\n    \"\"\"\n\n    stub_data_cache: dict[str, Any] = {}\n    service_name: str\n\n    @staticmethod\n    def get_stub_json(service_name, name):\n        \"\"\"\n        Get the stubbed data as a JSON string.\n\n        :param service_name: string\n        :param name: string\n        :return: string\n        \"\"\"\n        path = os.path.join(FIXTURE_DIRECTORY, service_name, \"stubs\", name)\n        with open(path) as f:\n            return f.read()\n\n    @staticmethod\n    def get_stub_data(service_name, name):\n        \"\"\"\n        Get the stubbed data as a python object.\n\n        :param service_name: string\n        :param name: string\n        :return: object\n        \"\"\"\n        cache_key = f\"{service_name}.{name}\"\n        cached = StubService.stub_data_cache.get(cache_key)\n        if cached:\n            data = cached\n        else:\n            data = orjson.loads(StubService.get_stub_json(service_name, name))\n            StubService.stub_data_cache[cache_key] = data\n        return deepcopy(data)\n\n    def _get_stub_data(self, name):\n        return StubService.get_stub_data(self.service_name, name)\n", "metadata": {"source": "github.com/getsentry/sentry", "file_path": "fixtures/integrations/stub_service.py", "language": "python", "complexity": "simple", "star_count": 41527, "quality_score": 0.9500000000000001, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": true, "has_type_hints": true, "has_testing": false, "has_logging": true, "function_count": 3, "class_count": 1, "line_count": 58}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "database_integration", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate configuration for Create database model with ORM relationships and queries\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# this file is necessary for some reason for test models such as tests/sentry/db/models/fields/bitfield/test_bitfield.py\n", "metadata": {"source": "github.com/getsentry/sentry", "file_path": "fixtures/models.py", "language": "python", "complexity": "simple", "star_count": 41527, "quality_score": 0.6300000000000001, "features": {"has_classes": false, "has_functions": false, "has_async": false, "has_decorators": false, "has_imports": false, "has_error_handling": false, "has_documentation": false, "has_type_hints": false, "has_testing": true, "has_logging": false, "function_count": 0, "class_count": 0, "line_count": 2}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "database_integration", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create database model with ORM relationships and queries\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from sentry.models.dashboard import Dashboard\nfrom sentry.models.organization import Organization\n\nfrom .base import BasePage\n\nEDIT_WIDGET_BUTTON = '[data-test-id=\"widget-edit\"]'\nWIDGET_DRAG_HANDLE = \".widget-drag\"\nWIDGET_RESIZE_HANDLE = \".widget-resize\"\nWIDGET_EDITABLE_TEXT_LABEL = '[data-test-id=\"editable-text-label\"]'\nWIDGET_TITLE_FIELD = 'input[aria-label=\"Widget title\"]'\n\n\nclass DashboardDetailPage(BasePage):\n    def __init__(self, browser, client, *, organization: Organization, dashboard: Dashboard):\n        super().__init__(browser)\n        self.client = client\n        self.organization = organization\n        self.dashboard = dashboard\n\n    def wait_until_loaded(self):\n        self.browser.wait_until_not('[data-test-id=\"events-request-loading\"]')\n        self.browser.wait_until_not('[data-test-id=\"loading-indicator\"]')\n        self.browser.wait_until_not('[data-test-id=\"loading-placeholder\"]')\n        self.browser.wait_until_not(\".loading\")\n\n    def visit_default_overview(self):\n        self.browser.get(f\"/organizations/{self.organization.slug}/dashboard/default-overview/\")\n        self.wait_until_loaded()\n        self.browser.driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight)\")\n        self.wait_until_loaded()\n\n    def visit_create_dashboard(self):\n        self.browser.get(f\"/organizations/{self.organization.slug}/dashboards/new/\")\n        self.wait_until_loaded()\n\n    def visit_dashboard_detail(self):\n        self.browser.get(f\"/organizations/{self.organization.slug}/dashboard/{self.dashboard.id}/\")\n        self.wait_until_loaded()\n\n    def enter_edit_state(self):\n        button = self.browser.element('[data-test-id=\"dashboard-edit\"]')\n        self.browser.wait_until_clickable('[data-test-id=\"dashboard-edit\"]')\n        button.click()\n        self.wait_until_loaded()\n\n    def click_dashboard_add_widget_button(self):\n        button = self.browser.element('[data-test-id=\"widget-add\"]')\n        # HACK: Use JavaScript to execute click to avoid click intercepted issues\n        self.browser.driver.execute_script(\"arguments[0].click()\", button)\n        self.wait_until_loaded()\n\n    def click_dashboard_header_add_widget_button(self):\n        button = self.browser.element('[data-test-id=\"add-widget-library\"]')\n        self.browser.wait_until_clickable('[data-test-id=\"add-widget-library\"]')\n        button.click()\n        self.wait_until_loaded()\n\n    def click_cancel_button(self):\n        button = self.browser.element('[data-test-id=\"dashboard-cancel\"]')\n        self.browser.wait_until_clickable('[data-test-id=\"dashboard-cancel\"]')\n        button.click()\n        self.wait_until_loaded()\n\n    def add_widget_through_dashboard(self, widget_title):\n        self.click_dashboard_add_widget_button()\n        title_input = self.browser.element(WIDGET_TITLE_FIELD)\n        title_input.clear()\n        title_input.send_keys(widget_title)\n        button = self.browser.element('[aria-label=\"Add Widget\"]')\n        button.click()\n        self.wait_until_loaded()\n\n    def save_dashboard(self):\n        button = self.browser.element('[data-test-id=\"dashboard-commit\"]')\n        self.browser.wait_until_clickable('[data-test-id=\"dashboard-commit\"]')\n        button.click()\n        # This is a kind of hack.\n        # After we click the button, an API call is made and we want to wait\n        # until the API call finishes. Since the loading indicator isn't used\n        # we can't rely on self.wait_until_loaded(). The UI shows a\n        # success toast, however if a previous step of a test shows a success\n        # toast, a wait_until([data-test-id=\"toast-success\"]) will return\n        # immediately due to the previous toast still being in the DOM.\n        # Since clicking the save dasboard button is removed once the API\n        # call is complete, we can wait for that as a signal\n        # that the API is complete.\n        self.browser.wait_until_not('[data-test-id=\"dashboard-commit\"]')\n        self.wait_until_loaded()\n", "metadata": {"source": "github.com/getsentry/sentry", "file_path": "fixtures/page_objects/dashboard_detail.py", "language": "python", "complexity": "medium", "star_count": 41527, "quality_score": 0.92, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 11, "class_count": 1, "line_count": 89}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "database_integration", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create database model with ORM relationships and queries\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from selenium.webdriver.common.by import By\n\nfrom .base import BasePage\nfrom .global_selection import GlobalSelectionPage\n\n\nclass ExploreLogsPage(BasePage):\n    def __init__(self, browser, client):\n        super().__init__(browser)\n        self.client = client\n        self.global_selection = GlobalSelectionPage(browser)\n\n    def visit_explore_logs(self, org):\n        self.browser.get(f\"/organizations/{org}/explore/logs/\")\n        self.wait_until_loaded()\n\n    def toggle_log_row_with_message(self, message):\n        row = self.get_log_row_with_message(message)\n        try:\n            expanded_count = len(\n                self.browser.find_elements(By.CSS_SELECTOR, '*[data-test-id=\"fields-tree\"]')\n            )\n        except Exception:\n            expanded_count = 0\n        if expanded_count > 0:\n            row.click()\n            # If this is breaking make sure to only have one row expanded at a time.\n            # TODO: Target the correct field-tree with xpath.\n            self.browser.wait_until_not('[data-test-id=\"fields-tree\"]')\n        else:\n            row.click()\n            self.browser.wait_until('[data-test-id=\"fields-tree\"]')\n\n        return row\n\n    def get_log_row_with_message(self, message):\n        row = self.browser.find_element(\n            by=By.XPATH,\n            value=f'//*[@data-test-id=\"log-table-row\" and .//*[contains(text(),\"{message}\")]]',\n        )\n        return row\n\n    def get_log_row_columns(self, row):\n        # The expanded row actually makes a new sibling row that contains the fields-tree.\n        columns = row.find_elements(\n            By.XPATH, 'following-sibling::*[1]//*[@data-test-id=\"attribute-tree-column\"]'\n        )\n        return columns\n\n    def wait_until_loaded(self):\n        self.browser.wait_until_not('[data-test-id=\"loading-indicator\"]')\n        self.browser.wait_until_test_id(\"logs-table\")\n", "metadata": {"source": "github.com/getsentry/sentry", "file_path": "fixtures/page_objects/explore_logs.py", "language": "python", "complexity": "medium", "star_count": 41527, "quality_score": 0.98, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": true, "has_documentation": false, "has_type_hints": true, "has_testing": true, "has_logging": true, "function_count": 6, "class_count": 1, "line_count": 53}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "database_integration", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create database model with ORM relationships and queries\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from .base import BasePage\n\n\nclass GlobalSelectionPage(BasePage):\n    def get_selected_project_slug(self):\n        return self.browser.element('[data-test-id=\"page-filter-project-selector\"]').text\n\n    def get_selected_environment(self):\n        return self.browser.element('[data-test-id=\"page-filter-environment-selector\"]').text\n\n    def get_selected_date(self):\n        return self.browser.element('[data-test-id=\"page-filter-timerange-selector\"]').text\n\n    def go_back_to_issues(self):\n        self.browser.click('[data-test-id=\"breadcrumb-link\"]')\n\n    def open_project_selector(self):\n        self.browser.click('[data-test-id=\"page-filter-project-selector\"]')\n\n    def select_project_by_slug(self, slug):\n        project_item_selector = f'//*[@data-test-id=\"menu-list-item-label\" and text()=\"{slug}\"]'\n\n        self.open_project_selector()\n        self.browser.wait_until(xpath=project_item_selector)\n        self.browser.click(xpath=project_item_selector)\n\n    def open_environment_selector(self):\n        self.browser.click('[data-test-id=\"page-filter-environment-selector\"]')\n\n    def select_environment(self, environment):\n        environment_path = f'//*[@data-test-id=\"menu-list-item-label\" and text()=\"{environment}\"]'\n\n        self.open_environment_selector()\n        self.browser.wait_until(xpath=environment_path)\n        self.browser.click(xpath=environment_path)\n\n    def open_date_selector(self):\n        self.browser.click('[data-test-id=\"page-filter-timerange-selector\"]')\n\n    def select_date(self, date):\n        date_path = f'//*[text()=\"{date}\"]'\n\n        self.open_date_selector()\n        self.browser.wait_until(xpath=date_path)\n        self.browser.click(xpath=date_path)\n", "metadata": {"source": "github.com/getsentry/sentry", "file_path": "fixtures/page_objects/global_selection.py", "language": "python", "complexity": "medium", "star_count": 41527, "quality_score": 0.92, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": false, "has_testing": false, "has_logging": false, "function_count": 10, "class_count": 1, "line_count": 46}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "database_integration", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create database model with ORM relationships and queries with SQLAlchemy/Django ORM patterns\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from django.conf import settings\nfrom django.contrib import admin\nfrom guardian.admin import GuardedModelAdmin\n\nfrom documents.models import Correspondent\nfrom documents.models import CustomField\nfrom documents.models import CustomFieldInstance\nfrom documents.models import Document\nfrom documents.models import DocumentType\nfrom documents.models import Note\nfrom documents.models import PaperlessTask\nfrom documents.models import SavedView\nfrom documents.models import SavedViewFilterRule\nfrom documents.models import ShareLink\nfrom documents.models import StoragePath\nfrom documents.models import Tag\n\nif settings.AUDIT_LOG_ENABLED:\n    from auditlog.admin import LogEntryAdmin\n    from auditlog.models import LogEntry\n\n\nclass CorrespondentAdmin(GuardedModelAdmin):\n    list_display = (\"name\", \"match\", \"matching_algorithm\")\n    list_filter = (\"matching_algorithm\",)\n    list_editable = (\"match\", \"matching_algorithm\")\n\n\nclass TagAdmin(GuardedModelAdmin):\n    list_display = (\"name\", \"color\", \"match\", \"matching_algorithm\")\n    list_filter = (\"matching_algorithm\",)\n    list_editable = (\"color\", \"match\", \"matching_algorithm\")\n    search_fields = (\"color\", \"name\")\n\n\nclass DocumentTypeAdmin(GuardedModelAdmin):\n    list_display = (\"name\", \"match\", \"matching_algorithm\")\n    list_filter = (\"matching_algorithm\",)\n    list_editable = (\"match\", \"matching_algorithm\")\n\n\nclass DocumentAdmin(GuardedModelAdmin):\n    search_fields = (\"correspondent__name\", \"title\", \"content\", \"tags__name\")\n    readonly_fields = (\n        \"added\",\n        \"modified\",\n        \"mime_type\",\n        \"storage_type\",\n        \"filename\",\n        \"checksum\",\n        \"archive_filename\",\n        \"archive_checksum\",\n        \"original_filename\",\n        \"deleted_at\",\n    )\n\n    list_display_links = (\"title\",)\n\n    list_display = (\"id\", \"title\", \"mime_type\", \"filename\", \"archive_filename\")\n\n    list_filter = (\n        (\"mime_type\"),\n        (\"archive_serial_number\", admin.EmptyFieldListFilter),\n        (\"archive_filename\", admin.EmptyFieldListFilter),\n    )\n\n    filter_horizontal = (\"tags\",)\n\n    ordering = [\"-id\"]\n\n    date_hierarchy = \"created\"\n\n    def has_add_permission(self, request):\n        return False\n\n    def created_(self, obj):\n        return obj.created.date().strftime(\"%Y-%m-%d\")\n\n    created_.short_description = \"Created\"\n\n    def get_queryset(self, request):  # pragma: no cover\n        \"\"\"\n        Include trashed documents\n        \"\"\"\n        return Document.global_objects.all()\n\n    def delete_queryset(self, request, queryset):\n        from documents import index\n\n        with index.open_index_writer() as writer:\n            for o in queryset:\n                index.remove_document(writer, o)\n\n        super().delete_queryset(request, queryset)\n\n    def delete_model(self, request, obj):\n        from documents import index\n\n        index.remove_document_from_index(obj)\n        super().delete_model(request, obj)\n\n    def save_model(self, request, obj, form, change):\n        from documents import index\n\n        index.add_or_update_document(obj)\n        super().save_model(request, obj, form, change)\n\n\nclass RuleInline(admin.TabularInline):\n    model = SavedViewFilterRule\n\n\nclass SavedViewAdmin(GuardedModelAdmin):\n    list_display = (\"name\", \"owner\")\n\n    inlines = [RuleInline]\n\n    def get_queryset(self, request):  # pragma: no cover\n        return super().get_queryset(request).select_related(\"owner\")\n\n\nclass StoragePathInline(admin.TabularInline):\n    model = StoragePath\n\n\nclass StoragePathAdmin(GuardedModelAdmin):\n    list_display = (\"name\", \"path\", \"match\", \"matching_algorithm\")\n    list_filter = (\"path\", \"matching_algorithm\")\n    list_editable = (\"path\", \"match\", \"matching_algorithm\")\n\n\nclass TaskAdmin(admin.ModelAdmin):\n    list_display = (\"task_id\", \"task_file_name\", \"task_name\", \"date_done\", \"status\")\n    list_filter = (\"status\", \"date_done\", \"task_name\")\n    search_fields = (\"task_name\", \"task_id\", \"status\", \"task_file_name\")\n    readonly_fields = (\n        \"task_id\",\n        \"task_file_name\",\n        \"task_name\",\n        \"status\",\n        \"date_created\",\n        \"date_started\",\n        \"date_done\",\n        \"result\",\n    )\n\n\nclass NotesAdmin(GuardedModelAdmin):\n    list_display = (\"user\", \"created\", \"note\", \"document\")\n    list_filter = (\"created\", \"user\")\n    list_display_links = (\"created\",)\n    raw_id_fields = (\"document\",)\n    search_fields = (\"document__title\",)\n\n    def get_queryset(self, request):  # pragma: no cover\n        return (\n            super()\n            .get_queryset(request)\n            .select_related(\"user\", \"document__correspondent\")\n        )\n\n\nclass ShareLinksAdmin(GuardedModelAdmin):\n    list_display = (\"created\", \"expiration\", \"document\")\n    list_filter = (\"created\", \"expiration\", \"owner\")\n    list_display_links = (\"created\",)\n    raw_id_fields = (\"document\",)\n\n    def get_queryset(self, request):  # pragma: no cover\n        return super().get_queryset(request).select_related(\"document__correspondent\")\n\n\nclass CustomFieldsAdmin(GuardedModelAdmin):\n    fields = (\"name\", \"created\", \"data_type\")\n    readonly_fields = (\"created\", \"data_type\")\n    list_display = (\"name\", \"created\", \"data_type\")\n    list_filter = (\"created\", \"data_type\")\n\n\nclass CustomFieldInstancesAdmin(GuardedModelAdmin):\n    fields = (\"field\", \"document\", \"created\", \"value\")\n    readonly_fields = (\"field\", \"document\", \"created\", \"value\")\n    list_display = (\"field\", \"document\", \"value\", \"created\")\n    search_fields = (\"document__title\",)\n    list_filter = (\"created\", \"field\")\n\n    def get_queryset(self, request):  # pragma: no cover\n        return (\n            super()\n            .get_queryset(request)\n            .select_related(\"field\", \"document__correspondent\")\n        )\n\n\nadmin.site.register(Correspondent, CorrespondentAdmin)\nadmin.site.register(Tag, TagAdmin)\nadmin.site.register(DocumentType, DocumentTypeAdmin)\nadmin.site.register(Document, DocumentAdmin)\nadmin.site.register(SavedView, SavedViewAdmin)\nadmin.site.register(StoragePath, StoragePathAdmin)\nadmin.site.register(PaperlessTask, TaskAdmin)\nadmin.site.register(Note, NotesAdmin)\nadmin.site.register(ShareLink, ShareLinksAdmin)\nadmin.site.register(CustomField, CustomFieldsAdmin)\nadmin.site.register(CustomFieldInstance, CustomFieldInstancesAdmin)\n\nif settings.AUDIT_LOG_ENABLED:\n\n    class LogEntryAUDIT(LogEntryAdmin):\n        def has_delete_permission(self, request, obj=None):\n            return False\n\n    admin.site.unregister(LogEntry)\n    admin.site.register(LogEntry, LogEntryAUDIT)\n", "metadata": {"source": "github.com/paperless-ngx/paperless-ngx", "file_path": "src/documents/admin.py", "language": "python", "complexity": "complex", "star_count": 29534, "quality_score": 0.98, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": true, "has_type_hints": true, "has_testing": false, "has_logging": true, "function_count": 11, "class_count": 14, "line_count": 215}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "database_integration", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create database model with ORM relationships and queries\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from __future__ import annotations\n\nimport logging\nimport re\nimport tempfile\nfrom dataclasses import dataclass\nfrom pathlib import Path\nfrom typing import TYPE_CHECKING\n\nfrom django.conf import settings\nfrom pdf2image import convert_from_path\nfrom pikepdf import Page\nfrom pikepdf import PasswordError\nfrom pikepdf import Pdf\n\nfrom documents.converters import convert_from_tiff_to_pdf\nfrom documents.data_models import ConsumableDocument\nfrom documents.data_models import DocumentMetadataOverrides\nfrom documents.models import Tag\nfrom documents.plugins.base import ConsumeTaskPlugin\nfrom documents.plugins.base import StopConsumeTaskError\nfrom documents.plugins.helpers import ProgressManager\nfrom documents.plugins.helpers import ProgressStatusOptions\nfrom documents.utils import copy_basic_file_stats\nfrom documents.utils import copy_file_with_basic_stats\nfrom documents.utils import maybe_override_pixel_limit\nfrom paperless.config import BarcodeConfig\n\nif TYPE_CHECKING:\n    from collections.abc import Callable\n\n    from PIL import Image\n\nlogger = logging.getLogger(\"paperless.barcodes\")\n\n\n@dataclass(frozen=True)\nclass Barcode:\n    \"\"\"\n    Holds the information about a single barcode and its location in a document\n    \"\"\"\n\n    page: int\n    value: str\n    settings: BarcodeConfig\n\n    @property\n    def is_separator(self) -> bool:\n        \"\"\"\n        Returns True if the barcode value equals the configured separation value,\n        False otherwise\n        \"\"\"\n        return self.value == self.settings.barcode_string\n\n    @property\n    def is_asn(self) -> bool:\n        \"\"\"\n        Returns True if the barcode value matches the configured ASN prefix,\n        False otherwise\n        \"\"\"\n        return self.value.startswith(self.settings.barcode_asn_prefix)\n\n\nclass BarcodePlugin(ConsumeTaskPlugin):\n    NAME: str = \"BarcodePlugin\"\n\n    @property\n    def able_to_run(self) -> bool:\n        \"\"\"\n        Able to run if:\n          - ASN from barcode detection is enabled or\n          - Barcode support is enabled and the mime type is supported\n        \"\"\"\n        if self.settings.barcode_enable_tiff_support:\n            supported_mimes: set[str] = {\"application/pdf\", \"image/tiff\"}\n        else:\n            supported_mimes = {\"application/pdf\"}\n\n        return (\n            self.settings.barcode_enable_asn\n            or self.settings.barcodes_enabled\n            or self.settings.barcode_enable_tag\n        ) and self.input_doc.mime_type in supported_mimes\n\n    def get_settings(self) -> BarcodeConfig:\n        \"\"\"\n        Returns the settings for this plugin (Django settings or app config)\n        \"\"\"\n        return BarcodeConfig()\n\n    def __init__(\n        self,\n        input_doc: ConsumableDocument,\n        metadata: DocumentMetadataOverrides,\n        status_mgr: ProgressManager,\n        base_tmp_dir: Path,\n        task_id: str,\n    ) -> None:\n        super().__init__(\n            input_doc,\n            metadata,\n            status_mgr,\n            base_tmp_dir,\n            task_id,\n        )\n        # need these for able_to_run\n        self.settings = self.get_settings()\n\n    def setup(self) -> None:\n        self.temp_dir = tempfile.TemporaryDirectory(\n            dir=self.base_tmp_dir,\n            prefix=\"barcode\",\n        )\n        self.pdf_file: Path = self.input_doc.original_file\n        self._tiff_conversion_done = False\n        self.barcodes: list[Barcode] = []\n\n    def run(self) -> None:\n        # Some operations may use PIL, override pixel setting if needed\n        maybe_override_pixel_limit()\n\n        # Maybe do the conversion of TIFF to PDF\n        self.convert_from_tiff_to_pdf()\n\n        # Locate any barcodes in the files\n        self.detect()\n\n        # try reading tags from barcodes\n        if (\n            self.settings.barcode_enable_tag\n            and (tags := self.tags) is not None\n            and len(tags) > 0\n        ):\n            if self.metadata.tag_ids:\n                self.metadata.tag_ids += tags\n            else:\n                self.metadata.tag_ids = tags\n            logger.info(f\"Found tags in barcode: {tags}\")\n\n        # Lastly attempt to split documents\n        if self.settings.barcodes_enabled and (\n            separator_pages := self.get_separation_pages()\n        ):\n            # We have pages to split against\n\n            # Note this does NOT use the base_temp_dir, as that will be removed\n            tmp_dir = Path(\n                tempfile.mkdtemp(\n                    dir=settings.SCRATCH_DIR,\n                    prefix=\"paperless-barcode-split-\",\n                ),\n            ).resolve()\n\n            from documents import tasks\n\n            # Create the split document tasks\n            for new_document in self.separate_pages(separator_pages):\n                copy_file_with_basic_stats(new_document, tmp_dir / new_document.name)\n\n                task = tasks.consume_file.delay(\n                    ConsumableDocument(\n                        # Same source, for templates\n                        source=self.input_doc.source,\n                        mailrule_id=self.input_doc.mailrule_id,\n                        # Can't use same folder or the consume might grab it again\n                        original_file=(tmp_dir / new_document.name).resolve(),\n                    ),\n                    # All the same metadata\n                    self.metadata,\n                )\n                logger.info(f\"Created new task {task.id} for {new_document.name}\")\n\n            # This file is now two or more files\n            self.input_doc.original_file.unlink()\n\n            msg = \"Barcode splitting complete!\"\n\n            # Update the progress to complete\n            self.status_mgr.send_progress(ProgressStatusOptions.SUCCESS, msg, 100, 100)\n\n            # Request the consume task stops\n            raise StopConsumeTaskError(msg)\n\n        # Update/overwrite an ASN if possible\n        # After splitting, as otherwise each split document gets the same ASN\n        if self.settings.barcode_enable_asn and (located_asn := self.asn) is not None:\n            logger.info(f\"Found ASN in barcode: {located_asn}\")\n            self.metadata.asn = located_asn\n\n    def cleanup(self) -> None:\n        self.temp_dir.cleanup()\n\n    def convert_from_tiff_to_pdf(self) -> None:\n        \"\"\"\n        May convert a TIFF image into a PDF, if the input is a TIFF and\n        the TIFF has not been made into a PDF\n        \"\"\"\n        # Nothing to do, pdf_file is already assigned correctly\n        if self.input_doc.mime_type != \"image/tiff\" or self._tiff_conversion_done:\n            return\n\n        self.pdf_file = convert_from_tiff_to_pdf(\n            self.input_doc.original_file,\n            Path(self.temp_dir.name),\n        )\n        self._tiff_conversion_done = True\n\n    @staticmethod\n    def read_barcodes_zxing(image: Image.Image) -> list[str]:\n        barcodes = []\n\n        import zxingcpp\n\n        detected_barcodes = zxingcpp.read_barcodes(image)\n        for barcode in detected_barcodes:\n            if barcode.text:\n                barcodes.append(barcode.text)\n                logger.debug(\n                    f\"Barcode of type {barcode.format} found: {barcode.text}\",\n                )\n\n        return barcodes\n\n    @staticmethod\n    def read_barcodes_pyzbar(image: Image.Image) -> list[str]:\n        barcodes = []\n\n        from pyzbar import pyzbar\n\n        # Decode the barcode image\n        detected_barcodes = pyzbar.decode(image)\n\n        # Traverse through all the detected barcodes in image\n        for barcode in detected_barcodes:\n            if barcode.data:\n                decoded_barcode = barcode.data.decode(\"utf-8\")\n                barcodes.append(decoded_barcode)\n                logger.debug(\n                    f\"Barcode of type {barcode.type} found: {decoded_barcode}\",\n                )\n\n        return barcodes\n\n    def detect(self) -> None:\n        \"\"\"\n        Scan all pages of the PDF as images, updating barcodes and the pages\n        found on as we go\n        \"\"\"\n        # Bail if barcodes already exist\n        if self.barcodes:\n            return\n\n        # No op if not a TIFF\n        self.convert_from_tiff_to_pdf()\n\n        # Choose the library for reading\n        if settings.CONSUMER_BARCODE_SCANNER == \"PYZBAR\":\n            reader: Callable[[Image.Image], list[str]] = self.read_barcodes_pyzbar\n            logger.debug(\"Scanning for barcodes using PYZBAR\")\n        else:\n            reader = self.read_barcodes_zxing\n            logger.debug(\"Scanning for barcodes using ZXING\")\n\n        try:\n            # Read number of pages from pdf\n            with Pdf.open(self.pdf_file) as pdf:\n                num_of_pages = len(pdf.pages)\n            logger.debug(f\"PDF has {num_of_pages} pages\")\n\n            # Get limit from configuration\n            barcode_max_pages: int = (\n                num_of_pages\n                if self.settings.barcode_max_pages == 0\n                else self.settings.barcode_max_pages\n            )\n\n            if barcode_max_pages < num_of_pages:  # pragma: no cover\n                logger.debug(\n                    f\"Barcodes detection will be limited to the first {barcode_max_pages} pages\",\n                )\n\n            # Loop al page\n            for current_page_number in range(min(num_of_pages, barcode_max_pages)):\n                logger.debug(f\"Processing page {current_page_number}\")\n\n                # Convert page to image\n                page = convert_from_path(\n                    self.pdf_file,\n                    dpi=self.settings.barcode_dpi,\n                    output_folder=self.temp_dir.name,\n                    first_page=current_page_number + 1,\n                    last_page=current_page_number + 1,\n                )[0]\n\n                # Remember filename, since it is lost by upscaling\n                page_filepath = Path(page.filename)\n                logger.debug(f\"Image is at {page_filepath}\")\n\n                # Upscale image if configured\n                factor = self.settings.barcode_upscale\n                if factor > 1.0:\n                    logger.debug(\n                        f\"Upscaling image by {factor} for better barcode detection\",\n                    )\n                    x, y = page.size\n                    page = page.resize(\n                        (round(x * factor), (round(y * factor))),\n                    )\n\n                # Detect barcodes\n                for barcode_value in reader(page):\n                    self.barcodes.append(\n                        Barcode(current_page_number, barcode_value, self.settings),\n                    )\n\n                # Delete temporary image file\n                page_filepath.unlink()\n\n        # Password protected files can't be checked\n        # This is the exception raised for those\n        except PasswordError as e:\n            logger.warning(\n                f\"File is likely password protected, not checking for barcodes: {e}\",\n            )\n        # This file is really borked, allow the consumption to continue\n        # but it may fail further on\n        except Exception as e:  # pragma: no cover\n            logger.warning(\n                f\"Exception during barcode scanning: {e}\",\n            )\n\n    @property\n    def asn(self) -> int | None:\n        \"\"\"\n        Search the parsed barcodes for any ASNs.\n        The first barcode that starts with barcode_asn_prefix\n        is considered the ASN to be used.\n        Returns the detected ASN (or None)\n        \"\"\"\n        asn = None\n\n        # Ensure the barcodes have been read\n        self.detect()\n\n        # get the first barcode that starts with barcode_asn_prefix\n        asn_text: str | None = next(\n            (x.value for x in self.barcodes if x.is_asn),\n            None,\n        )\n\n        if asn_text:\n            logger.debug(f\"Found ASN Barcode: {asn_text}\")\n            # remove the prefix and remove whitespace\n            asn_text = asn_text[len(self.settings.barcode_asn_prefix) :].strip()\n\n            # remove non-numeric parts of the remaining string\n            asn_text = re.sub(r\"\\D\", \"\", asn_text)\n\n            # now, try parsing the ASN number\n            try:\n                asn = int(asn_text)\n            except ValueError as e:\n                logger.warning(f\"Failed to parse ASN number because: {e}\")\n\n        return asn\n\n    @property\n    def tags(self) -> list[int]:\n        \"\"\"\n        Search the parsed barcodes for any tags.\n        Returns the detected tag ids (or empty list)\n        \"\"\"\n        tags: list[int] = []\n\n        # Ensure the barcodes have been read\n        self.detect()\n\n        for x in self.barcodes:\n            tag_texts: str = x.value\n\n            for raw in tag_texts.split(\",\"):\n                try:\n                    tag_str: str | None = None\n                    for regex in self.settings.barcode_tag_mapping:\n                        if re.match(regex, raw, flags=re.IGNORECASE):\n                            sub = self.settings.barcode_tag_mapping[regex]\n                            tag_str = (\n                                re.sub(regex, sub, raw, flags=re.IGNORECASE)\n                                if sub\n                                else raw\n                            )\n                            break\n\n                    if tag_str:\n                        tag, _ = Tag.objects.get_or_create(\n                            name__iexact=tag_str,\n                            defaults={\"name\": tag_str},\n                        )\n\n                        logger.debug(\n                            f\"Found Tag Barcode '{raw}', substituted \"\n                            f\"to '{tag}' and mapped to \"\n                            f\"tag #{tag.pk}.\",\n                        )\n                        tags.append(tag.pk)\n\n                except Exception as e:\n                    logger.error(\n                        f\"Failed to find or create TAG '{raw}' because: {e}\",\n                    )\n\n        return tags\n\n    def get_separation_pages(self) -> dict[int, bool]:\n        \"\"\"\n        Search the parsed barcodes for separators and returns a dict of page\n        numbers, which separate the file into new files, together with the\n        information whether to keep the page.\n        \"\"\"\n        # filter all barcodes for the separator string\n        # get the page numbers of the separating barcodes\n        retain = self.settings.barcode_retain_split_pages\n        separator_pages = {\n            bc.page: retain\n            for bc in self.barcodes\n            if bc.is_separator and (not retain or (retain and bc.page > 0))\n        }  # as below, dont include the first page if retain is enabled\n        if not self.settings.barcode_enable_asn:\n            return separator_pages\n\n        # add the page numbers of the ASN barcodes\n        # (except for first page, that might lead to infinite loops).\n        return {\n            **separator_pages,\n            **{bc.page: True for bc in self.barcodes if bc.is_asn and bc.page != 0},\n        }\n\n    def separate_pages(self, pages_to_split_on: dict[int, bool]) -> list[Path]:\n        \"\"\"\n        Separate the provided pdf file on the pages_to_split_on.\n        The pages which are defined by the keys in page_numbers\n        will be removed if the corresponding value is false.\n        Returns a list of (temporary) filepaths to consume.\n        These will need to be deleted later.\n        \"\"\"\n\n        document_paths = []\n        fname: str = self.input_doc.original_file.stem\n        with Pdf.open(self.pdf_file) as input_pdf:\n            # Start with an empty document\n            current_document: list[Page] = []\n            # A list of documents, ie a list of lists of pages\n            documents: list[list[Page]] = [current_document]\n\n            for idx, page in enumerate(input_pdf.pages):\n                # Keep building the new PDF as long as it is not a\n                # separator index\n                if idx not in pages_to_split_on:\n                    current_document.append(page)\n                    continue\n\n                # This is a split index\n                # Start a new destination page listing\n                logger.debug(f\"Starting new document at idx {idx}\")\n                current_document = []\n                documents.append(current_document)\n                keep_page: bool = pages_to_split_on[idx]\n                if keep_page:\n                    # Keep the page\n                    # (new document is started by asn barcode)\n                    current_document.append(page)\n\n            documents = [x for x in documents if len(x)]\n\n            logger.debug(f\"Split into {len(documents)} new documents\")\n\n            # Write the new documents out\n            for doc_idx, document in enumerate(documents):\n                dst = Pdf.new()\n                dst.pages.extend(document)\n\n                output_filename = f\"{fname}_document_{doc_idx}.pdf\"\n\n                logger.debug(f\"pdf no:{doc_idx} has {len(dst.pages)} pages\")\n                savepath = Path(self.temp_dir.name) / output_filename\n                with savepath.open(\"wb\") as out:\n                    dst.save(out)\n\n                copy_basic_file_stats(self.input_doc.original_file, savepath)\n\n                document_paths.append(savepath)\n\n            return document_paths\n", "metadata": {"source": "github.com/paperless-ngx/paperless-ngx", "file_path": "src/documents/barcodes.py", "language": "python", "complexity": "complex", "star_count": 29534, "quality_score": 1.0, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": true, "has_documentation": true, "has_type_hints": true, "has_testing": false, "has_logging": true, "function_count": 16, "class_count": 2, "line_count": 494}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "database_integration", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create database model with ORM relationships and queries\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from __future__ import annotations\n\nfrom pathlib import Path\nfrom typing import TYPE_CHECKING\nfrom typing import NoReturn\n\nif TYPE_CHECKING:\n    from collections.abc import Callable\n    from zipfile import ZipFile\n\n    from documents.models import Document\n\n\nclass BulkArchiveStrategy:\n    def __init__(self, zipf: ZipFile, *, follow_formatting: bool = False) -> None:\n        self.zipf: ZipFile = zipf\n        if follow_formatting:\n            self.make_unique_filename: Callable[..., Path | str] = (\n                self._formatted_filepath\n            )\n        else:\n            self.make_unique_filename = self._filename_only\n\n    def _filename_only(\n        self,\n        doc: Document,\n        *,\n        archive: bool = False,\n        folder: str = \"\",\n    ) -> str:\n        \"\"\"\n        Constructs a unique name for the given document to be used inside the\n        zip file.\n\n        The filename might not be unique enough, so a counter is appended if needed\n        \"\"\"\n        counter = 0\n        while True:\n            filename: str = folder + doc.get_public_filename(\n                archive=archive,\n                counter=counter,\n            )\n            if filename in self.zipf.namelist():\n                counter += 1\n            else:\n                return filename\n\n    def _formatted_filepath(\n        self,\n        doc: Document,\n        *,\n        archive: bool = False,\n        folder: str = \"\",\n    ) -> Path:\n        \"\"\"\n        Constructs a full file path for the given document to be used inside\n        the zipfile.\n\n        The path is already unique, as handled when a document is consumed or updated\n        \"\"\"\n        if archive and doc.has_archive_version:\n            if TYPE_CHECKING:\n                assert doc.archive_filename is not None\n            in_archive_path: Path = Path(folder) / doc.archive_filename\n        else:\n            if TYPE_CHECKING:\n                assert doc.filename is not None\n            in_archive_path = Path(folder) / doc.filename\n\n        return in_archive_path\n\n    def add_document(self, doc: Document) -> NoReturn:\n        raise NotImplementedError  # pragma: no cover\n\n\nclass OriginalsOnlyStrategy(BulkArchiveStrategy):\n    def add_document(self, doc: Document) -> None:\n        self.zipf.write(doc.source_path, self.make_unique_filename(doc))\n\n\nclass ArchiveOnlyStrategy(BulkArchiveStrategy):\n    def add_document(self, doc: Document) -> None:\n        if doc.has_archive_version:\n            if TYPE_CHECKING:\n                assert doc.archive_path is not None\n            self.zipf.write(\n                doc.archive_path,\n                self.make_unique_filename(doc, archive=True),\n            )\n        else:\n            self.zipf.write(doc.source_path, self.make_unique_filename(doc))\n\n\nclass OriginalAndArchiveStrategy(BulkArchiveStrategy):\n    def add_document(self, doc: Document) -> None:\n        if doc.has_archive_version:\n            if TYPE_CHECKING:\n                assert doc.archive_path is not None\n            self.zipf.write(\n                doc.archive_path,\n                self.make_unique_filename(doc, archive=True, folder=\"archive/\"),\n            )\n\n        self.zipf.write(\n            doc.source_path,\n            self.make_unique_filename(doc, folder=\"originals/\"),\n        )\n", "metadata": {"source": "github.com/paperless-ngx/paperless-ngx", "file_path": "src/documents/bulk_download.py", "language": "python", "complexity": "complex", "star_count": 29534, "quality_score": 0.98, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": true, "has_type_hints": true, "has_testing": true, "has_logging": false, "function_count": 7, "class_count": 4, "line_count": 108}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "database_integration", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create database model with ORM relationships and queries\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from __future__ import annotations\n\nimport hashlib\nimport itertools\nimport logging\nimport tempfile\nfrom pathlib import Path\nfrom typing import TYPE_CHECKING\nfrom typing import Literal\n\nfrom celery import chain\nfrom celery import chord\nfrom celery import group\nfrom celery import shared_task\nfrom django.conf import settings\nfrom django.db.models import Q\nfrom django.utils import timezone\n\nfrom documents.data_models import ConsumableDocument\nfrom documents.data_models import DocumentMetadataOverrides\nfrom documents.data_models import DocumentSource\nfrom documents.models import Correspondent\nfrom documents.models import CustomField\nfrom documents.models import CustomFieldInstance\nfrom documents.models import Document\nfrom documents.models import DocumentType\nfrom documents.models import StoragePath\nfrom documents.permissions import set_permissions_for_object\nfrom documents.plugins.helpers import DocumentsStatusManager\nfrom documents.tasks import bulk_update_documents\nfrom documents.tasks import consume_file\nfrom documents.tasks import update_document_content_maybe_archive_file\n\nif TYPE_CHECKING:\n    from django.contrib.auth.models import User\n\nlogger: logging.Logger = logging.getLogger(\"paperless.bulk_edit\")\n\n\ndef set_correspondent(\n    doc_ids: list[int],\n    correspondent: Correspondent,\n) -> Literal[\"OK\"]:\n    if correspondent:\n        correspondent = Correspondent.objects.only(\"pk\").get(id=correspondent)\n\n    qs = (\n        Document.objects.filter(Q(id__in=doc_ids) & ~Q(correspondent=correspondent))\n        .select_related(\"correspondent\")\n        .only(\"pk\", \"correspondent__id\")\n    )\n    affected_docs = list(qs.values_list(\"pk\", flat=True))\n    qs.update(correspondent=correspondent)\n\n    bulk_update_documents.delay(document_ids=affected_docs)\n\n    return \"OK\"\n\n\ndef set_storage_path(doc_ids: list[int], storage_path: StoragePath) -> Literal[\"OK\"]:\n    if storage_path:\n        storage_path = StoragePath.objects.only(\"pk\").get(id=storage_path)\n\n    qs = (\n        Document.objects.filter(\n            Q(id__in=doc_ids) & ~Q(storage_path=storage_path),\n        )\n        .select_related(\"storage_path\")\n        .only(\"pk\", \"storage_path__id\")\n    )\n    affected_docs = list(qs.values_list(\"pk\", flat=True))\n    qs.update(storage_path=storage_path)\n\n    bulk_update_documents.delay(\n        document_ids=affected_docs,\n    )\n\n    return \"OK\"\n\n\ndef set_document_type(doc_ids: list[int], document_type: DocumentType) -> Literal[\"OK\"]:\n    if document_type:\n        document_type = DocumentType.objects.only(\"pk\").get(id=document_type)\n\n    qs = (\n        Document.objects.filter(Q(id__in=doc_ids) & ~Q(document_type=document_type))\n        .select_related(\"document_type\")\n        .only(\"pk\", \"document_type__id\")\n    )\n    affected_docs = list(qs.values_list(\"pk\", flat=True))\n    qs.update(document_type=document_type)\n\n    bulk_update_documents.delay(document_ids=affected_docs)\n\n    return \"OK\"\n\n\ndef add_tag(doc_ids: list[int], tag: int) -> Literal[\"OK\"]:\n    qs = Document.objects.filter(Q(id__in=doc_ids) & ~Q(tags__id=tag)).only(\"pk\")\n    affected_docs = list(qs.values_list(\"pk\", flat=True))\n\n    DocumentTagRelationship = Document.tags.through\n\n    DocumentTagRelationship.objects.bulk_create(\n        [DocumentTagRelationship(document_id=doc, tag_id=tag) for doc in affected_docs],\n    )\n\n    bulk_update_documents.delay(document_ids=affected_docs)\n\n    return \"OK\"\n\n\ndef remove_tag(doc_ids: list[int], tag: int) -> Literal[\"OK\"]:\n    qs = Document.objects.filter(Q(id__in=doc_ids) & Q(tags__id=tag)).only(\"pk\")\n    affected_docs = list(qs.values_list(\"pk\", flat=True))\n\n    DocumentTagRelationship = Document.tags.through\n\n    DocumentTagRelationship.objects.filter(\n        Q(document_id__in=affected_docs) & Q(tag_id=tag),\n    ).delete()\n\n    bulk_update_documents.delay(document_ids=affected_docs)\n\n    return \"OK\"\n\n\ndef modify_tags(\n    doc_ids: list[int],\n    add_tags: list[int],\n    remove_tags: list[int],\n) -> Literal[\"OK\"]:\n    qs = Document.objects.filter(id__in=doc_ids).only(\"pk\")\n    affected_docs = list(qs.values_list(\"pk\", flat=True))\n\n    DocumentTagRelationship = Document.tags.through\n\n    DocumentTagRelationship.objects.filter(\n        document_id__in=affected_docs,\n        tag_id__in=remove_tags,\n    ).delete()\n\n    DocumentTagRelationship.objects.bulk_create(\n        [\n            DocumentTagRelationship(document_id=doc, tag_id=tag)\n            for (doc, tag) in itertools.product(affected_docs, add_tags)\n        ],\n        ignore_conflicts=True,\n    )\n\n    bulk_update_documents.delay(document_ids=affected_docs)\n\n    return \"OK\"\n\n\ndef modify_custom_fields(\n    doc_ids: list[int],\n    add_custom_fields: list[int] | dict,\n    remove_custom_fields: list[int],\n) -> Literal[\"OK\"]:\n    qs = Document.objects.filter(id__in=doc_ids).only(\"pk\")\n    affected_docs = list(qs.values_list(\"pk\", flat=True))\n    # Ensure add_custom_fields is a list of tuples, supports old API\n    add_custom_fields = (\n        add_custom_fields.items()\n        if isinstance(add_custom_fields, dict)\n        else [(field, None) for field in add_custom_fields]\n    )\n\n    custom_fields = CustomField.objects.filter(\n        id__in=[int(field) for field, _ in add_custom_fields],\n    ).distinct()\n    for field_id, value in add_custom_fields:\n        for doc_id in affected_docs:\n            defaults = {}\n            custom_field = custom_fields.get(id=field_id)\n            if custom_field:\n                value_field = CustomFieldInstance.TYPE_TO_DATA_STORE_NAME_MAP[\n                    custom_field.data_type\n                ]\n                defaults[value_field] = value\n                if (\n                    custom_field.data_type == CustomField.FieldDataType.DOCUMENTLINK\n                    and doc_id in value\n                ):\n                    # Prevent self-linking\n                    continue\n            CustomFieldInstance.objects.update_or_create(\n                document_id=doc_id,\n                field_id=field_id,\n                defaults=defaults,\n            )\n            if custom_field.data_type == CustomField.FieldDataType.DOCUMENTLINK:\n                doc = Document.objects.get(id=doc_id)\n                reflect_doclinks(doc, custom_field, value)\n\n    # For doc link fields that are being removed, remove symmetrical links\n    for doclink_being_removed_instance in CustomFieldInstance.objects.filter(\n        document_id__in=affected_docs,\n        field__id__in=remove_custom_fields,\n        field__data_type=CustomField.FieldDataType.DOCUMENTLINK,\n        value_document_ids__isnull=False,\n    ):\n        for target_doc_id in doclink_being_removed_instance.value:\n            remove_doclink(\n                document=Document.objects.get(\n                    id=doclink_being_removed_instance.document.id,\n                ),\n                field=doclink_being_removed_instance.field,\n                target_doc_id=target_doc_id,\n            )\n\n    # Finally, remove the custom fields\n    CustomFieldInstance.objects.filter(\n        document_id__in=affected_docs,\n        field_id__in=remove_custom_fields,\n    ).hard_delete()\n\n    bulk_update_documents.delay(document_ids=affected_docs)\n\n    return \"OK\"\n\n\n@shared_task\ndef delete(doc_ids: list[int]) -> Literal[\"OK\"]:\n    try:\n        Document.objects.filter(id__in=doc_ids).delete()\n\n        from documents import index\n\n        with index.open_index_writer() as writer:\n            for id in doc_ids:\n                index.remove_document_by_id(writer, id)\n\n        status_mgr = DocumentsStatusManager()\n        status_mgr.send_documents_deleted(doc_ids)\n    except Exception as e:\n        if \"Data too long for column\" in str(e):\n            logger.warning(\n                \"Detected a possible incompatible database column. See https://docs.paperless-ngx.com/troubleshooting/#convert-uuid-field\",\n            )\n        logger.error(f\"Error deleting documents: {e!s}\")\n\n    return \"OK\"\n\n\ndef reprocess(doc_ids: list[int]) -> Literal[\"OK\"]:\n    for document_id in doc_ids:\n        update_document_content_maybe_archive_file.delay(\n            document_id=document_id,\n        )\n\n    return \"OK\"\n\n\ndef set_permissions(\n    doc_ids: list[int],\n    set_permissions,\n    *,\n    owner=None,\n    merge=False,\n) -> Literal[\"OK\"]:\n    qs = Document.objects.filter(id__in=doc_ids).select_related(\"owner\")\n\n    if merge:\n        # If merging, only set owner for documents that don't have an owner\n        qs.filter(owner__isnull=True).update(owner=owner)\n    else:\n        qs.update(owner=owner)\n\n    for doc in qs:\n        set_permissions_for_object(permissions=set_permissions, object=doc, merge=merge)\n\n    affected_docs = list(qs.values_list(\"pk\", flat=True))\n\n    bulk_update_documents.delay(document_ids=affected_docs)\n\n    return \"OK\"\n\n\ndef rotate(doc_ids: list[int], degrees: int) -> Literal[\"OK\"]:\n    logger.info(\n        f\"Attempting to rotate {len(doc_ids)} documents by {degrees} degrees.\",\n    )\n    qs = Document.objects.filter(id__in=doc_ids)\n    affected_docs: list[int] = []\n    import pikepdf\n\n    rotate_tasks = []\n    for doc in qs:\n        if doc.mime_type != \"application/pdf\":\n            logger.warning(\n                f\"Document {doc.id} is not a PDF, skipping rotation.\",\n            )\n            continue\n        try:\n            with pikepdf.open(doc.source_path, allow_overwriting_input=True) as pdf:\n                for page in pdf.pages:\n                    page.rotate(degrees, relative=True)\n                pdf.save()\n                doc.checksum = hashlib.md5(doc.source_path.read_bytes()).hexdigest()\n                doc.save()\n                rotate_tasks.append(\n                    update_document_content_maybe_archive_file.s(\n                        document_id=doc.id,\n                    ),\n                )\n                logger.info(\n                    f\"Rotated document {doc.id} by {degrees} degrees\",\n                )\n                affected_docs.append(doc.id)\n        except Exception as e:\n            logger.exception(f\"Error rotating document {doc.id}: {e}\")\n\n    if len(affected_docs) > 0:\n        bulk_update_task = bulk_update_documents.si(document_ids=affected_docs)\n        chord(header=rotate_tasks, body=bulk_update_task).delay()\n\n    return \"OK\"\n\n\ndef merge(\n    doc_ids: list[int],\n    *,\n    metadata_document_id: int | None = None,\n    delete_originals: bool = False,\n    archive_fallback: bool = False,\n    user: User | None = None,\n) -> Literal[\"OK\"]:\n    logger.info(\n        f\"Attempting to merge {len(doc_ids)} documents into a single document.\",\n    )\n    qs = Document.objects.filter(id__in=doc_ids)\n    affected_docs: list[int] = []\n    import pikepdf\n\n    merged_pdf = pikepdf.new()\n    version: str = merged_pdf.pdf_version\n    # use doc_ids to preserve order\n    for doc_id in doc_ids:\n        doc = qs.get(id=doc_id)\n        try:\n            doc_path = (\n                doc.archive_path\n                if archive_fallback\n                and doc.mime_type != \"application/pdf\"\n                and doc.has_archive_version\n                else doc.source_path\n            )\n            with pikepdf.open(str(doc_path)) as pdf:\n                version = max(version, pdf.pdf_version)\n                merged_pdf.pages.extend(pdf.pages)\n            affected_docs.append(doc.id)\n        except Exception as e:\n            logger.exception(\n                f\"Error merging document {doc.id}, it will not be included in the merge: {e}\",\n            )\n    if len(affected_docs) == 0:\n        logger.warning(\"No documents were merged\")\n        return \"OK\"\n\n    filepath = (\n        Path(\n            tempfile.mkdtemp(dir=settings.SCRATCH_DIR),\n        )\n        / f\"{'_'.join([str(doc_id) for doc_id in affected_docs])[:100]}_merged.pdf\"\n    )\n    merged_pdf.remove_unreferenced_resources()\n    merged_pdf.save(filepath, min_version=version)\n    merged_pdf.close()\n\n    if metadata_document_id:\n        metadata_document = qs.get(id=metadata_document_id)\n        if metadata_document is not None:\n            overrides: DocumentMetadataOverrides = (\n                DocumentMetadataOverrides.from_document(metadata_document)\n            )\n            overrides.title = metadata_document.title + \" (merged)\"\n        else:\n            overrides = DocumentMetadataOverrides()\n    else:\n        overrides = DocumentMetadataOverrides()\n\n    if user is not None:\n        overrides.owner_id = user.id\n\n    logger.info(\"Adding merged document to the task queue.\")\n\n    consume_task = consume_file.s(\n        ConsumableDocument(\n            source=DocumentSource.ConsumeFolder,\n            original_file=filepath,\n        ),\n        overrides,\n    )\n\n    if delete_originals:\n        logger.info(\n            \"Queueing removal of original documents after consumption of merged document\",\n        )\n        chain(consume_task, delete.si(affected_docs)).delay()\n    else:\n        consume_task.delay()\n\n    return \"OK\"\n\n\ndef split(\n    doc_ids: list[int],\n    pages: list[list[int]],\n    *,\n    delete_originals: bool = False,\n    user: User | None = None,\n) -> Literal[\"OK\"]:\n    logger.info(\n        f\"Attempting to split document {doc_ids[0]} into {len(pages)} documents\",\n    )\n    doc = Document.objects.get(id=doc_ids[0])\n    import pikepdf\n\n    consume_tasks = []\n\n    try:\n        with pikepdf.open(doc.source_path) as pdf:\n            for idx, split_doc in enumerate(pages):\n                dst: pikepdf.Pdf = pikepdf.new()\n                for page in split_doc:\n                    dst.pages.append(pdf.pages[page - 1])\n                filepath: Path = (\n                    Path(\n                        tempfile.mkdtemp(dir=settings.SCRATCH_DIR),\n                    )\n                    / f\"{doc.id}_{split_doc[0]}-{split_doc[-1]}.pdf\"\n                )\n                dst.remove_unreferenced_resources()\n                dst.save(filepath)\n                dst.close()\n\n                overrides: DocumentMetadataOverrides = (\n                    DocumentMetadataOverrides().from_document(doc)\n                )\n                overrides.title = f\"{doc.title} (split {idx + 1})\"\n                if user is not None:\n                    overrides.owner_id = user.id\n                logger.info(\n                    f\"Adding split document with pages {split_doc} to the task queue.\",\n                )\n                consume_tasks.append(\n                    consume_file.s(\n                        ConsumableDocument(\n                            source=DocumentSource.ConsumeFolder,\n                            original_file=filepath,\n                        ),\n                        overrides,\n                    ),\n                )\n\n            if delete_originals:\n                logger.info(\n                    \"Queueing removal of original document after consumption of the split documents\",\n                )\n                chord(header=consume_tasks, body=delete.si([doc.id])).delay()\n            else:\n                group(consume_tasks).delay()\n\n    except Exception as e:\n        logger.exception(f\"Error splitting document {doc.id}: {e}\")\n\n    return \"OK\"\n\n\ndef delete_pages(doc_ids: list[int], pages: list[int]) -> Literal[\"OK\"]:\n    logger.info(\n        f\"Attempting to delete pages {pages} from {len(doc_ids)} documents\",\n    )\n    doc = Document.objects.get(id=doc_ids[0])\n    pages = sorted(pages)  # sort pages to avoid index issues\n    import pikepdf\n\n    try:\n        with pikepdf.open(doc.source_path, allow_overwriting_input=True) as pdf:\n            offset = 1  # pages are 1-indexed\n            for page_num in pages:\n                pdf.pages.remove(pdf.pages[page_num - offset])\n                offset += 1  # remove() changes the index of the pages\n            pdf.remove_unreferenced_resources()\n            pdf.save()\n            doc.checksum = hashlib.md5(doc.source_path.read_bytes()).hexdigest()\n            if doc.page_count is not None:\n                doc.page_count = doc.page_count - len(pages)\n            doc.save()\n            update_document_content_maybe_archive_file.delay(document_id=doc.id)\n            logger.info(f\"Deleted pages {pages} from document {doc.id}\")\n    except Exception as e:\n        logger.exception(f\"Error deleting pages from document {doc.id}: {e}\")\n\n    return \"OK\"\n\n\ndef reflect_doclinks(\n    document: Document,\n    field: CustomField,\n    target_doc_ids: list[int],\n):\n    \"\"\"\n    Add or remove 'symmetrical' links to `document` on all `target_doc_ids`\n    \"\"\"\n\n    if target_doc_ids is None:\n        target_doc_ids = []\n\n    # Check if any documents are going to be removed from the current list of links and remove the symmetrical links\n    current_field_instance = CustomFieldInstance.objects.filter(\n        field=field,\n        document=document,\n    ).first()\n    if current_field_instance is not None and current_field_instance.value is not None:\n        for doc_id in current_field_instance.value:\n            if doc_id not in target_doc_ids:\n                remove_doclink(\n                    document=document,\n                    field=field,\n                    target_doc_id=doc_id,\n                )\n\n    # Create an instance if target doc doesn't have this field or append it to an existing one\n    existing_custom_field_instances = {\n        custom_field.document_id: custom_field\n        for custom_field in CustomFieldInstance.objects.filter(\n            field=field,\n            document_id__in=target_doc_ids,\n        )\n    }\n    custom_field_instances_to_create = []\n    custom_field_instances_to_update = []\n    for target_doc_id in target_doc_ids:\n        target_doc_field_instance = existing_custom_field_instances.get(\n            target_doc_id,\n        )\n        if target_doc_field_instance is None:\n            custom_field_instances_to_create.append(\n                CustomFieldInstance(\n                    document_id=target_doc_id,\n                    field=field,\n                    value_document_ids=[document.id],\n                ),\n            )\n        elif target_doc_field_instance.value is None:\n            target_doc_field_instance.value_document_ids = [document.id]\n            custom_field_instances_to_update.append(target_doc_field_instance)\n        elif document.id not in target_doc_field_instance.value:\n            target_doc_field_instance.value_document_ids.append(document.id)\n            custom_field_instances_to_update.append(target_doc_field_instance)\n\n    CustomFieldInstance.objects.bulk_create(custom_field_instances_to_create)\n    CustomFieldInstance.objects.bulk_update(\n        custom_field_instances_to_update,\n        [\"value_document_ids\"],\n    )\n    Document.objects.filter(id__in=target_doc_ids).update(modified=timezone.now())\n\n\ndef remove_doclink(\n    document: Document,\n    field: CustomField,\n    target_doc_id: int,\n):\n    \"\"\"\n    Removes a 'symmetrical' link to `document` from the target document's existing custom field instance\n    \"\"\"\n    target_doc_field_instance = CustomFieldInstance.objects.filter(\n        document_id=target_doc_id,\n        field=field,\n    ).first()\n    if (\n        target_doc_field_instance is not None\n        and document.id in target_doc_field_instance.value\n    ):\n        target_doc_field_instance.value.remove(document.id)\n        target_doc_field_instance.save()\n    Document.objects.filter(id=target_doc_id).update(modified=timezone.now())\n", "metadata": {"source": "github.com/paperless-ngx/paperless-ngx", "file_path": "src/documents/bulk_edit.py", "language": "python", "complexity": "complex", "star_count": 29534, "quality_score": 1.0, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": true, "has_documentation": true, "has_type_hints": true, "has_testing": false, "has_logging": true, "function_count": 16, "class_count": 0, "line_count": 582}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "database_integration", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create database model with ORM relationships and queries\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from __future__ import annotations\n\nimport logging\nfrom binascii import hexlify\nfrom dataclasses import dataclass\nfrom typing import TYPE_CHECKING\nfrom typing import Final\n\nfrom django.core.cache import cache\n\nfrom documents.models import Document\n\nif TYPE_CHECKING:\n    from documents.classifier import DocumentClassifier\n\nlogger = logging.getLogger(\"paperless.caching\")\n\n\n@dataclass(frozen=True)\nclass MetadataCacheData:\n    original_checksum: str\n    original_metadata: list\n    archive_checksum: str | None\n    archive_metadata: list | None\n\n\n@dataclass(frozen=True)\nclass SuggestionCacheData:\n    classifier_version: int\n    classifier_hash: str\n    suggestions: dict\n\n\nCLASSIFIER_VERSION_KEY: Final[str] = \"classifier_version\"\nCLASSIFIER_HASH_KEY: Final[str] = \"classifier_hash\"\nCLASSIFIER_MODIFIED_KEY: Final[str] = \"classifier_modified\"\n\nCACHE_1_MINUTE: Final[int] = 60\nCACHE_5_MINUTES: Final[int] = 5 * CACHE_1_MINUTE\nCACHE_50_MINUTES: Final[int] = 50 * CACHE_1_MINUTE\n\n\ndef get_suggestion_cache_key(document_id: int) -> str:\n    \"\"\"\n    Returns the basic key for a document's suggestions\n    \"\"\"\n    return f\"doc_{document_id}_suggest\"\n\n\ndef get_suggestion_cache(document_id: int) -> SuggestionCacheData | None:\n    \"\"\"\n    If possible, return the cached suggestions for the given document ID.\n    The classifier needs to be matching in format and hash and the suggestions need to\n    have been cached once.\n    \"\"\"\n    from documents.classifier import DocumentClassifier\n\n    doc_key = get_suggestion_cache_key(document_id)\n    cache_hits = cache.get_many([CLASSIFIER_VERSION_KEY, CLASSIFIER_HASH_KEY, doc_key])\n    # The document suggestions are in the cache\n    if doc_key in cache_hits:\n        doc_suggestions: SuggestionCacheData = cache_hits[doc_key]\n        # The classifier format is the same\n        # The classifier hash is the same\n        # Then the suggestions can be used\n        if (\n            CLASSIFIER_VERSION_KEY in cache_hits\n            and cache_hits[CLASSIFIER_VERSION_KEY] == DocumentClassifier.FORMAT_VERSION\n            and cache_hits[CLASSIFIER_VERSION_KEY] == doc_suggestions.classifier_version\n        ) and (\n            CLASSIFIER_HASH_KEY in cache_hits\n            and cache_hits[CLASSIFIER_HASH_KEY] == doc_suggestions.classifier_hash\n        ):\n            return doc_suggestions\n        else:  # pragma: no cover\n            # Remove the key because something didn't match\n            cache.delete(doc_key)\n    return None\n\n\ndef set_suggestions_cache(\n    document_id: int,\n    suggestions: dict,\n    classifier: DocumentClassifier | None,\n    *,\n    timeout=CACHE_50_MINUTES,\n) -> None:\n    \"\"\"\n    Caches the given suggestions, which were generated by the given classifier.  If there is no classifier,\n    this function is a no-op (there won't be suggestions then anyway)\n    \"\"\"\n    if classifier is not None:\n        doc_key = get_suggestion_cache_key(document_id)\n        cache.set(\n            doc_key,\n            SuggestionCacheData(\n                classifier.FORMAT_VERSION,\n                hexlify(classifier.last_auto_type_hash).decode(),\n                suggestions,\n            ),\n            timeout,\n        )\n\n\ndef refresh_suggestions_cache(\n    document_id: int,\n    *,\n    timeout: int = CACHE_50_MINUTES,\n) -> None:\n    \"\"\"\n    Refreshes the expiration of the suggestions for the given document ID\n    to the given timeout\n    \"\"\"\n    doc_key = get_suggestion_cache_key(document_id)\n    cache.touch(doc_key, timeout)\n\n\ndef get_metadata_cache_key(document_id: int) -> str:\n    \"\"\"\n    Returns the basic key for a document's metadata\n    \"\"\"\n    return f\"doc_{document_id}_metadata\"\n\n\ndef get_metadata_cache(document_id: int) -> MetadataCacheData | None:\n    \"\"\"\n    Returns the cached document metadata for the given document ID, as long as the metadata\n    was cached once and the checksums have not changed\n    \"\"\"\n    doc_key = get_metadata_cache_key(document_id)\n    doc_metadata: MetadataCacheData | None = cache.get(doc_key)\n    # The metadata exists in the cache\n    if doc_metadata is not None:\n        try:\n            doc = Document.objects.only(\n                \"pk\",\n                \"checksum\",\n                \"archive_checksum\",\n                \"archive_filename\",\n            ).get(pk=document_id)\n            # The original checksums match\n            # If it has one, the archive checksums match\n            # Then, we can use the metadata\n            if (\n                doc_metadata.original_checksum == doc.checksum\n                and doc.has_archive_version\n                and doc_metadata.archive_checksum is not None\n                and doc_metadata.archive_checksum == doc.archive_checksum\n            ):\n                # Refresh cache\n                cache.touch(doc_key, CACHE_50_MINUTES)\n                return doc_metadata\n            else:  # pragma: no cover\n                # Something didn't match, delete the key\n                cache.delete(doc_key)\n        except Document.DoesNotExist:  # pragma: no cover\n            # Basically impossible, but the key existed, but the Document didn't\n            cache.delete(doc_key)\n    return None\n\n\ndef set_metadata_cache(\n    document: Document,\n    original_metadata: list,\n    archive_metadata: list | None,\n    *,\n    timeout=CACHE_50_MINUTES,\n) -> None:\n    \"\"\"\n    Sets the metadata into cache for the given Document\n    \"\"\"\n    doc_key = get_metadata_cache_key(document.pk)\n    cache.set(\n        doc_key,\n        MetadataCacheData(\n            document.checksum,\n            original_metadata,\n            document.archive_checksum,\n            archive_metadata,\n        ),\n        timeout,\n    )\n\n\ndef refresh_metadata_cache(\n    document_id: int,\n    *,\n    timeout: int = CACHE_50_MINUTES,\n) -> None:\n    \"\"\"\n    Refreshes the expiration of the metadata for the given document ID\n    to the given timeout\n    \"\"\"\n    doc_key = get_metadata_cache_key(document_id)\n    cache.touch(doc_key, timeout)\n\n\ndef get_thumbnail_modified_key(document_id: int) -> str:\n    \"\"\"\n    Builds the key to store a thumbnail's timestamp\n    \"\"\"\n    return f\"doc_{document_id}_thumbnail_modified\"\n\n\ndef clear_document_caches(document_id: int) -> None:\n    \"\"\"\n    Removes all cached items for the given document\n    \"\"\"\n    cache.delete_many(\n        [\n            get_suggestion_cache_key(document_id),\n            get_metadata_cache_key(document_id),\n            get_thumbnail_modified_key(document_id),\n        ],\n    )\n", "metadata": {"source": "github.com/paperless-ngx/paperless-ngx", "file_path": "src/documents/caching.py", "language": "python", "complexity": "complex", "star_count": 29534, "quality_score": 1.0, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": true, "has_documentation": true, "has_type_hints": true, "has_testing": false, "has_logging": true, "function_count": 10, "class_count": 2, "line_count": 216}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "database_integration", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create database model with ORM relationships and queries\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import textwrap\n\nfrom django.conf import settings\nfrom django.core.checks import Error\nfrom django.core.checks import Warning\nfrom django.core.checks import register\nfrom django.core.exceptions import FieldError\nfrom django.db.utils import OperationalError\nfrom django.db.utils import ProgrammingError\n\nfrom documents.signals import document_consumer_declaration\nfrom documents.templating.utils import convert_format_str_to_template_format\n\n\n@register()\ndef changed_password_check(app_configs, **kwargs):\n    from documents.models import Document\n    from paperless.db import GnuPG\n\n    try:\n        encrypted_doc = (\n            Document.objects.filter(\n                storage_type=Document.STORAGE_TYPE_GPG,\n            )\n            .only(\"pk\", \"storage_type\")\n            .first()\n        )\n    except (OperationalError, ProgrammingError, FieldError):\n        return []  # No documents table yet\n\n    if encrypted_doc:\n        if not settings.PASSPHRASE:\n            return [\n                Error(\n                    \"The database contains encrypted documents but no password is set.\",\n                ),\n            ]\n\n        if not GnuPG.decrypted(encrypted_doc.source_file):\n            return [\n                Error(\n                    textwrap.dedent(\n                        \"\"\"\n                The current password doesn't match the password of the\n                existing documents.\n\n                If you intend to change your password, you must first export\n                all of the old documents, start fresh with the new password\n                and then re-import them.\"\n                \"\"\",\n                    ),\n                ),\n            ]\n\n    return []\n\n\n@register()\ndef parser_check(app_configs, **kwargs):\n    parsers = []\n    for response in document_consumer_declaration.send(None):\n        parsers.append(response[1])\n\n    if len(parsers) == 0:\n        return [\n            Error(\n                \"No parsers found. This is a bug. The consumer won't be \"\n                \"able to consume any documents without parsers.\",\n            ),\n        ]\n    else:\n        return []\n\n\n@register()\ndef filename_format_check(app_configs, **kwargs):\n    if settings.FILENAME_FORMAT:\n        converted_format = convert_format_str_to_template_format(\n            settings.FILENAME_FORMAT,\n        )\n        if converted_format != settings.FILENAME_FORMAT:\n            return [\n                Warning(\n                    f\"Filename format {settings.FILENAME_FORMAT} is using the old style, please update to use double curly brackets\",\n                    hint=converted_format,\n                ),\n            ]\n    return []\n", "metadata": {"source": "github.com/paperless-ngx/paperless-ngx", "file_path": "src/documents/checks.py", "language": "python", "complexity": "medium", "star_count": 29534, "quality_score": 1.0, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": true, "has_documentation": true, "has_type_hints": false, "has_testing": false, "has_logging": false, "function_count": 3, "class_count": 0, "line_count": 89}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "database_integration", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create database model with ORM relationships and queries with SQLAlchemy/Django ORM patterns\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from __future__ import annotations\n\nimport logging\nimport pickle\nimport re\nimport warnings\nfrom hashlib import sha256\nfrom pathlib import Path\nfrom typing import TYPE_CHECKING\n\nif TYPE_CHECKING:\n    from collections.abc import Iterator\n    from datetime import datetime\n\n    from numpy import ndarray\n\nfrom django.conf import settings\nfrom django.core.cache import cache\n\nfrom documents.caching import CACHE_50_MINUTES\nfrom documents.caching import CLASSIFIER_HASH_KEY\nfrom documents.caching import CLASSIFIER_MODIFIED_KEY\nfrom documents.caching import CLASSIFIER_VERSION_KEY\nfrom documents.models import Document\nfrom documents.models import MatchingModel\n\nlogger = logging.getLogger(\"paperless.classifier\")\n\n\nclass IncompatibleClassifierVersionError(Exception):\n    def __init__(self, message: str, *args: object) -> None:\n        self.message: str = message\n        super().__init__(*args)\n\n\nclass ClassifierModelCorruptError(Exception):\n    pass\n\n\ndef load_classifier(*, raise_exception: bool = False) -> DocumentClassifier | None:\n    if not settings.MODEL_FILE.is_file():\n        logger.debug(\n            \"Document classification model does not exist (yet), not \"\n            \"performing automatic matching.\",\n        )\n        return None\n\n    classifier = DocumentClassifier()\n    try:\n        classifier.load()\n\n    except IncompatibleClassifierVersionError as e:\n        logger.info(f\"Classifier version incompatible: {e.message}, will re-train\")\n        Path(settings.MODEL_FILE).unlink()\n        classifier = None\n        if raise_exception:\n            raise e\n    except ClassifierModelCorruptError as e:\n        # there's something wrong with the model file.\n        logger.exception(\n            \"Unrecoverable error while loading document \"\n            \"classification model, deleting model file.\",\n        )\n        Path(settings.MODEL_FILE).unlink\n        classifier = None\n        if raise_exception:\n            raise e\n    except OSError as e:\n        logger.exception(\"IO error while loading document classification model\")\n        classifier = None\n        if raise_exception:\n            raise e\n    except Exception as e:  # pragma: no cover\n        logger.exception(\"Unknown error while loading document classification model\")\n        classifier = None\n        if raise_exception:\n            raise e\n\n    return classifier\n\n\nclass DocumentClassifier:\n    # v7 - Updated scikit-learn package version\n    # v8 - Added storage path classifier\n    # v9 - Changed from hashing to time/ids for re-train check\n    FORMAT_VERSION = 9\n\n    def __init__(self) -> None:\n        # last time a document changed and therefore training might be required\n        self.last_doc_change_time: datetime | None = None\n        # Hash of primary keys of AUTO matching values last used in training\n        self.last_auto_type_hash: bytes | None = None\n\n        self.data_vectorizer = None\n        self.tags_binarizer = None\n        self.tags_classifier = None\n        self.correspondent_classifier = None\n        self.document_type_classifier = None\n        self.storage_path_classifier = None\n\n        self._stemmer = None\n        self._stop_words = None\n\n    def load(self) -> None:\n        from sklearn.exceptions import InconsistentVersionWarning\n\n        # Catch warnings for processing\n        with warnings.catch_warnings(record=True) as w:\n            with Path(settings.MODEL_FILE).open(\"rb\") as f:\n                schema_version = pickle.load(f)\n\n                if schema_version != self.FORMAT_VERSION:\n                    raise IncompatibleClassifierVersionError(\n                        \"Cannot load classifier, incompatible versions.\",\n                    )\n                else:\n                    try:\n                        self.last_doc_change_time = pickle.load(f)\n                        self.last_auto_type_hash = pickle.load(f)\n\n                        self.data_vectorizer = pickle.load(f)\n                        self.tags_binarizer = pickle.load(f)\n\n                        self.tags_classifier = pickle.load(f)\n                        self.correspondent_classifier = pickle.load(f)\n                        self.document_type_classifier = pickle.load(f)\n                        self.storage_path_classifier = pickle.load(f)\n                    except Exception as err:\n                        raise ClassifierModelCorruptError from err\n\n            # Check for the warning about unpickling from differing versions\n            # and consider it incompatible\n            sk_learn_warning_url = (\n                \"https://scikit-learn.org/stable/\"\n                \"model_persistence.html\"\n                \"#security-maintainability-limitations\"\n            )\n            for warning in w:\n                # The warning is inconsistent, the MLPClassifier is a specific warning, others have not updated yet\n                if issubclass(warning.category, InconsistentVersionWarning) or (\n                    issubclass(warning.category, UserWarning)\n                    and sk_learn_warning_url in str(warning.message)\n                ):\n                    raise IncompatibleClassifierVersionError(\"sklearn version update\")\n\n    def save(self) -> None:\n        target_file: Path = settings.MODEL_FILE\n        target_file_temp: Path = target_file.with_suffix(\".pickle.part\")\n\n        with target_file_temp.open(\"wb\") as f:\n            pickle.dump(self.FORMAT_VERSION, f)\n\n            pickle.dump(self.last_doc_change_time, f)\n            pickle.dump(self.last_auto_type_hash, f)\n\n            pickle.dump(self.data_vectorizer, f)\n\n            pickle.dump(self.tags_binarizer, f)\n            pickle.dump(self.tags_classifier, f)\n\n            pickle.dump(self.correspondent_classifier, f)\n            pickle.dump(self.document_type_classifier, f)\n            pickle.dump(self.storage_path_classifier, f)\n\n        target_file_temp.rename(target_file)\n\n    def train(self) -> bool:\n        # Get non-inbox documents\n        docs_queryset = (\n            Document.objects.exclude(\n                tags__is_inbox_tag=True,\n            )\n            .select_related(\"document_type\", \"correspondent\", \"storage_path\")\n            .prefetch_related(\"tags\")\n            .order_by(\"pk\")\n        )\n\n        # No documents exit to train against\n        if docs_queryset.count() == 0:\n            raise ValueError(\"No training data available.\")\n\n        labels_tags = []\n        labels_correspondent = []\n        labels_document_type = []\n        labels_storage_path = []\n\n        # Step 1: Extract and preprocess training data from the database.\n        logger.debug(\"Gathering data from database...\")\n        hasher = sha256()\n        for doc in docs_queryset:\n            y = -1\n            dt = doc.document_type\n            if dt and dt.matching_algorithm == MatchingModel.MATCH_AUTO:\n                y = dt.pk\n            hasher.update(y.to_bytes(4, \"little\", signed=True))\n            labels_document_type.append(y)\n\n            y = -1\n            cor = doc.correspondent\n            if cor and cor.matching_algorithm == MatchingModel.MATCH_AUTO:\n                y = cor.pk\n            hasher.update(y.to_bytes(4, \"little\", signed=True))\n            labels_correspondent.append(y)\n\n            tags: list[int] = list(\n                doc.tags.filter(matching_algorithm=MatchingModel.MATCH_AUTO)\n                .order_by(\"pk\")\n                .values_list(\"pk\", flat=True),\n            )\n            for tag in tags:\n                hasher.update(tag.to_bytes(4, \"little\", signed=True))\n            labels_tags.append(tags)\n\n            y = -1\n            sp = doc.storage_path\n            if sp and sp.matching_algorithm == MatchingModel.MATCH_AUTO:\n                y = sp.pk\n            hasher.update(y.to_bytes(4, \"little\", signed=True))\n            labels_storage_path.append(y)\n\n        labels_tags_unique = {tag for tags in labels_tags for tag in tags}\n\n        num_tags = len(labels_tags_unique)\n\n        # Check if retraining is actually required.\n        # A document has been updated since the classifier was trained\n        # New auto tags, types, correspondent, storage paths exist\n        latest_doc_change = docs_queryset.latest(\"modified\").modified\n        if (\n            self.last_doc_change_time is not None\n            and self.last_doc_change_time >= latest_doc_change\n        ) and self.last_auto_type_hash == hasher.digest():\n            logger.info(\"No updates since last training\")\n            # Set the classifier information into the cache\n            # Caching for 50 minutes, so slightly less than the normal retrain time\n            cache.set(\n                CLASSIFIER_MODIFIED_KEY,\n                self.last_doc_change_time,\n                CACHE_50_MINUTES,\n            )\n            cache.set(CLASSIFIER_HASH_KEY, hasher.hexdigest(), CACHE_50_MINUTES)\n            cache.set(CLASSIFIER_VERSION_KEY, self.FORMAT_VERSION, CACHE_50_MINUTES)\n            return False\n\n        # subtract 1 since -1 (null) is also part of the classes.\n\n        # union with {-1} accounts for cases where all documents have\n        # correspondents and types assigned, so -1 isn't part of labels_x, which\n        # it usually is.\n        num_correspondents: int = len(set(labels_correspondent) | {-1}) - 1\n        num_document_types: int = len(set(labels_document_type) | {-1}) - 1\n        num_storage_paths: int = len(set(labels_storage_path) | {-1}) - 1\n\n        logger.debug(\n            f\"{docs_queryset.count()} documents, {num_tags} tag(s), {num_correspondents} correspondent(s), \"\n            f\"{num_document_types} document type(s). {num_storage_paths} storage path(s)\",\n        )\n\n        from sklearn.feature_extraction.text import CountVectorizer\n        from sklearn.neural_network import MLPClassifier\n        from sklearn.preprocessing import LabelBinarizer\n        from sklearn.preprocessing import MultiLabelBinarizer\n\n        # Step 2: vectorize data\n        logger.debug(\"Vectorizing data...\")\n\n        def content_generator() -> Iterator[str]:\n            \"\"\"\n            Generates the content for documents, but once at a time\n            \"\"\"\n            for doc in docs_queryset:\n                yield self.preprocess_content(doc.content)\n\n        self.data_vectorizer = CountVectorizer(\n            analyzer=\"word\",\n            ngram_range=(1, 2),\n            min_df=0.01,\n        )\n\n        data_vectorized: ndarray = self.data_vectorizer.fit_transform(\n            content_generator(),\n        )\n\n        # See the notes here:\n        # https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n        # This attribute isn't needed to function and can be large\n        self.data_vectorizer.stop_words_ = None\n\n        # Step 3: train the classifiers\n        if num_tags > 0:\n            logger.debug(\"Training tags classifier...\")\n\n            if num_tags == 1:\n                # Special case where only one tag has auto:\n                # Fallback to binary classification.\n                labels_tags = [\n                    label[0] if len(label) == 1 else -1 for label in labels_tags\n                ]\n                self.tags_binarizer = LabelBinarizer()\n                labels_tags_vectorized: ndarray = self.tags_binarizer.fit_transform(\n                    labels_tags,\n                ).ravel()\n            else:\n                self.tags_binarizer = MultiLabelBinarizer()\n                labels_tags_vectorized = self.tags_binarizer.fit_transform(labels_tags)\n\n            self.tags_classifier = MLPClassifier(tol=0.01)\n            self.tags_classifier.fit(data_vectorized, labels_tags_vectorized)\n        else:\n            self.tags_classifier = None\n            logger.debug(\"There are no tags. Not training tags classifier.\")\n\n        if num_correspondents > 0:\n            logger.debug(\"Training correspondent classifier...\")\n            self.correspondent_classifier = MLPClassifier(tol=0.01)\n            self.correspondent_classifier.fit(data_vectorized, labels_correspondent)\n        else:\n            self.correspondent_classifier = None\n            logger.debug(\n                \"There are no correspondents. Not training correspondent classifier.\",\n            )\n\n        if num_document_types > 0:\n            logger.debug(\"Training document type classifier...\")\n            self.document_type_classifier = MLPClassifier(tol=0.01)\n            self.document_type_classifier.fit(data_vectorized, labels_document_type)\n        else:\n            self.document_type_classifier = None\n            logger.debug(\n                \"There are no document types. Not training document type classifier.\",\n            )\n\n        if num_storage_paths > 0:\n            logger.debug(\n                \"Training storage paths classifier...\",\n            )\n            self.storage_path_classifier = MLPClassifier(tol=0.01)\n            self.storage_path_classifier.fit(\n                data_vectorized,\n                labels_storage_path,\n            )\n        else:\n            self.storage_path_classifier = None\n            logger.debug(\n                \"There are no storage paths. Not training storage path classifier.\",\n            )\n\n        self.last_doc_change_time = latest_doc_change\n        self.last_auto_type_hash = hasher.digest()\n\n        # Set the classifier information into the cache\n        # Caching for 50 minutes, so slightly less than the normal retrain time\n        cache.set(CLASSIFIER_MODIFIED_KEY, self.last_doc_change_time, CACHE_50_MINUTES)\n        cache.set(CLASSIFIER_HASH_KEY, hasher.hexdigest(), CACHE_50_MINUTES)\n        cache.set(CLASSIFIER_VERSION_KEY, self.FORMAT_VERSION, CACHE_50_MINUTES)\n\n        return True\n\n    def preprocess_content(self, content: str) -> str:  # pragma: no cover\n        \"\"\"\n        Process to contents of a document, distilling it down into\n        words which are meaningful to the content\n        \"\"\"\n\n        # Lower case the document\n        content = content.lower().strip()\n        # Reduce spaces\n        content = re.sub(r\"\\s+\", \" \", content)\n        # Get only the letters\n        content = re.sub(r\"[^\\w\\s]\", \" \", content)\n\n        # If the NLTK language is supported, do further processing\n        if settings.NLTK_LANGUAGE is not None and settings.NLTK_ENABLED:\n            import nltk\n            from nltk.corpus import stopwords\n            from nltk.stem import SnowballStemmer\n            from nltk.tokenize import word_tokenize\n\n            # Not really hacky, since it isn't private and is documented, but\n            # set the search path for NLTK data to the single location it should be in\n            nltk.data.path = [settings.NLTK_DIR]\n\n            try:\n                # Preload the corpus early, to force the lazy loader to transform\n                stopwords.ensure_loaded()\n\n                # Do some one time setup\n                # Sometimes, somehow, there's multiple threads loading the corpus\n                # and it's not thread safe, raising an AttributeError\n                if self._stemmer is None:\n                    self._stemmer = SnowballStemmer(settings.NLTK_LANGUAGE)\n                if self._stop_words is None:\n                    self._stop_words = set(stopwords.words(settings.NLTK_LANGUAGE))\n\n                # Tokenize\n                # This splits the content into tokens, roughly words\n                words: list[str] = word_tokenize(\n                    content,\n                    language=settings.NLTK_LANGUAGE,\n                )\n\n                meaningful_words = []\n                for word in words:\n                    # Skip stop words\n                    # These are words like \"a\", \"and\", \"the\" which add little meaning\n                    if word in self._stop_words:\n                        continue\n                    # Stem the words\n                    # This reduces the words to their stems.\n                    # \"amazement\" returns \"amaz\"\n                    # \"amaze\" returns \"amaz\n                    # \"amazed\" returns \"amaz\"\n                    meaningful_words.append(self._stemmer.stem(word))\n\n                return \" \".join(meaningful_words)\n\n            except AttributeError:\n                return content\n\n        return content\n\n    def predict_correspondent(self, content: str) -> int | None:\n        if self.correspondent_classifier:\n            X = self.data_vectorizer.transform([self.preprocess_content(content)])\n            correspondent_id = self.correspondent_classifier.predict(X)\n            if correspondent_id != -1:\n                return correspondent_id\n            else:\n                return None\n        else:\n            return None\n\n    def predict_document_type(self, content: str) -> int | None:\n        if self.document_type_classifier:\n            X = self.data_vectorizer.transform([self.preprocess_content(content)])\n            document_type_id = self.document_type_classifier.predict(X)\n            if document_type_id != -1:\n                return document_type_id\n            else:\n                return None\n        else:\n            return None\n\n    def predict_tags(self, content: str) -> list[int]:\n        from sklearn.utils.multiclass import type_of_target\n\n        if self.tags_classifier:\n            X = self.data_vectorizer.transform([self.preprocess_content(content)])\n            y = self.tags_classifier.predict(X)\n            tags_ids = self.tags_binarizer.inverse_transform(y)[0]\n            if type_of_target(y).startswith(\"multilabel\"):\n                # the usual case when there are multiple tags.\n                return list(tags_ids)\n            elif type_of_target(y) == \"binary\" and tags_ids != -1:\n                # This is for when we have binary classification with only one\n                # tag and the result is to assign this tag.\n                return [tags_ids]\n            else:\n                # Usually binary as well with -1 as the result, but we're\n                # going to catch everything else here as well.\n                return []\n        else:\n            return []\n\n    def predict_storage_path(self, content: str) -> int | None:\n        if self.storage_path_classifier:\n            X = self.data_vectorizer.transform([self.preprocess_content(content)])\n            storage_path_id = self.storage_path_classifier.predict(X)\n            if storage_path_id != -1:\n                return storage_path_id\n            else:\n                return None\n        else:\n            return None\n", "metadata": {"source": "github.com/paperless-ngx/paperless-ngx", "file_path": "src/documents/classifier.py", "language": "python", "complexity": "complex", "star_count": 29534, "quality_score": 1.0, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": true, "has_documentation": true, "has_type_hints": true, "has_testing": true, "has_logging": true, "function_count": 12, "class_count": 4, "line_count": 475}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "database_integration", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create database model with ORM relationships and queries\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from datetime import datetime\nfrom datetime import timezone\n\nfrom django.conf import settings\nfrom django.core.cache import cache\n\nfrom documents.caching import CACHE_5_MINUTES\nfrom documents.caching import CACHE_50_MINUTES\nfrom documents.caching import CLASSIFIER_HASH_KEY\nfrom documents.caching import CLASSIFIER_MODIFIED_KEY\nfrom documents.caching import CLASSIFIER_VERSION_KEY\nfrom documents.caching import get_thumbnail_modified_key\nfrom documents.classifier import DocumentClassifier\nfrom documents.models import Document\n\n\ndef suggestions_etag(request, pk: int) -> str | None:\n    \"\"\"\n    Returns an optional string for the ETag, allowing browser caching of\n    suggestions if the classifier has not been changed and the suggested dates\n    setting is also unchanged\n\n    \"\"\"\n    # If no model file, no etag at all\n    if not settings.MODEL_FILE.exists():\n        return None\n    # Check cache information\n    cache_hits = cache.get_many(\n        [CLASSIFIER_VERSION_KEY, CLASSIFIER_HASH_KEY],\n    )\n    # If the version differs somehow, no etag\n    if (\n        CLASSIFIER_VERSION_KEY in cache_hits\n        and cache_hits[CLASSIFIER_VERSION_KEY] != DocumentClassifier.FORMAT_VERSION\n    ):\n        return None\n    elif CLASSIFIER_HASH_KEY in cache_hits:\n        # Refresh the cache and return the hash digest and the dates setting\n        cache.touch(CLASSIFIER_HASH_KEY, CACHE_5_MINUTES)\n        return f\"{cache_hits[CLASSIFIER_HASH_KEY]}:{settings.NUMBER_OF_SUGGESTED_DATES}\"\n    return None\n\n\ndef suggestions_last_modified(request, pk: int) -> datetime | None:\n    \"\"\"\n    Returns the datetime of classifier last modification.  This is slightly off,\n    as there is not way to track the suggested date setting modification, but it seems\n    unlikely that changes too often\n    \"\"\"\n    # No file, no last modified\n    if not settings.MODEL_FILE.exists():\n        return None\n    cache_hits = cache.get_many(\n        [CLASSIFIER_VERSION_KEY, CLASSIFIER_MODIFIED_KEY],\n    )\n    # If the version differs somehow, no last modified\n    if (\n        CLASSIFIER_VERSION_KEY in cache_hits\n        and cache_hits[CLASSIFIER_VERSION_KEY] != DocumentClassifier.FORMAT_VERSION\n    ):\n        return None\n    elif CLASSIFIER_MODIFIED_KEY in cache_hits:\n        # Refresh the cache and return the last modified\n        cache.touch(CLASSIFIER_MODIFIED_KEY, CACHE_5_MINUTES)\n        return cache_hits[CLASSIFIER_MODIFIED_KEY]\n    return None\n\n\ndef metadata_etag(request, pk: int) -> str | None:\n    \"\"\"\n    Metadata is extracted from the original file, so use its checksum as the\n    ETag\n    \"\"\"\n    try:\n        doc = Document.objects.only(\"checksum\").get(pk=pk)\n        return doc.checksum\n    except Document.DoesNotExist:  # pragma: no cover\n        return None\n    return None\n\n\ndef metadata_last_modified(request, pk: int) -> datetime | None:\n    \"\"\"\n    Metadata is extracted from the original file, so use its modified.  Strictly speaking, this is\n    not the modification of the original file, but of the database object, but might as well\n    error on the side of more cautious\n    \"\"\"\n    try:\n        doc = Document.objects.only(\"modified\").get(pk=pk)\n        return doc.modified\n    except Document.DoesNotExist:  # pragma: no cover\n        return None\n    return None\n\n\ndef preview_etag(request, pk: int) -> str | None:\n    \"\"\"\n    ETag for the document preview, using the original or archive checksum, depending on the request\n    \"\"\"\n    try:\n        doc = Document.objects.only(\"checksum\", \"archive_checksum\").get(pk=pk)\n        use_original = (\n            \"original\" in request.query_params\n            and request.query_params[\"original\"] == \"true\"\n        )\n        return doc.checksum if use_original else doc.archive_checksum\n    except Document.DoesNotExist:  # pragma: no cover\n        return None\n    return None\n\n\ndef preview_last_modified(request, pk: int) -> datetime | None:\n    \"\"\"\n    Uses the documents modified time to set the Last-Modified header.  Not strictly\n    speaking correct, but close enough and quick\n    \"\"\"\n    try:\n        doc = Document.objects.only(\"modified\").get(pk=pk)\n        return doc.modified\n    except Document.DoesNotExist:  # pragma: no cover\n        return None\n    return None\n\n\ndef thumbnail_last_modified(request, pk: int) -> datetime | None:\n    \"\"\"\n    Returns the filesystem last modified either from cache or from filesystem.\n    Cache should be (slightly?) faster than filesystem\n    \"\"\"\n    try:\n        doc = Document.objects.only(\"storage_type\").get(pk=pk)\n        if not doc.thumbnail_path.exists():\n            return None\n        doc_key = get_thumbnail_modified_key(pk)\n\n        cache_hit = cache.get(doc_key)\n        if cache_hit is not None:\n            cache.touch(doc_key, CACHE_50_MINUTES)\n            return cache_hit\n\n        # No cache, get the timestamp and cache the datetime\n        last_modified = datetime.fromtimestamp(\n            doc.thumbnail_path.stat().st_mtime,\n            tz=timezone.utc,\n        )\n        cache.set(doc_key, last_modified, CACHE_50_MINUTES)\n        return last_modified\n    except Document.DoesNotExist:  # pragma: no cover\n        return None\n", "metadata": {"source": "github.com/paperless-ngx/paperless-ngx", "file_path": "src/documents/conditionals.py", "language": "python", "complexity": "medium", "star_count": 29534, "quality_score": 0.98, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": true, "has_documentation": true, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 7, "class_count": 0, "line_count": 150}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "database_integration", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create database model with ORM relationships and queries\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import datetime\nimport hashlib\nimport os\nimport tempfile\nfrom enum import Enum\nfrom pathlib import Path\nfrom typing import TYPE_CHECKING\n\nimport magic\nfrom django.conf import settings\nfrom django.contrib.auth.models import User\nfrom django.db import transaction\nfrom django.db.models import Q\nfrom django.utils import timezone\nfrom filelock import FileLock\nfrom rest_framework.reverse import reverse\n\nfrom documents.classifier import load_classifier\nfrom documents.data_models import ConsumableDocument\nfrom documents.data_models import DocumentMetadataOverrides\nfrom documents.file_handling import create_source_path_directory\nfrom documents.file_handling import generate_unique_filename\nfrom documents.loggers import LoggingMixin\nfrom documents.models import Correspondent\nfrom documents.models import CustomField\nfrom documents.models import CustomFieldInstance\nfrom documents.models import Document\nfrom documents.models import DocumentType\nfrom documents.models import StoragePath\nfrom documents.models import Tag\nfrom documents.models import WorkflowTrigger\nfrom documents.parsers import DocumentParser\nfrom documents.parsers import ParseError\nfrom documents.parsers import get_parser_class_for_mime_type\nfrom documents.parsers import parse_date\nfrom documents.permissions import set_permissions_for_object\nfrom documents.plugins.base import AlwaysRunPluginMixin\nfrom documents.plugins.base import ConsumeTaskPlugin\nfrom documents.plugins.base import NoCleanupPluginMixin\nfrom documents.plugins.base import NoSetupPluginMixin\nfrom documents.plugins.helpers import ProgressManager\nfrom documents.plugins.helpers import ProgressStatusOptions\nfrom documents.signals import document_consumption_finished\nfrom documents.signals import document_consumption_started\nfrom documents.signals.handlers import run_workflows\nfrom documents.templating.workflows import parse_w_workflow_placeholders\nfrom documents.utils import copy_basic_file_stats\nfrom documents.utils import copy_file_with_basic_stats\nfrom documents.utils import run_subprocess\nfrom paperless_mail.parsers import MailDocumentParser\n\n\nclass WorkflowTriggerPlugin(\n    NoCleanupPluginMixin,\n    NoSetupPluginMixin,\n    AlwaysRunPluginMixin,\n    ConsumeTaskPlugin,\n):\n    NAME: str = \"WorkflowTriggerPlugin\"\n\n    def run(self) -> str | None:\n        \"\"\"\n        Get overrides from matching workflows\n        \"\"\"\n        overrides, msg = run_workflows(\n            trigger_type=WorkflowTrigger.WorkflowTriggerType.CONSUMPTION,\n            document=self.input_doc,\n            logging_group=None,\n            overrides=DocumentMetadataOverrides(),\n        )\n        if overrides:\n            self.metadata.update(overrides)\n        return msg\n\n\nclass ConsumerError(Exception):\n    pass\n\n\nclass ConsumerStatusShortMessage(str, Enum):\n    DOCUMENT_ALREADY_EXISTS = \"document_already_exists\"\n    DOCUMENT_ALREADY_EXISTS_IN_TRASH = \"document_already_exists_in_trash\"\n    ASN_ALREADY_EXISTS = \"asn_already_exists\"\n    ASN_ALREADY_EXISTS_IN_TRASH = \"asn_already_exists_in_trash\"\n    ASN_RANGE = \"asn_value_out_of_range\"\n    FILE_NOT_FOUND = \"file_not_found\"\n    PRE_CONSUME_SCRIPT_NOT_FOUND = \"pre_consume_script_not_found\"\n    PRE_CONSUME_SCRIPT_ERROR = \"pre_consume_script_error\"\n    POST_CONSUME_SCRIPT_NOT_FOUND = \"post_consume_script_not_found\"\n    POST_CONSUME_SCRIPT_ERROR = \"post_consume_script_error\"\n    NEW_FILE = \"new_file\"\n    UNSUPPORTED_TYPE = \"unsupported_type\"\n    PARSING_DOCUMENT = \"parsing_document\"\n    GENERATING_THUMBNAIL = \"generating_thumbnail\"\n    PARSE_DATE = \"parse_date\"\n    SAVE_DOCUMENT = \"save_document\"\n    FINISHED = \"finished\"\n    FAILED = \"failed\"\n\n\nclass ConsumerPluginMixin:\n    def __init__(\n        self,\n        input_doc: ConsumableDocument,\n        metadata: DocumentMetadataOverrides,\n        status_mgr: ProgressManager,\n        base_tmp_dir: Path,\n        task_id: str,\n    ) -> None:\n        super().__init__(input_doc, metadata, status_mgr, base_tmp_dir, task_id)\n\n        self.renew_logging_group()\n\n        self.filename = self.metadata.filename or self.input_doc.original_file.name\n\n    def _send_progress(\n        self,\n        current_progress: int,\n        max_progress: int,\n        status: ProgressStatusOptions,\n        message: ConsumerStatusShortMessage | str | None = None,\n        document_id=None,\n    ):  # pragma: no cover\n        self.status_mgr.send_progress(\n            status,\n            message,\n            current_progress,\n            max_progress,\n            extra_args={\n                \"document_id\": document_id,\n                \"owner_id\": self.metadata.owner_id if self.metadata.owner_id else None,\n                \"users_can_view\": (self.metadata.view_users or [])\n                + (self.metadata.change_users or []),\n                \"groups_can_view\": (self.metadata.view_groups or [])\n                + (self.metadata.change_groups or []),\n            },\n        )\n\n    def _fail(\n        self,\n        message: ConsumerStatusShortMessage | str,\n        log_message: str | None = None,\n        exc_info=None,\n        exception: Exception | None = None,\n    ):\n        self._send_progress(100, 100, ProgressStatusOptions.FAILED, message)\n        self.log.error(log_message or message, exc_info=exc_info)\n        raise ConsumerError(f\"{self.filename}: {log_message or message}\") from exception\n\n\nclass ConsumerPlugin(\n    AlwaysRunPluginMixin,\n    NoSetupPluginMixin,\n    NoCleanupPluginMixin,\n    LoggingMixin,\n    ConsumerPluginMixin,\n    ConsumeTaskPlugin,\n):\n    logging_name = \"paperless.consumer\"\n\n    def run_pre_consume_script(self):\n        \"\"\"\n        If one is configured and exists, run the pre-consume script and\n        handle its output and/or errors\n        \"\"\"\n        if not settings.PRE_CONSUME_SCRIPT:\n            return\n\n        if not Path(settings.PRE_CONSUME_SCRIPT).is_file():\n            self._fail(\n                ConsumerStatusShortMessage.PRE_CONSUME_SCRIPT_NOT_FOUND,\n                f\"Configured pre-consume script \"\n                f\"{settings.PRE_CONSUME_SCRIPT} does not exist.\",\n            )\n\n        self.log.info(f\"Executing pre-consume script {settings.PRE_CONSUME_SCRIPT}\")\n\n        working_file_path = str(self.working_copy)\n        original_file_path = str(self.input_doc.original_file)\n\n        script_env = os.environ.copy()\n        script_env[\"DOCUMENT_SOURCE_PATH\"] = original_file_path\n        script_env[\"DOCUMENT_WORKING_PATH\"] = working_file_path\n        script_env[\"TASK_ID\"] = self.task_id or \"\"\n\n        try:\n            run_subprocess(\n                [\n                    settings.PRE_CONSUME_SCRIPT,\n                    original_file_path,\n                ],\n                script_env,\n                self.log,\n            )\n\n        except Exception as e:\n            self._fail(\n                ConsumerStatusShortMessage.PRE_CONSUME_SCRIPT_ERROR,\n                f\"Error while executing pre-consume script: {e}\",\n                exc_info=True,\n                exception=e,\n            )\n\n    def run_post_consume_script(self, document: Document):\n        \"\"\"\n        If one is configured and exists, run the pre-consume script and\n        handle its output and/or errors\n        \"\"\"\n        if not settings.POST_CONSUME_SCRIPT:\n            return\n\n        if not Path(settings.POST_CONSUME_SCRIPT).is_file():\n            self._fail(\n                ConsumerStatusShortMessage.POST_CONSUME_SCRIPT_NOT_FOUND,\n                f\"Configured post-consume script \"\n                f\"{settings.POST_CONSUME_SCRIPT} does not exist.\",\n            )\n\n        self.log.info(\n            f\"Executing post-consume script {settings.POST_CONSUME_SCRIPT}\",\n        )\n\n        script_env = os.environ.copy()\n\n        script_env[\"DOCUMENT_ID\"] = str(document.pk)\n        script_env[\"DOCUMENT_TYPE\"] = str(document.document_type)\n        script_env[\"DOCUMENT_CREATED\"] = str(document.created)\n        script_env[\"DOCUMENT_MODIFIED\"] = str(document.modified)\n        script_env[\"DOCUMENT_ADDED\"] = str(document.added)\n        script_env[\"DOCUMENT_FILE_NAME\"] = document.get_public_filename()\n        script_env[\"DOCUMENT_SOURCE_PATH\"] = os.path.normpath(document.source_path)\n        script_env[\"DOCUMENT_ARCHIVE_PATH\"] = os.path.normpath(\n            str(document.archive_path),\n        )\n        script_env[\"DOCUMENT_THUMBNAIL_PATH\"] = os.path.normpath(\n            document.thumbnail_path,\n        )\n        script_env[\"DOCUMENT_DOWNLOAD_URL\"] = reverse(\n            \"document-download\",\n            kwargs={\"pk\": document.pk},\n        )\n        script_env[\"DOCUMENT_THUMBNAIL_URL\"] = reverse(\n            \"document-thumb\",\n            kwargs={\"pk\": document.pk},\n        )\n        script_env[\"DOCUMENT_OWNER\"] = (\n            document.owner.get_username() if document.owner else \"\"\n        )\n        script_env[\"DOCUMENT_CORRESPONDENT\"] = str(document.correspondent)\n        script_env[\"DOCUMENT_TAGS\"] = str(\n            \",\".join(document.tags.all().values_list(\"name\", flat=True)),\n        )\n        script_env[\"DOCUMENT_ORIGINAL_FILENAME\"] = str(document.original_filename)\n        script_env[\"TASK_ID\"] = self.task_id or \"\"\n\n        try:\n            run_subprocess(\n                [\n                    settings.POST_CONSUME_SCRIPT,\n                    str(document.pk),\n                    document.get_public_filename(),\n                    os.path.normpath(document.source_path),\n                    os.path.normpath(document.thumbnail_path),\n                    reverse(\"document-download\", kwargs={\"pk\": document.pk}),\n                    reverse(\"document-thumb\", kwargs={\"pk\": document.pk}),\n                    str(document.correspondent),\n                    str(\",\".join(document.tags.all().values_list(\"name\", flat=True))),\n                ],\n                script_env,\n                self.log,\n            )\n\n        except Exception as e:\n            self._fail(\n                ConsumerStatusShortMessage.POST_CONSUME_SCRIPT_ERROR,\n                f\"Error while executing post-consume script: {e}\",\n                exc_info=True,\n                exception=e,\n            )\n\n    def run(self) -> str:\n        \"\"\"\n        Return the document object if it was successfully created.\n        \"\"\"\n\n        tempdir = None\n\n        try:\n            # Preflight has already run including progress update to 0%\n            self.log.info(f\"Consuming {self.filename}\")\n\n            # For the actual work, copy the file into a tempdir\n            tempdir = tempfile.TemporaryDirectory(\n                prefix=\"paperless-ngx\",\n                dir=settings.SCRATCH_DIR,\n            )\n            self.working_copy = Path(tempdir.name) / Path(self.filename)\n            copy_file_with_basic_stats(self.input_doc.original_file, self.working_copy)\n            self.unmodified_original = None\n\n            # Determine the parser class.\n\n            mime_type = magic.from_file(self.working_copy, mime=True)\n\n            self.log.debug(f\"Detected mime type: {mime_type}\")\n\n            if (\n                Path(self.filename).suffix.lower() == \".pdf\"\n                and mime_type in settings.CONSUMER_PDF_RECOVERABLE_MIME_TYPES\n            ):\n                try:\n                    # The file might be a pdf, but the mime type is wrong.\n                    # Try to clean with qpdf\n                    self.log.debug(\n                        \"Detected possible PDF with wrong mime type, trying to clean with qpdf\",\n                    )\n                    run_subprocess(\n                        [\n                            \"qpdf\",\n                            \"--replace-input\",\n                            self.working_copy,\n                        ],\n                        logger=self.log,\n                    )\n                    mime_type = magic.from_file(self.working_copy, mime=True)\n                    self.log.debug(f\"Detected mime type after qpdf: {mime_type}\")\n                    # Save the original file for later\n                    self.unmodified_original = (\n                        Path(tempdir.name) / Path(\"uo\") / Path(self.filename)\n                    )\n                    self.unmodified_original.parent.mkdir(exist_ok=True)\n                    copy_file_with_basic_stats(\n                        self.input_doc.original_file,\n                        self.unmodified_original,\n                    )\n                except Exception as e:\n                    self.log.error(f\"Error attempting to clean PDF: {e}\")\n\n            # Based on the mime type, get the parser for that type\n            parser_class: type[DocumentParser] | None = get_parser_class_for_mime_type(\n                mime_type,\n            )\n            if not parser_class:\n                tempdir.cleanup()\n                self._fail(\n                    ConsumerStatusShortMessage.UNSUPPORTED_TYPE,\n                    f\"Unsupported mime type {mime_type}\",\n                )\n\n            # Notify all listeners that we're going to do some work.\n\n            document_consumption_started.send(\n                sender=self.__class__,\n                filename=self.working_copy,\n                logging_group=self.logging_group,\n            )\n\n            self.run_pre_consume_script()\n        except:\n            if tempdir:\n                tempdir.cleanup()\n            raise\n\n        def progress_callback(current_progress, max_progress):  # pragma: no cover\n            # recalculate progress to be within 20 and 80\n            p = int((current_progress / max_progress) * 50 + 20)\n            self._send_progress(p, 100, ProgressStatusOptions.WORKING)\n\n        # This doesn't parse the document yet, but gives us a parser.\n\n        document_parser: DocumentParser = parser_class(\n            self.logging_group,\n            progress_callback=progress_callback,\n        )\n\n        self.log.debug(f\"Parser: {type(document_parser).__name__}\")\n\n        # Parse the document. This may take some time.\n\n        text = None\n        date = None\n        thumbnail = None\n        archive_path = None\n        page_count = None\n\n        try:\n            self._send_progress(\n                20,\n                100,\n                ProgressStatusOptions.WORKING,\n                ConsumerStatusShortMessage.PARSING_DOCUMENT,\n            )\n            self.log.debug(f\"Parsing {self.filename}...\")\n            if (\n                isinstance(document_parser, MailDocumentParser)\n                and self.input_doc.mailrule_id\n            ):\n                document_parser.parse(\n                    self.working_copy,\n                    mime_type,\n                    self.filename,\n                    self.input_doc.mailrule_id,\n                )\n            else:\n                document_parser.parse(self.working_copy, mime_type, self.filename)\n\n            self.log.debug(f\"Generating thumbnail for {self.filename}...\")\n            self._send_progress(\n                70,\n                100,\n                ProgressStatusOptions.WORKING,\n                ConsumerStatusShortMessage.GENERATING_THUMBNAIL,\n            )\n            thumbnail = document_parser.get_thumbnail(\n                self.working_copy,\n                mime_type,\n                self.filename,\n            )\n\n            text = document_parser.get_text()\n            date = document_parser.get_date()\n            if date is None:\n                self._send_progress(\n                    90,\n                    100,\n                    ProgressStatusOptions.WORKING,\n                    ConsumerStatusShortMessage.PARSE_DATE,\n                )\n                date = parse_date(self.filename, text)\n            archive_path = document_parser.get_archive_path()\n            page_count = document_parser.get_page_count(self.working_copy, mime_type)\n\n        except ParseError as e:\n            document_parser.cleanup()\n            if tempdir:\n                tempdir.cleanup()\n            self._fail(\n                str(e),\n                f\"Error occurred while consuming document {self.filename}: {e}\",\n                exc_info=True,\n                exception=e,\n            )\n        except Exception as e:\n            document_parser.cleanup()\n            if tempdir:\n                tempdir.cleanup()\n            self._fail(\n                str(e),\n                f\"Unexpected error while consuming document {self.filename}: {e}\",\n                exc_info=True,\n                exception=e,\n            )\n\n        # Prepare the document classifier.\n\n        # TODO: I don't really like to do this here, but this way we avoid\n        #   reloading the classifier multiple times, since there are multiple\n        #   post-consume hooks that all require the classifier.\n\n        classifier = load_classifier()\n\n        self._send_progress(\n            95,\n            100,\n            ProgressStatusOptions.WORKING,\n            ConsumerStatusShortMessage.SAVE_DOCUMENT,\n        )\n        # now that everything is done, we can start to store the document\n        # in the system. This will be a transaction and reasonably fast.\n        try:\n            with transaction.atomic():\n                # store the document.\n                document = self._store(\n                    text=text,\n                    date=date,\n                    page_count=page_count,\n                    mime_type=mime_type,\n                )\n\n                # If we get here, it was successful. Proceed with post-consume\n                # hooks. If they fail, nothing will get changed.\n\n                document_consumption_finished.send(\n                    sender=self.__class__,\n                    document=document,\n                    logging_group=self.logging_group,\n                    classifier=classifier,\n                    original_file=self.unmodified_original\n                    if self.unmodified_original\n                    else self.working_copy,\n                )\n\n                # After everything is in the database, copy the files into\n                # place. If this fails, we'll also rollback the transaction.\n                with FileLock(settings.MEDIA_LOCK):\n                    document.filename = generate_unique_filename(document)\n                    create_source_path_directory(document.source_path)\n\n                    self._write(\n                        document.storage_type,\n                        self.unmodified_original\n                        if self.unmodified_original is not None\n                        else self.working_copy,\n                        document.source_path,\n                    )\n\n                    self._write(\n                        document.storage_type,\n                        thumbnail,\n                        document.thumbnail_path,\n                    )\n\n                    if archive_path and Path(archive_path).is_file():\n                        document.archive_filename = generate_unique_filename(\n                            document,\n                            archive_filename=True,\n                        )\n                        create_source_path_directory(document.archive_path)\n                        self._write(\n                            document.storage_type,\n                            archive_path,\n                            document.archive_path,\n                        )\n\n                        with Path(archive_path).open(\"rb\") as f:\n                            document.archive_checksum = hashlib.md5(\n                                f.read(),\n                            ).hexdigest()\n\n                # Don't save with the lock active. Saving will cause the file\n                # renaming logic to acquire the lock as well.\n                # This triggers things like file renaming\n                document.save()\n\n                # Delete the file only if it was successfully consumed\n                self.log.debug(f\"Deleting original file {self.input_doc.original_file}\")\n                self.input_doc.original_file.unlink()\n                self.log.debug(f\"Deleting working copy {self.working_copy}\")\n                self.working_copy.unlink()\n                if self.unmodified_original is not None:  # pragma: no cover\n                    self.log.debug(\n                        f\"Deleting unmodified original file {self.unmodified_original}\",\n                    )\n                    self.unmodified_original.unlink()\n\n                # https://github.com/jonaswinkler/paperless-ng/discussions/1037\n                shadow_file = (\n                    Path(self.input_doc.original_file).parent\n                    / f\"._{Path(self.input_doc.original_file).name}\"\n                )\n\n                if Path(shadow_file).is_file():\n                    self.log.debug(f\"Deleting shadow file {shadow_file}\")\n                    Path(shadow_file).unlink()\n\n        except Exception as e:\n            self._fail(\n                str(e),\n                f\"The following error occurred while storing document \"\n                f\"{self.filename} after parsing: {e}\",\n                exc_info=True,\n                exception=e,\n            )\n        finally:\n            document_parser.cleanup()\n            tempdir.cleanup()\n\n        self.run_post_consume_script(document)\n\n        self.log.info(f\"Document {document} consumption finished\")\n\n        self._send_progress(\n            100,\n            100,\n            ProgressStatusOptions.SUCCESS,\n            ConsumerStatusShortMessage.FINISHED,\n            document.id,\n        )\n\n        # Return the most up to date fields\n        document.refresh_from_db()\n\n        return f\"Success. New document id {document.pk} created\"\n\n    def _parse_title_placeholders(self, title: str) -> str:\n        local_added = timezone.localtime(timezone.now())\n\n        correspondent_name = (\n            Correspondent.objects.get(pk=self.metadata.correspondent_id).name\n            if self.metadata.correspondent_id is not None\n            else None\n        )\n        doc_type_name = (\n            DocumentType.objects.get(pk=self.metadata.document_type_id).name\n            if self.metadata.document_type_id is not None\n            else None\n        )\n        owner_username = (\n            User.objects.get(pk=self.metadata.owner_id).username\n            if self.metadata.owner_id is not None\n            else None\n        )\n\n        return parse_w_workflow_placeholders(\n            title,\n            correspondent_name,\n            doc_type_name,\n            owner_username,\n            local_added,\n            self.filename,\n            self.filename,\n        )\n\n    def _store(\n        self,\n        text: str,\n        date: datetime.datetime | None,\n        page_count: int | None,\n        mime_type: str,\n    ) -> Document:\n        # If someone gave us the original filename, use it instead of doc.\n\n        self.log.debug(\"Saving record to database\")\n\n        if self.metadata.created is not None:\n            create_date = self.metadata.created\n            self.log.debug(\n                f\"Creation date from post_documents parameter: {create_date}\",\n            )\n        elif date is not None:\n            create_date = date\n            self.log.debug(f\"Creation date from parse_date: {create_date}\")\n        else:\n            stats = Path(self.input_doc.original_file).stat()\n            create_date = timezone.make_aware(\n                datetime.datetime.fromtimestamp(stats.st_mtime),\n            )\n            self.log.debug(f\"Creation date from st_mtime: {create_date}\")\n\n        storage_type = Document.STORAGE_TYPE_UNENCRYPTED\n\n        if self.metadata.filename:\n            title = Path(self.metadata.filename).stem\n        else:\n            title = self.input_doc.original_file.stem\n\n        if self.metadata.title is not None:\n            try:\n                title = self._parse_title_placeholders(self.metadata.title)\n            except Exception as e:\n                self.log.error(\n                    f\"Error occurred parsing title override '{self.metadata.title}', falling back to original. Exception: {e}\",\n                )\n\n        file_for_checksum = (\n            self.unmodified_original\n            if self.unmodified_original is not None\n            else self.working_copy\n        )\n\n        document = Document.objects.create(\n            title=title[:127],\n            content=text,\n            mime_type=mime_type,\n            checksum=hashlib.md5(file_for_checksum.read_bytes()).hexdigest(),\n            created=create_date,\n            modified=create_date,\n            storage_type=storage_type,\n            page_count=page_count,\n            original_filename=self.filename,\n        )\n\n        self.apply_overrides(document)\n\n        document.save()\n\n        return document\n\n    def apply_overrides(self, document):\n        if self.metadata.correspondent_id:\n            document.correspondent = Correspondent.objects.get(\n                pk=self.metadata.correspondent_id,\n            )\n\n        if self.metadata.document_type_id:\n            document.document_type = DocumentType.objects.get(\n                pk=self.metadata.document_type_id,\n            )\n\n        if self.metadata.tag_ids:\n            for tag_id in self.metadata.tag_ids:\n                document.tags.add(Tag.objects.get(pk=tag_id))\n\n        if self.metadata.storage_path_id:\n            document.storage_path = StoragePath.objects.get(\n                pk=self.metadata.storage_path_id,\n            )\n\n        if self.metadata.asn is not None:\n            document.archive_serial_number = self.metadata.asn\n\n        if self.metadata.owner_id:\n            document.owner = User.objects.get(\n                pk=self.metadata.owner_id,\n            )\n\n        if (\n            self.metadata.view_users is not None\n            or self.metadata.view_groups is not None\n            or self.metadata.change_users is not None\n            or self.metadata.change_users is not None\n        ):\n            permissions = {\n                \"view\": {\n                    \"users\": self.metadata.view_users or [],\n                    \"groups\": self.metadata.view_groups or [],\n                },\n                \"change\": {\n                    \"users\": self.metadata.change_users or [],\n                    \"groups\": self.metadata.change_groups or [],\n                },\n            }\n            set_permissions_for_object(permissions=permissions, object=document)\n\n        if self.metadata.custom_fields:\n            for field in CustomField.objects.filter(\n                id__in=self.metadata.custom_fields.keys(),\n            ).distinct():\n                value_field_name = CustomFieldInstance.get_value_field_name(\n                    data_type=field.data_type,\n                )\n                args = {\n                    \"field\": field,\n                    \"document\": document,\n                    value_field_name: self.metadata.custom_fields.get(field.id, None),\n                }\n                CustomFieldInstance.objects.create(**args)  # adds to document\n\n    def _write(self, storage_type, source, target):\n        with (\n            Path(source).open(\"rb\") as read_file,\n            Path(target).open(\"wb\") as write_file,\n        ):\n            write_file.write(read_file.read())\n\n        # Attempt to copy file's original stats, but it's ok if we can't\n        try:\n            copy_basic_file_stats(source, target)\n        except Exception:  # pragma: no cover\n            pass\n\n\nclass ConsumerPreflightPlugin(\n    NoCleanupPluginMixin,\n    NoSetupPluginMixin,\n    AlwaysRunPluginMixin,\n    LoggingMixin,\n    ConsumerPluginMixin,\n    ConsumeTaskPlugin,\n):\n    NAME: str = \"ConsumerPreflightPlugin\"\n    logging_name = \"paperless.consumer\"\n\n    def pre_check_file_exists(self):\n        \"\"\"\n        Confirm the input file still exists where it should\n        \"\"\"\n        if TYPE_CHECKING:\n            assert isinstance(self.input_doc.original_file, Path), (\n                self.input_doc.original_file\n            )\n        if not self.input_doc.original_file.is_file():\n            self._fail(\n                ConsumerStatusShortMessage.FILE_NOT_FOUND,\n                f\"Cannot consume {self.input_doc.original_file}: File not found.\",\n            )\n\n    def pre_check_duplicate(self):\n        \"\"\"\n        Using the MD5 of the file, check this exact file doesn't already exist\n        \"\"\"\n        with Path(self.input_doc.original_file).open(\"rb\") as f:\n            checksum = hashlib.md5(f.read()).hexdigest()\n        existing_doc = Document.global_objects.filter(\n            Q(checksum=checksum) | Q(archive_checksum=checksum),\n        )\n        if existing_doc.exists():\n            msg = ConsumerStatusShortMessage.DOCUMENT_ALREADY_EXISTS\n            log_msg = f\"Not consuming {self.filename}: It is a duplicate of {existing_doc.get().title} (#{existing_doc.get().pk}).\"\n\n            if existing_doc.first().deleted_at is not None:\n                msg = ConsumerStatusShortMessage.DOCUMENT_ALREADY_EXISTS_IN_TRASH\n                log_msg += \" Note: existing document is in the trash.\"\n\n            if settings.CONSUMER_DELETE_DUPLICATES:\n                Path(self.input_doc.original_file).unlink()\n            self._fail(\n                msg,\n                log_msg,\n            )\n\n    def pre_check_directories(self):\n        \"\"\"\n        Ensure all required directories exist before attempting to use them\n        \"\"\"\n        settings.SCRATCH_DIR.mkdir(parents=True, exist_ok=True)\n        settings.THUMBNAIL_DIR.mkdir(parents=True, exist_ok=True)\n        settings.ORIGINALS_DIR.mkdir(parents=True, exist_ok=True)\n        settings.ARCHIVE_DIR.mkdir(parents=True, exist_ok=True)\n\n    def pre_check_asn_value(self):\n        \"\"\"\n        Check that if override_asn is given, it is unique and within a valid range\n        \"\"\"\n        if self.metadata.asn is None:\n            # check not necessary in case no ASN gets set\n            return\n        # Validate the range is above zero and less than uint32_t max\n        # otherwise, Whoosh can't handle it in the index\n        if (\n            self.metadata.asn < Document.ARCHIVE_SERIAL_NUMBER_MIN\n            or self.metadata.asn > Document.ARCHIVE_SERIAL_NUMBER_MAX\n        ):\n            self._fail(\n                ConsumerStatusShortMessage.ASN_RANGE,\n                f\"Not consuming {self.filename}: \"\n                f\"Given ASN {self.metadata.asn} is out of range \"\n                f\"[{Document.ARCHIVE_SERIAL_NUMBER_MIN:,}, \"\n                f\"{Document.ARCHIVE_SERIAL_NUMBER_MAX:,}]\",\n            )\n        existing_asn_doc = Document.global_objects.filter(\n            archive_serial_number=self.metadata.asn,\n        )\n        if existing_asn_doc.exists():\n            msg = ConsumerStatusShortMessage.ASN_ALREADY_EXISTS\n            log_msg = f\"Not consuming {self.filename}: Given ASN {self.metadata.asn} already exists!\"\n\n            if existing_asn_doc.first().deleted_at is not None:\n                msg = ConsumerStatusShortMessage.ASN_ALREADY_EXISTS_IN_TRASH\n                log_msg += \" Note: existing document is in the trash.\"\n\n            self._fail(\n                msg,\n                log_msg,\n            )\n\n    def run(self) -> None:\n        self._send_progress(\n            0,\n            100,\n            ProgressStatusOptions.STARTED,\n            ConsumerStatusShortMessage.NEW_FILE,\n        )\n\n        # Make sure that preconditions for consuming the file are met.\n\n        self.pre_check_file_exists()\n        self.pre_check_duplicate()\n        self.pre_check_directories()\n        self.pre_check_asn_value()\n", "metadata": {"source": "github.com/paperless-ngx/paperless-ngx", "file_path": "src/documents/consumer.py", "language": "python", "complexity": "complex", "star_count": 29534, "quality_score": 1.0, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": true, "has_documentation": true, "has_type_hints": true, "has_testing": true, "has_logging": true, "function_count": 17, "class_count": 6, "line_count": 861}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "database_integration", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create database model with ORM relationships and queries\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from django.conf import settings as django_settings\nfrom django.contrib.auth.models import User\n\nfrom documents.models import Document\nfrom paperless.config import GeneralConfig\n\n\ndef settings(request):\n    general_config = GeneralConfig()\n\n    app_title = (\n        django_settings.APP_TITLE\n        if general_config.app_title is None or len(general_config.app_title) == 0\n        else general_config.app_title\n    )\n    app_logo = (\n        django_settings.APP_LOGO\n        if general_config.app_logo is None or len(general_config.app_logo) == 0\n        else django_settings.BASE_URL + general_config.app_logo.lstrip(\"/\")\n    )\n\n    return {\n        \"EMAIL_ENABLED\": django_settings.EMAIL_ENABLED,\n        \"DISABLE_REGULAR_LOGIN\": django_settings.DISABLE_REGULAR_LOGIN,\n        \"REDIRECT_LOGIN_TO_SSO\": django_settings.REDIRECT_LOGIN_TO_SSO,\n        \"ACCOUNT_ALLOW_SIGNUPS\": django_settings.ACCOUNT_ALLOW_SIGNUPS,\n        \"domain\": getattr(django_settings, \"PAPERLESS_URL\", request.get_host()),\n        \"APP_TITLE\": app_title,\n        \"APP_LOGO\": app_logo,\n        \"FIRST_INSTALL\": User.objects.exclude(\n            username__in=[\"consumer\", \"AnonymousUser\"],\n        ).count()\n        == 0\n        and Document.global_objects.count() == 0,\n    }\n", "metadata": {"source": "github.com/paperless-ngx/paperless-ngx", "file_path": "src/documents/context_processors.py", "language": "python", "complexity": "simple", "star_count": 29534, "quality_score": 0.9500000000000001, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": false, "has_logging": true, "function_count": 1, "class_count": 0, "line_count": 36}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "database_integration", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create database model with ORM relationships and queries\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import os\n\nfrom django.conf import settings\n\nfrom documents.models import Document\nfrom documents.templating.filepath import validate_filepath_template_and_render\nfrom documents.templating.utils import convert_format_str_to_template_format\n\n\ndef create_source_path_directory(source_path):\n    os.makedirs(os.path.dirname(source_path), exist_ok=True)\n\n\ndef delete_empty_directories(directory, root):\n    if not os.path.isdir(directory):\n        return\n\n    # Go up in the directory hierarchy and try to delete all directories\n    directory = os.path.normpath(directory)\n    root = os.path.normpath(root)\n\n    if not directory.startswith(root + os.path.sep):\n        # don't do anything outside our originals folder.\n\n        # append os.path.set so that we avoid these cases:\n        #   directory = /home/originals2/test\n        #   root = /home/originals (\"/\" gets appended and startswith fails)\n        return\n\n    while directory != root:\n        if not os.listdir(directory):\n            # it's empty\n            try:\n                os.rmdir(directory)\n            except OSError:\n                # whatever. empty directories aren't that bad anyway.\n                return\n        else:\n            # it's not empty.\n            return\n\n        # go one level up\n        directory = os.path.normpath(os.path.dirname(directory))\n\n\ndef generate_unique_filename(doc, *, archive_filename=False):\n    \"\"\"\n    Generates a unique filename for doc in settings.ORIGINALS_DIR.\n\n    The returned filename is guaranteed to be either the current filename\n    of the document if unchanged, or a new filename that does not correspondent\n    to any existing files. The function will append _01, _02, etc to the\n    filename before the extension to avoid conflicts.\n\n    If archive_filename is True, return a unique archive filename instead.\n\n    \"\"\"\n    if archive_filename:\n        old_filename = doc.archive_filename\n        root = settings.ARCHIVE_DIR\n    else:\n        old_filename = doc.filename\n        root = settings.ORIGINALS_DIR\n\n    # If generating archive filenames, try to make a name that is similar to\n    # the original filename first.\n\n    if archive_filename and doc.filename:\n        new_filename = os.path.splitext(doc.filename)[0] + \".pdf\"\n        if new_filename == old_filename or not os.path.exists(\n            os.path.join(root, new_filename),\n        ):\n            return new_filename\n\n    counter = 0\n\n    while True:\n        new_filename = generate_filename(\n            doc,\n            counter=counter,\n            archive_filename=archive_filename,\n        )\n        if new_filename == old_filename:\n            # still the same as before.\n            return new_filename\n\n        if os.path.exists(os.path.join(root, new_filename)):\n            counter += 1\n        else:\n            return new_filename\n\n\ndef generate_filename(\n    doc: Document,\n    *,\n    counter=0,\n    append_gpg=True,\n    archive_filename=False,\n):\n    path = \"\"\n\n    def format_filename(document: Document, template_str: str) -> str | None:\n        rendered_filename = validate_filepath_template_and_render(\n            template_str,\n            document,\n        )\n        if rendered_filename is None:\n            return None\n\n        # Apply this setting.  It could become a filter in the future (or users could use |default)\n        if settings.FILENAME_FORMAT_REMOVE_NONE:\n            rendered_filename = rendered_filename.replace(\"/-none-/\", \"/\")\n            rendered_filename = rendered_filename.replace(\" -none-\", \"\")\n            rendered_filename = rendered_filename.replace(\"-none-\", \"\")\n            rendered_filename = rendered_filename.strip(os.sep)\n\n        rendered_filename = rendered_filename.replace(\n            \"-none-\",\n            \"none\",\n        )  # backward compatibility\n\n        return rendered_filename\n\n    # Determine the source of the format string\n    if doc.storage_path is not None:\n        filename_format = doc.storage_path.path\n    elif settings.FILENAME_FORMAT is not None:\n        # Maybe convert old to new style\n        filename_format = convert_format_str_to_template_format(\n            settings.FILENAME_FORMAT,\n        )\n    else:\n        filename_format = None\n\n    # If we have one, render it\n    if filename_format is not None:\n        path = format_filename(doc, filename_format)\n\n    counter_str = f\"_{counter:02}\" if counter else \"\"\n    filetype_str = \".pdf\" if archive_filename else doc.file_type\n\n    if path:\n        filename = f\"{path}{counter_str}{filetype_str}\"\n    else:\n        filename = f\"{doc.pk:07}{counter_str}{filetype_str}\"\n\n    if append_gpg and doc.storage_type == doc.STORAGE_TYPE_GPG:\n        filename += \".gpg\"\n\n    return filename\n", "metadata": {"source": "github.com/paperless-ngx/paperless-ngx", "file_path": "src/documents/file_handling.py", "language": "python", "complexity": "complex", "star_count": 29534, "quality_score": 0.98, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": true, "has_documentation": true, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 5, "class_count": 0, "line_count": 151}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "database_integration", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create database model with ORM relationships and queries with SQLAlchemy/Django ORM patterns\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from __future__ import annotations\n\nimport functools\nimport inspect\nimport json\nimport operator\nfrom contextlib import contextmanager\nfrom typing import TYPE_CHECKING\n\nfrom django.contrib.contenttypes.models import ContentType\nfrom django.db.models import Case\nfrom django.db.models import CharField\nfrom django.db.models import Count\nfrom django.db.models import Exists\nfrom django.db.models import IntegerField\nfrom django.db.models import OuterRef\nfrom django.db.models import Q\nfrom django.db.models import Subquery\nfrom django.db.models import Sum\nfrom django.db.models import Value\nfrom django.db.models import When\nfrom django.db.models.functions import Cast\nfrom django.utils.translation import gettext_lazy as _\nfrom django_filters import DateFilter\nfrom django_filters.rest_framework import BooleanFilter\nfrom django_filters.rest_framework import Filter\nfrom django_filters.rest_framework import FilterSet\nfrom drf_spectacular.utils import extend_schema_field\nfrom guardian.utils import get_group_obj_perms_model\nfrom guardian.utils import get_user_obj_perms_model\nfrom rest_framework import serializers\nfrom rest_framework.filters import OrderingFilter\nfrom rest_framework_guardian.filters import ObjectPermissionsFilter\n\nfrom documents.models import Correspondent\nfrom documents.models import CustomField\nfrom documents.models import CustomFieldInstance\nfrom documents.models import Document\nfrom documents.models import DocumentType\nfrom documents.models import PaperlessTask\nfrom documents.models import ShareLink\nfrom documents.models import StoragePath\nfrom documents.models import Tag\n\nif TYPE_CHECKING:\n    from collections.abc import Callable\n\nCHAR_KWARGS = [\"istartswith\", \"iendswith\", \"icontains\", \"iexact\"]\nID_KWARGS = [\"in\", \"exact\"]\nINT_KWARGS = [\"exact\", \"gt\", \"gte\", \"lt\", \"lte\", \"isnull\"]\nDATE_KWARGS = [\n    \"year\",\n    \"month\",\n    \"day\",\n    \"gt\",\n    \"gte\",\n    \"lt\",\n    \"lte\",\n]\nDATETIME_KWARGS = [\n    \"year\",\n    \"month\",\n    \"day\",\n    \"date__gt\",\n    \"date__gte\",\n    \"gt\",\n    \"gte\",\n    \"date__lt\",\n    \"date__lte\",\n    \"lt\",\n    \"lte\",\n]\n\nCUSTOM_FIELD_QUERY_MAX_DEPTH = 10\nCUSTOM_FIELD_QUERY_MAX_ATOMS = 20\n\n\nclass CorrespondentFilterSet(FilterSet):\n    class Meta:\n        model = Correspondent\n        fields = {\n            \"id\": ID_KWARGS,\n            \"name\": CHAR_KWARGS,\n        }\n\n\nclass TagFilterSet(FilterSet):\n    class Meta:\n        model = Tag\n        fields = {\n            \"id\": ID_KWARGS,\n            \"name\": CHAR_KWARGS,\n        }\n\n\nclass DocumentTypeFilterSet(FilterSet):\n    class Meta:\n        model = DocumentType\n        fields = {\n            \"id\": ID_KWARGS,\n            \"name\": CHAR_KWARGS,\n        }\n\n\nclass StoragePathFilterSet(FilterSet):\n    class Meta:\n        model = StoragePath\n        fields = {\n            \"id\": ID_KWARGS,\n            \"name\": CHAR_KWARGS,\n            \"path\": CHAR_KWARGS,\n        }\n\n\nclass ObjectFilter(Filter):\n    def __init__(self, *, exclude=False, in_list=False, field_name=\"\"):\n        super().__init__()\n        self.exclude = exclude\n        self.in_list = in_list\n        self.field_name = field_name\n\n    def filter(self, qs, value):\n        if not value:\n            return qs\n\n        try:\n            object_ids = [int(x) for x in value.split(\",\")]\n        except ValueError:\n            return qs\n\n        if self.in_list:\n            qs = qs.filter(**{f\"{self.field_name}__id__in\": object_ids}).distinct()\n        else:\n            for obj_id in object_ids:\n                if self.exclude:\n                    qs = qs.exclude(**{f\"{self.field_name}__id\": obj_id})\n                else:\n                    qs = qs.filter(**{f\"{self.field_name}__id\": obj_id})\n\n        return qs\n\n\n@extend_schema_field(serializers.BooleanField)\nclass InboxFilter(Filter):\n    def filter(self, qs, value):\n        if value == \"true\":\n            return qs.filter(tags__is_inbox_tag=True)\n        elif value == \"false\":\n            return qs.exclude(tags__is_inbox_tag=True)\n        else:\n            return qs\n\n\n@extend_schema_field(serializers.CharField)\nclass TitleContentFilter(Filter):\n    def filter(self, qs, value):\n        if value:\n            return qs.filter(Q(title__icontains=value) | Q(content__icontains=value))\n        else:\n            return qs\n\n\n@extend_schema_field(serializers.BooleanField)\nclass SharedByUser(Filter):\n    def filter(self, qs, value):\n        ctype = ContentType.objects.get_for_model(self.model)\n        UserObjectPermission = get_user_obj_perms_model()\n        GroupObjectPermission = get_group_obj_perms_model()\n        # see https://github.com/paperless-ngx/paperless-ngx/issues/5392, we limit subqueries\n        # to 1 because Postgres doesn't like returning > 1 row, but all we care about is > 0\n        return (\n            qs.filter(\n                owner_id=value,\n            )\n            .annotate(\n                num_shared_users=Count(\n                    UserObjectPermission.objects.filter(\n                        content_type=ctype,\n                        object_pk=Cast(OuterRef(\"pk\"), CharField()),\n                    ).values(\"user_id\")[:1],\n                ),\n            )\n            .annotate(\n                num_shared_groups=Count(\n                    GroupObjectPermission.objects.filter(\n                        content_type=ctype,\n                        object_pk=Cast(OuterRef(\"pk\"), CharField()),\n                    ).values(\"group_id\")[:1],\n                ),\n            )\n            .filter(\n                Q(num_shared_users__gt=0) | Q(num_shared_groups__gt=0),\n            )\n            if value is not None\n            else qs\n        )\n\n\nclass CustomFieldFilterSet(FilterSet):\n    class Meta:\n        model = CustomField\n        fields = {\n            \"id\": ID_KWARGS,\n            \"name\": CHAR_KWARGS,\n        }\n\n\n@extend_schema_field(serializers.CharField)\nclass CustomFieldsFilter(Filter):\n    def filter(self, qs, value):\n        if value:\n            fields_with_matching_selects = CustomField.objects.filter(\n                extra_data__icontains=value,\n            )\n            option_ids = []\n            if fields_with_matching_selects.count() > 0:\n                for field in fields_with_matching_selects:\n                    options = field.extra_data.get(\"select_options\", [])\n                    for _, option in enumerate(options):\n                        if option.get(\"label\").lower().find(value.lower()) != -1:\n                            option_ids.extend([option.get(\"id\")])\n            return (\n                qs.filter(custom_fields__field__name__icontains=value)\n                | qs.filter(custom_fields__value_text__icontains=value)\n                | qs.filter(custom_fields__value_bool__icontains=value)\n                | qs.filter(custom_fields__value_int__icontains=value)\n                | qs.filter(custom_fields__value_float__icontains=value)\n                | qs.filter(custom_fields__value_date__icontains=value)\n                | qs.filter(custom_fields__value_url__icontains=value)\n                | qs.filter(custom_fields__value_monetary__icontains=value)\n                | qs.filter(custom_fields__value_document_ids__icontains=value)\n                | qs.filter(custom_fields__value_select__in=option_ids)\n            )\n        else:\n            return qs\n\n\nclass MimeTypeFilter(Filter):\n    def filter(self, qs, value):\n        if value:\n            return qs.filter(mime_type__icontains=value)\n        else:\n            return qs\n\n\nclass SelectField(serializers.CharField):\n    def __init__(self, custom_field: CustomField):\n        self._options = custom_field.extra_data[\"select_options\"]\n        super().__init__(max_length=16)\n\n    def to_internal_value(self, data):\n        # If the supplied value is the option label instead of the ID\n        try:\n            data = next(\n                option.get(\"id\")\n                for option in self._options\n                if option.get(\"label\") == data\n            )\n        except StopIteration:\n            pass\n        return super().to_internal_value(data)\n\n\ndef handle_validation_prefix(func: Callable):\n    \"\"\"\n    Catch ValidationErrors raised by the wrapped function\n    and add a prefix to the exception detail to track what causes the exception,\n    similar to nested serializers.\n    \"\"\"\n\n    def wrapper(*args, validation_prefix=None, **kwargs):\n        try:\n            return func(*args, **kwargs)\n        except serializers.ValidationError as e:\n            raise serializers.ValidationError({validation_prefix: e.detail})\n\n    # Update the signature to include the validation_prefix argument\n    old_sig = inspect.signature(func)\n    new_param = inspect.Parameter(\"validation_prefix\", inspect.Parameter.KEYWORD_ONLY)\n    new_sig = old_sig.replace(parameters=[*old_sig.parameters.values(), new_param])\n\n    # Apply functools.wraps and manually set the new signature\n    functools.update_wrapper(wrapper, func)\n    wrapper.__signature__ = new_sig\n\n    return wrapper\n\n\nclass CustomFieldQueryParser:\n    EXPR_BY_CATEGORY = {\n        \"basic\": [\"exact\", \"in\", \"isnull\", \"exists\"],\n        \"string\": [\n            \"icontains\",\n            \"istartswith\",\n            \"iendswith\",\n        ],\n        \"arithmetic\": [\n            \"gt\",\n            \"gte\",\n            \"lt\",\n            \"lte\",\n            \"range\",\n        ],\n        \"containment\": [\"contains\"],\n    }\n\n    SUPPORTED_EXPR_CATEGORIES = {\n        CustomField.FieldDataType.STRING: (\"basic\", \"string\"),\n        CustomField.FieldDataType.URL: (\"basic\", \"string\"),\n        CustomField.FieldDataType.DATE: (\"basic\", \"arithmetic\"),\n        CustomField.FieldDataType.BOOL: (\"basic\",),\n        CustomField.FieldDataType.INT: (\"basic\", \"arithmetic\"),\n        CustomField.FieldDataType.FLOAT: (\"basic\", \"arithmetic\"),\n        CustomField.FieldDataType.MONETARY: (\"basic\", \"string\", \"arithmetic\"),\n        CustomField.FieldDataType.DOCUMENTLINK: (\"basic\", \"containment\"),\n        CustomField.FieldDataType.SELECT: (\"basic\",),\n    }\n\n    DATE_COMPONENTS = [\n        \"year\",\n        \"iso_year\",\n        \"month\",\n        \"day\",\n        \"week\",\n        \"week_day\",\n        \"iso_week_day\",\n        \"quarter\",\n    ]\n\n    def __init__(\n        self,\n        validation_prefix,\n        max_query_depth=10,\n        max_atom_count=20,\n    ) -> None:\n        \"\"\"\n        A helper class that parses the query string into a `django.db.models.Q` for filtering\n        documents based on custom field values.\n\n        The syntax of the query expression is illustrated with the below pseudo code rules:\n        1. parse([`custom_field`, \"exists\", true]):\n            matches documents with Q(custom_fields__field=`custom_field`)\n        2. parse([`custom_field`, \"exists\", false]):\n            matches documents with ~Q(custom_fields__field=`custom_field`)\n        3. parse([`custom_field`, `op`, `value`]):\n            matches documents with\n            Q(custom_fields__field=`custom_field`, custom_fields__value_`type`__`op`= `value`)\n        4. parse([\"AND\", [`q0`, `q1`, ..., `qn`]])\n            -> parse(`q0`) & parse(`q1`) & ... & parse(`qn`)\n        5. parse([\"OR\", [`q0`, `q1`, ..., `qn`]])\n            -> parse(`q0`) | parse(`q1`) | ... | parse(`qn`)\n        6. parse([\"NOT\", `q`])\n            -> ~parse(`q`)\n\n        Args:\n            validation_prefix: Used to generate the ValidationError message.\n            max_query_depth: Limits the maximum nesting depth of queries.\n            max_atom_count: Limits the maximum number of atoms (i.e., rule 1, 2, 3) in the query.\n\n        `max_query_depth` and `max_atom_count` can be set to guard against generating arbitrarily\n        complex SQL queries.\n        \"\"\"\n        self._custom_fields: dict[int | str, CustomField] = {}\n        self._validation_prefix = validation_prefix\n        # Dummy ModelSerializer used to convert a Django models.Field to serializers.Field.\n        self._model_serializer = serializers.ModelSerializer()\n        # Used for sanity check\n        self._max_query_depth = max_query_depth\n        self._max_atom_count = max_atom_count\n        self._current_depth = 0\n        self._atom_count = 0\n        # The set of annotations that we need to apply to the queryset\n        self._annotations = {}\n\n    def parse(self, query: str) -> tuple[Q, dict[str, Count]]:\n        \"\"\"\n        Parses the query string into a `django.db.models.Q`\n        and a set of annotations to be applied to the queryset.\n        \"\"\"\n        try:\n            expr = json.loads(query)\n        except json.JSONDecodeError:\n            raise serializers.ValidationError(\n                {self._validation_prefix: [_(\"Value must be valid JSON.\")]},\n            )\n        return (\n            self._parse_expr(expr, validation_prefix=self._validation_prefix),\n            self._annotations,\n        )\n\n    @handle_validation_prefix\n    def _parse_expr(self, expr) -> Q:\n        \"\"\"\n        Applies rule (1, 2, 3) or (4, 5, 6) based on the length of the expr.\n        \"\"\"\n        with self._track_query_depth():\n            if isinstance(expr, list | tuple):\n                if len(expr) == 2:\n                    return self._parse_logical_expr(*expr)\n                elif len(expr) == 3:\n                    return self._parse_atom(*expr)\n            raise serializers.ValidationError(\n                [_(\"Invalid custom field query expression\")],\n            )\n\n    @handle_validation_prefix\n    def _parse_expr_list(self, exprs) -> list[Q]:\n        \"\"\"\n        Handles [`q0`, `q1`, ..., `qn`] in rule 4 & 5.\n        \"\"\"\n        if not isinstance(exprs, list | tuple) or not exprs:\n            raise serializers.ValidationError(\n                [_(\"Invalid expression list. Must be nonempty.\")],\n            )\n        return [\n            self._parse_expr(expr, validation_prefix=i) for i, expr in enumerate(exprs)\n        ]\n\n    def _parse_logical_expr(self, op, args) -> Q:\n        \"\"\"\n        Handles rule 4, 5, 6.\n        \"\"\"\n        op_lower = op.lower()\n\n        if op_lower == \"not\":\n            return ~self._parse_expr(args, validation_prefix=1)\n\n        if op_lower == \"and\":\n            op_func = operator.and_\n        elif op_lower == \"or\":\n            op_func = operator.or_\n        else:\n            raise serializers.ValidationError(\n                {\"0\": [_(\"Invalid logical operator {op!r}\").format(op=op)]},\n            )\n\n        qs = self._parse_expr_list(args, validation_prefix=\"1\")\n        return functools.reduce(op_func, qs)\n\n    def _parse_atom(self, id_or_name, op, value) -> Q:\n        \"\"\"\n        Handles rule 1, 2, 3.\n        \"\"\"\n        # Guard against queries with too many conditions.\n        self._atom_count += 1\n        if self._atom_count > self._max_atom_count:\n            raise serializers.ValidationError(\n                [_(\"Maximum number of query conditions exceeded.\")],\n            )\n\n        custom_field = self._get_custom_field(id_or_name, validation_prefix=\"0\")\n        op = self._validate_atom_op(custom_field, op, validation_prefix=\"1\")\n        value = self._validate_atom_value(\n            custom_field,\n            op,\n            value,\n            validation_prefix=\"2\",\n        )\n\n        # Needed because not all DB backends support Array __contains\n        if (\n            custom_field.data_type == CustomField.FieldDataType.DOCUMENTLINK\n            and op == \"contains\"\n        ):\n            return self._parse_atom_doc_link_contains(custom_field, value)\n\n        value_field_name = CustomFieldInstance.get_value_field_name(\n            custom_field.data_type,\n        )\n        if (\n            custom_field.data_type == CustomField.FieldDataType.MONETARY\n            and op in self.EXPR_BY_CATEGORY[\"arithmetic\"]\n        ):\n            value_field_name = \"value_monetary_amount\"\n        has_field = Q(custom_fields__field=custom_field)\n\n        # We need to use an annotation here because different atoms\n        # might be referring to different instances of custom fields.\n        annotation_name = f\"_custom_field_filter_{len(self._annotations)}\"\n\n        # Our special exists operator.\n        if op == \"exists\":\n            annotation = Count(\"custom_fields\", filter=has_field)\n            # A Document should have > 0 match if it has this field, or 0 if doesn't.\n            query_op = \"gt\" if value else \"exact\"\n            query = Q(**{f\"{annotation_name}__{query_op}\": 0})\n        else:\n            # Check if 1) custom field name matches, and 2) value satisfies condition\n            field_filter = has_field & Q(\n                **{f\"custom_fields__{value_field_name}__{op}\": value},\n            )\n            # Annotate how many matching custom fields each document has\n            annotation = Count(\"custom_fields\", filter=field_filter)\n            # Filter document by count\n            query = Q(**{f\"{annotation_name}__gt\": 0})\n\n        self._annotations[annotation_name] = annotation\n        return query\n\n    @handle_validation_prefix\n    def _get_custom_field(self, id_or_name):\n        \"\"\"Get the CustomField instance by id or name.\"\"\"\n        if id_or_name in self._custom_fields:\n            return self._custom_fields[id_or_name]\n\n        kwargs = (\n            {\"id\": id_or_name} if isinstance(id_or_name, int) else {\"name\": id_or_name}\n        )\n        try:\n            custom_field = CustomField.objects.get(**kwargs)\n        except CustomField.DoesNotExist:\n            raise serializers.ValidationError(\n                [_(\"{name!r} is not a valid custom field.\").format(name=id_or_name)],\n            )\n        self._custom_fields[custom_field.id] = custom_field\n        self._custom_fields[custom_field.name] = custom_field\n        return custom_field\n\n    @staticmethod\n    def _split_op(full_op):\n        *prefix, op = str(full_op).rsplit(\"__\", maxsplit=1)\n        prefix = prefix[0] if prefix else None\n        return prefix, op\n\n    @handle_validation_prefix\n    def _validate_atom_op(self, custom_field, raw_op):\n        \"\"\"Check if the `op` is compatible with the type of the custom field.\"\"\"\n        prefix, op = self._split_op(raw_op)\n\n        # Check if the operator is supported for the current data_type.\n        supported = False\n        for category in self.SUPPORTED_EXPR_CATEGORIES[custom_field.data_type]:\n            if op in self.EXPR_BY_CATEGORY[category]:\n                supported = True\n                break\n\n        # Check prefix\n        if prefix is not None:\n            if (\n                prefix in self.DATE_COMPONENTS\n                and custom_field.data_type == CustomField.FieldDataType.DATE\n            ):\n                pass  # ok - e.g., \"year__exact\" for date field\n            else:\n                supported = False  # anything else is invalid\n\n        if not supported:\n            raise serializers.ValidationError(\n                [\n                    _(\"{data_type} does not support query expr {expr!r}.\").format(\n                        data_type=custom_field.data_type,\n                        expr=raw_op,\n                    ),\n                ],\n            )\n\n        return raw_op\n\n    def _get_serializer_field(self, custom_field, full_op):\n        \"\"\"Return a serializers.Field for value validation.\"\"\"\n        prefix, op = self._split_op(full_op)\n        field = None\n\n        if op in (\"isnull\", \"exists\"):\n            # `isnull` takes either True or False regardless of the data_type.\n            field = serializers.BooleanField()\n        elif (\n            custom_field.data_type == CustomField.FieldDataType.DATE\n            and prefix in self.DATE_COMPONENTS\n        ):\n            # DateField admits queries in the form of `year__exact`, etc. These take integers.\n            field = serializers.IntegerField()\n        elif custom_field.data_type == CustomField.FieldDataType.DOCUMENTLINK:\n            # We can be more specific here and make sure the value is a list.\n            field = serializers.ListField(child=serializers.IntegerField())\n        elif custom_field.data_type == CustomField.FieldDataType.SELECT:\n            # We use this custom field to permit SELECT option names.\n            field = SelectField(custom_field)\n        elif custom_field.data_type == CustomField.FieldDataType.URL:\n            # For URL fields we don't need to be strict about validation (e.g., for istartswith).\n            field = serializers.CharField()\n        else:\n            # The general case: inferred from the corresponding field in CustomFieldInstance.\n            value_field_name = CustomFieldInstance.get_value_field_name(\n                custom_field.data_type,\n            )\n            model_field = CustomFieldInstance._meta.get_field(value_field_name)\n            field_name = model_field.deconstruct()[0]\n            field_class, field_kwargs = self._model_serializer.build_standard_field(\n                field_name,\n                model_field,\n            )\n            field = field_class(**field_kwargs)\n            field.allow_null = False\n\n            # Need to set allow_blank manually because of the inconsistency in CustomFieldInstance validation.\n            # See https://github.com/paperless-ngx/paperless-ngx/issues/7361.\n            if isinstance(field, serializers.CharField):\n                field.allow_blank = True\n\n        if op == \"in\":\n            # `in` takes a list of values.\n            field = serializers.ListField(child=field, allow_empty=False)\n        elif op == \"range\":\n            # `range` takes a list of values, i.e., [start, end].\n            field = serializers.ListField(\n                child=field,\n                min_length=2,\n                max_length=2,\n            )\n\n        return field\n\n    @handle_validation_prefix\n    def _validate_atom_value(self, custom_field, op, value):\n        \"\"\"Check if `value` is valid for the custom field and `op`. Returns the validated value.\"\"\"\n        serializer_field = self._get_serializer_field(custom_field, op)\n        return serializer_field.run_validation(value)\n\n    def _parse_atom_doc_link_contains(self, custom_field, value) -> Q:\n        \"\"\"\n        Handles document link `contains` in a way that is supported by all DB backends.\n        \"\"\"\n\n        # If the value is an empty set,\n        # this is trivially true for any document with not null document links.\n        if not value:\n            return Q(\n                custom_fields__field=custom_field,\n                custom_fields__value_document_ids__isnull=False,\n            )\n\n        # First we look up reverse links from the requested documents.\n        links = CustomFieldInstance.objects.filter(\n            document_id__in=value,\n            field__data_type=CustomField.FieldDataType.DOCUMENTLINK,\n        )\n\n        # Check if any of the requested IDs are missing.\n        missing_ids = set(value) - set(link.document_id for link in links)\n        if missing_ids:\n            # The result should be an empty set in this case.\n            return Q(id__in=[])\n\n        # Take the intersection of the reverse links - this should be what we are looking for.\n        document_ids_we_want = functools.reduce(\n            operator.and_,\n            (set(link.value_document_ids) for link in links),\n        )\n\n        return Q(id__in=document_ids_we_want)\n\n    @contextmanager\n    def _track_query_depth(self):\n        # guard against queries that are too deeply nested\n        self._current_depth += 1\n        if self._current_depth > self._max_query_depth:\n            raise serializers.ValidationError([_(\"Maximum nesting depth exceeded.\")])\n        try:\n            yield\n        finally:\n            self._current_depth -= 1\n\n\n@extend_schema_field(serializers.CharField)\nclass CustomFieldQueryFilter(Filter):\n    def __init__(self, validation_prefix):\n        \"\"\"\n        A filter that filters documents based on custom field name and value.\n\n        Args:\n            validation_prefix: Used to generate the ValidationError message.\n        \"\"\"\n        super().__init__()\n        self._validation_prefix = validation_prefix\n\n    def filter(self, qs, value):\n        if not value:\n            return qs\n\n        parser = CustomFieldQueryParser(\n            self._validation_prefix,\n            max_query_depth=CUSTOM_FIELD_QUERY_MAX_DEPTH,\n            max_atom_count=CUSTOM_FIELD_QUERY_MAX_ATOMS,\n        )\n        q, annotations = parser.parse(value)\n\n        return qs.annotate(**annotations).filter(q)\n\n\nclass DocumentFilterSet(FilterSet):\n    is_tagged = BooleanFilter(\n        label=\"Is tagged\",\n        field_name=\"tags\",\n        lookup_expr=\"isnull\",\n        exclude=True,\n    )\n\n    tags__id__all = ObjectFilter(field_name=\"tags\")\n\n    tags__id__none = ObjectFilter(field_name=\"tags\", exclude=True)\n\n    tags__id__in = ObjectFilter(field_name=\"tags\", in_list=True)\n\n    correspondent__id__none = ObjectFilter(field_name=\"correspondent\", exclude=True)\n\n    document_type__id__none = ObjectFilter(field_name=\"document_type\", exclude=True)\n\n    storage_path__id__none = ObjectFilter(field_name=\"storage_path\", exclude=True)\n\n    is_in_inbox = InboxFilter()\n\n    title_content = TitleContentFilter()\n\n    owner__id__none = ObjectFilter(field_name=\"owner\", exclude=True)\n\n    custom_fields__icontains = CustomFieldsFilter()\n\n    custom_fields__id__all = ObjectFilter(field_name=\"custom_fields__field\")\n\n    custom_fields__id__none = ObjectFilter(\n        field_name=\"custom_fields__field\",\n        exclude=True,\n    )\n\n    custom_fields__id__in = ObjectFilter(\n        field_name=\"custom_fields__field\",\n        in_list=True,\n    )\n\n    has_custom_fields = BooleanFilter(\n        label=\"Has custom field\",\n        field_name=\"custom_fields\",\n        lookup_expr=\"isnull\",\n        exclude=True,\n    )\n\n    custom_field_query = CustomFieldQueryFilter(\"custom_field_query\")\n\n    shared_by__id = SharedByUser()\n\n    mime_type = MimeTypeFilter()\n\n    # Backwards compatibility\n    created__date__gt = DateFilter(field_name=\"created\", lookup_expr=\"gt\")\n    created__date__gte = DateFilter(field_name=\"created\", lookup_expr=\"gte\")\n    created__date__lt = DateFilter(field_name=\"created\", lookup_expr=\"lt\")\n    created__date__lte = DateFilter(field_name=\"created\", lookup_expr=\"lte\")\n\n    class Meta:\n        model = Document\n        fields = {\n            \"id\": ID_KWARGS,\n            \"title\": CHAR_KWARGS,\n            \"content\": CHAR_KWARGS,\n            \"archive_serial_number\": INT_KWARGS,\n            \"created\": DATE_KWARGS,\n            \"added\": DATETIME_KWARGS,\n            \"modified\": DATETIME_KWARGS,\n            \"original_filename\": CHAR_KWARGS,\n            \"checksum\": CHAR_KWARGS,\n            \"correspondent\": [\"isnull\"],\n            \"correspondent__id\": ID_KWARGS,\n            \"correspondent__name\": CHAR_KWARGS,\n            \"tags__id\": ID_KWARGS,\n            \"tags__name\": CHAR_KWARGS,\n            \"document_type\": [\"isnull\"],\n            \"document_type__id\": ID_KWARGS,\n            \"document_type__name\": CHAR_KWARGS,\n            \"storage_path\": [\"isnull\"],\n            \"storage_path__id\": ID_KWARGS,\n            \"storage_path__name\": CHAR_KWARGS,\n            \"owner\": [\"isnull\"],\n            \"owner__id\": ID_KWARGS,\n            \"custom_fields\": [\"icontains\"],\n        }\n\n\nclass ShareLinkFilterSet(FilterSet):\n    class Meta:\n        model = ShareLink\n        fields = {\n            \"created\": DATETIME_KWARGS,\n            \"expiration\": DATETIME_KWARGS,\n        }\n\n\nclass PaperlessTaskFilterSet(FilterSet):\n    acknowledged = BooleanFilter(\n        label=\"Acknowledged\",\n        field_name=\"acknowledged\",\n    )\n\n    class Meta:\n        model = PaperlessTask\n        fields = {\n            \"type\": [\"exact\"],\n            \"task_name\": [\"exact\"],\n            \"status\": [\"exact\"],\n        }\n\n\nclass ObjectOwnedOrGrantedPermissionsFilter(ObjectPermissionsFilter):\n    \"\"\"\n    A filter backend that limits results to those where the requesting user\n    has read object level permissions, owns the objects, or objects without\n    an owner (for backwards compat)\n    \"\"\"\n\n    def filter_queryset(self, request, queryset, view):\n        objects_with_perms = super().filter_queryset(request, queryset, view)\n        objects_owned = queryset.filter(owner=request.user)\n        objects_unowned = queryset.filter(owner__isnull=True)\n        return objects_with_perms | objects_owned | objects_unowned\n\n\nclass ObjectOwnedPermissionsFilter(ObjectPermissionsFilter):\n    \"\"\"\n    A filter backend that limits results to those where the requesting user\n    owns the objects or objects without an owner (for backwards compat)\n    \"\"\"\n\n    def filter_queryset(self, request, queryset, view):\n        if request.user.is_superuser:\n            return queryset\n        objects_owned = queryset.filter(owner=request.user)\n        objects_unowned = queryset.filter(owner__isnull=True)\n        return objects_owned | objects_unowned\n\n\nclass DocumentsOrderingFilter(OrderingFilter):\n    field_name = \"ordering\"\n    prefix = \"custom_field_\"\n\n    def filter_queryset(self, request, queryset, view):\n        param = request.query_params.get(\"ordering\")\n        if param and self.prefix in param:\n            custom_field_id = int(param.split(self.prefix)[1])\n            try:\n                field = CustomField.objects.get(pk=custom_field_id)\n            except CustomField.DoesNotExist:\n                raise serializers.ValidationError(\n                    {self.prefix + str(custom_field_id): [_(\"Custom field not found\")]},\n                )\n\n            annotation = None\n            match field.data_type:\n                case CustomField.FieldDataType.STRING:\n                    annotation = Subquery(\n                        CustomFieldInstance.objects.filter(\n                            document_id=OuterRef(\"id\"),\n                            field_id=custom_field_id,\n                        ).values(\"value_text\")[:1],\n                    )\n                case CustomField.FieldDataType.INT:\n                    annotation = Subquery(\n                        CustomFieldInstance.objects.filter(\n                            document_id=OuterRef(\"id\"),\n                            field_id=custom_field_id,\n                        ).values(\"value_int\")[:1],\n                    )\n                case CustomField.FieldDataType.FLOAT:\n                    annotation = Subquery(\n                        CustomFieldInstance.objects.filter(\n                            document_id=OuterRef(\"id\"),\n                            field_id=custom_field_id,\n                        ).values(\"value_float\")[:1],\n                    )\n                case CustomField.FieldDataType.DATE:\n                    annotation = Subquery(\n                        CustomFieldInstance.objects.filter(\n                            document_id=OuterRef(\"id\"),\n                            field_id=custom_field_id,\n                        ).values(\"value_date\")[:1],\n                    )\n                case CustomField.FieldDataType.MONETARY:\n                    annotation = Subquery(\n                        CustomFieldInstance.objects.filter(\n                            document_id=OuterRef(\"id\"),\n                            field_id=custom_field_id,\n                        ).values(\"value_monetary_amount\")[:1],\n                    )\n                case CustomField.FieldDataType.SELECT:\n                    # Select options are a little more complicated since the value is the id of the option, not\n                    # the label. Additionally, to support sqlite we can't use StringAgg, so we need to create a\n                    # case statement for each option, setting the value to the index of the option in a list\n                    # sorted by label, and then summing the results to give a single value for the annotation\n\n                    select_options = sorted(\n                        field.extra_data.get(\"select_options\", []),\n                        key=lambda x: x.get(\"label\"),\n                    )\n                    whens = [\n                        When(\n                            custom_fields__field_id=custom_field_id,\n                            custom_fields__value_select=option.get(\"id\"),\n                            then=Value(idx, output_field=IntegerField()),\n                        )\n                        for idx, option in enumerate(select_options)\n                    ]\n                    whens.append(\n                        When(\n                            custom_fields__field_id=custom_field_id,\n                            custom_fields__value_select__isnull=True,\n                            then=Value(\n                                len(select_options),\n                                output_field=IntegerField(),\n                            ),\n                        ),\n                    )\n                    annotation = Sum(\n                        Case(\n                            *whens,\n                            default=Value(0),\n                            output_field=IntegerField(),\n                        ),\n                    )\n                case CustomField.FieldDataType.DOCUMENTLINK:\n                    annotation = Subquery(\n                        CustomFieldInstance.objects.filter(\n                            document_id=OuterRef(\"id\"),\n                            field_id=custom_field_id,\n                        ).values(\"value_document_ids\")[:1],\n                    )\n                case CustomField.FieldDataType.URL:\n                    annotation = Subquery(\n                        CustomFieldInstance.objects.filter(\n                            document_id=OuterRef(\"id\"),\n                            field_id=custom_field_id,\n                        ).values(\"value_url\")[:1],\n                    )\n                case CustomField.FieldDataType.BOOL:\n                    annotation = Subquery(\n                        CustomFieldInstance.objects.filter(\n                            document_id=OuterRef(\"id\"),\n                            field_id=custom_field_id,\n                        ).values(\"value_bool\")[:1],\n                    )\n\n            if not annotation:\n                # Only happens if a new data type is added and not handled here\n                raise ValueError(\"Invalid custom field data type\")\n\n            queryset = (\n                queryset.annotate(\n                    # We need to annotate the queryset with the custom field value\n                    custom_field_value=annotation,\n                    # We also need to annotate the queryset with a boolean for sorting whether the field exists\n                    has_field=Exists(\n                        CustomFieldInstance.objects.filter(\n                            document_id=OuterRef(\"id\"),\n                            field_id=custom_field_id,\n                        ),\n                    ),\n                )\n                .order_by(\n                    \"-has_field\",\n                    param.replace(\n                        self.prefix + str(custom_field_id),\n                        \"custom_field_value\",\n                    ),\n                )\n                .distinct()\n            )\n\n        return super().filter_queryset(request, queryset, view)\n", "metadata": {"source": "github.com/paperless-ngx/paperless-ngx", "file_path": "src/documents/filters.py", "language": "python", "complexity": "complex", "star_count": 29534, "quality_score": 1.0, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": true, "has_documentation": true, "has_type_hints": true, "has_testing": false, "has_logging": true, "function_count": 29, "class_count": 29, "line_count": 967}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "database_integration", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create database model with ORM relationships and queries with SQLAlchemy/Django ORM patterns\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from __future__ import annotations\n\nimport logging\nimport math\nimport re\nfrom collections import Counter\nfrom contextlib import contextmanager\nfrom datetime import datetime\nfrom datetime import time\nfrom datetime import timedelta\nfrom datetime import timezone\nfrom shutil import rmtree\nfrom typing import TYPE_CHECKING\nfrom typing import Literal\n\nfrom django.conf import settings\nfrom django.utils import timezone as django_timezone\nfrom django.utils.timezone import get_current_timezone\nfrom django.utils.timezone import now\nfrom guardian.shortcuts import get_users_with_perms\nfrom whoosh import classify\nfrom whoosh import highlight\nfrom whoosh import query\nfrom whoosh.fields import BOOLEAN\nfrom whoosh.fields import DATETIME\nfrom whoosh.fields import KEYWORD\nfrom whoosh.fields import NUMERIC\nfrom whoosh.fields import TEXT\nfrom whoosh.fields import Schema\nfrom whoosh.highlight import HtmlFormatter\nfrom whoosh.idsets import BitSet\nfrom whoosh.idsets import DocIdSet\nfrom whoosh.index import FileIndex\nfrom whoosh.index import create_in\nfrom whoosh.index import exists_in\nfrom whoosh.index import open_dir\nfrom whoosh.qparser import MultifieldParser\nfrom whoosh.qparser import QueryParser\nfrom whoosh.qparser.dateparse import DateParserPlugin\nfrom whoosh.qparser.dateparse import English\nfrom whoosh.qparser.plugins import FieldsPlugin\nfrom whoosh.scoring import TF_IDF\nfrom whoosh.util.times import timespan\nfrom whoosh.writing import AsyncWriter\n\nfrom documents.models import CustomFieldInstance\nfrom documents.models import Document\nfrom documents.models import Note\nfrom documents.models import User\n\nif TYPE_CHECKING:\n    from django.db.models import QuerySet\n    from whoosh.reading import IndexReader\n    from whoosh.searching import ResultsPage\n    from whoosh.searching import Searcher\n\nlogger = logging.getLogger(\"paperless.index\")\n\n\ndef get_schema() -> Schema:\n    return Schema(\n        id=NUMERIC(stored=True, unique=True),\n        title=TEXT(sortable=True),\n        content=TEXT(),\n        asn=NUMERIC(sortable=True, signed=False),\n        correspondent=TEXT(sortable=True),\n        correspondent_id=NUMERIC(),\n        has_correspondent=BOOLEAN(),\n        tag=KEYWORD(commas=True, scorable=True, lowercase=True),\n        tag_id=KEYWORD(commas=True, scorable=True),\n        has_tag=BOOLEAN(),\n        type=TEXT(sortable=True),\n        type_id=NUMERIC(),\n        has_type=BOOLEAN(),\n        created=DATETIME(sortable=True),\n        modified=DATETIME(sortable=True),\n        added=DATETIME(sortable=True),\n        path=TEXT(sortable=True),\n        path_id=NUMERIC(),\n        has_path=BOOLEAN(),\n        notes=TEXT(),\n        num_notes=NUMERIC(sortable=True, signed=False),\n        custom_fields=TEXT(),\n        custom_field_count=NUMERIC(sortable=True, signed=False),\n        has_custom_fields=BOOLEAN(),\n        custom_fields_id=KEYWORD(commas=True),\n        owner=TEXT(),\n        owner_id=NUMERIC(),\n        has_owner=BOOLEAN(),\n        viewer_id=KEYWORD(commas=True),\n        checksum=TEXT(),\n        page_count=NUMERIC(sortable=True),\n        original_filename=TEXT(sortable=True),\n        is_shared=BOOLEAN(),\n    )\n\n\ndef open_index(*, recreate=False) -> FileIndex:\n    try:\n        if exists_in(settings.INDEX_DIR) and not recreate:\n            return open_dir(settings.INDEX_DIR, schema=get_schema())\n    except Exception:\n        logger.exception(\"Error while opening the index, recreating.\")\n\n    # create_in doesn't handle corrupted indexes very well, remove the directory entirely first\n    if settings.INDEX_DIR.is_dir():\n        rmtree(settings.INDEX_DIR)\n    settings.INDEX_DIR.mkdir(parents=True, exist_ok=True)\n\n    return create_in(settings.INDEX_DIR, get_schema())\n\n\n@contextmanager\ndef open_index_writer(*, optimize=False) -> AsyncWriter:\n    writer = AsyncWriter(open_index())\n\n    try:\n        yield writer\n    except Exception as e:\n        logger.exception(str(e))\n        writer.cancel()\n    finally:\n        writer.commit(optimize=optimize)\n\n\n@contextmanager\ndef open_index_searcher() -> Searcher:\n    searcher = open_index().searcher()\n\n    try:\n        yield searcher\n    finally:\n        searcher.close()\n\n\ndef update_document(writer: AsyncWriter, doc: Document) -> None:\n    tags = \",\".join([t.name for t in doc.tags.all()])\n    tags_ids = \",\".join([str(t.id) for t in doc.tags.all()])\n    notes = \",\".join([str(c.note) for c in Note.objects.filter(document=doc)])\n    custom_fields = \",\".join(\n        [str(c) for c in CustomFieldInstance.objects.filter(document=doc)],\n    )\n    custom_fields_ids = \",\".join(\n        [str(f.field.id) for f in CustomFieldInstance.objects.filter(document=doc)],\n    )\n    asn: int | None = doc.archive_serial_number\n    if asn is not None and (\n        asn < Document.ARCHIVE_SERIAL_NUMBER_MIN\n        or asn > Document.ARCHIVE_SERIAL_NUMBER_MAX\n    ):\n        logger.error(\n            f\"Not indexing Archive Serial Number {asn} of document {doc.pk}. \"\n            f\"ASN is out of range \"\n            f\"[{Document.ARCHIVE_SERIAL_NUMBER_MIN:,}, \"\n            f\"{Document.ARCHIVE_SERIAL_NUMBER_MAX:,}.\",\n        )\n        asn = 0\n    users_with_perms = get_users_with_perms(\n        doc,\n        only_with_perms_in=[\"view_document\"],\n    )\n    viewer_ids: str = \",\".join([str(u.id) for u in users_with_perms])\n    writer.update_document(\n        id=doc.pk,\n        title=doc.title,\n        content=doc.content,\n        correspondent=doc.correspondent.name if doc.correspondent else None,\n        correspondent_id=doc.correspondent.id if doc.correspondent else None,\n        has_correspondent=doc.correspondent is not None,\n        tag=tags if tags else None,\n        tag_id=tags_ids if tags_ids else None,\n        has_tag=len(tags) > 0,\n        type=doc.document_type.name if doc.document_type else None,\n        type_id=doc.document_type.id if doc.document_type else None,\n        has_type=doc.document_type is not None,\n        created=datetime.combine(doc.created, time.min),\n        added=doc.added,\n        asn=asn,\n        modified=doc.modified,\n        path=doc.storage_path.name if doc.storage_path else None,\n        path_id=doc.storage_path.id if doc.storage_path else None,\n        has_path=doc.storage_path is not None,\n        notes=notes,\n        num_notes=len(notes),\n        custom_fields=custom_fields,\n        custom_field_count=len(doc.custom_fields.all()),\n        has_custom_fields=len(custom_fields) > 0,\n        custom_fields_id=custom_fields_ids if custom_fields_ids else None,\n        owner=doc.owner.username if doc.owner else None,\n        owner_id=doc.owner.id if doc.owner else None,\n        has_owner=doc.owner is not None,\n        viewer_id=viewer_ids if viewer_ids else None,\n        checksum=doc.checksum,\n        page_count=doc.page_count,\n        original_filename=doc.original_filename,\n        is_shared=len(viewer_ids) > 0,\n    )\n\n\ndef remove_document(writer: AsyncWriter, doc: Document) -> None:\n    remove_document_by_id(writer, doc.pk)\n\n\ndef remove_document_by_id(writer: AsyncWriter, doc_id) -> None:\n    writer.delete_by_term(\"id\", doc_id)\n\n\ndef add_or_update_document(document: Document) -> None:\n    with open_index_writer() as writer:\n        update_document(writer, document)\n\n\ndef remove_document_from_index(document: Document) -> None:\n    with open_index_writer() as writer:\n        remove_document(writer, document)\n\n\nclass MappedDocIdSet(DocIdSet):\n    \"\"\"\n    A DocIdSet backed by a set of `Document` IDs.\n    Supports efficiently looking up if a whoosh docnum is in the provided `filter_queryset`.\n    \"\"\"\n\n    def __init__(self, filter_queryset: QuerySet, ixreader: IndexReader) -> None:\n        super().__init__()\n        document_ids = filter_queryset.order_by(\"id\").values_list(\"id\", flat=True)\n        max_id = document_ids.last() or 0\n        self.document_ids = BitSet(document_ids, size=max_id)\n        self.ixreader = ixreader\n\n    def __contains__(self, docnum) -> bool:\n        document_id = self.ixreader.stored_fields(docnum)[\"id\"]\n        return document_id in self.document_ids\n\n    def __bool__(self) -> Literal[True]:\n        # searcher.search ignores a filter if it's \"falsy\".\n        # We use this hack so this DocIdSet, when used as a filter, is never ignored.\n        return True\n\n\nclass DelayedQuery:\n    def _get_query(self):\n        raise NotImplementedError  # pragma: no cover\n\n    def _get_query_sortedby(self) -> tuple[None, Literal[False]] | tuple[str, bool]:\n        if \"ordering\" not in self.query_params:\n            return None, False\n\n        field: str = self.query_params[\"ordering\"]\n\n        sort_fields_map: dict[str, str] = {\n            \"created\": \"created\",\n            \"modified\": \"modified\",\n            \"added\": \"added\",\n            \"title\": \"title\",\n            \"correspondent__name\": \"correspondent\",\n            \"document_type__name\": \"type\",\n            \"archive_serial_number\": \"asn\",\n            \"num_notes\": \"num_notes\",\n            \"owner\": \"owner\",\n            \"page_count\": \"page_count\",\n        }\n\n        if field.startswith(\"-\"):\n            field = field[1:]\n            reverse = True\n        else:\n            reverse = False\n\n        if field not in sort_fields_map:\n            return None, False\n        else:\n            return sort_fields_map[field], reverse\n\n    def __init__(\n        self,\n        searcher: Searcher,\n        query_params,\n        page_size,\n        filter_queryset: QuerySet,\n    ) -> None:\n        self.searcher = searcher\n        self.query_params = query_params\n        self.page_size = page_size\n        self.saved_results = dict()\n        self.first_score = None\n        self.filter_queryset = filter_queryset\n        self.suggested_correction = None\n\n    def __len__(self) -> int:\n        page = self[0:1]\n        return len(page)\n\n    def __getitem__(self, item):\n        if item.start in self.saved_results:\n            return self.saved_results[item.start]\n\n        q, mask, suggested_correction = self._get_query()\n        self.suggested_correction = suggested_correction\n        sortedby, reverse = self._get_query_sortedby()\n\n        page: ResultsPage = self.searcher.search_page(\n            q,\n            mask=mask,\n            filter=MappedDocIdSet(self.filter_queryset, self.searcher.ixreader),\n            pagenum=math.floor(item.start / self.page_size) + 1,\n            pagelen=self.page_size,\n            sortedby=sortedby,\n            reverse=reverse,\n        )\n        page.results.fragmenter = highlight.ContextFragmenter(surround=50)\n        page.results.formatter = HtmlFormatter(tagname=\"span\", between=\" ... \")\n\n        if not self.first_score and len(page.results) > 0 and sortedby is None:\n            self.first_score = page.results[0].score\n\n        page.results.top_n = list(\n            map(\n                lambda hit: (\n                    (hit[0] / self.first_score) if self.first_score else None,\n                    hit[1],\n                ),\n                page.results.top_n,\n            ),\n        )\n\n        self.saved_results[item.start] = page\n\n        return page\n\n\nclass LocalDateParser(English):\n    def reverse_timezone_offset(self, d):\n        return (d.replace(tzinfo=django_timezone.get_current_timezone())).astimezone(\n            timezone.utc,\n        )\n\n    def date_from(self, *args, **kwargs):\n        d = super().date_from(*args, **kwargs)\n        if isinstance(d, timespan):\n            d.start = self.reverse_timezone_offset(d.start)\n            d.end = self.reverse_timezone_offset(d.end)\n        elif isinstance(d, datetime):\n            d = self.reverse_timezone_offset(d)\n        return d\n\n\nclass DelayedFullTextQuery(DelayedQuery):\n    def _get_query(self) -> tuple:\n        q_str = self.query_params[\"query\"]\n        q_str = rewrite_natural_date_keywords(q_str)\n        qp = MultifieldParser(\n            [\n                \"content\",\n                \"title\",\n                \"correspondent\",\n                \"tag\",\n                \"type\",\n                \"notes\",\n                \"custom_fields\",\n            ],\n            self.searcher.ixreader.schema,\n        )\n        qp.add_plugin(\n            DateParserPlugin(\n                basedate=django_timezone.now(),\n                dateparser=LocalDateParser(),\n            ),\n        )\n        q = qp.parse(q_str)\n        suggested_correction = None\n        try:\n            corrected = self.searcher.correct_query(q, q_str)\n            if corrected.string != q_str:\n                suggested_correction = corrected.string\n        except Exception as e:\n            logger.info(\n                \"Error while correcting query %s: %s\",\n                f\"{q_str!r}\",\n                e,\n            )\n\n        return q, None, suggested_correction\n\n\nclass DelayedMoreLikeThisQuery(DelayedQuery):\n    def _get_query(self) -> tuple:\n        more_like_doc_id = int(self.query_params[\"more_like_id\"])\n        content = Document.objects.get(id=more_like_doc_id).content\n\n        docnum = self.searcher.document_number(id=more_like_doc_id)\n        kts = self.searcher.key_terms_from_text(\n            \"content\",\n            content,\n            numterms=20,\n            model=classify.Bo1Model,\n            normalize=False,\n        )\n        q = query.Or(\n            [query.Term(\"content\", word, boost=weight) for word, weight in kts],\n        )\n        mask: set = {docnum}\n\n        return q, mask, None\n\n\ndef autocomplete(\n    ix: FileIndex,\n    term: str,\n    limit: int = 10,\n    user: User | None = None,\n) -> list:\n    \"\"\"\n    Mimics whoosh.reading.IndexReader.most_distinctive_terms with permissions\n    and without scoring\n    \"\"\"\n    terms = []\n\n    with ix.searcher(weighting=TF_IDF()) as s:\n        qp = QueryParser(\"content\", schema=ix.schema)\n        # Don't let searches with a query that happen to match a field override the\n        # content field query instead and return bogus, not text data\n        qp.remove_plugin_class(FieldsPlugin)\n        q = qp.parse(f\"{term.lower()}*\")\n        user_criterias: list = get_permissions_criterias(user)\n\n        results = s.search(\n            q,\n            terms=True,\n            filter=query.Or(user_criterias) if user_criterias is not None else None,\n        )\n\n        termCounts = Counter()\n        if results.has_matched_terms():\n            for hit in results:\n                for _, match in hit.matched_terms():\n                    termCounts[match] += 1\n            terms = [t for t, _ in termCounts.most_common(limit)]\n\n        term_encoded: bytes = term.encode(\"UTF-8\")\n        if term_encoded in terms:\n            terms.insert(0, terms.pop(terms.index(term_encoded)))\n\n    return terms\n\n\ndef get_permissions_criterias(user: User | None = None) -> list:\n    user_criterias = [query.Term(\"has_owner\", text=False)]\n    if user is not None:\n        if user.is_superuser:  # superusers see all docs\n            user_criterias = []\n        else:\n            user_criterias.append(query.Term(\"owner_id\", user.id))\n            user_criterias.append(\n                query.Term(\"viewer_id\", str(user.id)),\n            )\n    return user_criterias\n\n\ndef rewrite_natural_date_keywords(query_string: str) -> str:\n    \"\"\"\n    Rewrites natural date keywords (e.g. added:today or added:\"yesterday\") to UTC range syntax for Whoosh.\n    \"\"\"\n\n    tz = get_current_timezone()\n    local_now = now().astimezone(tz)\n\n    today = local_now.date()\n    yesterday = today - timedelta(days=1)\n\n    ranges = {\n        \"today\": (\n            datetime.combine(today, time.min, tzinfo=tz),\n            datetime.combine(today, time.max, tzinfo=tz),\n        ),\n        \"yesterday\": (\n            datetime.combine(yesterday, time.min, tzinfo=tz),\n            datetime.combine(yesterday, time.max, tzinfo=tz),\n        ),\n    }\n\n    pattern = r\"(\\b(?:added|created))\\s*:\\s*[\\\"']?(today|yesterday)[\\\"']?\"\n\n    def repl(m):\n        field, keyword = m.group(1), m.group(2)\n        start, end = ranges[keyword]\n        start_str = start.astimezone(timezone.utc).strftime(\"%Y%m%d%H%M%S\")\n        end_str = end.astimezone(timezone.utc).strftime(\"%Y%m%d%H%M%S\")\n        return f\"{field}:[{start_str} TO {end_str}]\"\n\n    return re.sub(pattern, repl, query_string)\n", "metadata": {"source": "github.com/paperless-ngx/paperless-ngx", "file_path": "src/documents/index.py", "language": "python", "complexity": "complex", "star_count": 29534, "quality_score": 1.0, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": true, "has_documentation": true, "has_type_hints": true, "has_testing": false, "has_logging": true, "function_count": 25, "class_count": 5, "line_count": 492}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "database_integration", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create database model with ORM relationships and queries\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from django.core.management.base import BaseCommand\nfrom django.db import connection\nfrom django.db import models\n\nfrom documents.models import Document\n\n\nclass Command(BaseCommand):\n    # This code is taken almost entirely from https://github.com/wagtail/wagtail/pull/11912 with all credit to the original author.\n    help = \"Converts UUID columns from char type to the native UUID type used in MariaDB 10.7+ and Django 5.0+.\"\n\n    def convert_field(self, model, field_name, *, null=False):\n        if model._meta.get_field(field_name).model != model:  # pragma: no cover\n            # Field is inherited from a parent model\n            return\n\n        if not model._meta.managed:  # pragma: no cover\n            # The migration framework skips unmanaged models, so we should too\n            return\n\n        old_field = models.CharField(null=null, max_length=36)\n        old_field.set_attributes_from_name(field_name)\n\n        new_field = models.UUIDField(null=null)\n        new_field.set_attributes_from_name(field_name)\n\n        with connection.schema_editor() as schema_editor:\n            schema_editor.alter_field(model, old_field, new_field)\n            self.stdout.write(\n                self.style.SUCCESS(\n                    f\"Successfully converted {model._meta.label} {field_name} field to UUID type.\",\n                ),\n            )\n\n    def handle(self, **options):\n        self.convert_field(Document, \"transaction_id\", null=True)\n", "metadata": {"source": "github.com/paperless-ngx/paperless-ngx", "file_path": "src/documents/management/commands/convert_mariadb_uuid.py", "language": "python", "complexity": "simple", "star_count": 29534, "quality_score": 0.92, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 2, "class_count": 1, "line_count": 37}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "database_integration", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create database model with ORM relationships and queries\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from pathlib import Path\n\nfrom django.conf import settings\nfrom django.core.management.base import BaseCommand\nfrom django.core.management.base import CommandError\n\nfrom documents.models import Document\nfrom paperless.db import GnuPG\n\n\nclass Command(BaseCommand):\n    help = (\n        \"This is how you migrate your stored documents from an encrypted \"\n        \"state to an unencrypted one (or vice-versa)\"\n    )\n\n    def add_arguments(self, parser) -> None:\n        parser.add_argument(\n            \"--passphrase\",\n            help=(\n                \"If PAPERLESS_PASSPHRASE isn't set already, you need to specify it here\"\n            ),\n        )\n\n    def handle(self, *args, **options) -> None:\n        try:\n            self.stdout.write(\n                self.style.WARNING(\n                    \"\\n\\n\"\n                    \"WARNING: This script is going to work directly on your \"\n                    \"document originals, so\\n\"\n                    \"WARNING: you probably shouldn't run \"\n                    \"this unless you've got a recent backup\\n\"\n                    \"WARNING: handy.  It \"\n                    \"*should* work without a hitch, but be safe and backup your\\n\"\n                    \"WARNING: stuff first.\\n\\n\"\n                    \"Hit Ctrl+C to exit now, or Enter to \"\n                    \"continue.\\n\\n\",\n                ),\n            )\n            _ = input()\n        except KeyboardInterrupt:\n            return\n\n        passphrase = options[\"passphrase\"] or settings.PASSPHRASE\n        if not passphrase:\n            raise CommandError(\n                \"Passphrase not defined.  Please set it with --passphrase or \"\n                \"by declaring it in your environment or your config.\",\n            )\n\n        self.__gpg_to_unencrypted(passphrase)\n\n    def __gpg_to_unencrypted(self, passphrase: str) -> None:\n        encrypted_files = Document.objects.filter(\n            storage_type=Document.STORAGE_TYPE_GPG,\n        )\n\n        for document in encrypted_files:\n            self.stdout.write(f\"Decrypting {document}\")\n\n            old_paths = [document.source_path, document.thumbnail_path]\n\n            with document.source_file as file_handle:\n                raw_document = GnuPG.decrypted(file_handle, passphrase)\n            with document.thumbnail_file as file_handle:\n                raw_thumb = GnuPG.decrypted(file_handle, passphrase)\n\n            document.storage_type = Document.STORAGE_TYPE_UNENCRYPTED\n\n            ext: str = Path(document.filename).suffix\n\n            if not ext == \".gpg\":\n                raise CommandError(\n                    f\"Abort: encrypted file {document.source_path} does not \"\n                    f\"end with .gpg\",\n                )\n\n            document.filename = Path(document.filename).stem\n\n            with document.source_path.open(\"wb\") as f:\n                f.write(raw_document)\n\n            with document.thumbnail_path.open(\"wb\") as f:\n                f.write(raw_thumb)\n\n            Document.objects.filter(id=document.id).update(\n                storage_type=document.storage_type,\n                filename=document.filename,\n            )\n\n            for path in old_paths:\n                path.unlink()\n", "metadata": {"source": "github.com/paperless-ngx/paperless-ngx", "file_path": "src/documents/management/commands/decrypt_documents.py", "language": "python", "complexity": "medium", "star_count": 29534, "quality_score": 1.0, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": true, "has_documentation": false, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 3, "class_count": 1, "line_count": 94}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "database_integration", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create database model with ORM relationships and queries\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"\nProvides various authentication policies.\n\"\"\"\nimport base64\nimport binascii\n\nfrom django.contrib.auth import authenticate, get_user_model\nfrom django.middleware.csrf import CsrfViewMiddleware\nfrom django.utils.translation import gettext_lazy as _\n\nfrom rest_framework import HTTP_HEADER_ENCODING, exceptions\n\n\ndef get_authorization_header(request):\n    \"\"\"\n    Return request's 'Authorization:' header, as a bytestring.\n\n    Hide some test client ickyness where the header can be unicode.\n    \"\"\"\n    auth = request.META.get('HTTP_AUTHORIZATION', b'')\n    if isinstance(auth, str):\n        # Work around django test client oddness\n        auth = auth.encode(HTTP_HEADER_ENCODING)\n    return auth\n\n\nclass CSRFCheck(CsrfViewMiddleware):\n    def _reject(self, request, reason):\n        # Return the failure reason instead of an HttpResponse\n        return reason\n\n\nclass BaseAuthentication:\n    \"\"\"\n    All authentication classes should extend BaseAuthentication.\n    \"\"\"\n\n    def authenticate(self, request):\n        \"\"\"\n        Authenticate the request and return a two-tuple of (user, token).\n        \"\"\"\n        raise NotImplementedError(\".authenticate() must be overridden.\")\n\n    def authenticate_header(self, request):\n        \"\"\"\n        Return a string to be used as the value of the `WWW-Authenticate`\n        header in a `401 Unauthenticated` response, or `None` if the\n        authentication scheme should return `403 Permission Denied` responses.\n        \"\"\"\n        pass\n\n\nclass BasicAuthentication(BaseAuthentication):\n    \"\"\"\n    HTTP Basic authentication against username/password.\n    \"\"\"\n    www_authenticate_realm = 'api'\n\n    def authenticate(self, request):\n        \"\"\"\n        Returns a `User` if a correct username and password have been supplied\n        using HTTP Basic authentication.  Otherwise returns `None`.\n        \"\"\"\n        auth = get_authorization_header(request).split()\n\n        if not auth or auth[0].lower() != b'basic':\n            return None\n\n        if len(auth) == 1:\n            msg = _('Invalid basic header. No credentials provided.')\n            raise exceptions.AuthenticationFailed(msg)\n        elif len(auth) > 2:\n            msg = _('Invalid basic header. Credentials string should not contain spaces.')\n            raise exceptions.AuthenticationFailed(msg)\n\n        try:\n            try:\n                auth_decoded = base64.b64decode(auth[1]).decode('utf-8')\n            except UnicodeDecodeError:\n                auth_decoded = base64.b64decode(auth[1]).decode('latin-1')\n\n            userid, password = auth_decoded.split(':', 1)\n        except (TypeError, ValueError, UnicodeDecodeError, binascii.Error):\n            msg = _('Invalid basic header. Credentials not correctly base64 encoded.')\n            raise exceptions.AuthenticationFailed(msg)\n\n        return self.authenticate_credentials(userid, password, request)\n\n    def authenticate_credentials(self, userid, password, request=None):\n        \"\"\"\n        Authenticate the userid and password against username and password\n        with optional request for context.\n        \"\"\"\n        credentials = {\n            get_user_model().USERNAME_FIELD: userid,\n            'password': password\n        }\n        user = authenticate(request=request, **credentials)\n\n        if user is None:\n            raise exceptions.AuthenticationFailed(_('Invalid username/password.'))\n\n        if not user.is_active:\n            raise exceptions.AuthenticationFailed(_('User inactive or deleted.'))\n\n        return (user, None)\n\n    def authenticate_header(self, request):\n        return 'Basic realm=\"%s\"' % self.www_authenticate_realm\n\n\nclass SessionAuthentication(BaseAuthentication):\n    \"\"\"\n    Use Django's session framework for authentication.\n    \"\"\"\n\n    def authenticate(self, request):\n        \"\"\"\n        Returns a `User` if the request session currently has a logged in user.\n        Otherwise returns `None`.\n        \"\"\"\n\n        # Get the session-based user from the underlying HttpRequest object\n        user = getattr(request._request, 'user', None)\n\n        # Unauthenticated, CSRF validation not required\n        if not user or not user.is_active:\n            return None\n\n        self.enforce_csrf(request)\n\n        # CSRF passed with authenticated user\n        return (user, None)\n\n    def enforce_csrf(self, request):\n        \"\"\"\n        Enforce CSRF validation for session based authentication.\n        \"\"\"\n        def dummy_get_response(request):  # pragma: no cover\n            return None\n\n        check = CSRFCheck(dummy_get_response)\n        # populates request.META['CSRF_COOKIE'], which is used in process_view()\n        check.process_request(request)\n        reason = check.process_view(request, None, (), {})\n        if reason:\n            # CSRF failed, bail with explicit error message\n            raise exceptions.PermissionDenied('CSRF Failed: %s' % reason)\n\n\nclass TokenAuthentication(BaseAuthentication):\n    \"\"\"\n    Simple token based authentication.\n\n    Clients should authenticate by passing the token key in the \"Authorization\"\n    HTTP header, prepended with the string \"Token \".  For example:\n\n        Authorization: Token 401f7ac837da42b97f613d789819ff93537bee6a\n    \"\"\"\n\n    keyword = 'Token'\n    model = None\n\n    def get_model(self):\n        if self.model is not None:\n            return self.model\n        from rest_framework.authtoken.models import Token\n        return Token\n\n    \"\"\"\n    A custom token model may be used, but must have the following properties.\n\n    * key -- The string identifying the token\n    * user -- The user to which the token belongs\n    \"\"\"\n\n    def authenticate(self, request):\n        auth = get_authorization_header(request).split()\n\n        if not auth or auth[0].lower() != self.keyword.lower().encode():\n            return None\n\n        if len(auth) == 1:\n            msg = _('Invalid token header. No credentials provided.')\n            raise exceptions.AuthenticationFailed(msg)\n        elif len(auth) > 2:\n            msg = _('Invalid token header. Token string should not contain spaces.')\n            raise exceptions.AuthenticationFailed(msg)\n\n        try:\n            token = auth[1].decode()\n        except UnicodeError:\n            msg = _('Invalid token header. Token string should not contain invalid characters.')\n            raise exceptions.AuthenticationFailed(msg)\n\n        return self.authenticate_credentials(token)\n\n    def authenticate_credentials(self, key):\n        model = self.get_model()\n        try:\n            token = model.objects.select_related('user').get(key=key)\n        except model.DoesNotExist:\n            raise exceptions.AuthenticationFailed(_('Invalid token.'))\n\n        if not token.user.is_active:\n            raise exceptions.AuthenticationFailed(_('User inactive or deleted.'))\n\n        return (token.user, token)\n\n    def authenticate_header(self, request):\n        return self.keyword\n\n\nclass RemoteUserAuthentication(BaseAuthentication):\n    \"\"\"\n    REMOTE_USER authentication.\n\n    To use this, set up your web server to perform authentication, which will\n    set the REMOTE_USER environment variable. You will need to have\n    'django.contrib.auth.backends.RemoteUserBackend in your\n    AUTHENTICATION_BACKENDS setting\n    \"\"\"\n\n    # Name of request header to grab username from.  This will be the key as\n    # used in the request.META dictionary, i.e. the normalization of headers to\n    # all uppercase and the addition of \"HTTP_\" prefix apply.\n    header = \"REMOTE_USER\"\n\n    def authenticate(self, request):\n        user = authenticate(request=request, remote_user=request.META.get(self.header))\n        if user and user.is_active:\n            return (user, None)\n", "metadata": {"source": "github.com/encode/django-rest-framework", "file_path": "rest_framework/authentication.py", "language": "python", "complexity": "complex", "star_count": 29333, "quality_score": 0.98, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": true, "has_documentation": true, "has_type_hints": true, "has_testing": false, "has_logging": true, "function_count": 15, "class_count": 6, "line_count": 233}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "database_integration", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create database model with ORM relationships and queries with SQLAlchemy/Django ORM patterns\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from django.contrib import admin\nfrom django.contrib.admin.utils import quote\nfrom django.contrib.admin.views.main import ChangeList\nfrom django.contrib.auth import get_user_model\nfrom django.core.exceptions import ValidationError\nfrom django.urls import reverse\nfrom django.utils.translation import gettext_lazy as _\n\nfrom rest_framework.authtoken.models import Token, TokenProxy\n\nUser = get_user_model()\n\n\nclass TokenChangeList(ChangeList):\n    \"\"\"Map to matching User id\"\"\"\n    def url_for_result(self, result):\n        pk = result.user.pk\n        return reverse('admin:%s_%s_change' % (self.opts.app_label,\n                                               self.opts.model_name),\n                       args=(quote(pk),),\n                       current_app=self.model_admin.admin_site.name)\n\n\nclass TokenAdmin(admin.ModelAdmin):\n    list_display = ('key', 'user', 'created')\n    fields = ('user',)\n    search_fields = ('user__username',)\n    search_help_text = _('Username')\n    ordering = ('-created',)\n    actions = None  # Actions not compatible with mapped IDs.\n\n    def get_changelist(self, request, **kwargs):\n        return TokenChangeList\n\n    def get_object(self, request, object_id, from_field=None):\n        \"\"\"\n        Map from User ID to matching Token.\n        \"\"\"\n        queryset = self.get_queryset(request)\n        field = User._meta.pk\n        try:\n            object_id = field.to_python(object_id)\n            user = User.objects.get(**{field.name: object_id})\n            return queryset.get(user=user)\n        except (queryset.model.DoesNotExist, User.DoesNotExist, ValidationError, ValueError):\n            return None\n\n    def delete_model(self, request, obj):\n        # Map back to actual Token, since delete() uses pk.\n        token = Token.objects.get(key=obj.key)\n        return super().delete_model(request, token)\n\n\nadmin.site.register(TokenProxy, TokenAdmin)\n", "metadata": {"source": "github.com/encode/django-rest-framework", "file_path": "rest_framework/authtoken/admin.py", "language": "python", "complexity": "medium", "star_count": 29333, "quality_score": 0.98, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": true, "has_documentation": true, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 4, "class_count": 2, "line_count": 55}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "database_integration", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create database model with ORM relationships and queries with SQLAlchemy/Django ORM patterns\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from django.contrib.auth import get_user_model\nfrom django.core.management.base import BaseCommand, CommandError\n\nfrom rest_framework.authtoken.models import Token\n\nUserModel = get_user_model()\n\n\nclass Command(BaseCommand):\n    help = 'Create DRF Token for a given user'\n\n    def create_user_token(self, username, reset_token):\n        user = UserModel._default_manager.get_by_natural_key(username)\n\n        if reset_token:\n            Token.objects.filter(user=user).delete()\n\n        token = Token.objects.get_or_create(user=user)\n        return token[0]\n\n    def add_arguments(self, parser):\n        parser.add_argument('username', type=str)\n\n        parser.add_argument(\n            '-r',\n            '--reset',\n            action='store_true',\n            dest='reset_token',\n            default=False,\n            help='Reset existing User token and create a new one',\n        )\n\n    def handle(self, *args, **options):\n        username = options['username']\n        reset_token = options['reset_token']\n\n        try:\n            token = self.create_user_token(username, reset_token)\n        except UserModel.DoesNotExist:\n            raise CommandError(\n                'Cannot create the Token: user {} does not exist'.format(\n                    username)\n            )\n        self.stdout.write(\n            f'Generated token {token.key} for user {username}')\n", "metadata": {"source": "github.com/encode/django-rest-framework", "file_path": "rest_framework/authtoken/management/commands/drf_create_token.py", "language": "python", "complexity": "simple", "star_count": 29333, "quality_score": 0.9500000000000001, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": true, "has_documentation": false, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 3, "class_count": 1, "line_count": 46}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "database_integration", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create database model with ORM relationships and queries with SQLAlchemy/Django ORM patterns\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from django.conf import settings\nfrom django.db import migrations, models\n\n\nclass Migration(migrations.Migration):\n\n    dependencies = [\n        migrations.swappable_dependency(settings.AUTH_USER_MODEL),\n    ]\n\n    operations = [\n        migrations.CreateModel(\n            name='Token',\n            fields=[\n                ('key', models.CharField(primary_key=True, serialize=False, max_length=40)),\n                ('created', models.DateTimeField(auto_now_add=True)),\n                ('user', models.OneToOneField(to=settings.AUTH_USER_MODEL, related_name='auth_token', on_delete=models.CASCADE)),\n            ],\n            options={\n            },\n            bases=(models.Model,),\n        ),\n    ]\n", "metadata": {"source": "github.com/encode/django-rest-framework", "file_path": "rest_framework/authtoken/migrations/0001_initial.py", "language": "python", "complexity": "simple", "star_count": 29333, "quality_score": 0.92, "features": {"has_classes": true, "has_functions": false, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": false, "has_testing": false, "has_logging": false, "function_count": 0, "class_count": 1, "line_count": 24}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "database_integration", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create database model with ORM relationships and queries with SQLAlchemy/Django ORM patterns\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from django.conf import settings\nfrom django.db import migrations, models\n\n\nclass Migration(migrations.Migration):\n\n    dependencies = [\n        ('authtoken', '0001_initial'),\n    ]\n\n    operations = [\n        migrations.AlterModelOptions(\n            name='token',\n            options={'verbose_name_plural': 'Tokens', 'verbose_name': 'Token'},\n        ),\n        migrations.AlterField(\n            model_name='token',\n            name='created',\n            field=models.DateTimeField(verbose_name='Created', auto_now_add=True),\n        ),\n        migrations.AlterField(\n            model_name='token',\n            name='key',\n            field=models.CharField(verbose_name='Key', max_length=40, primary_key=True, serialize=False),\n        ),\n        migrations.AlterField(\n            model_name='token',\n            name='user',\n            field=models.OneToOneField(to=settings.AUTH_USER_MODEL, verbose_name='User', related_name='auth_token', on_delete=models.CASCADE),\n        ),\n    ]\n", "metadata": {"source": "github.com/encode/django-rest-framework", "file_path": "rest_framework/authtoken/migrations/0002_auto_20160226_1747.py", "language": "python", "complexity": "simple", "star_count": 29333, "quality_score": 0.92, "features": {"has_classes": true, "has_functions": false, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 0, "class_count": 1, "line_count": 32}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "database_integration", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create database model with ORM relationships and queries with SQLAlchemy/Django ORM patterns\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Generated by Django 3.1.1 on 2020-09-28 09:34\n\nfrom django.db import migrations\n\n\nclass Migration(migrations.Migration):\n\n    dependencies = [\n        ('authtoken', '0002_auto_20160226_1747'),\n    ]\n\n    operations = [\n        migrations.CreateModel(\n            name='TokenProxy',\n            fields=[\n            ],\n            options={\n                'verbose_name': 'token',\n                'proxy': True,\n                'indexes': [],\n                'constraints': [],\n            },\n            bases=('authtoken.token',),\n        ),\n    ]\n", "metadata": {"source": "github.com/encode/django-rest-framework", "file_path": "rest_framework/authtoken/migrations/0003_tokenproxy.py", "language": "python", "complexity": "simple", "star_count": 29333, "quality_score": 0.89, "features": {"has_classes": true, "has_functions": false, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 0, "class_count": 1, "line_count": 26}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "database_integration", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create database model with ORM relationships and queries with SQLAlchemy/Django ORM patterns\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Generated by Django 4.1.3 on 2022-11-24 21:07\n\nfrom django.db import migrations\n\n\nclass Migration(migrations.Migration):\n\n    dependencies = [\n        ('authtoken', '0003_tokenproxy'),\n    ]\n\n    operations = [\n        migrations.AlterModelOptions(\n            name='tokenproxy',\n            options={'verbose_name': 'Token', 'verbose_name_plural': 'Tokens'},\n        ),\n    ]\n", "metadata": {"source": "github.com/encode/django-rest-framework", "file_path": "rest_framework/authtoken/migrations/0004_alter_tokenproxy_options.py", "language": "python", "complexity": "simple", "star_count": 29333, "quality_score": 0.89, "features": {"has_classes": true, "has_functions": false, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 0, "class_count": 1, "line_count": 18}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "database_integration", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create database model with ORM relationships and queries with SQLAlchemy/Django ORM patterns\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import binascii\nimport os\n\nfrom django.conf import settings\nfrom django.db import models\nfrom django.utils.translation import gettext_lazy as _\n\n\nclass Token(models.Model):\n    \"\"\"\n    The default authorization token model.\n    \"\"\"\n    key = models.CharField(_(\"Key\"), max_length=40, primary_key=True)\n    user = models.OneToOneField(\n        settings.AUTH_USER_MODEL, related_name='auth_token',\n        on_delete=models.CASCADE, verbose_name=_(\"User\")\n    )\n    created = models.DateTimeField(_(\"Created\"), auto_now_add=True)\n\n    class Meta:\n        # Work around for a bug in Django:\n        # https://code.djangoproject.com/ticket/19422\n        #\n        # Also see corresponding ticket:\n        # https://github.com/encode/django-rest-framework/issues/705\n        abstract = 'rest_framework.authtoken' not in settings.INSTALLED_APPS\n        verbose_name = _(\"Token\")\n        verbose_name_plural = _(\"Tokens\")\n\n    def save(self, *args, **kwargs):\n        if not self.key:\n            self.key = self.generate_key()\n        return super().save(*args, **kwargs)\n\n    @classmethod\n    def generate_key(cls):\n        return binascii.hexlify(os.urandom(20)).decode()\n\n    def __str__(self):\n        return self.key\n\n\nclass TokenProxy(Token):\n    \"\"\"\n    Proxy mapping pk to user pk for use in admin.\n    \"\"\"\n    @property\n    def pk(self):\n        return self.user_id\n\n    class Meta:\n        proxy = 'rest_framework.authtoken' in settings.INSTALLED_APPS\n        abstract = 'rest_framework.authtoken' not in settings.INSTALLED_APPS\n        verbose_name = _(\"Token\")\n        verbose_name_plural = _(\"Tokens\")\n", "metadata": {"source": "github.com/encode/django-rest-framework", "file_path": "rest_framework/authtoken/models.py", "language": "python", "complexity": "medium", "star_count": 29333, "quality_score": 0.98, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": true, "has_type_hints": false, "has_testing": false, "has_logging": false, "function_count": 4, "class_count": 4, "line_count": 56}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "database_integration", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create database model with ORM relationships and queries with SQLAlchemy/Django ORM patterns\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from django.contrib.auth import authenticate\nfrom django.utils.translation import gettext_lazy as _\n\nfrom rest_framework import serializers\n\n\nclass AuthTokenSerializer(serializers.Serializer):\n    username = serializers.CharField(\n        label=_(\"Username\"),\n        write_only=True\n    )\n    password = serializers.CharField(\n        label=_(\"Password\"),\n        style={'input_type': 'password'},\n        trim_whitespace=False,\n        write_only=True\n    )\n    token = serializers.CharField(\n        label=_(\"Token\"),\n        read_only=True\n    )\n\n    def validate(self, attrs):\n        username = attrs.get('username')\n        password = attrs.get('password')\n\n        if username and password:\n            user = authenticate(request=self.context.get('request'),\n                                username=username, password=password)\n\n            # The authenticate call simply returns None for is_active=False\n            # users. (Assuming the default ModelBackend authentication\n            # backend.)\n            if not user:\n                msg = _('Unable to log in with provided credentials.')\n                raise serializers.ValidationError(msg, code='authorization')\n        else:\n            msg = _('Must include \"username\" and \"password\".')\n            raise serializers.ValidationError(msg, code='authorization')\n\n        attrs['user'] = user\n        return attrs\n", "metadata": {"source": "github.com/encode/django-rest-framework", "file_path": "rest_framework/authtoken/serializers.py", "language": "python", "complexity": "simple", "star_count": 29333, "quality_score": 0.92, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": false, "has_logging": true, "function_count": 1, "class_count": 1, "line_count": 43}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "database_integration", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create database model with ORM relationships and queries\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from rest_framework import parsers, renderers\nfrom rest_framework.authtoken.models import Token\nfrom rest_framework.authtoken.serializers import AuthTokenSerializer\nfrom rest_framework.compat import coreapi, coreschema\nfrom rest_framework.response import Response\nfrom rest_framework.schemas import ManualSchema\nfrom rest_framework.schemas import coreapi as coreapi_schema\nfrom rest_framework.views import APIView\n\n\nclass ObtainAuthToken(APIView):\n    throttle_classes = ()\n    permission_classes = ()\n    parser_classes = (parsers.FormParser, parsers.MultiPartParser, parsers.JSONParser,)\n    renderer_classes = (renderers.JSONRenderer,)\n    serializer_class = AuthTokenSerializer\n\n    if coreapi_schema.is_enabled():\n        schema = ManualSchema(\n            fields=[\n                coreapi.Field(\n                    name=\"username\",\n                    required=True,\n                    location='form',\n                    schema=coreschema.String(\n                        title=\"Username\",\n                        description=\"Valid username for authentication\",\n                    ),\n                ),\n                coreapi.Field(\n                    name=\"password\",\n                    required=True,\n                    location='form',\n                    schema=coreschema.String(\n                        title=\"Password\",\n                        description=\"Valid password for authentication\",\n                    ),\n                ),\n            ],\n            encoding=\"application/json\",\n        )\n\n    def get_serializer_context(self):\n        return {\n            'request': self.request,\n            'format': self.format_kwarg,\n            'view': self\n        }\n\n    def get_serializer(self, *args, **kwargs):\n        kwargs['context'] = self.get_serializer_context()\n        return self.serializer_class(*args, **kwargs)\n\n    def post(self, request, *args, **kwargs):\n        serializer = self.get_serializer(data=request.data)\n        serializer.is_valid(raise_exception=True)\n        user = serializer.validated_data['user']\n        token, created = Token.objects.get_or_create(user=user)\n        return Response({'token': token.key})\n\n\nobtain_auth_token = ObtainAuthToken.as_view()\n", "metadata": {"source": "github.com/encode/django-rest-framework", "file_path": "rest_framework/authtoken/views.py", "language": "python", "complexity": "simple", "star_count": 29333, "quality_score": 0.92, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 3, "class_count": 2, "line_count": 63}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "database_integration", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create database model with ORM relationships and queries\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"\nThe `compat` module provides support for backwards compatibility with older\nversions of Django/Python, and compatibility wrappers around optional packages.\n\"\"\"\nimport django\nfrom django.db import models\nfrom django.db.models.constants import LOOKUP_SEP\nfrom django.db.models.sql.query import Node\nfrom django.views.generic import View\n\n\ndef unicode_http_header(value):\n    # Coerce HTTP header value to unicode.\n    if isinstance(value, bytes):\n        return value.decode('iso-8859-1')\n    return value\n\n\n# django.contrib.postgres requires psycopg2\ntry:\n    from django.contrib.postgres import fields as postgres_fields\nexcept ImportError:\n    postgres_fields = None\n\n\n# coreapi is required for CoreAPI schema generation\ntry:\n    import coreapi\nexcept ImportError:\n    coreapi = None\n\n# uritemplate is required for OpenAPI and CoreAPI schema generation\ntry:\n    import uritemplate\nexcept ImportError:\n    uritemplate = None\n\n\n# coreschema is optional\ntry:\n    import coreschema\nexcept ImportError:\n    coreschema = None\n\n\n# pyyaml is optional\ntry:\n    import yaml\nexcept ImportError:\n    yaml = None\n\n# inflection is optional\ntry:\n    import inflection\nexcept ImportError:\n    inflection = None\n\n\n# requests is optional\ntry:\n    import requests\nexcept ImportError:\n    requests = None\n\n\n# PATCH method is not implemented by Django\nif 'patch' not in View.http_method_names:\n    View.http_method_names = View.http_method_names + ['patch']\n\n\n# Markdown is optional (version 3.0+ required)\ntry:\n    import markdown\n\n    HEADERID_EXT_PATH = 'markdown.extensions.toc'\n    LEVEL_PARAM = 'baselevel'\n\n    def apply_markdown(text):\n        \"\"\"\n        Simple wrapper around :func:`markdown.markdown` to set the base level\n        of '#' style headers to <h2>.\n        \"\"\"\n        extensions = [HEADERID_EXT_PATH]\n        extension_configs = {\n            HEADERID_EXT_PATH: {\n                LEVEL_PARAM: '2'\n            }\n        }\n        md = markdown.Markdown(\n            extensions=extensions, extension_configs=extension_configs\n        )\n        md_filter_add_syntax_highlight(md)\n        return md.convert(text)\nexcept ImportError:\n    apply_markdown = None\n    markdown = None\n\n\ntry:\n    import pygments\n    from pygments.formatters import HtmlFormatter\n    from pygments.lexers import TextLexer, get_lexer_by_name\n\n    def pygments_highlight(text, lang, style):\n        lexer = get_lexer_by_name(lang, stripall=False)\n        formatter = HtmlFormatter(nowrap=True, style=style)\n        return pygments.highlight(text, lexer, formatter)\n\n    def pygments_css(style):\n        formatter = HtmlFormatter(style=style)\n        return formatter.get_style_defs('.highlight')\n\nexcept ImportError:\n    pygments = None\n\n    def pygments_highlight(text, lang, style):\n        return text\n\n    def pygments_css(style):\n        return None\n\nif markdown is not None and pygments is not None:\n    # starting from this blogpost and modified to support current markdown extensions API\n    # https://zerokspot.com/weblog/2008/06/18/syntax-highlighting-in-markdown-with-pygments/\n\n    import re\n\n    from markdown.preprocessors import Preprocessor\n\n    class CodeBlockPreprocessor(Preprocessor):\n        pattern = re.compile(\n            r'^\\s*``` *([^\\n]+)\\n(.+?)^\\s*```', re.M | re.S)\n\n        formatter = HtmlFormatter()\n\n        def run(self, lines):\n            def repl(m):\n                try:\n                    lexer = get_lexer_by_name(m.group(1))\n                except (ValueError, NameError):\n                    lexer = TextLexer()\n                code = m.group(2).replace('\\t', '    ')\n                code = pygments.highlight(code, lexer, self.formatter)\n                code = code.replace('\\n\\n', '\\n&nbsp;\\n').replace('\\n', '<br />').replace('\\\\@', '@')\n                return '\\n\\n%s\\n\\n' % code\n            ret = self.pattern.sub(repl, \"\\n\".join(lines))\n            return ret.split(\"\\n\")\n\n    def md_filter_add_syntax_highlight(md):\n        md.preprocessors.register(CodeBlockPreprocessor(), 'highlight', 40)\n        return True\nelse:\n    def md_filter_add_syntax_highlight(md):\n        return False\n\n\nif django.VERSION >= (5, 1):\n    # Django 5.1+: use the stock ip_address_validators function\n    # Note: Before Django 5.1, ip_address_validators returns a tuple containing\n    #       1) the list of validators and 2) the error message. Starting from\n    #       Django 5.1 ip_address_validators only returns the list of validators\n    from django.core.validators import ip_address_validators\n\n    def get_referenced_base_fields_from_q(q):\n        return q.referenced_base_fields\n\nelse:\n    # Django <= 5.1: create a compatibility shim for ip_address_validators\n    from django.core.validators import \\\n        ip_address_validators as _ip_address_validators\n\n    def ip_address_validators(protocol, unpack_ipv4):\n        return _ip_address_validators(protocol, unpack_ipv4)[0]\n\n    # Django < 5.1: create a compatibility shim for Q.referenced_base_fields\n    # https://github.com/django/django/blob/5.1a1/django/db/models/query_utils.py#L179\n    def _get_paths_from_expression(expr):\n        if isinstance(expr, models.F):\n            yield expr.name\n        elif hasattr(expr, 'flatten'):\n            for child in expr.flatten():\n                if isinstance(child, models.F):\n                    yield child.name\n                elif isinstance(child, models.Q):\n                    yield from _get_children_from_q(child)\n\n    def _get_children_from_q(q):\n        for child in q.children:\n            if isinstance(child, Node):\n                yield from _get_children_from_q(child)\n            elif isinstance(child, tuple):\n                lhs, rhs = child\n                yield lhs\n                if hasattr(rhs, 'resolve_expression'):\n                    yield from _get_paths_from_expression(rhs)\n            elif hasattr(child, 'resolve_expression'):\n                yield from _get_paths_from_expression(child)\n\n    def get_referenced_base_fields_from_q(q):\n        return {\n            child.split(LOOKUP_SEP, 1)[0] for child in _get_children_from_q(q)\n        }\n\n\n# `separators` argument to `json.dumps()` differs between 2.x and 3.x\n# See: https://bugs.python.org/issue22767\nSHORT_SEPARATORS = (',', ':')\nLONG_SEPARATORS = (', ', ': ')\nINDENT_SEPARATORS = (',', ': ')\n", "metadata": {"source": "github.com/encode/django-rest-framework", "file_path": "rest_framework/compat.py", "language": "python", "complexity": "complex", "star_count": 29333, "quality_score": 1.0, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": true, "has_documentation": true, "has_type_hints": true, "has_testing": false, "has_logging": true, "function_count": 15, "class_count": 1, "line_count": 210}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "testing_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate module integration for Create comprehensive test suite with fixtures and mocks\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from __future__ import annotations\n\nimport sys\n\n\nif __name__ == \"__main__\":\n    import cProfile\n    import pstats\n\n    import pytest  # noqa: F401\n\n    script = sys.argv[1:] if len(sys.argv) > 1 else [\"empty.py\"]\n    cProfile.run(f\"pytest.cmdline.main({script!r})\", \"prof\")\n    p = pstats.Stats(\"prof\")\n    p.strip_dirs()\n    p.sort_stats(\"cumulative\")\n    print(p.print_stats(500))\n", "metadata": {"source": "github.com/pytest-dev/pytest", "file_path": "bench/bench.py", "language": "python", "complexity": "simple", "star_count": 12913, "quality_score": 0.8600000000000001, "features": {"has_classes": false, "has_functions": false, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 0, "class_count": 0, "line_count": 18}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "testing_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate module integration for Create comprehensive test suite with fixtures and mocks\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# 10000 iterations, just for relative comparison\n#                      2.7.5     3.3.2\n# FilesCompleter       75.1109   69.2116\n# FastFilesCompleter    0.7383    1.0760\nfrom __future__ import annotations\n\nimport timeit\n\n\nimports = [\n    \"from argcomplete.completers import FilesCompleter as completer\",\n    \"from _pytest._argcomplete import FastFilesCompleter as completer\",\n]\n\ncount = 1000  # only a few seconds\nsetup = \"%s\\nfc = completer()\"\nrun = 'fc(\"/d\")'\n\n\nif __name__ == \"__main__\":\n    print(timeit.timeit(run, setup=setup % imports[0], number=count))\n    print(timeit.timeit(run, setup=setup % imports[1], number=count))\n", "metadata": {"source": "github.com/pytest-dev/pytest", "file_path": "bench/bench_argcomplete.py", "language": "python", "complexity": "simple", "star_count": 12913, "quality_score": 0.8600000000000001, "features": {"has_classes": false, "has_functions": false, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": false, "has_testing": false, "has_logging": false, "function_count": 0, "class_count": 0, "line_count": 23}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "testing_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create comprehensive test suite with fixtures and mocks\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from __future__ import annotations\n\n\nfor i in range(1000):\n    exec(f\"def test_func_{i}(): pass\")\n", "metadata": {"source": "github.com/pytest-dev/pytest", "file_path": "bench/empty.py", "language": "python", "complexity": "simple", "star_count": 12913, "quality_score": 0.72, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": true, "has_logging": false, "function_count": 1, "class_count": 0, "line_count": 6}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "testing_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create comprehensive test suite with fixtures and mocks\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from __future__ import annotations\n\nimport pytest\n\n\n@pytest.fixture(scope=\"module\", params=range(966))\ndef foo(request):\n    return request.param\n\n\ndef test_it(foo):\n    pass\n\n\ndef test_it2(foo):\n    pass\n", "metadata": {"source": "github.com/pytest-dev/pytest", "file_path": "bench/manyparam.py", "language": "python", "complexity": "simple", "star_count": 12913, "quality_score": 0.92, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": false, "has_testing": true, "has_logging": false, "function_count": 3, "class_count": 0, "line_count": 17}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "testing_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create comprehensive test suite with fixtures and mocks\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from __future__ import annotations\n\nimport pytest\n\n\nSKIP = True\n\n\n@pytest.mark.parametrize(\"x\", range(5000))\ndef test_foo(x):\n    if SKIP:\n        pytest.skip(\"heh\")\n", "metadata": {"source": "github.com/pytest-dev/pytest", "file_path": "bench/skip.py", "language": "python", "complexity": "simple", "star_count": 12913, "quality_score": 0.92, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": false, "has_testing": true, "has_logging": false, "function_count": 1, "class_count": 0, "line_count": 13}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "testing_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create comprehensive test suite with fixtures and mocks\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from __future__ import annotations\n\nfrom unittest import TestCase  # noqa: F401\n\n\nfor i in range(15000):\n    exec(\n        f\"\"\"\nclass Test{i}(TestCase):\n    @classmethod\n    def setUpClass(cls): pass\n    def test_1(self): pass\n    def test_2(self): pass\n    def test_3(self): pass\n\"\"\"\n    )\n", "metadata": {"source": "github.com/pytest-dev/pytest", "file_path": "bench/unit_test.py", "language": "python", "complexity": "medium", "star_count": 12913, "quality_score": 0.98, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": true, "has_type_hints": true, "has_testing": true, "has_logging": false, "function_count": 4, "class_count": 1, "line_count": 17}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "testing_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create comprehensive test suite with fixtures and mocks\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from __future__ import annotations\n\n\nfor i in range(5000):\n    exec(\n        f\"\"\"\nclass Test{i}:\n    @classmethod\n    def setup_class(cls): pass\n    def test_1(self): pass\n    def test_2(self): pass\n    def test_3(self): pass\n\"\"\"\n    )\n", "metadata": {"source": "github.com/pytest-dev/pytest", "file_path": "bench/xunit.py", "language": "python", "complexity": "medium", "star_count": 12913, "quality_score": 0.98, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": true, "has_type_hints": true, "has_testing": true, "has_logging": false, "function_count": 4, "class_count": 1, "line_count": 15}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "testing_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create comprehensive test suite with fixtures and mocks\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from __future__ import annotations\n\nimport os\nfrom pathlib import Path\nimport shutil\nfrom textwrap import dedent\nfrom typing import TYPE_CHECKING\n\nfrom pytest import __version__ as full_version\n\n\nif TYPE_CHECKING:\n    import sphinx.application\n\nPROJECT_ROOT_DIR = Path(__file__).parents[2].resolve()\n\n# -- Project information ---------------------------------------------------------------\n# https://www.sphinx-doc.org/en/master/usage/configuration.html#project-information\n\nproject = \"pytest\"\ncopyright = \"2015, holger krekel and pytest-dev team\"\nversion = full_version.split(\"+\")[0]\nrelease = \".\".join(version.split(\".\")[:2])\n\n# -- General configuration -------------------------------------------------------------\n# https://www.sphinx-doc.org/en/master/usage/configuration.html#general-configuration\n\nroot_doc = \"index\"\nextensions = [\n    \"pygments_pytest\",\n    \"sphinx.ext.autodoc\",\n    \"sphinx.ext.autosummary\",\n    \"sphinx.ext.intersphinx\",\n    \"sphinx.ext.todo\",\n    \"sphinx.ext.viewcode\",\n    \"sphinx_removed_in\",\n    \"sphinxcontrib_trio\",\n    \"sphinxcontrib.towncrier.ext\",  # provides `towncrier-draft-entries` directive\n    \"sphinx_issues\",  # implements `:issue:`, `:pr:` and other GH-related roles\n]\n\n# Building PDF docs on readthedocs requires inkscape for svg to pdf\n# conversion. The relevant plugin is not useful for normal HTML builds, but\n# it still raises warnings and fails CI if inkscape is not available. So\n# only use the plugin if inkscape is actually available.\nif shutil.which(\"inkscape\"):\n    extensions.append(\"sphinxcontrib.inkscapeconverter\")\n\nexclude_patterns = [\n    \"_build\",\n    \"naming20.rst\",\n    \"test/*\",\n    \"old_*\",\n    \"*attic*\",\n    \"*/attic*\",\n    \"funcargs.rst\",\n    \"setup.rst\",\n    \"example/remoteinterp.rst\",\n]\ntemplates_path = [\"_templates\"]\ndefault_role = \"literal\"\n\nnitpicky = True\nnitpick_ignore = [\n    # TODO (fix in pluggy?)\n    (\"py:class\", \"HookCaller\"),\n    (\"py:class\", \"HookspecMarker\"),\n    (\"py:exc\", \"PluginValidationError\"),\n    # Might want to expose/TODO (https://github.com/pytest-dev/pytest/issues/7469)\n    (\"py:class\", \"ExceptionRepr\"),\n    (\"py:class\", \"Exit\"),\n    (\"py:class\", \"SubRequest\"),\n    (\"py:class\", \"SubRequest\"),\n    (\"py:class\", \"TerminalReporter\"),\n    (\"py:class\", \"_pytest._code.code.TerminalRepr\"),\n    (\"py:class\", \"TerminalRepr\"),\n    (\"py:class\", \"_pytest.fixtures.FixtureFunctionMarker\"),\n    (\"py:class\", \"_pytest.fixtures.FixtureFunctionDefinition\"),\n    (\"py:class\", \"_pytest.logging.LogCaptureHandler\"),\n    (\"py:class\", \"_pytest.mark.structures.ParameterSet\"),\n    # Intentionally undocumented/private\n    (\"py:class\", \"_pytest._code.code.Traceback\"),\n    (\"py:class\", \"_pytest._py.path.LocalPath\"),\n    (\"py:class\", \"_pytest.capture.CaptureResult\"),\n    (\"py:class\", \"_pytest.compat.NotSetType\"),\n    (\"py:class\", \"_pytest.python.PyCollector\"),\n    (\"py:class\", \"_pytest.python.PyobjMixin\"),\n    (\"py:class\", \"_pytest.python_api.RaisesContext\"),\n    (\"py:class\", \"_pytest.recwarn.WarningsChecker\"),\n    (\"py:class\", \"_pytest.reports.BaseReport\"),\n    # Undocumented third parties\n    (\"py:class\", \"_tracing.TagTracerSub\"),\n    (\"py:class\", \"warnings.WarningMessage\"),\n    # Undocumented type aliases\n    (\"py:class\", \"LEGACY_PATH\"),\n    (\"py:class\", \"_PluggyPlugin\"),\n    # TypeVars\n    (\"py:class\", \"_pytest._code.code.E\"),\n    (\"py:class\", \"E\"),  # due to delayed annotation\n    (\"py:class\", \"_pytest.fixtures.FixtureFunction\"),\n    (\"py:class\", \"_pytest.nodes._NodeType\"),\n    (\"py:class\", \"_NodeType\"),  # due to delayed annotation\n    (\"py:class\", \"_pytest.python_api.E\"),\n    (\"py:class\", \"_pytest.recwarn.T\"),\n    (\"py:class\", \"_pytest.runner.TResult\"),\n    (\"py:obj\", \"_pytest.fixtures.FixtureValue\"),\n    (\"py:obj\", \"_pytest.stash.T\"),\n    (\"py:class\", \"_ScopeName\"),\n    (\"py:class\", \"BaseExcT_1\"),\n    (\"py:class\", \"ExcT_1\"),\n]\n\nadd_module_names = False\n\n# -- Options for Autodoc --------------------------------------------------------------\n# https://www.sphinx-doc.org/en/master/usage/extensions/autodoc.html#configuration\n\nautodoc_member_order = \"bysource\"\nautodoc_typehints = \"description\"\nautodoc_typehints_description_target = \"documented\"\n\n# -- Options for intersphinx ----------------------------------------------------------\n# https://www.sphinx-doc.org/en/master/usage/extensions/intersphinx.html#configuration\n\nintersphinx_mapping = {\n    \"pluggy\": (\"https://pluggy.readthedocs.io/en/stable\", None),\n    \"python\": (\"https://docs.python.org/3\", None),\n    \"numpy\": (\"https://numpy.org/doc/stable\", None),\n    \"pip\": (\"https://pip.pypa.io/en/stable\", None),\n    \"tox\": (\"https://tox.wiki/en/stable\", None),\n    \"virtualenv\": (\"https://virtualenv.pypa.io/en/stable\", None),\n    \"setuptools\": (\"https://setuptools.pypa.io/en/stable\", None),\n    \"packaging\": (\"https://packaging.python.org/en/latest\", None),\n}\n\n# -- Options for todo -----------------------------------------------------------------\n# https://www.sphinx-doc.org/en/master/usage/extensions/todo.html#configuration\n\ntodo_include_todos = True\n\n# -- Options for linkcheck builder ----------------------------------------------------\n# https://www.sphinx-doc.org/en/master/usage/configuration.html#options-for-the-linkcheck-builder\n\nlinkcheck_ignore = [\n    \"https://blogs.msdn.microsoft.com/bharry/2017/06/28/testing-in-a-cloud-delivery-cadence/\",\n    \"http://pythontesting.net/framework/pytest-introduction/\",\n    r\"https://github.com/pytest-dev/pytest/issues/\\d+\",\n    r\"https://github.com/pytest-dev/pytest/pull/\\d+\",\n]\nlinkcheck_workers = 5\n\n# -- Options for HTML output ----------------------------------------------------------\n# https://www.sphinx-doc.org/en/master/usage/configuration.html#options-for-html-output\n\nhtml_theme = \"furo\"\nhtml_theme_options = {\"sidebar_hide_name\": True}\n\nhtml_static_path = [\"_static\"]\nhtml_css_files = [\n    \"pytest-custom.css\",\n]\n\nhtml_title = \"pytest documentation\"\nhtml_short_title = f\"pytest-{release}\"\n\nhtml_logo = \"_static/pytest1.png\"\nhtml_favicon = \"img/favicon.png\"\n\nhtml_use_index = False\nhtml_show_sourcelink = False\n\nhtml_baseurl = \"https://docs.pytest.org/en/stable/\"\n\n# -- Options for HTML Help output -----------------------------------------------------\n# https://www.sphinx-doc.org/en/master/usage/configuration.html#options-for-html-help-output\n\nhtmlhelp_basename = \"pytestdoc\"\n\n\n# -- Options for manual page output ---------------------------------------------------\n# https://www.sphinx-doc.org/en/master/usage/configuration.html#options-for-manual-page-output\n\nman_pages = [\n    (\"how-to/usage\", \"pytest\", \"pytest usage\", [\"holger krekel at merlinux eu\"], 1)\n]\n\n# -- Options for epub output ----------------------------------------------------------\n# https://www.sphinx-doc.org/en/master/usage/configuration.html#options-for-epub-output\n\nepub_title = \"pytest\"\nepub_author = \"holger krekel at merlinux eu\"\nepub_publisher = \"holger krekel at merlinux eu\"\nepub_copyright = \"2013, holger krekel et alii\"\n\n# -- Options for LaTeX output --------------------------------------------------------\n# https://www.sphinx-doc.org/en/master/usage/configuration.html#options-for-latex-output\n\nlatex_documents = [\n    (\n        \"contents\",\n        \"pytest.tex\",\n        \"pytest Documentation\",\n        \"holger krekel, trainer and consultant, https://merlinux.eu/\",\n        \"manual\",\n    )\n]\nlatex_domain_indices = False\nlatex_engine = \"lualatex\"\nlatex_elements = {\n    \"preamble\": dedent(\n        r\"\"\"\n        \\directlua{\n            luaotfload.add_fallback(\"fallbacks\", {\n                \"Noto Serif CJK SC:style=Regular;\",\n                \"Symbola:Style=Regular;\"\n            })\n        }\n\n        \\setmainfont{FreeSerif}[RawFeature={fallback=fallbacks}]\n        \"\"\"\n    )\n}\n\n# -- Options for texinfo output -------------------------------------------------------\n# https://www.sphinx-doc.org/en/master/usage/configuration.html#options-for-texinfo-output\n\ntexinfo_documents = [\n    (\n        root_doc,\n        \"pytest\",\n        \"pytest Documentation\",\n        (\n            \"Holger Krekel@*Benjamin Peterson@*Ronny Pfannschmidt@*\"\n            \"Floris Bruynooghe@*others\"\n        ),\n        \"pytest\",\n        \"simple powerful testing with Python\",\n        \"Programming\",\n        1,\n    )\n]\n\n# -- Options for towncrier_draft extension --------------------------------------------\n# https://sphinxcontrib-towncrier.readthedocs.io/en/latest/#how-to-use-this\n\ntowncrier_draft_autoversion_mode = \"draft\"  # or: 'sphinx-version', 'sphinx-release'\ntowncrier_draft_include_empty = True\ntowncrier_draft_working_directory = PROJECT_ROOT_DIR\ntowncrier_draft_config_path = \"pyproject.toml\"  # relative to cwd\n\n# -- Options for sphinx_issues extension -----------------------------------\n# https://github.com/sloria/sphinx-issues#installation-and-configuration\n\nissues_github_path = \"pytest-dev/pytest\"\n\n# -- Custom Read the Docs build configuration -----------------------------------------\n# https://docs.readthedocs.io/en/stable/reference/environment-variables.html#environment-variable-reference\n# https://www.sphinx-doc.org/en/master/usage/restructuredtext/directives.html#including-content-based-on-tags\n\nIS_RELEASE_ON_RTD = (\n    os.getenv(\"READTHEDOCS\", \"False\") == \"True\"\n    and os.environ[\"READTHEDOCS_VERSION_TYPE\"] == \"tag\"\n)\nif IS_RELEASE_ON_RTD:\n    tags: set[str]\n    # pylint: disable-next=used-before-assignment\n    tags.add(\"is_release\")  # noqa: F821\n\n# -- Custom documentation plugin ------------------------------------------------------\n# https://www.sphinx-doc.org/en/master/development/tutorials/extending_syntax.html#the-setup-function\n\n\ndef setup(app: sphinx.application.Sphinx) -> None:\n    app.add_crossref_type(\n        \"fixture\",\n        \"fixture\",\n        objname=\"built-in fixture\",\n        indextemplate=\"pair: %s; fixture\",\n    )\n\n    app.add_object_type(\n        \"confval\",\n        \"confval\",\n        objname=\"configuration value\",\n        indextemplate=\"pair: %s; configuration value\",\n    )\n\n    app.add_object_type(\n        \"globalvar\",\n        \"globalvar\",\n        objname=\"global variable interpreted by pytest\",\n        indextemplate=\"pair: %s; global variable interpreted by pytest\",\n    )\n\n    app.add_crossref_type(\n        directivename=\"hook\",\n        rolename=\"hook\",\n        objname=\"pytest hook\",\n        indextemplate=\"pair: %s; hook\",\n    )\n\n    # legacypath.py monkey-patches pytest.Testdir in. Import the file so\n    # that autodoc can discover references to it.\n    import _pytest.legacypath  # noqa: F401\n", "metadata": {"source": "github.com/pytest-dev/pytest", "file_path": "doc/en/conf.py", "language": "python", "complexity": "complex", "star_count": 12913, "quality_score": 0.98, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": true, "has_type_hints": true, "has_testing": false, "has_logging": true, "function_count": 1, "class_count": 0, "line_count": 305}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "testing_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create comprehensive test suite with fixtures and mocks with pytest testing framework\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from __future__ import annotations\n\nimport pytest\nfrom pytest import raises\n\n\ndef otherfunc(a, b):\n    assert a == b\n\n\ndef somefunc(x, y):\n    otherfunc(x, y)\n\n\ndef otherfunc_multi(a, b):\n    assert a == b\n\n\n@pytest.mark.parametrize(\"param1, param2\", [(3, 6)])\ndef test_generative(param1, param2):\n    assert param1 * 2 < param2\n\n\nclass TestFailing:\n    def test_simple(self):\n        def f():\n            return 42\n\n        def g():\n            return 43\n\n        assert f() == g()\n\n    def test_simple_multiline(self):\n        otherfunc_multi(42, 6 * 9)\n\n    def test_not(self):\n        def f():\n            return 42\n\n        assert not f()\n\n\nclass TestSpecialisedExplanations:\n    def test_eq_text(self):\n        assert \"spam\" == \"eggs\"\n\n    def test_eq_similar_text(self):\n        assert \"foo 1 bar\" == \"foo 2 bar\"\n\n    def test_eq_multiline_text(self):\n        assert \"foo\\nspam\\nbar\" == \"foo\\neggs\\nbar\"\n\n    def test_eq_long_text(self):\n        a = \"1\" * 100 + \"a\" + \"2\" * 100\n        b = \"1\" * 100 + \"b\" + \"2\" * 100\n        assert a == b\n\n    def test_eq_long_text_multiline(self):\n        a = \"1\\n\" * 100 + \"a\" + \"2\\n\" * 100\n        b = \"1\\n\" * 100 + \"b\" + \"2\\n\" * 100\n        assert a == b\n\n    def test_eq_list(self):\n        assert [0, 1, 2] == [0, 1, 3]\n\n    def test_eq_list_long(self):\n        a = [0] * 100 + [1] + [3] * 100\n        b = [0] * 100 + [2] + [3] * 100\n        assert a == b\n\n    def test_eq_dict(self):\n        assert {\"a\": 0, \"b\": 1, \"c\": 0} == {\"a\": 0, \"b\": 2, \"d\": 0}\n\n    def test_eq_set(self):\n        assert {0, 10, 11, 12} == {0, 20, 21}\n\n    def test_eq_longer_list(self):\n        assert [1, 2] == [1, 2, 3]\n\n    def test_in_list(self):\n        assert 1 in [0, 2, 3, 4, 5]\n\n    def test_not_in_text_multiline(self):\n        text = \"some multiline\\ntext\\nwhich\\nincludes foo\\nand a\\ntail\"\n        assert \"foo\" not in text\n\n    def test_not_in_text_single(self):\n        text = \"single foo line\"\n        assert \"foo\" not in text\n\n    def test_not_in_text_single_long(self):\n        text = \"head \" * 50 + \"foo \" + \"tail \" * 20\n        assert \"foo\" not in text\n\n    def test_not_in_text_single_long_term(self):\n        text = \"head \" * 50 + \"f\" * 70 + \"tail \" * 20\n        assert \"f\" * 70 not in text\n\n    def test_eq_dataclass(self):\n        from dataclasses import dataclass\n\n        @dataclass\n        class Foo:\n            a: int\n            b: str\n\n        left = Foo(1, \"b\")\n        right = Foo(1, \"c\")\n        assert left == right\n\n    def test_eq_attrs(self):\n        import attr\n\n        @attr.s\n        class Foo:\n            a = attr.ib()\n            b = attr.ib()\n\n        left = Foo(1, \"b\")\n        right = Foo(1, \"c\")\n        assert left == right\n\n\ndef test_attribute():\n    class Foo:\n        b = 1\n\n    i = Foo()\n    assert i.b == 2\n\n\ndef test_attribute_instance():\n    class Foo:\n        b = 1\n\n    assert Foo().b == 2\n\n\ndef test_attribute_failure():\n    class Foo:\n        def _get_b(self):\n            raise Exception(\"Failed to get attrib\")\n\n        b = property(_get_b)\n\n    i = Foo()\n    assert i.b == 2\n\n\ndef test_attribute_multiple():\n    class Foo:\n        b = 1\n\n    class Bar:\n        b = 2\n\n    assert Foo().b == Bar().b\n\n\ndef globf(x):\n    return x + 1\n\n\nclass TestRaises:\n    def test_raises(self):\n        s = \"qwe\"\n        raises(TypeError, int, s)\n\n    def test_raises_doesnt(self):\n        raises(OSError, int, \"3\")\n\n    def test_raise(self):\n        raise ValueError(\"demo error\")\n\n    def test_tupleerror(self):\n        a, b = [1]  # noqa: F841\n\n    def test_reinterpret_fails_with_print_for_the_fun_of_it(self):\n        items = [1, 2, 3]\n        print(f\"items is {items!r}\")\n        a, b = items.pop()\n\n    def test_some_error(self):\n        if namenotexi:  # noqa: F821\n            pass\n\n    def func1(self):\n        assert 41 == 42\n\n\n# thanks to Matthew Scott for this test\ndef test_dynamic_compile_shows_nicely():\n    import importlib.util\n    import sys\n\n    src = \"def foo():\\n assert 1 == 0\\n\"\n    name = \"abc-123\"\n    spec = importlib.util.spec_from_loader(name, loader=None)\n    module = importlib.util.module_from_spec(spec)\n    code = compile(src, name, \"exec\")\n    exec(code, module.__dict__)\n    sys.modules[name] = module\n    module.foo()\n\n\nclass TestMoreErrors:\n    def test_complex_error(self):\n        def f():\n            return 44\n\n        def g():\n            return 43\n\n        somefunc(f(), g())\n\n    def test_z1_unpack_error(self):\n        items = []\n        a, b = items\n\n    def test_z2_type_error(self):\n        items = 3\n        a, b = items\n\n    def test_startswith(self):\n        s = \"123\"\n        g = \"456\"\n        assert s.startswith(g)\n\n    def test_startswith_nested(self):\n        def f():\n            return \"123\"\n\n        def g():\n            return \"456\"\n\n        assert f().startswith(g())\n\n    def test_global_func(self):\n        assert isinstance(globf(42), float)\n\n    def test_instance(self):\n        self.x = 6 * 7\n        assert self.x != 42\n\n    def test_compare(self):\n        assert globf(10) < 5\n\n    def test_try_finally(self):\n        x = 1\n        try:\n            assert x == 0\n        finally:\n            x = 0\n\n\nclass TestCustomAssertMsg:\n    def test_single_line(self):\n        class A:\n            a = 1\n\n        b = 2\n        assert A.a == b, \"A.a appears not to be b\"\n\n    def test_multiline(self):\n        class A:\n            a = 1\n\n        b = 2\n        assert A.a == b, (\n            \"A.a appears not to be b\\nor does not appear to be b\\none of those\"\n        )\n\n    def test_custom_repr(self):\n        class JSON:\n            a = 1\n\n            def __repr__(self):\n                return \"This is JSON\\n{\\n  'foo': 'bar'\\n}\"\n\n        a = JSON()\n        b = 2\n        assert a.a == b, a\n", "metadata": {"source": "github.com/pytest-dev/pytest", "file_path": "doc/en/example/assertion/failure_demo.py", "language": "python", "complexity": "complex", "star_count": 12913, "quality_score": 1.0, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": true, "has_logging": false, "function_count": 59, "class_count": 15, "line_count": 284}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "testing_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create comprehensive test suite with fixtures and mocks\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from __future__ import annotations\n\nimport os.path\n\nimport pytest\n\n\nmydir = os.path.dirname(__file__)\n\n\ndef pytest_runtest_setup(item):\n    if isinstance(item, pytest.Function):\n        if not item.fspath.relto(mydir):\n            return\n        mod = item.getparent(pytest.Module).obj\n        if hasattr(mod, \"hello\"):\n            print(f\"mod.hello {mod.hello!r}\")\n", "metadata": {"source": "github.com/pytest-dev/pytest", "file_path": "doc/en/example/assertion/global_testmodule_config/conftest.py", "language": "python", "complexity": "simple", "star_count": 12913, "quality_score": 0.92, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": false, "has_testing": true, "has_logging": false, "function_count": 1, "class_count": 0, "line_count": 18}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "testing_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create comprehensive test suite with fixtures and mocks\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from __future__ import annotations\n\n\nhello = \"world\"\n\n\ndef test_func():\n    pass\n", "metadata": {"source": "github.com/pytest-dev/pytest", "file_path": "doc/en/example/assertion/global_testmodule_config/test_hello_world.py", "language": "python", "complexity": "simple", "star_count": 12913, "quality_score": 0.72, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": false, "has_testing": true, "has_logging": false, "function_count": 1, "class_count": 0, "line_count": 9}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "testing_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create comprehensive test suite with fixtures and mocks with pytest testing framework\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from __future__ import annotations\n\nimport os.path\nimport shutil\n\n\nfailure_demo = os.path.join(os.path.dirname(__file__), \"failure_demo.py\")\npytest_plugins = (\"pytester\",)\n\n\ndef test_failure_demo_fails_properly(pytester):\n    target = pytester.path.joinpath(os.path.basename(failure_demo))\n    shutil.copy(failure_demo, target)\n    result = pytester.runpytest(target, syspathinsert=True)\n    result.stdout.fnmatch_lines([\"*44 failed*\"])\n    assert result.ret != 0\n", "metadata": {"source": "github.com/pytest-dev/pytest", "file_path": "doc/en/example/assertion/test_failures.py", "language": "python", "complexity": "simple", "star_count": 12913, "quality_score": 0.9500000000000001, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": false, "has_testing": true, "has_logging": false, "function_count": 1, "class_count": 0, "line_count": 17}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "testing_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create comprehensive test suite with fixtures and mocks with pytest testing framework\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from __future__ import annotations\n\n\ndef setup_module(module):\n    module.TestStateFullThing.classcount = 0\n\n\nclass TestStateFullThing:\n    def setup_class(cls):\n        cls.classcount += 1\n\n    def teardown_class(cls):\n        cls.classcount -= 1\n\n    def setup_method(self, method):\n        self.id = eval(method.__name__[5:])\n\n    def test_42(self):\n        assert self.classcount == 1\n        assert self.id == 42\n\n    def test_23(self):\n        assert self.classcount == 1\n        assert self.id == 23\n\n\ndef teardown_module(module):\n    assert module.TestStateFullThing.classcount == 0\n\n\n\"\"\" For this example the control flow happens as follows::\n    import test_setup_flow_example\n    setup_module(test_setup_flow_example)\n       setup_class(TestStateFullThing)\n           instance = TestStateFullThing()\n           setup_method(instance, instance.test_42)\n              instance.test_42()\n           setup_method(instance, instance.test_23)\n              instance.test_23()\n       teardown_class(TestStateFullThing)\n    teardown_module(test_setup_flow_example)\n\nNote that ``setup_class(TestStateFullThing)`` is called and not\n``TestStateFullThing.setup_class()`` which would require you\nto insert ``setup_class = classmethod(setup_class)`` to make\nyour setup function callable.\n\"\"\"\n", "metadata": {"source": "github.com/pytest-dev/pytest", "file_path": "doc/en/example/assertion/test_setup_flow_example.py", "language": "python", "complexity": "medium", "star_count": 12913, "quality_score": 1.0, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": true, "has_type_hints": false, "has_testing": true, "has_logging": false, "function_count": 7, "class_count": 2, "line_count": 48}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "testing_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create comprehensive test suite with fixtures and mocks\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# content of conftest.py\nfrom __future__ import annotations\n\nimport json\n\nimport pytest\n\n\nclass ManifestDirectory(pytest.Directory):\n    def collect(self):\n        # The standard pytest behavior is to loop over all `test_*.py` files and\n        # call `pytest_collect_file` on each file. This collector instead reads\n        # the `manifest.json` file and only calls `pytest_collect_file` for the\n        # files defined there.\n        manifest_path = self.path / \"manifest.json\"\n        manifest = json.loads(manifest_path.read_text(encoding=\"utf-8\"))\n        ihook = self.ihook\n        for file in manifest[\"files\"]:\n            yield from ihook.pytest_collect_file(\n                file_path=self.path / file, parent=self\n            )\n\n\n@pytest.hookimpl\ndef pytest_collect_directory(path, parent):\n    # Use our custom collector for directories containing a `manifest.json` file.\n    if path.joinpath(\"manifest.json\").is_file():\n        return ManifestDirectory.from_parent(parent=parent, path=path)\n    # Otherwise fallback to the standard behavior.\n    return None\n", "metadata": {"source": "github.com/pytest-dev/pytest", "file_path": "doc/en/example/customdirectory/conftest.py", "language": "python", "complexity": "simple", "star_count": 12913, "quality_score": 0.9500000000000001, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": false, "has_testing": true, "has_logging": false, "function_count": 2, "class_count": 1, "line_count": 31}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "testing_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create comprehensive test suite with fixtures and mocks\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# content of test_first.py\nfrom __future__ import annotations\n\n\ndef test_1():\n    pass\n", "metadata": {"source": "github.com/pytest-dev/pytest", "file_path": "doc/en/example/customdirectory/tests/test_first.py", "language": "python", "complexity": "simple", "star_count": 12913, "quality_score": 0.72, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": false, "has_testing": true, "has_logging": false, "function_count": 1, "class_count": 0, "line_count": 7}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "testing_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create comprehensive test suite with fixtures and mocks with pytest testing framework\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"Add an item to a shopping cart. Verify. Remove item. Verify.\"\"\"\nfrom seleniumbase import BaseCase\nBaseCase.main(__name__, __file__)\n\n\nclass MyTestClass(BaseCase):\n    def test_basics(self):\n        self.open(\"https://www.saucedemo.com\")\n        self.type(\"#user-name\", \"standard_user\")\n        self.type(\"#password\", \"secret_sauce\\n\")\n        self.assert_element(\"div.inventory_list\")\n        self.assert_exact_text(\"Products\", \"span.title\")\n        self.click('button[name*=\"backpack\"]')\n        self.click(\"#shopping_cart_container a\")\n        self.assert_exact_text(\"Your Cart\", \"span.title\")\n        self.assert_text(\"Backpack\", \"div.cart_item\")\n        self.click('button:contains(\"Remove\")')  # HTML innerText\n        self.assert_text_not_visible(\"Backpack\", \"div.cart_item\")\n        self.js_click(\"a#logout_sidebar_link\")\n        self.assert_element(\"div#login_button_container\")\n", "metadata": {"source": "github.com/seleniumbase/SeleniumBase", "file_path": "examples/basic_test.py", "language": "python", "complexity": "simple", "star_count": 11448, "quality_score": 1.0, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": true, "has_type_hints": false, "has_testing": true, "has_logging": true, "function_count": 1, "class_count": 1, "line_count": 21}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "testing_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create comprehensive test suite with fixtures and mocks\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from behave import step\n\n\n@step(\"Open the Calculator App\")\ndef go_to_calculator(context):\n    context.sb.open(\"https://seleniumbase.io/apps/calculator\")\n\n\n@step(\"Press C\")\ndef press_c(context):\n    context.sb.click(\"button#clear\")\n\n\n@step(\"Press (\")\ndef press_open_paren(context):\n    context.sb.click('button[id=\"(\"]')\n\n\n@step(\"Press )\")\ndef press_close_paren(context):\n    context.sb.click('button[id=\")\"]')\n\n\n@step(\"Press ÷\")\ndef press_divide(context):\n    context.sb.click(\"button#divide\")\n\n\n@step(\"Press ×\")\ndef press_multiply(context):\n    context.sb.click(\"button#multiply\")\n\n\n@step(\"Press -\")\ndef press_subtract(context):\n    context.sb.click(\"button#subtract\")\n\n\n@step(\"Press +\")\ndef press_add(context):\n    context.sb.click(\"button#add\")\n\n\n@step(\"Press =\")\ndef press_equal(context):\n    context.sb.click(\"button#equal\")\n\n\n@step(\"Press 1\")\ndef press_1(context):\n    context.sb.click('button[id=\"1\"]')\n\n\n@step(\"Press 2\")\ndef press_2(context):\n    context.sb.click('button[id=\"2\"]')\n\n\n@step(\"Press 3\")\ndef press_3(context):\n    context.sb.click('button[id=\"3\"]')\n\n\n@step(\"Press 4\")\ndef press_4(context):\n    context.sb.click('button[id=\"4\"]')\n\n\n@step(\"Press 5\")\ndef press_5(context):\n    context.sb.click('button[id=\"5\"]')\n\n\n@step(\"Press 6\")\ndef press_6(context):\n    context.sb.click('button[id=\"6\"]')\n\n\n@step(\"Press 7\")\ndef press_7(context):\n    context.sb.click('button[id=\"7\"]')\n\n\n@step(\"Press 8\")\ndef press_8(context):\n    context.sb.click('button[id=\"8\"]')\n\n\n@step(\"Press 9\")\ndef press_9(context):\n    context.sb.click('button[id=\"9\"]')\n\n\n@step(\"Press 0\")\ndef press_0(context):\n    context.sb.click('button[id=\"0\"]')\n\n\n@step(\"Press ←\")\ndef press_delete(context):\n    context.sb.click(\"button#delete\")\n\n\n@step(\"Press .\")\ndef press_dot(context):\n    context.sb.click('button[id=\".\"]')\n\n\n@step(\"Press [{number}]\")\ndef enter_number_into_calc(context, number):\n    sb = context.sb\n    for digit in number:\n        sb.click('button[id=\"%s\"]' % digit)\n\n\n@step(\"Evaluate [{equation}]\")\ndef evaluate_equation(context, equation):\n    sb = context.sb\n    for key in equation:\n        if key == \" \":\n            continue\n        elif key == \"÷\":\n            sb.click(\"button#divide\")\n        elif key == \"×\":\n            sb.click(\"button#multiply\")\n        elif key == \"-\":\n            sb.click(\"button#subtract\")\n        elif key == \"+\":\n            sb.click(\"button#add\")\n        else:\n            sb.click('button[id=\"%s\"]' % key)\n    sb.click(\"button#equal\")\n\n\n@step('Verify output is \"{output}\"')\ndef verify_output(context, output):\n    sb = context.sb\n    sb.assert_exact_text(output, \"#output\")\n\n\n@step(\"Save calculator screenshot to logs\")\ndef save_calculator_screenshot_to_logs(context):\n    sb = context.sb\n    sb.save_screenshot_to_logs()\n", "metadata": {"source": "github.com/seleniumbase/SeleniumBase", "file_path": "examples/behave_bdd/features/steps/calculator.py", "language": "python", "complexity": "complex", "star_count": 11448, "quality_score": 0.92, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": false, "has_testing": true, "has_logging": true, "function_count": 25, "class_count": 0, "line_count": 145}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "testing_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create comprehensive test suite with fixtures and mocks\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from behave import step\n\n\n@step(\"Open the Swag Labs Login Page\")\ndef go_to_swag_labs(context):\n    sb = context.sb\n    sb.open(\"https://www.saucedemo.com\")\n    sb.clear_local_storage()\n\n\n@step(\"Login to Swag Labs with {user}\")\ndef login_to_swag_labs(context, user):\n    sb = context.sb\n    sb.type(\"#user-name\", user)\n    sb.type(\"#password\", \"secret_sauce\\n\")\n\n\n@step(\"Verify that the current user is logged in\")\ndef verify_logged_in(context):\n    sb = context.sb\n    sb.assert_element(\"#header_container\")\n    sb.assert_element(\"#react-burger-menu-btn\")\n    sb.assert_element(\"#shopping_cart_container\")\n\n\n@step('Add \"{item}\" to cart')\ndef add_item_to_cart(context, item):\n    sb = context.sb\n    sb.click('div.inventory_item:contains(\"%s\") button[name*=\"add\"]' % item)\n\n\n@step('Save price of \"{item}\" to <{var}>')\ndef save_price_of_item(context, item, var):\n    sb = context.sb\n    price = sb.get_text(\n        'div.inventory_item:contains(\"%s\") .inventory_item_price' % item\n    )\n    sb.variables[var] = price\n\n\n@step('Remove \"{item}\" from cart')\ndef remove_item_to_cart(context, item):\n    sb = context.sb\n    sb.click('div.inventory_item:contains(\"%s\") button[name*=\"remove\"]' % item)\n\n\n@step(\"Verify shopping cart badge shows {number} item(s)\")\ndef verify_badge_number(context, number):\n    sb = context.sb\n    sb.assert_exact_text(number, \"span.shopping_cart_badge\")\n\n\n@step(\"Verify shopping cart badge is missing\")\ndef verify_badge_missing(context):\n    sb = context.sb\n    sb.assert_element_not_visible(\"span.shopping_cart_badge\")\n\n\n@step(\"Click on shopping cart icon\")\ndef click_shopping_cart(context):\n    sb = context.sb\n    sb.click(\"#shopping_cart_container a\")\n\n\n@step(\"Click Checkout\")\ndef click_checkout(context):\n    sb = context.sb\n    sb.click(\"#checkout\")\n\n\n@step(\"Enter checkout info: {first_name}, {last_name}, {zip_code}\")\ndef enter_checkout_info(context, first_name, last_name, zip_code):\n    sb = context.sb\n    sb.type(\"#first-name\", first_name)\n    sb.type(\"#last-name\", last_name)\n    sb.type(\"#postal-code\", zip_code)\n\n\n@step(\"Click Continue\")\ndef click_continue(context):\n    sb = context.sb\n    sb.click(\"input#continue\")\n\n\n@step('Verify {quantity} \"{item}\"(s) in cart')\ndef verify_item_in_cart(context, quantity, item):\n    sb = context.sb\n    sb.assert_exact_text(\n        quantity, 'div.cart_item:contains(\"%s\") div.cart_quantity' % item\n    )\n\n\n@step('Verify cost of \"{item}\" is <{var}>')\ndef verify_cost_of_item(context, item, var):\n    sb = context.sb\n    earlier_price = sb.variables[var]\n    sb.assert_exact_text(\n        earlier_price,\n        'div.cart_item_label:contains(\"%s\") .inventory_item_price' % item,\n    )\n\n\n@step(\"Verify item total is {item_total}\")\ndef verify_item_total(context, item_total):\n    sb = context.sb\n    sb.assert_exact_text(\n        \"Item total: %s\" % item_total, \"div.summary_subtotal_label\", timeout=1\n    )\n\n\n@step(\"Verify tax amount is {tax_amount}\")\ndef verify_tax_amount(context, tax_amount):\n    sb = context.sb\n    sb.assert_exact_text(\n        \"Tax: %s\" % tax_amount, \"div.summary_tax_label\", timeout=1\n    )\n\n\n@step(\"Verify total cost is {total_cost}\")\ndef verify_total_cost(context, total_cost):\n    sb = context.sb\n    sb.assert_exact_text(\n        \"Total: %s\" % total_cost, \"div.summary_total_label\", timeout=1\n    )\n\n\n@step(\"Click Finish\")\ndef click_finish(context):\n    sb = context.sb\n    sb.click(\"button#finish\")\n\n\n@step(\"Verify order complete\")\ndef verify_order_complete(context):\n    sb = context.sb\n    sb.assert_exact_text(\"Thank you for your order!\", \"h2\")\n    sb.assert_element('img[alt=\"Pony Express\"]')\n\n\n@step(\"Logout from Swag Labs\")\ndef logout_from_swag_labs(context):\n    sb = context.sb\n    sb.js_click(\"a#logout_sidebar_link\")\n\n\n@step(\"Verify on Login page\")\ndef verify_on_login_page(context):\n    sb = context.sb\n    sb.assert_element(\"#login-button\")\n\n\n@step(\"Sort items from Z to A\")\ndef sort_items_from_z_to_a(context):\n    sb = context.sb\n    sb.select_option_by_text(\"select.product_sort_container\", \"Name (Z to A)\")\n\n\n@step('Verify \"{item}\" on top')\ndef verify_item_on_top(context, item):\n    sb = context.sb\n    sb.assert_text(item, \"div.inventory_item_name\")\n", "metadata": {"source": "github.com/seleniumbase/SeleniumBase", "file_path": "examples/behave_bdd/features/steps/swag_labs.py", "language": "python", "complexity": "complex", "star_count": 11448, "quality_score": 0.92, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": true, "has_logging": true, "function_count": 23, "class_count": 0, "line_count": 162}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "testing_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create comprehensive test suite with fixtures and mocks\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"Use this as a boilerplate for your test framework.\nDefine customized library methods in a class like this.\nThen have your test classes inherit it.\nBaseTestCase inherits SeleniumBase methods from BaseCase.\"\"\"\nfrom seleniumbase import BaseCase\n\n\nclass BaseTestCase(BaseCase):\n    def setUp(self):\n        super().setUp()\n        # <<< Run custom setUp() code for tests AFTER the super().setUp() >>>\n\n    def tearDown(self):\n        self.save_teardown_screenshot()  # If test fails, or if \"--screenshot\"\n        if self.has_exception():\n            # <<< Run custom code if the test failed. >>>\n            pass\n        else:\n            # <<< Run custom code if the test passed. >>>\n            pass\n        # (Wrap unreliable tearDown() code in a try/except block.)\n        # <<< Run custom tearDown() code BEFORE the super().tearDown() >>>\n        super().tearDown()\n\n    def login(self):\n        # <<< Placeholder. Add your code here. >>>\n        # Reduce duplicate code in tests by having reusable methods like this.\n        # If the UI changes, the fix can be applied in one place.\n        pass\n\n    def example_method(self):\n        # <<< Placeholder. Add your code here. >>>\n        pass\n\n\n\"\"\"\n# Now you can do something like this in your test files:\n\nfrom base_test_case import BaseTestCase\n\nclass MyTests(BaseTestCase):\n\n    def test_example(self):\n        self.login()\n        self.example_method()\n        self.type(\"input\", \"Name\")\n        self.click(\"form button\")\n        ...\n\"\"\"\n", "metadata": {"source": "github.com/seleniumbase/SeleniumBase", "file_path": "examples/boilerplates/base_test_case.py", "language": "python", "complexity": "medium", "star_count": 11448, "quality_score": 0.98, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": true, "has_type_hints": false, "has_testing": true, "has_logging": true, "function_count": 5, "class_count": 3, "line_count": 50}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "testing_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create comprehensive test suite with fixtures and mocks with pytest testing framework\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "try:  # Run with \"pytest\" (relative imports are valid)\n    from .base_test_case import BaseTestCase\n    from .page_objects import Page\nexcept (ImportError, ValueError):  # Run with \"python\"\n    from base_test_case import BaseTestCase\n    from page_objects import Page\n    BaseTestCase.main(__name__, __file__)\n\n\nclass MyTestClass(BaseTestCase):\n    def test_boilerplate(self):\n        self.login()\n        self.example_method()\n        self.assert_element(Page.html)\n", "metadata": {"source": "github.com/seleniumbase/SeleniumBase", "file_path": "examples/boilerplates/boilerplate_test.py", "language": "python", "complexity": "simple", "star_count": 11448, "quality_score": 1.0, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": true, "has_documentation": false, "has_type_hints": true, "has_testing": true, "has_logging": true, "function_count": 1, "class_count": 1, "line_count": 15}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "testing_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create comprehensive test suite with fixtures and mocks with SQLAlchemy/Django ORM patterns\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"Classic Page Object Model with BaseCase inheritance.\"\"\"\r\nfrom seleniumbase import BaseCase\r\nBaseCase.main(__name__, __file__)\r\n\r\n\r\nclass DataPage:\r\n    def go_to_data_url(self, sb):\r\n        sb.open(\"data:text/html,<p>Hello!</p><input />\")\r\n\r\n    def add_input_text(self, sb, text):\r\n        sb.type(\"input\", text)\r\n\r\n\r\nclass ObjTests(BaseCase):\r\n    def test_data_url_page(self):\r\n        DataPage().go_to_data_url(self)\r\n        self.assert_text(\"Hello!\", \"p\")\r\n        DataPage().add_input_text(self, \"Goodbye!\")\r\n", "metadata": {"source": "github.com/seleniumbase/SeleniumBase", "file_path": "examples/boilerplates/classic_obj_test.py", "language": "python", "complexity": "simple", "star_count": 11448, "quality_score": 1.0, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": true, "has_type_hints": false, "has_testing": true, "has_logging": false, "function_count": 3, "class_count": 2, "line_count": 19}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "testing_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create comprehensive test suite with fixtures and mocks with pytest testing framework\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"Example of using the Page Object Pattern in tests.\nMakes code more Readable, Maintainable, and Reusable.\nImport files like this at the top of your test files.\"\"\"\n\n\nclass Page(object):\n    html = \"html\"\n    ok_button = \"#ok\"\n    cancel_button = \"#cancel\"\n    see_items_button = \"button.items\"\n\n\nclass HomePage(object):\n    see_items_button = \"button.items\"\n\n\nclass ShoppingPage(object):\n    buyable_item = 'img[alt=\"Item\"]'\n    add_to_cart = \"button.add\"\n    go_to_checkout = \"#checkout\"\n\n\nclass CheckoutPage(object):\n    remove_from_cart = \"button.remove\"\n    buy_now = \"#buy-now\"\n    shop_more = \"#shop-more\"\n\n\n\"\"\"\n# Now you can do something like this in your test files:\n\nfrom .base_test_case import BaseTestCase\nfrom .page_objects import HomePage, ShoppingPage, CheckoutPage\n\nclass MyTests(BaseTestCase):\n\n    def test_example(self):\n        self.login()\n        self.click(HomePage.see_items_button)\n        self.click(ShoppingPage.buyable_item)\n        self.click(ShoppingPage.add_to_cart)\n        self.click(CheckoutPage.buy_now)\n        self.assert_element(\"#success\")\n        self.assert_text(\"Order Received!\", \"#h2\")\n\"\"\"\n", "metadata": {"source": "github.com/seleniumbase/SeleniumBase", "file_path": "examples/boilerplates/page_objects.py", "language": "python", "complexity": "medium", "star_count": 11448, "quality_score": 1.0, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": true, "has_type_hints": false, "has_testing": true, "has_logging": true, "function_count": 1, "class_count": 5, "line_count": 46}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "testing_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create comprehensive test suite with fixtures and mocks\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"Example of parsing data from files.\"\"\"\nfrom seleniumbase import BaseCase\nBaseCase.main(__name__, __file__)\n\n\nclass ParseTestCase(BaseCase):\n    def get_login_credentials(self, user_type):\n        # Example of parsing data from a file (Method 1)\n        with open(\"qa_login_example.txt\") as f:\n            file_lines = [line.rstrip() for line in f]\n        for line in file_lines:\n            line_items = line.split(\",\")\n            if line_items[0] == user_type:\n                return line_items[1], line_items[2]\n\n    def get_all_login_credentials(self):\n        # Example of parsing data from a file (Method 2)\n        keys = {}\n        with open(\"staging_login_example.txt\") as f:\n            file_lines = [line.rstrip() for line in f]\n        for line in file_lines:\n            line_items = line.split(\",\")\n            if line_items[0] == \"admin\":\n                keys[\"admin\"] = {\n                    \"username\": line_items[1],\n                    \"password\": line_items[2],\n                }\n            if line_items[0] == \"employee\":\n                keys[\"employee\"] = {\n                    \"username\": line_items[1],\n                    \"password\": line_items[2],\n                }\n            if line_items[0] == \"customer\":\n                keys[\"customer\"] = {\n                    \"username\": line_items[1],\n                    \"password\": line_items[2],\n                }\n        return keys\n\n\nclass ParseTests(ParseTestCase):\n    def test_get_login_credentials(self):\n        print(\"\\nExample 1 of getting login info from parsing a config file:\")\n        print(\"\")\n        username, password = self.get_login_credentials(\"admin\")\n        print(\"Getting Admin User login data:\")\n        print(\"Username: %s\" % username)\n        print(\"Password: %s\" % password)\n\n        print(\"\\nExample 2 of getting login info from parsing a config file:\")\n        print(\"\")\n        keys = self.get_all_login_credentials()\n        print(\"Getting Customer login data:\")\n        print(\"Username: %s\" % keys[\"customer\"][\"username\"])\n        print(\"Password: %s\" % keys[\"customer\"][\"password\"])\n", "metadata": {"source": "github.com/seleniumbase/SeleniumBase", "file_path": "examples/boilerplates/samples/file_parsing/parse_files.py", "language": "python", "complexity": "simple", "star_count": 11448, "quality_score": 1.0, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": true, "has_type_hints": true, "has_testing": true, "has_logging": true, "function_count": 3, "class_count": 2, "line_count": 56}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "testing_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create comprehensive test suite with fixtures and mocks with pytest testing framework\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"google.com example test that uses page objects\"\"\"\nfrom seleniumbase import BaseCase\ntry:\n    from .google_objects import HomePage, ResultsPage\nexcept Exception:\n    from google_objects import HomePage, ResultsPage\n    BaseCase.main(__name__, __file__, \"--uc\")\n\n\nclass GoogleTests(BaseCase):\n    def test_google_dot_com(self):\n        if self.headless and self._multithreaded:\n            self.open_if_not_url(\"about:blank\")\n            print(\"\\n  Skipping test in headless multi-threaded mode.\")\n            self.skip(\"Skipping test in headless multi-threaded mode.\")\n        self.open(\"https://google.com/ncr\")\n        self.assert_title_contains(\"Google\")\n        self.sleep(0.05)\n        self.save_screenshot_to_logs()  # (\"./latest_logs\" folder)\n        if not self.is_element_visible(\"iframe\"):\n            self.sleep(1.5)  # A slow pop-up might appear\n        self.hide_elements('iframe')  # Hide \"Sign in\" pop-up\n        self.sleep(0.05)\n        self.save_screenshot_to_logs()\n        self.type(HomePage.search_box, \"github.com\")\n        self.assert_element(HomePage.search_button)\n        self.assert_element(HomePage.feeling_lucky_button)\n        self.click(HomePage.search_button)\n        self.assert_text(\"github.com\", ResultsPage.search_results)\n", "metadata": {"source": "github.com/seleniumbase/SeleniumBase", "file_path": "examples/boilerplates/samples/google_test.py", "language": "python", "complexity": "simple", "star_count": 11448, "quality_score": 1.0, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": true, "has_documentation": true, "has_type_hints": false, "has_testing": true, "has_logging": true, "function_count": 1, "class_count": 1, "line_count": 30}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "testing_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create comprehensive test suite with fixtures and mocks with SQLAlchemy/Django ORM patterns\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"Classic Page Object Model with the \"sb\" fixture.\"\"\"\r\n\r\n\r\nclass LoginPage:\r\n    def login_to_swag_labs(self, sb, username):\r\n        sb.open(\"https://www.saucedemo.com\")\r\n        sb.type(\"#user-name\", username)\r\n        sb.type(\"#password\", \"secret_sauce\")\r\n        sb.click('input[type=\"submit\"]')\r\n\r\n\r\nclass MyTests:\r\n    def test_swag_labs_login(self, sb):\r\n        LoginPage().login_to_swag_labs(sb, \"standard_user\")\r\n        sb.assert_element(\"div.inventory_list\")\r\n        sb.assert_element('div:contains(\"Sauce Labs Backpack\")')\r\n        sb.js_click(\"a#logout_sidebar_link\")\r\n        sb.assert_element(\"div#login_button_container\")\r\n", "metadata": {"source": "github.com/seleniumbase/SeleniumBase", "file_path": "examples/boilerplates/samples/sb_swag_test.py", "language": "python", "complexity": "medium", "star_count": 11448, "quality_score": 0.9500000000000001, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": false, "has_error_handling": false, "has_documentation": true, "has_type_hints": false, "has_testing": true, "has_logging": true, "function_count": 2, "class_count": 2, "line_count": 19}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "testing_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create comprehensive test suite with fixtures and mocks with SQLAlchemy/Django ORM patterns\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"Classic Page Object Model with BaseCase inheritance.\"\"\"\r\nfrom seleniumbase import BaseCase\r\nBaseCase.main(__name__, __file__)\r\n\r\n\r\nclass LoginPage:\r\n    def login_to_swag_labs(self, sb, username):\r\n        sb.open(\"https://www.saucedemo.com\")\r\n        sb.type(\"#user-name\", username)\r\n        sb.type(\"#password\", \"secret_sauce\")\r\n        sb.click('input[type=\"submit\"]')\r\n\r\n\r\nclass MyTests(BaseCase):\r\n    def test_swag_labs_login(self):\r\n        LoginPage().login_to_swag_labs(self, \"standard_user\")\r\n        self.assert_element(\"div.inventory_list\")\r\n        self.assert_element('div:contains(\"Sauce Labs Backpack\")')\r\n        self.js_click(\"a#logout_sidebar_link\")\r\n        self.assert_element(\"div#login_button_container\")\r\n", "metadata": {"source": "github.com/seleniumbase/SeleniumBase", "file_path": "examples/boilerplates/samples/swag_labs_test.py", "language": "python", "complexity": "simple", "star_count": 11448, "quality_score": 1.0, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": true, "has_type_hints": false, "has_testing": true, "has_logging": true, "function_count": 2, "class_count": 2, "line_count": 21}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "testing_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create comprehensive test suite with fixtures and mocks with SQLAlchemy/Django ORM patterns\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"An example using the Classic Page Object Model.\"\"\"\r\nfrom seleniumbase import BaseCase\r\nBaseCase.main(__name__, __file__, \"--uc\")\r\n\r\n\r\nclass GooglePage:\r\n    def go_to_google(self, sb):\r\n        sb.open(\"https://google.com/ncr\")\r\n\r\n    def assert_google_title(self, sb):\r\n        sb.assert_title_contains(\"Google\")\r\n\r\n    def hide_sign_in_pop_up(self, sb):\r\n        if not sb.is_element_visible(\"iframe\"):\r\n            sb.sleep(1.5)  # A slow pop-up might appear\r\n        sb.hide_elements('iframe')\r\n        sb.sleep(0.05)\r\n\r\n    def do_search(self, sb, search_term):\r\n        sb.sleep(0.05)\r\n        sb.click('[title=\"Search\"]')\r\n        sb.type('[title=\"Search\"]', search_term + \"\\n\")\r\n\r\n    def click_search_result(self, sb, content):\r\n        sb.click('a:contains(\"%s\")' % content)\r\n\r\n\r\nclass SeleniumBaseIOPage:\r\n    def do_search_and_click(self, sb, search_term):\r\n        sb.sleep(0.05)\r\n        sb.type('form[name=\"search\"] input', search_term)\r\n        sb.click(\"li.md-search-result__item h1:contains(%s)\" % search_term)\r\n\r\n\r\nclass MyTests(BaseCase):\r\n    def test_page_objects(self):\r\n        if self.headless and self._multithreaded:\r\n            self.open_if_not_url(\"about:blank\")\r\n            print(\"\\n  Skipping test in headless multi-threaded mode.\")\r\n            self.skip(\"Skipping test in headless multi-threaded mode.\")\r\n        search_term = \"SeleniumBase.io Docs\"\r\n        expected_text = \"SeleniumBase\"\r\n        GooglePage().go_to_google(self)\r\n        GooglePage().assert_google_title(self)\r\n        GooglePage().hide_sign_in_pop_up(self)\r\n        GooglePage().do_search(self, search_term)\r\n        self.assert_text(expected_text, \"#search\")\r\n        GooglePage().click_search_result(self, expected_text)\r\n        SeleniumBaseIOPage().do_search_and_click(self, \"Dashboard\")\r\n        self.assert_text(\"Dashboard\", \"main h1\")\r\n", "metadata": {"source": "github.com/seleniumbase/SeleniumBase", "file_path": "examples/boilerplates/samples/test_page_objects.py", "language": "python", "complexity": "medium", "star_count": 11448, "quality_score": 1.0, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": true, "has_type_hints": false, "has_testing": true, "has_logging": false, "function_count": 7, "class_count": 3, "line_count": 51}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "testing_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create comprehensive test suite with fixtures and mocks with SQLAlchemy/Django ORM patterns\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"Classic Page Object Model with the \"sb\" fixture.\"\"\"\r\n\r\n\r\nclass DataPage:\r\n    def go_to_data_url(self, sb):\r\n        sb.open(\"data:text/html,<p>Hello!</p><input />\")\r\n\r\n    def add_input_text(self, sb, text):\r\n        sb.type(\"input\", text)\r\n\r\n\r\nclass ObjTests:\r\n    def test_data_url_page(self, sb):\r\n        DataPage().go_to_data_url(sb)\r\n        sb.assert_text(\"Hello!\", \"p\")\r\n        DataPage().add_input_text(sb, \"Goodbye!\")\r\n", "metadata": {"source": "github.com/seleniumbase/SeleniumBase", "file_path": "examples/boilerplates/sb_fixture_test.py", "language": "python", "complexity": "medium", "star_count": 11448, "quality_score": 0.9500000000000001, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": false, "has_error_handling": false, "has_documentation": true, "has_type_hints": false, "has_testing": true, "has_logging": false, "function_count": 3, "class_count": 2, "line_count": 17}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "testing_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate module integration for Create comprehensive test suite with fixtures and mocks\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from seleniumbase import SB\n\nwith SB(uc=True, test=True) as sb:\n    url = \"https://seleniumbase.io/antibot/login\"\n    sb.activate_cdp_mode(url)\n    sb.press_keys(\"input#username\", \"demo_user\")\n    sb.press_keys(\"input#password\", \"secret_pass\")\n    x, y = sb.cdp.get_gui_element_center(\"button#myButton\")\n    sb.uc_gui_click_x_y(x, y)\n    sb.sleep(1.5)\n    x, y = sb.cdp.get_gui_element_center(\"a#log-in\")\n    sb.uc_gui_click_x_y(x, y)\n    sb.assert_text(\"Welcome!\", \"h1\")\n    sb.set_messenger_theme(location=\"bottom_center\")\n    sb.post_message(\"SeleniumBase wasn't detected!\")\n    sb.sleep(1.5)\n", "metadata": {"source": "github.com/seleniumbase/SeleniumBase", "file_path": "examples/cdp_mode/raw_antibot.py", "language": "python", "complexity": "simple", "star_count": 11448, "quality_score": 0.89, "features": {"has_classes": false, "has_functions": false, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": false, "has_testing": true, "has_logging": true, "function_count": 0, "class_count": 0, "line_count": 17}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "testing_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate module integration for Create comprehensive test suite with fixtures and mocks\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from seleniumbase import SB\n\nwith SB(uc=True, test=True, ad_block=True) as sb:\n    url = \"https://www.browserscan.net/bot-detection\"\n    sb.activate_cdp_mode(url)\n    sb.sleep(1)\n    sb.cdp.flash(\"Test Results\", duration=4)\n    sb.sleep(1)\n    sb.cdp.assert_element('strong:contains(\"Normal\")')\n    sb.cdp.flash('strong:contains(\"Normal\")', duration=4, pause=4)\n", "metadata": {"source": "github.com/seleniumbase/SeleniumBase", "file_path": "examples/cdp_mode/raw_browserscan.py", "language": "python", "complexity": "simple", "star_count": 11448, "quality_score": 0.89, "features": {"has_classes": false, "has_functions": false, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": false, "has_testing": true, "has_logging": false, "function_count": 0, "class_count": 0, "line_count": 11}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "testing_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create comprehensive test suite with fixtures and mocks\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from __future__ import annotations\n\n\n__copyright__ = \"\"\"\nCopyright (C) 2009-2017 Andreas Kloeckner\nCopyright (C) 2014-2017 Aaron Meurer\n\"\"\"\n\n__license__ = \"\"\"\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in\nall copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\nTHE SOFTWARE.\n\"\"\"\n\n\nimport re\nimport sys\nfrom importlib import metadata\n\nfrom pudb.settings import load_config\n\n\nVERSION = metadata.version(\"pudb\")\n_ver_match = re.match(r\"^([0-9.]+)([a-z0-9]*?)$\", VERSION)\nassert _ver_match\nNUM_VERSION = tuple(int(nr) for nr in _ver_match.group(1).split(\".\"))\n__version__ = VERSION\n\n\nclass PudbShortcuts:\n    @property\n    def db(self):\n        dbg = _get_debugger()\n\n        import threading\n        if isinstance(threading.current_thread(), threading._MainThread):\n            set_interrupt_handler()\n        dbg.set_trace(sys._getframe().f_back)\n\n    @property\n    def go(self):\n        dbg = _get_debugger()\n\n        import threading\n        if isinstance(threading.current_thread(), threading._MainThread):\n            set_interrupt_handler()\n        dbg.set_trace(sys._getframe().f_back, paused=False)\n\n\nimport builtins\n\n\nbuiltins.__dict__[\"pu\"] = PudbShortcuts()\n\n\ndef _tty_override():\n    import os\n    return os.environ.get(\"PUDB_TTY\")\n\n\ndef _open_tty(tty_path):\n    import io\n    import os\n    tty_file = io.TextIOWrapper(open(tty_path, \"r+b\", buffering=0))\n    term_size = os.get_terminal_size(tty_file.fileno())\n\n    return tty_file, term_size\n\n\ndef _get_debugger(**kwargs):\n    from pudb.debugger import Debugger\n    if not Debugger._current_debugger:\n        tty_path = _tty_override()\n        if tty_path and (\"stdin\" not in kwargs or \"stdout\" not in kwargs):\n            tty_file, term_size = _open_tty(tty_path)\n            kwargs.setdefault(\"stdin\", tty_file)\n            kwargs.setdefault(\"stdout\", tty_file)\n            kwargs.setdefault(\"term_size\", term_size)\n            kwargs.setdefault(\"tty_file\", tty_file)\n\n        from pudb.debugger import Debugger\n        dbg = Debugger(**kwargs)\n\n        return dbg\n    else:\n        return Debugger._current_debugger[0]\n\n\ndef _have_debugger():\n    try:\n        from pudb.debugger import Debugger\n        return bool(Debugger._current_debugger)\n    except ImportError:\n        # Import cycles may happen if function is called during early startup\n        return False\n\n\nimport signal  # noqa\nDEFAULT_SIGNAL = signal.SIGINT\ndel signal\n\n\ndef runmodule(*args, **kwargs):\n    kwargs[\"run_as_module\"] = True\n    runscript(*args, **kwargs)\n\n\ndef runscript(mainpyfile, steal_output=False, _continue_at_start=False,\n              **kwargs):\n    try:\n        dbg = _get_debugger(\n            steal_output=steal_output,\n            _continue_at_start=_continue_at_start,\n        )\n        _runscript(mainpyfile, dbg, **kwargs)\n    finally:\n        dbg.__del__()\n\n\ndef _runscript(mainpyfile, dbg, args=None, pre_run=\"\", run_as_module=False):\n\n    # Note on saving/restoring sys.argv: it's a good idea when sys.argv was\n    # modified by the script being debugged. It's a bad idea when it was\n    # changed by the user from the command line. The best approach would be to\n    # have a \"restart\" command which would allow explicit specification of\n    # command line arguments.\n\n    if args is not None:\n        prev_sys_argv = sys.argv[:]\n        if run_as_module:\n            sys.argv = args\n        else:\n            sys.argv = [mainpyfile, *args]\n\n    # replace pudb's dir with script's dir in front of module search path.\n    from pathlib import Path\n    prev_sys_path = sys.path[:]\n    sys.path[0] = str(Path(mainpyfile).resolve().parent)\n\n    import os\n    cwd = os.getcwd()\n\n    while True:\n        # Script may have changed directory. Restore cwd before restart.\n        os.chdir(cwd)\n\n        if pre_run:\n            from subprocess import call\n            retcode = call(pre_run, close_fds=True, shell=True)\n            if retcode:\n                print(\"*** WARNING: pre-run process exited with code %d.\" % retcode)\n                input(\"[Hit Enter]\")\n\n        status_msg = \"\"\n\n        try:\n            if run_as_module:\n                try:\n                    dbg._runmodule(mainpyfile)\n                except ImportError as e:\n                    print(e, file=sys.stderr)\n                    sys.exit(1)\n            else:\n                try:\n                    dbg._runscript(mainpyfile)\n                except SystemExit:\n                    se = sys.exc_info()[1]\n                    status_msg = \"The debuggee exited normally with \" \\\n                            f\"status code {se.code}.\\n\\n\"\n        except Exception:\n            dbg.post_mortem = True\n            dbg.interaction(None, sys.exc_info())\n\n        while True:\n            import urwid\n            pre_run_edit = urwid.Edit(\"\", pre_run)\n\n            if not load_config()[\"prompt_on_quit\"]:\n                return\n\n            result = dbg.ui.call_with_ui(dbg.ui.dialog,\n                urwid.ListBox(urwid.SimpleListWalker([urwid.Text(\n                    f\"Your PuDB session has ended.\\n\\n{status_msg}\"\n                    \"Would you like to quit PuDB or restart your program?\\n\"\n                    \"You may hit 'q' to quit.\"),\n                    urwid.Text(\"\\n\\nIf you decide to restart, this command \"\n                    \"will be run prior to actually restarting:\"),\n                    urwid.AttrMap(pre_run_edit, \"input\", \"focused input\")\n                    ])),\n                [\n                    (\"Restart\", \"restart\"),\n                    (\"Quit\", \"quit\"),\n                    ],\n                focus_buttons=True,\n                bind_enter_esc=False,\n                title=\"Finished\",\n                extra_bindings=[\n                    (\"q\", \"quit\"),\n                    ])\n\n            if result == \"quit\":\n                return\n\n            if result == \"restart\":\n                break\n\n        pre_run = pre_run_edit.get_edit_text()\n\n        dbg.restart()\n\n    if args is not None:\n        sys.argv = prev_sys_argv\n\n    sys.path = prev_sys_path\n\n\ndef runstatement(statement, globals=None, locals=None):\n    return _get_debugger().run(statement, globals, locals)\n\n\ndef runeval(expression, globals=None, locals=None):\n    return _get_debugger().runeval(expression, globals, locals)\n\n\ndef runcall(*args, **kwargs):\n    return _get_debugger().runcall(*args, **kwargs)\n\n\ndef set_trace(paused=True):\n    \"\"\"\n    Start the debugger\n\n    If paused=False (the default is True), the debugger will not stop here\n    (same as immediately pressing 'c' to continue).\n    \"\"\"\n    import sys\n    dbg = _get_debugger()\n\n    import threading\n    if isinstance(threading.current_thread(), threading._MainThread):\n        set_interrupt_handler()\n\n    dbg.set_trace(sys._getframe().f_back, paused=paused)\n\n\nstart = set_trace\n\n\ndef _interrupt_handler(signum, frame):\n    from pudb import _get_debugger\n    _get_debugger().set_trace(frame, as_breakpoint=False)\n\n\ndef set_interrupt_handler(interrupt_signal=None):\n    \"\"\"\n    Set up an interrupt handler, to activate PuDB when Python receives the\n    signal `interrupt_signal`.  By default it is SIGINT (i.e., Ctrl-c).\n\n    To use a different signal, pass it as the argument to this function, like\n    `set_interrupt_handler(signal.SIGALRM)`.  You can then break your code\n    with `kill -ALRM pid`, where `pid` is the process ID of the Python\n    process.  Note that PuDB will still use SIGINT once it is running to allow\n    breaking running code.  If that is an issue, you can change the default\n    signal by hooking `pudb.DEFAULT_SIGNAL`, like\n\n    >>> import pudb\n    >>> import signal\n    >>> pudb.DEFAULT_SIGNAL = signal.SIGALRM\n\n    Note, this may not work if you use threads or subprocesses.\n\n    Note, this only works when called from the main thread.\n    \"\"\"\n\n    if interrupt_signal is None:\n        interrupt_signal = DEFAULT_SIGNAL\n\n    import signal\n    old_handler = signal.getsignal(interrupt_signal)\n\n    if old_handler is not signal.default_int_handler \\\n            and old_handler != signal.SIG_DFL and old_handler != _interrupt_handler:\n        # Since we don't currently have support for a non-default signal handlers,\n        # let's avoid undefined-behavior territory and just show a warning.\n        from warnings import warn\n        if old_handler is None:\n            # This is the documented meaning of getsignal()->None.\n            old_handler = \"not installed from python\"\n        return warn(\"A non-default handler for signal %d is already installed (%s). \"\n                \"Skipping pudb interrupt support.\"\n                % (interrupt_signal, old_handler),\n                stacklevel=2)\n\n    import threading\n    if not isinstance(threading.current_thread(), threading._MainThread):\n        from warnings import warn\n        # Setting signals from a non-main thread will not work\n        return warn(\"Setting the interrupt handler can only be done on the main \"\n                \"thread. The interrupt handler was NOT installed.\",\n                stacklevel=2)\n\n    try:\n        signal.signal(interrupt_signal, _interrupt_handler)\n    except ValueError:\n        import sys\n        from traceback import format_exception\n        from warnings import warn\n        warn(\"setting interrupt handler on signal %d failed: %s\"\n                % (interrupt_signal, \"\".join(format_exception(*sys.exc_info()))),\n                stacklevel=2)\n\n\ndef post_mortem(tb=None, e_type=None, e_value=None):\n    if tb is None:\n        import sys\n        exc_info = sys.exc_info()\n    else:\n        exc_info = (e_type, e_value, tb)\n\n    dbg = _get_debugger()\n    dbg.reset()\n    dbg.interaction(None, exc_info)\n\n\ndef pm():\n    import sys\n    exc_type, _exc_val, _tb = sys.exc_info()\n\n    if exc_type is None:\n        # No exception on record. Do nothing.\n        return\n    post_mortem()\n\n\nif __name__ == \"__main__\":\n    print(\"You now need to type 'python -m pudb.run'. Sorry.\")\n\n# vim: foldmethod=marker:expandtab:softtabstop=4\n", "metadata": {"source": "github.com/inducer/pudb", "file_path": "pudb/__init__.py", "language": "python", "complexity": "complex", "star_count": 3122, "quality_score": 0.8999999999999999, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": true, "has_documentation": true, "has_type_hints": true, "has_testing": true, "has_logging": true, "function_count": 17, "class_count": 1, "line_count": 353}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "testing_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create comprehensive test suite with fixtures and mocks\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from __future__ import annotations\n\n\n__copyright__ = \"\"\"\nCopyright (C) 2009-2017 Andreas Kloeckner\nCopyright (C) 2014-2017 Aaron Meurer\nCopyright (C) 2024 Gerhard Sittig\n\"\"\"\n\n__license__ = \"\"\"\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in\nall copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\nTHE SOFTWARE.\n\"\"\"\n\n\nimport logging\nimport sys\nfrom datetime import datetime\nfrom enum import Enum, auto\n\n\nlogfile = [None]\n\n\ndef getlogfile():\n    return logfile[0]\n\n\ndef setlogfile(destfile):\n    logfile[0] = destfile\n    with open(destfile, \"a\") as openfile:\n        openfile.write(\n            f\"\\n*** Pudb session error log started at {datetime.now()} ***\\n\")\n\n\nclass TerminalOrStreamHandler(logging.StreamHandler):\n    \"\"\"\n    Logging handler that sends errors either to the terminal window or to\n    stderr, depending on whether the debugger is active.\n    \"\"\"\n    def emit(self, record):\n        from pudb import _get_debugger, _have_debugger\n        logfile = getlogfile()\n\n        self.acquire()\n        try:\n            if logfile is not None:\n                message = self.format(record)\n                with open(logfile, \"a\") as openfile:\n                    openfile.write(f\"\\n{message}\\n\")\n            elif _have_debugger():\n                dbg = _get_debugger()\n                message = self.format(record)\n                dbg.ui.add_cmdline_content(message, \"command line error\")\n            else:\n                super().emit(record)\n        finally:\n            self.release()\n\n\ndef _init_loggers():\n    ui_handler = TerminalOrStreamHandler()\n    ui_formatter = logging.Formatter(\n        fmt=\"*** Pudb UI Exception Encountered: %(message)s ***\\n\"\n    )\n    ui_handler.setFormatter(ui_formatter)\n    ui_log = logging.getLogger(\"ui\")\n    ui_log.addHandler(ui_handler)\n\n    settings_handler = TerminalOrStreamHandler()\n    settings_formatter = logging.Formatter(\n        fmt=\"*** Pudb Settings Exception Encountered: %(message)s ***\\n\"\n    )\n    settings_handler.setFormatter(settings_formatter)\n    settings_log = logging.getLogger(\"settings\")\n    settings_log.addHandler(settings_handler)\n\n    return ui_log, settings_log\n\n\nui_log, settings_log = _init_loggers()\n\n\n# {{{ breakpoint validity\n\ndef generate_executable_lines_for_code(code):\n    if sys.version_info >= (3, 10):\n        for _start, _end, lineno in code.co_lines():\n            if lineno is not None:\n                yield lineno\n    else:\n        lineno = code.co_firstlineno\n        yield lineno\n        # See https://github.com/python/cpython/blob/master/Objects/lnotab_notes.txt\n\n        for line_incr in code.co_lnotab[1::2]:\n            # NB: This code is specific to Python 3.6 and higher\n            # https://github.com/python/cpython/blob/v3.6.0/Objects/lnotab_notes.txt\n            if line_incr >= 0x80:\n                line_incr -= 0x100\n            lineno += line_incr\n            yield lineno\n\n\ndef get_executable_lines_for_codes_recursive(codes):\n    codes = codes[:]\n\n    from types import CodeType\n\n    execable_lines = set()\n\n    while codes:\n        code = codes.pop()\n        execable_lines |= set(generate_executable_lines_for_code(code))\n        codes.extend(const\n                for const in code.co_consts\n                if isinstance(const, CodeType))\n\n    return execable_lines\n\n\ndef get_executable_lines_for_file(filename):\n    # inspired by rpdb2\n\n    from linecache import getlines\n    codes = [compile(\"\".join(getlines(filename)), filename, \"exec\", dont_inherit=1)]\n\n    return get_executable_lines_for_codes_recursive(codes)\n\n\ndef get_breakpoint_invalid_reason(filename, lineno):\n    # simple logic stolen from pdb\n    import linecache\n    line = linecache.getline(filename, lineno)\n    if not line:\n        return \"Line is beyond end of file.\"\n\n    try:\n        executable_lines = get_executable_lines_for_file(filename)\n    except SyntaxError:\n        return \"File failed to compile.\"\n\n    if lineno not in executable_lines:\n        return \"No executable statement found in line.\"\n\n\ndef lookup_module(filename):\n    \"\"\"Helper function for break/clear parsing -- may be overridden.\n\n    lookupmodule() translates (possibly incomplete) file or module name\n    into an absolute file name.\n    \"\"\"\n\n    # stolen from pdb\n    import os\n    import sys\n\n    if os.path.isabs(filename) and os.path.exists(filename):\n        return filename\n    f = os.path.join(sys.path[0], filename)\n    if os.path.exists(f):  # and self.canonic(f) == self.mainpyfile:\n        return f\n    _root, ext = os.path.splitext(filename)\n    if ext == \"\":\n        filename = filename + \".py\"\n    if os.path.isabs(filename):\n        return filename\n    for dirname in sys.path:\n        while os.path.islink(dirname):\n            dirname = os.readlink(dirname)\n        fullname = os.path.join(dirname, filename)\n        if os.path.exists(fullname):\n            return fullname\n    return None\n\n# }}}\n\n\n# {{{ file encoding detection\n# the main idea stolen from Python 3.1's tokenize.py, by Ka-Ping Yee\n\nimport re\n\n\ncookie_re = re.compile(br\"^\\s*#.*coding[:=]\\s*([-\\w.]+)\")\nfrom codecs import BOM_UTF8, lookup\n\n\ndef detect_encoding(line_iter):\n    \"\"\"\n    The detect_encoding() function is used to detect the encoding that should\n    be used to decode a Python source file. It requires one argument, line_iter,\n    an iterator on the lines to be read.\n\n    It will read a maximum of two lines, and return the encoding used\n    (as a string) and a list of any lines (left as bytes) it has read\n    in.\n\n    It detects the encoding from the presence of a utf-8 bom or an encoding\n    cookie as specified in pep-0263. If both a bom and a cookie are present,\n    but disagree, a SyntaxError will be raised. If the encoding cookie is an\n    invalid charset, raise a SyntaxError.\n\n    If no encoding is specified, then the default of 'utf-8' will be returned.\n    \"\"\"\n    bom_found = False\n\n    def read_or_stop():\n        try:\n            return next(line_iter)\n        except StopIteration:\n            return \"\"\n\n    def find_cookie(line):\n        try:\n            line_string = line\n        except UnicodeDecodeError:\n            return None\n\n        matches = cookie_re.findall(line_string)\n        if not matches:\n            return None\n        encoding = matches[0].decode()\n        try:\n            codec = lookup(encoding)\n        except LookupError as err:\n            # This behaviour mimics the Python interpreter\n            raise SyntaxError(\"unknown encoding: \" + encoding) from err\n\n        if bom_found and codec.name != \"utf-8\":\n            # This behaviour mimics the Python interpreter\n            raise SyntaxError(\"encoding problem: utf-8\")\n        return encoding\n\n    first = read_or_stop()\n    if isinstance(first, str):\n        return None, [first]\n\n    if first.startswith(BOM_UTF8):\n        bom_found = True\n        first = first[3:]\n    if not first:\n        return \"utf-8\", []\n\n    encoding = find_cookie(first)\n    if encoding:\n        return encoding, [first]\n\n    second = read_or_stop()\n    if not second:\n        return \"utf-8\", [first]\n\n    encoding = find_cookie(second)\n    if encoding:\n        return encoding, [first, second]\n\n    return \"utf-8\", [first, second]\n\n\ndef decode_lines(lines):\n    line_iter = iter(lines)\n    source_enc, detection_read_lines = detect_encoding(line_iter)\n\n    from itertools import chain\n\n    for line in chain(detection_read_lines, line_iter):\n        if hasattr(line, \"decode\") and source_enc is not None:\n            yield line.decode(source_enc)\n        else:\n            yield line\n\n# }}}\n\n\n# {{{ get single key press from console outside of curses\n\nclass KeyReadImpl(Enum):\n    INPUT = auto()\n    GETCH = auto()\n    SELECT = auto()\n\n\n_keyread_impl = KeyReadImpl.INPUT\nif sys.platform in (\"emscripten\", \"wasi\"):\n    pass\nelif sys.platform in (\"win32\",):\n    _keyread_impl = KeyReadImpl.GETCH\nelse:\n    _keyread_impl = KeyReadImpl.SELECT\n\n\nclass ConsoleSingleKeyReader:\n    \"\"\"\n    Get a single key press from a terminal without a prompt.\n\n    Eliminates the necessity to press ENTER before other input also\n    becomes available. Avoids the accumulation of prompts on the screen\n    as was the case with Python's input() call. Is used in situations\n    where urwid is disabled and curses calls are not available.\n\n    Supports major desktop platforms with special cases (msvcrt getch(),\n    termios and select). Transparently falls back to Python's input()\n    method. Call sites remain simple and straight forward.\n    \"\"\"\n\n    def __enter__(self):\n        if _keyread_impl == KeyReadImpl.SELECT:\n            import termios\n            import tty\n            self.prev_settings = termios.tcgetattr(sys.stdin)\n            tty.setcbreak(sys.stdin.fileno())\n        return self\n\n    def __exit__(self, type, value, traceback):\n        if _keyread_impl == KeyReadImpl.SELECT:\n            import termios\n            termios.tcsetattr(sys.stdin, termios.TCSADRAIN, self.prev_settings)\n\n    def get_single_key(self):\n        if _keyread_impl == KeyReadImpl.GETCH:\n            import msvcrt\n            # https://docs.python.org/3/library/msvcrt.html#msvcrt.getch\n            # Most keys are returned in the first getch() call. Some\n            # special keys (function keys, cursor, keypad) require\n            # another call when the first returned '\\0' or '\\xe0'.\n            c = msvcrt.getch()\n            if c in (\"\\x00\", \"\\xe0\"):\n                c = msvcrt.getch()\n            return c\n\n        elif _keyread_impl == KeyReadImpl.SELECT:\n            import select\n            rset, _, _ = select.select([sys.stdin], [], [], None)\n            assert sys.stdin in rset\n            return sys.stdin.read(1)\n\n        # Strictly speaking putting the fallback here which requires\n        # pressing ENTER is not correct, this is the \"non buffered\"\n        # console support code. But it simplifies call sites. And is\n        # easy to tell by users because a prompt is provided. This is\n        # the most portable approach, and backwards compatible with\n        # earlier PuDB releases. It's a most appropriate default for\n        # otherwise unsupported platforms. Or when users choose to\n        # not accept single key presses, or keys other than ENTER.\n        else:\n            input(\"Hit Enter to return:\")\n            return None\n\n# }}}\n\n# vim: foldmethod=marker\n", "metadata": {"source": "github.com/inducer/pudb", "file_path": "pudb/lowlevel.py", "language": "python", "complexity": "complex", "star_count": 3122, "quality_score": 0.8999999999999999, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": true, "has_documentation": true, "has_type_hints": true, "has_testing": true, "has_logging": true, "function_count": 16, "class_count": 3, "line_count": 367}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "testing_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create comprehensive test suite with fixtures and mocks\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"\n.. autoclass:: RemoteDebugger\n\n.. autofunction:: set_trace\n.. autofunction:: debugger\n.. autofunction:: debug_remote_on_single_rank\n\"\"\"\nfrom __future__ import annotations\n\n\n__copyright__ = \"\"\"\nCopyright (C) 2009-2017 Andreas Kloeckner\nCopyright (C) 2014-2017 Aaron Meurer\nCopyright (C) 2020-2020 Son Geon\n\"\"\"\n\n__license__ = \"\"\"\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in\nall copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\nTHE SOFTWARE.\n\"\"\"\n\n\n# mostly stolen from celery.contrib.rdb\n\n\nimport atexit\nimport errno\nimport os\nimport socket\nimport sys\nfrom typing import Any, Callable\n\nfrom pudb.debugger import Debugger\n\n\n__all__ = [\n    \"PUDB_RDB_HOST\",\n    \"PUDB_RDB_PORT\",\n    \"PUDB_RDB_REVERSE\",\n    \"debug_remote_on_single_rank\",\n    \"debugger\",\n    \"default_port\",\n    \"set_trace\",\n]\n\ndefault_port = 6899\n\nPUDB_RDB_HOST = os.environ.get(\"PUDB_RDB_HOST\") or \"127.0.0.1\"\nPUDB_RDB_PORT = int(os.environ.get(\"PUDB_RDB_PORT\") or default_port)\nPUDB_RDB_REVERSE = bool(os.environ.get(\"PUDB_RDB_REVERSE\"))\n\n#: Holds the currently active debugger.\n_current = [None]\n\n_frame = sys._getframe\n\nNO_AVAILABLE_PORT = \"\"\"\\\n{self.ident}: Couldn't find an available port.\n\nPlease specify one using the PUDB_RDB_PORT environment variable.\n\"\"\"\n\nBANNER = \"\"\"\\\n{self.ident}: Please start a telnet session using a command like:\ntelnet {self.host} {self.port}\n{self.ident}: Waiting for client...\n\"\"\"\n\nSESSION_STARTED = \"{self.ident}: Now in session with {self.remote_addr}.\"\nSESSION_ENDED = \"{self.ident}: Session with {self.remote_addr} ended.\"\n\nCONN_REFUSED = \"\"\"\\\nCannot connect to the reverse telnet client {self.host} {self.port}.\n\nTry to open reverse client by running\nstty -echo -icanon && nc -l -p 6899  # Linux\nstty -echo -icanon && nc -l 6899  # BSD/MacOS\n\nPlease specify one using the PUDB_RDB_PORT environment variable.\n\"\"\"\n\n\nclass TelnetCharacters:\n    \"\"\"Collection of characters from the telnet protocol RFC 854\n\n    This format for the telnet characters was adapted from the telnetlib module\n    which was removed from the C Python standard library in version 3.13. Only\n    the characters needed by pudb have been copied here. Additional characters\n    can be found by looking in the telnetlib code in Python 3.12 or in the\n    telnet RFC.\n\n    .. note::\n\n        This class is not intended to be instantiated.\n    \"\"\"\n    # Telnet protocol characters\n    IAC = b\"\\xff\"  # \"Interpret As Command\"\n    DO = b\"\\xfd\"\n    WILL = b\"\\xfb\"\n\n    # Telnet protocol options codes\n    # These ones all come from arpa/telnet.h\n    ECHO = b\"\\x01\"  # echo\n    SGA = b\"\\x03\"  # suppress go ahead\n\n\nclass RemoteDebugger(Debugger):\n    \"\"\"\n    .. automethod:: __init__\n    \"\"\"\n\n    me = \"pudb\"\n    _prev_outs = None\n    _sock = None\n\n    def __init__(\n        self,\n        host=PUDB_RDB_HOST,\n        port=PUDB_RDB_PORT,\n        port_search_limit=100,\n        out=sys.stdout,\n        term_size=None,\n        reverse=PUDB_RDB_REVERSE,\n    ):\n        \"\"\"\n        :arg term_size: A two-tuple ``(columns, rows)``, or *None*. If *None*,\n            try to determine the terminal size automatically.\n\n            Currently, this uses a heuristic: It uses the terminal size of the\n            debuggee as that for the debugger. The idea is that you might be\n            running both in two tabs of the same terminal window, hence using\n            terminals of the same size.\n        \"\"\"\n        self.out = out\n\n        if term_size is None:\n            term_size = os.environ.get(\"PUDB_TERM_SIZE\")\n            if term_size is not None:\n                term_size = tuple(map(int, term_size.split(\"x\")))\n                if len(term_size) != 2:\n                    raise ValueError(\"PUDB_TERM_SIZE should have two dimensions\")\n            else:\n                try:\n                    s = os.get_terminal_size()\n                    term_size = (s.columns, s.lines)\n                except Exception:\n                    term_size = (80, 24)\n\n        self._prev_handles = sys.stdin, sys.stdout\n        self._client, (address, port) = self.get_client(\n            host=host, port=port, search_limit=port_search_limit, reverse=reverse\n        )\n        self.remote_addr = \":\".join(str(v) for v in address)\n\n        self.say(SESSION_STARTED.format(self=self))\n\n        # makefile ignores encoding if there's no buffering.\n        raw_sock_file = self._client.makefile(\"rwb\", 0)\n        import codecs\n\n        sock_file = codecs.StreamReaderWriter(\n            raw_sock_file, codecs.getreader(\"utf-8\"), codecs.getwriter(\"utf-8\"))\n\n        self._handle = sys.stdin = sys.stdout = sock_file\n\n        # nc negotiation doesn't support telnet options\n        if not reverse:\n            tn = TelnetCharacters\n\n            raw_sock_file.write(tn.IAC + tn.WILL + tn.SGA)\n            resp = raw_sock_file.read(3)\n            assert resp == tn.IAC + tn.DO + tn.SGA\n\n            raw_sock_file.write(tn.IAC + tn.WILL + tn.ECHO)\n            resp = raw_sock_file.read(3)\n            assert resp == tn.IAC + tn.DO + tn.ECHO\n\n        Debugger.__init__(\n            self, stdin=self._handle, stdout=self._handle, term_size=term_size\n        )\n\n    def get_client(self, host, port, search_limit=100, reverse=False):\n        if reverse:\n            self.host, self.port = host, port\n            client, address = self.get_reverse_socket_client(host, port)\n            self.ident = f\"{self.me}:{self.port}\"\n        else:\n            self._sock, conn_info = self.get_socket_client(\n                host, port, search_limit=search_limit,\n            )\n            self.host, self.port = conn_info\n            self.ident = f\"{self.me}:{self.port}\"\n            self.say(BANNER.format(self=self))\n            client, address = self._sock.accept()\n        client.setblocking(1)\n        return client, (address, self.port)\n\n    def get_reverse_socket_client(self, host, port):\n        sock = socket.socket()\n        try:\n            sock.connect((host, port))\n            sock.setblocking(1)\n        except OSError as exc:\n            if exc.errno == errno.ECONNREFUSED:\n                raise ValueError(CONN_REFUSED.format(self=self)) from exc\n            raise exc\n        return sock, sock.getpeername()\n\n    def get_socket_client(self, host, port, search_limit):\n        sock, this_port = self.get_avail_port(host, port, search_limit)\n        sock.setblocking(1)\n        sock.listen(1)\n        return sock, (host, this_port)\n\n    def get_avail_port(self, host, port, search_limit=100, skew=+0):\n        this_port = None\n        for i in range(search_limit):\n            sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n            sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n            this_port = port + i\n            try:\n                sock.bind((host, this_port))\n            except OSError as exc:\n                if exc.errno in [errno.EADDRINUSE, errno.EINVAL]:\n                    continue\n                raise\n            else:\n                return sock, this_port\n        else:\n            raise Exception(NO_AVAILABLE_PORT.format(self=self))\n\n    def say(self, m):\n        print(m, file=self.out)\n\n    def close_remote_session(self):\n        self.stdin, self.stdout = sys.stdin, sys.stdout = self._prev_handles\n        self._handle.close()\n        self._client.close()\n        if self._sock:\n            self._sock.close()\n        self.say(SESSION_ENDED.format(self=self))\n\n\ndef debugger(\n    term_size=None,\n    host=PUDB_RDB_HOST,\n    port=PUDB_RDB_PORT,\n    reverse=PUDB_RDB_REVERSE\n):\n    \"\"\"Return the current debugger instance (if any),\n    or creates a new one.\"\"\"\n    rdb = _current[0]\n    if rdb is None:\n        rdb = _current[0] = RemoteDebugger(\n            host=host, port=port, term_size=term_size, reverse=reverse\n        )\n        atexit.register(lambda e: e.close_remote_session(), rdb)\n    return rdb\n\n\ndef set_trace(\n    frame=None,\n    term_size=None,\n    host=PUDB_RDB_HOST,\n    port=PUDB_RDB_PORT,\n    reverse=PUDB_RDB_REVERSE\n):\n    \"\"\"Set breakpoint at current location, or a specified frame\"\"\"\n    if frame is None:\n        frame = _frame().f_back\n\n    return debugger(\n        term_size=term_size, host=host, port=port, reverse=reverse\n    ).set_trace(frame)\n\n\ndef debug_remote_on_single_rank(comm: Any, rank: int, func: Callable,\n                                *args: Any, **kwargs: Any) -> None:\n    \"\"\"Run a remote debugger on a single rank of an ``mpi4py`` application.\n    *func* will be called on rank *rank* running in a :class:`RemoteDebugger`,\n    and will be called normally on all other ranks.\n\n    :param comm: an ``mpi4py`` ``Comm`` object.\n    :param rank: the rank to debug. All other ranks will spin until this rank exits.\n    :param func: the callable to debug.\n    :param args: the arguments passed to ``func``.\n    :param kwargs: the kwargs passed to ``func``.\n    \"\"\"\n    if comm.rank == rank:\n        debugger().runcall(func, *args, **kwargs)\n    else:\n        try:\n            func(*args, **kwargs)\n        finally:\n            from time import sleep\n            while True:\n                sleep(1)\n", "metadata": {"source": "github.com/inducer/pudb", "file_path": "pudb/remote.py", "language": "python", "complexity": "complex", "star_count": 3122, "quality_score": 0.84, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": true, "has_documentation": true, "has_type_hints": true, "has_testing": true, "has_logging": false, "function_count": 10, "class_count": 3, "line_count": 313}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "testing_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create comprehensive test suite with fixtures and mocks\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from __future__ import annotations\n\n\n__copyright__ = \"\"\"\nCopyright (C) 2009-2017 Andreas Kloeckner\nCopyright (C) 2014-2017 Aaron Meurer\n\"\"\"\n\n__license__ = \"\"\"\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in\nall copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\nTHE SOFTWARE.\n\"\"\"\n\nimport os\nimport sys\nfrom configparser import ConfigParser\n\nfrom pudb.lowlevel import get_breakpoint_invalid_reason, lookup_module, settings_log\n\n\n# see https://github.com/inducer/pudb/pull/453 for context\n_home = os.environ.get(\"HOME\", os.path.expanduser(\"~\"))\nXDG_CONF_RESOURCE = \"pudb\"\nXDG_CONFIG_HOME = os.environ.get(\n    \"XDG_CONFIG_HOME\",\n    os.path.join(_home, \".config\") if _home else None)\n\nif XDG_CONFIG_HOME:\n    XDG_CONFIG_DIRS = [XDG_CONFIG_HOME]\nelse:\n    XDG_CONFIG_DIRS = os.environ.get(\"XDG_CONFIG_DIRS\", \"/etc/xdg\").split(\":\")\n\n\ndef get_save_config_path():\n    # This may not raise, as it is called during import.\n    if not XDG_CONFIG_HOME:\n        return None\n\n    path = os.path.join(XDG_CONFIG_HOME, XDG_CONF_RESOURCE)\n    try:\n        os.makedirs(path, mode=0o700, exist_ok=True)\n    except Exception:\n        settings_log.exception(\"Failed to make config dir\")\n\n    return path\n\n\nCONF_SECTION = \"pudb\"\nCONF_FILE_NAME = \"pudb.cfg\"\n\nSAVED_BREAKPOINTS_FILE_NAME = \"saved-breakpoints-%d.%d\" % sys.version_info[:2]\nBREAKPOINTS_FILE_NAME = \"breakpoints-%d.%d\" % sys.version_info[:2]\n\n\n_config_ = [None]\n\n\ndef load_config():\n    # This may not raise, as it is called during import.\n\n    # Only ever do this once\n    if _config_[0] is not None:\n        return _config_[0]\n\n    from os.path import isdir, join\n\n    cparser = ConfigParser()\n\n    conf_dict = {}\n    try:\n        cparser.read([\n            join(cdir, XDG_CONF_RESOURCE, CONF_FILE_NAME)\n            for cdir in XDG_CONFIG_DIRS if isdir(cdir)])\n\n        if cparser.has_section(CONF_SECTION):\n            conf_dict.update(dict(cparser.items(CONF_SECTION)))\n    except Exception:\n        settings_log.exception(\"Failed to load config\")\n\n    conf_dict.setdefault(\"shell\", \"internal\")\n    conf_dict.setdefault(\"theme\", \"classic\")\n    conf_dict.setdefault(\"line_numbers\", \"False\")\n    conf_dict.setdefault(\"seen_welcome\", \"a\")\n\n    conf_dict.setdefault(\"sidebar_width\", 0.5)\n    conf_dict.setdefault(\"variables_weight\", 1)\n    conf_dict.setdefault(\"stack_weight\", 1)\n    conf_dict.setdefault(\"breakpoints_weight\", 1)\n\n    conf_dict.setdefault(\"current_stack_frame\", \"top\")\n\n    conf_dict.setdefault(\"stringifier\", \"default\")\n\n    conf_dict.setdefault(\"custom_theme\", \"\")\n    conf_dict.setdefault(\"custom_stringifier\", \"\")\n    conf_dict.setdefault(\"custom_shell\", \"\")\n\n    conf_dict.setdefault(\"wrap_variables\", \"True\")\n    conf_dict.setdefault(\"default_variables_access_level\", \"public\")\n\n    conf_dict.setdefault(\"display\", \"auto\")\n\n    conf_dict.setdefault(\"prompt_on_quit\", \"True\")\n\n    conf_dict.setdefault(\"hide_cmdline_win\", \"False\")\n\n    # hotkeys\n    conf_dict.setdefault(\"hotkeys_code\", \"C\")\n    conf_dict.setdefault(\"hotkeys_variables\", \"V\")\n    conf_dict.setdefault(\"hotkeys_stack\", \"S\")\n    conf_dict.setdefault(\"hotkeys_breakpoints\", \"B\")\n    conf_dict.setdefault(\"hotkeys_toggle_cmdline_focus\", \"ctrl x\")\n\n    def normalize_bool_inplace(name):\n        try:\n            if conf_dict[name].lower() in [\"0\", \"false\", \"off\"]:\n                conf_dict[name] = False\n            else:\n                conf_dict[name] = True\n        except Exception:\n            settings_log.exception(\"Failed to process config\")\n\n    normalize_bool_inplace(\"line_numbers\")\n    normalize_bool_inplace(\"wrap_variables\")\n    normalize_bool_inplace(\"prompt_on_quit\")\n    normalize_bool_inplace(\"hide_cmdline_win\")\n\n    _config_[0] = conf_dict\n    return conf_dict\n\n\ndef save_config(conf_dict):\n    # This may not raise, as it is called during import.\n\n    from os.path import join\n\n    cparser = ConfigParser()\n    cparser.add_section(CONF_SECTION)\n\n    for key in sorted(conf_dict):\n        cparser.set(CONF_SECTION, key, str(conf_dict[key]))\n\n    try:\n        save_path = get_save_config_path()\n        if not save_path:\n            return\n        outf = open(join(save_path, CONF_FILE_NAME), \"w\")\n        cparser.write(outf)\n        outf.close()\n    except Exception:\n        settings_log.exception(\"Failed to save config\")\n\n\ndef edit_config(ui, conf_dict):\n    import urwid\n\n    old_conf_dict = conf_dict.copy()\n\n    def _update_theme():\n        ui.setup_palette(ui.screen)\n        ui.screen.clear()\n\n    def _update_line_numbers():\n        for sl in ui.source:\n            sl._invalidate()\n\n    def _update_prompt_on_quit():\n        pass\n\n    def _update_hide_cmdline_win():\n        ui.update_cmdline_win()\n\n    def _update_current_stack_frame():\n        ui.update_stack()\n\n    def _update_stringifier():\n        import pudb.var_view\n        pudb.var_view.custom_stringifier_dict = {}\n        ui.update_var_view()\n\n    def _update_default_variables_access_level():\n        ui.update_var_view()\n\n    def _update_wrap_variables():\n        ui.update_var_view()\n\n    def _update_config(check_box, new_state, option_newvalue):\n        option, newvalue = option_newvalue\n        new_conf_dict = {option: newvalue}\n        if option == \"theme\":\n            # only activate if the new state of the radio button is 'on'\n            if new_state:\n                if newvalue is None:\n                    # Select the custom theme entry dialog\n                    lb.set_focus(lb_contents.index(theme_edit_list_item))\n                    return\n\n                conf_dict.update(theme=newvalue)\n                _update_theme()\n\n        elif option == \"line_numbers\":\n            new_conf_dict[\"line_numbers\"] = not check_box.get_state()\n            conf_dict.update(new_conf_dict)\n            _update_line_numbers()\n\n        elif option == \"prompt_on_quit\":\n            new_conf_dict[\"prompt_on_quit\"] = not check_box.get_state()\n            conf_dict.update(new_conf_dict)\n            _update_prompt_on_quit()\n\n        elif option == \"hide_cmdline_win\":\n            new_conf_dict[\"hide_cmdline_win\"] = not check_box.get_state()\n            conf_dict.update(new_conf_dict)\n            _update_hide_cmdline_win()\n\n        elif option == \"current_stack_frame\":\n            # only activate if the new state of the radio button is 'on'\n            if new_state:\n                conf_dict.update(new_conf_dict)\n                _update_current_stack_frame()\n\n        elif option == \"stringifier\":\n            # only activate if the new state of the radio button is 'on'\n            if new_state:\n                if newvalue is None:\n                    lb.set_focus(lb_contents.index(stringifier_edit_list_item))\n                    return\n\n                conf_dict.update(stringifier=newvalue)\n                _update_stringifier()\n\n        elif option == \"default_variables_access_level\":\n            # only activate if the new state of the radio button is 'on'\n            if new_state:\n                conf_dict.update(default_variables_access_level=newvalue)\n                _update_default_variables_access_level()\n\n        elif option == \"wrap_variables\":\n            new_conf_dict[\"wrap_variables\"] = not check_box.get_state()\n            conf_dict.update(new_conf_dict)\n            _update_wrap_variables()\n\n    heading = urwid.Text(\"This is the preferences screen for PuDB. \"\n        \"Hit Ctrl-P at any time to get back to it.\\n\\n\"\n        \"Configuration settings are saved in \"\n        \"$HOME/.config/pudb or $XDG_CONFIG_HOME/pudb \"\n        \"environment variable. If both variables are not set \"\n        \"configurations settings will not be saved.\\n\")\n\n    cb_line_numbers = urwid.CheckBox(\"Show Line Numbers\",\n            bool(conf_dict[\"line_numbers\"]), on_state_change=_update_config,\n                user_data=(\"line_numbers\", None))\n\n    cb_prompt_on_quit = urwid.CheckBox(\"Prompt before quitting\",\n            bool(conf_dict[\"prompt_on_quit\"]), on_state_change=_update_config,\n                user_data=(\"prompt_on_quit\", None))\n\n    hide_cmdline_win = urwid.CheckBox(\"Hide command line\"\n            f\"({conf_dict['hotkeys_toggle_cmdline_focus']}) window \"\n                                      \"when not in use\",\n            bool(conf_dict[\"hide_cmdline_win\"]), on_state_change=_update_config,\n                user_data=(\"hide_cmdline_win\", None))\n\n    # {{{ shells\n\n    shell_info = urwid.Text(\"This is the shell that will be \"\n            \"used when you hit '!'.\\n\")\n    shells = [\"internal\", \"classic\", \"ipython\", \"ipython_kernel\", \"bpython\",\n              \"ptpython\", \"ptipython\"]\n    known_shell = conf_dict[\"shell\"] in shells\n    shell_edit = urwid.Edit(edit_text=conf_dict[\"custom_shell\"])\n    shell_edit_list_item = urwid.AttrMap(shell_edit, \"input\", \"focused input\")\n\n    shell_rb_group = []\n    shell_rbs = [\n            urwid.RadioButton(shell_rb_group, name,\n                conf_dict[\"shell\"] == name)\n            for name in shells]+[\n                urwid.RadioButton(shell_rb_group, \"Custom:\",\n                not known_shell, on_state_change=_update_config,\n                user_data=(\"shell\", None)),\n                shell_edit_list_item,\n                urwid.Text(\"\\nTo use a custom shell, see examples/shell.py \"\n                    \"in the pudb distribution. Enter the full path to a \"\n                    \"file like it in the box above. '~' will be expanded \"\n                    \"to your home directory. The file should contain a \"\n                    \"function called pudb_shell(_globals, _locals) \"\n                    \"at the module level. See the PuDB documentation for \"\n                    \"more information.\"),\n            ]\n\n    # }}}\n\n    # {{{ themes\n\n    from pudb.themes import THEMES\n\n    known_theme = conf_dict[\"theme\"] in THEMES\n\n    theme_rb_group = []\n    theme_edit = urwid.Edit(edit_text=conf_dict[\"custom_theme\"])\n    theme_edit_list_item = urwid.AttrMap(theme_edit, \"input\", \"focused input\")\n    theme_rbs = [\n            urwid.RadioButton(theme_rb_group, name,\n                conf_dict[\"theme\"] == name, on_state_change=_update_config,\n                user_data=(\"theme\", name))\n            for name in THEMES]+[\n                urwid.RadioButton(theme_rb_group, \"Custom:\",\n                    not known_theme, on_state_change=_update_config,\n                    user_data=(\"theme\", None)),\n                theme_edit_list_item,\n                urwid.Text(\"\\nTo use a custom theme, see examples/theme.py in the \"\n                    \"pudb distribution. Enter the full path to a file like it in \"\n                    \"the box above. '~' will be expanded to your home directory. \"\n                    \"Note that a custom theme will not be applied until you close \"\n                    \"this dialog.\"),\n            ]\n\n    # }}}\n\n    # {{{ stack\n\n    stack_rb_group = []\n    stack_opts = [\"top\", \"bottom\"]\n    stack_info = urwid.Text(\"Show the current stack frame at the\\n\")\n    stack_rbs = [\n            urwid.RadioButton(stack_rb_group, name,\n                conf_dict[\"current_stack_frame\"] == name,\n                on_state_change=_update_config,\n                user_data=(\"current_stack_frame\", name))\n            for name in stack_opts\n            ]\n\n    # }}}\n\n    # {{{ stringifier\n\n    from pudb.var_view import STRINGIFIERS\n    stringifier_opts = list(STRINGIFIERS.keys())\n    known_stringifier = conf_dict[\"stringifier\"] in stringifier_opts\n    stringifier_rb_group = []\n    stringifier_edit = urwid.Edit(edit_text=conf_dict[\"custom_stringifier\"])\n    stringifier_info = urwid.Text(\n        \"This is the default function that will be called on variables in the \"\n        \"variables list. You can also change this on a per-variable basis by \"\n        \"selecting a variable and typing 'e' to edit the variable's display \"\n        \"settings, or by typing one of d/t/r/s/i/c. Note that str and repr will \"\n        \"be slower than the default, type, or id stringifiers.\\n\")\n    stringifier_edit_list_item = urwid.AttrMap(stringifier_edit,\n                                               \"input\", \"focused input\")\n    stringifier_rbs = [\n            urwid.RadioButton(stringifier_rb_group, name,\n                conf_dict[\"stringifier\"] == name,\n                on_state_change=_update_config,\n                user_data=(\"stringifier\", name))\n            for name in stringifier_opts\n            ]+[\n                urwid.RadioButton(stringifier_rb_group, \"Custom:\",\n                    not known_stringifier, on_state_change=_update_config,\n                    user_data=(\"stringifier\", None)),\n                stringifier_edit_list_item,\n                urwid.Text(\"\\nTo use a custom stringifier, see \"\n                    \"examples/stringifier.py in the pudb distribution. Enter the \"\n                    \"full path to a file like it in the box above. \"\n                    \"'~' will be expanded to your home directory. \"\n                    \"The file should contain a function called pudb_stringifier() \"\n                    \"at the module level, which should take a single argument and \"\n                    \"return the desired string form of the object passed to it. \"\n                    \"Note that if you choose a custom stringifier, the variables \"\n                    \"view will not be updated until you close this dialog.\"),\n            ]\n\n    # }}}\n\n    # {{{ variables access level\n\n    default_variables_access_level_opts = [\"public\", \"private\", \"all\"]\n    default_variables_access_level_rb_group = []\n    default_variables_access_level_info = urwid.Text(\n            \"Set the default attribute visibility \"\n            \"of variables in the variables list.\\n\"\n            \"\\nNote that you can change this option on \"\n            \"a per-variable basis by selecting the \"\n            \"variable and pressing '*'.\")\n    default_variables_access_level_rbs = [\n            urwid.RadioButton(default_variables_access_level_rb_group, name,\n                conf_dict[\"default_variables_access_level\"] == name,\n                on_state_change=_update_config,\n                user_data=(\"default_variables_access_level\", name))\n            for name in default_variables_access_level_opts\n            ]\n\n    # }}}\n\n    # {{{ wrap variables\n\n    cb_wrap_variables = urwid.CheckBox(\"Wrap variables\",\n            bool(conf_dict[\"wrap_variables\"]), on_state_change=_update_config,\n                user_data=(\"wrap_variables\", None))\n\n    wrap_variables_info = urwid.Text(\"\\nNote that you can change this option on \"\n                                     \"a per-variable basis by selecting the \"\n                                     \"variable and pressing 'w'.\")\n\n    # }}}\n\n    # {{{ display\n\n    display_info = urwid.Text(\"What driver is used to talk to your terminal. \"\n            \"'raw' has the most features (colors and highlighting), \"\n            \"but is only correct for \"\n            \"XTerm and terminals like it. 'curses' \"\n            \"has fewer \"\n            \"features, but it will work with just about any terminal. 'auto' \"\n            \"will attempt to pick between the two based on availability and \"\n            \"the $TERM environment variable.\\n\\n\"\n            \"Changing this setting requires a restart of PuDB.\")\n\n    displays = [\"auto\", \"raw\", \"curses\"]\n\n    display_rb_group = []\n    display_rbs = [\n            urwid.RadioButton(display_rb_group, name,\n                conf_dict[\"display\"] == name)\n            for name in displays]\n\n    # }}}\n\n    lb_contents = (\n            [heading,\n                urwid.AttrMap(\n                              urwid.Text(\"General:\\n\"),\n                              \"group head\"),\n                cb_line_numbers,\n                cb_prompt_on_quit,\n                hide_cmdline_win,\n                urwid.AttrMap(\n                              urwid.Text(\"\\nShell:\\n\"),\n                              \"group head\"),\n                shell_info,\n                *shell_rbs,\n                urwid.AttrMap(\n                              urwid.Text(\"\\nTheme:\\n\"),\n                              \"group head\"),\n                *theme_rbs,\n                urwid.AttrMap(\n                                          urwid.Text(\"\\nStack Order:\\n\"),\n                                          \"group head\"),\n                stack_info,\n                *stack_rbs,\n                urwid.AttrMap(\n                              urwid.Text(\"\\nVariable Stringifier:\\n\"),\n                              \"group head\"),\n                stringifier_info,\n                *stringifier_rbs,\n                urwid.AttrMap(\n                              urwid.Text(\"\\nVariables Attribute Visibility:\\n\"),\n                              \"group head\"),\n                default_variables_access_level_info,\n                *default_variables_access_level_rbs,\n                urwid.AttrMap(\n                              urwid.Text(\"\\nWrap Variables:\\n\"),\n                              \"group head\"),\n                cb_wrap_variables,\n                wrap_variables_info,\n                urwid.AttrMap(\n                              urwid.Text(\"\\nDisplay driver:\\n\"),\n                              \"group head\"),\n                display_info,\n                *display_rbs]\n            )\n\n    lb = urwid.ListBox(urwid.SimpleListWalker(lb_contents))\n\n    if ui.dialog(lb,         [\n            (\"OK\", True),\n            (\"Cancel\", False),\n            ],\n            title=\"Edit Preferences\"):\n        # Only update the settings here that instant-apply (above) doesn't take\n        # care of.\n\n        # if we had a custom theme, it wasn't updated live\n        if theme_rb_group[-1].state:\n            newvalue = theme_edit.get_edit_text()\n            conf_dict.update(theme=newvalue, custom_theme=newvalue)\n            _update_theme()\n\n        # Ditto for custom stringifiers\n        if stringifier_rb_group[-1].state:\n            newvalue = stringifier_edit.get_edit_text()\n            conf_dict.update(stringifier=newvalue, custom_stringifier=newvalue)\n            _update_stringifier()\n\n        if shell_rb_group[-1].state:\n            newvalue = shell_edit.get_edit_text()\n            conf_dict.update(shell=newvalue, custom_shell=newvalue)\n        else:\n            for shell, shell_rb in zip(shells, shell_rbs):\n                if shell_rb.get_state():\n                    conf_dict[\"shell\"] = shell\n\n        for display, display_rb in zip(displays, display_rbs):\n            if display_rb.get_state():\n                conf_dict[\"display\"] = display\n\n    else:  # The user chose cancel, revert changes\n        conf_dict.update(old_conf_dict)\n        _update_theme()\n        # _update_line_numbers() is equivalent to _update_theme()\n        _update_current_stack_frame()\n        _update_stringifier()\n\n\n# {{{ breakpoint saving\n\ndef parse_breakpoints(lines):\n    # b [ (filename:lineno | function) [, \"condition\"] ]\n\n    breakpoints = []\n    for arg in lines:\n        if not arg:\n            continue\n        arg = arg[1:]\n\n        filename = None\n        lineno = None\n        cond = None\n        comma = arg.find(\",\")\n\n        if comma > 0:\n            # parse stuff after comma: \"condition\"\n            cond = arg[comma+1:].lstrip()\n            arg = arg[:comma].rstrip()\n\n        colon = arg.rfind(\":\")\n        funcname = None\n\n        if colon > 0:\n            filename = arg[:colon].strip()\n\n            f = lookup_module(filename)\n            if not f:\n                continue\n            else:\n                filename = f\n\n            arg = arg[colon+1:].lstrip()\n            try:\n                lineno = int(arg)\n            except ValueError:\n                continue\n        else:\n            continue\n\n        if get_breakpoint_invalid_reason(filename, lineno) is None:\n            breakpoints.append((filename, lineno, False, cond, funcname))\n\n    return breakpoints\n\n\ndef get_breakpoints_file_name():\n    from os.path import join\n    save_path = get_save_config_path()\n    if not save_path:\n        return None\n    else:\n        return join(save_path, SAVED_BREAKPOINTS_FILE_NAME)\n\n\ndef load_breakpoints():\n    \"\"\"\n    Loads and check saved breakpoints out from files\n    Returns: list of tuples\n    \"\"\"\n    from os.path import isdir, join\n\n    file_names = []\n    for cdir in XDG_CONFIG_DIRS:\n        if isdir(cdir):\n            for name in [SAVED_BREAKPOINTS_FILE_NAME, BREAKPOINTS_FILE_NAME]:\n                file_names.append(join(cdir, XDG_CONF_RESOURCE, name))\n\n    lines = []\n    for fname in file_names:\n        try:\n            rc_file = open(fname)\n        except OSError:\n            pass\n        else:\n            lines.extend([line.strip() for line in rc_file.readlines()])\n            rc_file.close()\n\n    return parse_breakpoints(lines)\n\n\ndef save_breakpoints(bp_list):\n    \"\"\"\n    :arg bp_list: a list of `bdb.Breakpoint` objects\n    \"\"\"\n    save_path = get_breakpoints_file_name()\n    if not save_path:\n        return\n\n    histfile = open(get_breakpoints_file_name(), \"w\")\n    bp_list = {(bp.file, bp.line, bp.cond) for bp in bp_list}\n    for bp in bp_list:\n        line = \"b %s:%d\" % (bp[0], bp[1])\n        if bp[2]:\n            line += f\", {bp[2]}\"\n        line += \"\\n\"\n        histfile.write(line)\n    histfile.close()\n\n# }}}\n\n# vim:foldmethod=marker\n", "metadata": {"source": "github.com/inducer/pudb", "file_path": "pudb/settings.py", "language": "python", "complexity": "complex", "star_count": 3122, "quality_score": 0.8099999999999999, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": true, "has_documentation": true, "has_type_hints": true, "has_testing": false, "has_logging": true, "function_count": 18, "class_count": 0, "line_count": 633}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "testing_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create comprehensive test suite with fixtures and mocks\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from __future__ import annotations\n\n\n__copyright__ = \"\"\"\nCopyright (C) 2009-2017 Andreas Kloeckner\nCopyright (C) 2014-2017 Aaron Meurer\n\"\"\"\n\n__license__ = \"\"\"\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in\nall copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\nTHE SOFTWARE.\n\"\"\"\n\n\nimport urwid\n\n\nTABSTOP = 8\n\n\nclass SourceLine(urwid.Widget):\n    _sizing = frozenset([urwid.Sizing.FLOW])\n\n    def __init__(self, dbg_ui, text, line_nr=\"\", attr=None, has_breakpoint=False):\n        super().__init__()\n\n        self.dbg_ui = dbg_ui\n        self.text = text\n        self.attr = attr\n        self.line_nr = line_nr\n        self.has_breakpoint = has_breakpoint\n        self.is_current = False\n        self.highlight = False\n\n    def selectable(self):\n        return True\n\n    def set_current(self, is_current):\n        self.is_current = is_current\n        self._invalidate()\n\n    def set_highlight(self, highlight):\n        self.highlight = highlight\n        self._invalidate()\n\n    def set_breakpoint(self, has_breakpoint):\n        self.has_breakpoint = has_breakpoint\n        self._invalidate()\n\n    def rows(self, size, focus=False):\n        return 1\n\n    def render(self, size, focus=False):\n        from pudb.debugger import CONFIG\n        render_line_nr = CONFIG[\"line_numbers\"]\n\n        maxcol = size[0]\n        hscroll = self.dbg_ui.source_hscroll_start\n\n        # attrs is a list of words like \"focused\" and \"breakpoint\"\n        attrs = []\n\n        if self.is_current:\n            crnt = \">\"\n            attrs.append(\"current\")\n        else:\n            crnt = \" \"\n\n        if self.has_breakpoint:\n            bp = \"*\"\n            attrs.append(\"breakpoint\")\n        else:\n            bp = \" \"\n\n        if focus:\n            attrs.append(\"focused\")\n        elif self.highlight:\n            if not self.has_breakpoint:\n                attrs.append(\"highlighted\")\n\n        text = self.text\n        if not attrs and self.attr is not None:\n            attr = [*self.attr, (\"source\", None)]\n        else:\n            attr = [(\" \".join([*attrs, \"source\"]), None)]\n\n        from urwid.util import apply_target_encoding, trim_text_attr_cs\n\n        # build line prefix ---------------------------------------------------\n        line_prefix = \"\"\n        line_prefix_attr = []\n\n        if render_line_nr and self.line_nr:\n            line_prefix_attr = [(\"line number\", len(self.line_nr))]\n            line_prefix = self.line_nr\n\n        line_prefix = crnt+bp+line_prefix\n        line_prefix_attr = [\n            (\"current line marker\", 1),\n             (\"breakpoint marker\", 1),\n             *line_prefix_attr]\n\n        # assume rendered width is same as len\n        line_prefix_len = len(line_prefix)\n\n        encoded_line_prefix, line_prefix_cs = apply_target_encoding(line_prefix)\n\n        assert len(encoded_line_prefix) == len(line_prefix)\n        # otherwise we'd have to adjust line_prefix_attr... :/\n\n        # shipout, encoding ---------------------------------------------------\n        cs = []\n        encoded_text_segs = []\n        encoded_attr = []\n\n        i = 0\n        for seg_attr, seg_len in attr:\n            if seg_len is None:\n                # means: gobble up remainder of text and rest of line\n                # and fill with attribute\n\n                rowlen = hscroll+maxcol\n                remaining_text = text[i:]\n                encoded_seg_text, seg_cs = apply_target_encoding(\n                        remaining_text + rowlen*\" \")\n                encoded_attr.append((seg_attr, len(remaining_text)+rowlen))\n            else:\n                unencoded_seg_text = text[i:i+seg_len]\n                encoded_seg_text, seg_cs = apply_target_encoding(unencoded_seg_text)\n\n                adjustment = len(encoded_seg_text) - len(unencoded_seg_text)\n\n                encoded_attr.append((seg_attr, seg_len + adjustment))\n\n                i += seg_len\n\n            encoded_text_segs.append(encoded_seg_text)\n            cs.extend(seg_cs)\n\n        encoded_text = b\"\".join(encoded_text_segs)\n        encoded_text, encoded_attr, cs = trim_text_attr_cs(\n                encoded_text, encoded_attr, cs,\n                hscroll, hscroll+maxcol-line_prefix_len)\n\n        encoded_text = encoded_line_prefix + encoded_text\n        encoded_attr = line_prefix_attr + encoded_attr\n        cs = line_prefix_cs + cs\n\n        return urwid.TextCanvas([encoded_text], [encoded_attr], [cs], maxcol=maxcol)\n\n    def keypress(self, size, key):\n        return key\n\n\nclass ParseState:\n    \"\"\"States for the ArgumentParser class\"\"\"\n    idle = 1\n    found_function = 2\n    found_open_paren = 3\n\n\nclass ArgumentParser:\n    \"\"\"Parse source code tokens and identify function arguments.\n\n    This parser implements a state machine which accepts\n    Pygments tokens, delivered sequentially from the beginning\n    of a source file to its end.\n\n    parse_token() processes each token (and its associated string)\n    and returns None if that token does not require modification.\n    When it finds a token which represents a function\n    argument, it returns the correct token type for that\n    item (the caller should then replace the associated item's\n    token type with the returned type)\n    \"\"\"\n\n    def __init__(self, pygments_token):\n        self.t = pygments_token\n        self.state = ParseState.idle\n        self.paren_level = 0\n\n    def parse_token(self, token, s):\n        \"\"\"Parse token. Return None or replacement token type\"\"\"\n        if self.state == ParseState.idle:\n            if token in (self.t.Name.Function, self.t.Name.Function.Magic):\n                self.state = ParseState.found_function\n                self.paren_level = 0\n        elif self.state == ParseState.found_function:\n            if token is self.t.Punctuation and s == \"(\":\n                self.state = ParseState.found_open_paren\n                self.paren_level = 1\n        else:\n            if (token is self.t.Name):\n                return self.t.Token.Argument\n            elif token is self.t.Punctuation and s == \")\":\n                self.paren_level -= 1\n            elif token is self.t.Punctuation and s == \"(\":\n                self.paren_level += 1\n            if self.paren_level == 0:\n                self.state = ParseState.idle\n        return None\n\n\ntry:\n    import pygments  # noqa\nexcept ImportError:\n    def format_source(debugger_ui, lines, breakpoints):\n        lineno_format = \"%%%dd \" % (len(str(len(lines))))\n        return [\n            SourceLine(\n                debugger_ui,\n                line.rstrip(\"\\n\\r\").expandtabs(TABSTOP),\n                lineno_format % (i+1),\n                None,\n                has_breakpoint=i+1 in breakpoints,\n            )\n            for i, line in enumerate(lines)\n        ]\nelse:\n    import pygments.token as t\n    from pygments import highlight\n    from pygments.formatter import Formatter\n    from pygments.lexers import PythonLexer\n\n    argument_parser = ArgumentParser(t)\n\n    # NOTE: Tokens of the form t.Token.<name> are not native\n    #       Pygments token types; they are user defined token\n    #       types.\n    #\n    #       t.Token is a Pygments token creator object\n    #       (see http://pygments.org/docs/tokens/)\n    #\n    #       The user defined token types get assigned by\n    #       one of several translation operations at the\n    #       beginning of add_snippet().\n    #\n    ATTR_MAP = {\n        t.Token: \"source\",\n        t.Keyword.Namespace: \"namespace\",\n        t.Token.Argument: \"argument\",\n        t.Token.Dunder: \"dunder\",\n        t.Token.Keyword2: \"keyword2\",\n        t.Keyword: \"keyword\",\n        t.Literal: \"literal\",\n        t.Name: \"name\",\n        t.Name.Exception: \"exception\",\n        t.Name.Function: \"function\",\n        t.Name.Function.Magic: \"magic\",\n        t.Name.Class: \"class\",\n        t.Name.Builtin: \"builtin\",\n        t.Name.Builtin.Pseudo: \"pseudo\",\n        t.Name.Variable.Magic: \"magic\",\n        t.Punctuation: \"punctuation\",\n        t.Operator: \"operator\",\n        t.String: \"string\",\n        t.String.Double: \"doublestring\",\n        t.String.Single: \"singlestring\",\n        t.String.Backtick: \"backtick\",\n        t.String.Doc: \"docstring\",\n        t.Comment: \"comment\",\n    }\n\n    # Token translation table. Maps token types and their\n    # associated strings to new token types.\n    ATTR_TRANSLATE = {\n            t.Keyword: {\n                \"class\": t.Token.Keyword2,\n                \"def\": t.Token.Keyword2,\n                \"exec\": t.Token.Keyword2,\n                \"lambda\": t.Token.Keyword2,\n                \"print\": t.Token.Keyword2,\n                },\n            t.Operator: {\n                \".\": t.Token,\n                },\n            }\n\n    class UrwidFormatter(Formatter):\n        def __init__(self, debugger_ui, lineno_format, breakpoints, **options):\n            Formatter.__init__(self, **options)\n            self.current_line = \"\"\n            self.current_attr = []\n            self.lineno = 1\n            self.result = []\n            self.debugger_ui = debugger_ui\n            self.lineno_format = lineno_format\n            self.breakpoints = breakpoints\n\n        def add_snippet(self, ttype, s):\n            if not s:\n                return\n\n            # Find function arguments. When found, change their\n            # ttype to t.Token.Argument\n            new_ttype = argument_parser.parse_token(ttype, s)\n            if new_ttype:\n                ttype = new_ttype\n\n            # Translate tokens\n            if ttype in ATTR_TRANSLATE:\n                if s in ATTR_TRANSLATE[ttype]:\n                    ttype = ATTR_TRANSLATE[ttype][s]\n\n            # Translate dunder method tokens\n            # NOTE: leaves \"Magic\" name tokens alone\n            if (ttype == t.Name.Function\n                    and s.startswith(\"__\")\n                    and s.endswith(\"__\")):\n                ttype = t.Token.Dunder\n\n            while ttype not in ATTR_MAP:\n                if ttype.parent is not None:\n                    ttype = ttype.parent\n                else:\n                    raise RuntimeError(\n                            f\"untreated token type: {ttype!s}\")\n\n            attr = ATTR_MAP[ttype]\n\n            self.current_line += s\n            self.current_attr.append((attr, len(s)))\n\n        def shipout_line(self):\n            self.result.append(\n                SourceLine(\n                    self.debugger_ui,\n                    self.current_line,\n                    self.lineno_format % self.lineno,\n                    self.current_attr,\n                    has_breakpoint=self.lineno in self.breakpoints,\n                ))\n            self.current_line = \"\"\n            self.current_attr = []\n            self.lineno += 1\n\n        def format(self, tokensource, outfile):\n            for ttype, value in tokensource:\n                while True:\n                    newline_pos = value.find(\"\\n\")\n                    if newline_pos == -1:\n                        self.add_snippet(ttype, value)\n                        break\n                    else:\n                        self.add_snippet(ttype, value[:newline_pos])\n                        self.shipout_line()\n                        value = value[newline_pos+1:]\n\n            if self.current_line:\n                self.shipout_line()\n\n    def format_source(debugger_ui, lines, breakpoints):\n        lineno_format = \"%%%dd \" % (len(str(len(lines))))\n        formatter = UrwidFormatter(debugger_ui, lineno_format, breakpoints)\n        highlight(\n            \"\".join(line.expandtabs(TABSTOP) for line in lines),\n            PythonLexer(stripnl=False),\n            formatter,\n        )\n        return formatter.result\n", "metadata": {"source": "github.com/inducer/pudb", "file_path": "pudb/source_view.py", "language": "python", "complexity": "complex", "star_count": 3122, "quality_score": 0.8099999999999999, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": true, "has_documentation": true, "has_type_hints": true, "has_testing": true, "has_logging": false, "function_count": 16, "class_count": 4, "line_count": 376}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "testing_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create comprehensive test suite with fixtures and mocks with pytest testing framework\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import sys\n\nfrom pudb.lowlevel import decode_lines, detect_encoding\n\n\ndef test_detect_encoding_nocookie():\n    lines = [\"Test Проверка\"]\n    lines = [line.encode(\"utf-8\") for line in lines]\n    encoding, _ = detect_encoding(iter(lines))\n    assert encoding == \"utf-8\"\n\n\ndef test_detect_encoding_cookie():\n    lines = [\n        \"# coding=utf-8\",\n        \"Test\",\n        \"Проверка\"\n    ]\n    lines = [line.encode(\"utf-8\") for line in lines]\n    encoding, _ = detect_encoding(iter(lines))\n    assert encoding == \"utf-8\"\n\n\ndef test_decode_lines():\n    unicode_lines = [\n        \"# coding=utf-8\",\n        \"Test\",\n        \"Проверка\",\n    ]\n    lines = [line.encode(\"utf-8\") for line in unicode_lines]\n    assert unicode_lines == list(decode_lines(iter(lines)))\n\n\n# {{{ remove common indentation\n\ndef _remove_common_indentation(code, require_leading_newline=True):\n    if \"\\n\" not in code:\n        return code\n\n    if require_leading_newline and not code.startswith(\"\\n\"):\n        return code\n\n    lines = code.split(\"\\n\")\n    while lines[0].strip() == \"\":\n        lines.pop(0)\n    while lines[-1].strip() == \"\":\n        lines.pop(-1)\n\n    if lines:\n        base_indent = 0\n        while lines[0][base_indent] in \" \\t\":\n            base_indent += 1\n\n        for line in lines[1:]:\n            if line[:base_indent].strip():\n                raise ValueError(\"inconsistent indentation\")\n\n    return \"\\n\".join(line[base_indent:] for line in lines)\n\n# }}}\n\n\ndef test_executable_lines():\n    def get_exec_lines(src):\n        code = compile(\n                    _remove_common_indentation(test_code),\n                    \"<tmp>\", \"exec\")\n        from pudb.lowlevel import get_executable_lines_for_codes_recursive\n        return get_executable_lines_for_codes_recursive([code])\n\n    test_code = \"\"\"\n        def main():\n            import pudb; pu.db\n            conf = ''. \\\\\n                replace('', '')\n\n            conf_tpl = ''  # <-- impossible to set breakpoint here\n\n        main()\n        \"\"\"\n    expected = {1, 2, 3, 4, 6, 8}\n    if sys.version_info >= (3, 11):\n        # See https://github.com/python/cpython/pull/94562 and\n        # https://peps.python.org/pep-0626/\n        expected |= {0}\n\n    assert get_exec_lines(test_code) == expected\n\n    test_code = \"a = 3*5\\n\" + 333 * \"\\n\" + \"b = 15\"\n    expected = {\n        1,\n        335\n    }\n    if sys.version_info < (3, 10):\n        # if co_lines is unavailable, we appear to see these\n        expected.update([\n            128,  # bogus,\n            255,  # bogus,\n        ])\n    if sys.version_info >= (3, 11):\n        # See https://github.com/python/cpython/pull/94562 and\n        # https://peps.python.org/pep-0626/\n        expected |= {0}\n\n    assert get_exec_lines(test_code) == expected\n\n\nif __name__ == \"__main__\":\n    if len(sys.argv) > 1:\n        exec(sys.argv[1])\n    else:\n        from pytest import main\n        main([__file__])\n", "metadata": {"source": "github.com/inducer/pudb", "file_path": "pudb/test/test_lowlevel.py", "language": "python", "complexity": "medium", "star_count": 3122, "quality_score": 0.78, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": true, "has_type_hints": false, "has_testing": true, "has_logging": false, "function_count": 7, "class_count": 0, "line_count": 114}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "testing_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create comprehensive test suite with fixtures and mocks with pytest testing framework\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from pudb.ui_tools import make_canvas\n\n\ndef test_simple():\n    text = \"aaaaaa\"\n    canvas = make_canvas(\n        txt=[text],\n        attr=[[(\"var value\", len(text))]],\n        maxcol=len(text) + 5\n    )\n    content = list(canvas.content())\n    assert content == [\n        [(\"var value\", None, b\"aaaaaa\"), (None, None, b\" \" * 5)]\n    ]\n\n\ndef test_multiple():\n    canvas = make_canvas(\n        txt=[\"Return: None\"],\n        attr=[[(\"return label\", 8), (\"return value\", 4)]],\n        maxcol=100\n    )\n    content = list(canvas.content())\n    assert content == [\n        [(\"return label\", None, b\"Return: \"),\n         (\"return value\", None, b\"None\"),\n         (None, None, b\" \" * 88)]\n    ]\n\n\ndef test_boundary():\n    text = \"aaaaaa\"\n    canvas = make_canvas(\n        txt=[text],\n        attr=[[(\"var value\", len(text))]],\n        maxcol=len(text)\n    )\n    assert list(canvas.content()) == [[(\"var value\", None, b\"aaaaaa\")]]\n\n\ndef test_byte_boundary():\n    text = \"aaaaaaé\"\n    canvas = make_canvas(\n        txt=[text],\n        attr=[[(\"var value\", len(text))]],\n        maxcol=len(text)\n    )\n    assert list(canvas.content()) == [[(\"var value\", None, b\"aaaaaa\\xc3\\xa9\")]]\n\n\ndef test_wide_chars():\n    text = \"data: '中文'\"\n    canvas = make_canvas(\n        txt=[text],\n        attr=[[(\"var label\", 6), (\"var value\", 4)]],\n        maxcol=47,\n    )\n    assert list(canvas.content()) == [[\n        (\"var label\", None, b\"data: \"),\n        (\"var value\", None, \"'中文'\".encode()),\n        (None, None, b\" \"*(47 - 12)),  # 10 chars, 2 of which are double width\n        ]]\n\n\nif __name__ == \"__main__\":\n    import sys\n    if len(sys.argv) > 1:\n        exec(sys.argv[1])\n    else:\n        from pytest import main\n        main([__file__])\n", "metadata": {"source": "github.com/inducer/pudb", "file_path": "pudb/test/test_make_canvas.py", "language": "python", "complexity": "simple", "star_count": 3122, "quality_score": 0.75, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": true, "has_logging": false, "function_count": 5, "class_count": 0, "line_count": 72}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "testing_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create comprehensive test suite with fixtures and mocks with pytest testing framework\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "#!/usr/bin/env python3\n\nimport os\n\nimport pytest\n\nfrom pudb.run import main\n\n\ndef csv(x):\n    return \"('\" + \"', '\".join(x) + \"',)\"\n\n\nmain_version_scenarios = [\n    (\"-v\",),\n    (\"--version\",),\n    (\"--version\", \"dont_look_at_me.py\"),\n]\n\n\n@pytest.mark.parametrize(\n    \"argv\",\n    [pytest.param(s, id=csv(s)) for s in main_version_scenarios],\n)\ndef test_main_version(capsys, mocker, argv):\n    mocker.patch(\"sys.argv\", [os.path.basename(main.__code__.co_filename), *argv])\n\n    with pytest.raises(SystemExit) as ex:\n        main()\n        assert ex.value == 0\n\n    captured = capsys.readouterr()\n\n    assert \"pudb v\" in captured.out\n\n\ndef test_main_v_with_args(capsys, mocker):\n    \"\"\"\n    This will fail, because args is not only ``-v``,\n    and that's reserved for future use ...\n    \"\"\"\n    mocker.patch(\"sys.argv\", [\n        os.path.basename(main.__code__.co_filename),\n        \"-v\",\n        \"dont_look_at_me.py\"\n    ])\n\n    with pytest.raises(SystemExit) as ex:\n        main()\n        assert ex.value == 2\n\n    captured = capsys.readouterr()\n\n    assert \"error: unrecognized arguments: -v\" in captured.err\n    assert not captured.out\n", "metadata": {"source": "github.com/inducer/pudb", "file_path": "pudb/test/test_run.py", "language": "python", "complexity": "medium", "star_count": 3122, "quality_score": 0.78, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": true, "has_type_hints": true, "has_testing": true, "has_logging": false, "function_count": 3, "class_count": 0, "line_count": 56}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "testing_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create comprehensive test suite with fixtures and mocks with pytest testing framework\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import builtins\nimport collections\n\nimport pytest  # noqa: F401\n\nfrom pudb.settings import load_breakpoints, save_breakpoints\n\n\ndef test_load_breakpoints(mocker):\n    fake_data = [\"b /home/user/test.py:41\"], [\"b /home/user/test.py:50\"]\n    mock_open = mocker.mock_open()\n    mock_open.return_value.readlines.side_effect = fake_data\n    mocker.patch.object(builtins, \"open\", mock_open)\n    mocker.patch(\"pudb.settings.lookup_module\",\n            mocker.Mock(return_value=\"/home/user/test.py\"))\n    mocker.patch(\"pudb.settings.get_breakpoint_invalid_reason\",\n            mocker.Mock(return_value=None))\n    result = load_breakpoints()\n    expected = [(\"/home/user/test.py\", 41, False, None, None),\n                (\"/home/user/test.py\", 50, False, None, None)]\n    assert result == expected\n\n\ndef test_save_breakpoints(mocker):\n    MockBP = collections.namedtuple(\"MockBreakpoint\", \"file line cond\")\n    mock_breakpoints = [MockBP(\"/home/user/test.py\", 41, None),\n                        MockBP(\"/home/user/test.py\", 50, None)]\n    mocker.patch(\"pudb.settings.get_breakpoints_file_name\",\n                 mocker.Mock(return_value=\"saved-breakpoints\"))\n    mock_open = mocker.mock_open()\n    mocker.patch.object(builtins, \"open\", mock_open)\n\n    save_breakpoints(mock_breakpoints)\n    mock_open.assert_called_with(\"saved-breakpoints\", \"w\")\n", "metadata": {"source": "github.com/inducer/pudb", "file_path": "pudb/test/test_settings.py", "language": "python", "complexity": "simple", "star_count": 3122, "quality_score": 0.78, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": true, "has_logging": false, "function_count": 2, "class_count": 0, "line_count": 35}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "testing_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create comprehensive test suite with fixtures and mocks with pytest testing framework\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import pytest  # noqa: F401\n\nfrom pudb.debugger import (\n    DirectSourceCodeProvider,\n    FileSourceCodeProvider,\n    NullSourceCodeProvider,\n)\nfrom pudb.source_view import SourceLine\n\n\nclass TestNullSourceCodeProvider:\n    def test_get_lines(self, mocker):\n        provider = NullSourceCodeProvider()\n        result = provider.get_lines(mocker.Mock())\n        assert len(result) == 10\n        assert isinstance(result[0], SourceLine)\n\n\nclass TestFileSourceCodeProvider:\n    def test_string_file_name(self, mocker):\n        mock_debugger = mocker.Mock()\n        mock_debugger.canonic = mocker.Mock(return_value=\"<string>\")\n        provider = FileSourceCodeProvider(mock_debugger, \"test file name\")\n        result = provider.get_lines(mocker.MagicMock())\n        assert len(result) == 1\n        assert isinstance(result[0], SourceLine)\n\n    def test_get_lines(self, mocker):\n        provider = FileSourceCodeProvider(mocker.Mock(), \"test file name\")\n        result = provider.get_lines(mocker.MagicMock())\n        assert len(result) == 1\n        assert isinstance(result[0], SourceLine)\n\n\nclass TestDirectSourceCodeProvider:\n    def test_get_lines(self, mocker):\n        provider = DirectSourceCodeProvider(mocker.Mock(), \"test code\")\n        result = provider.get_lines(mocker.Mock())\n        assert len(result) == 1\n        assert isinstance(result[0], SourceLine)\n", "metadata": {"source": "github.com/inducer/pudb", "file_path": "pudb/test/test_source_code_providers.py", "language": "python", "complexity": "medium", "star_count": 3122, "quality_score": 0.78, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": true, "has_logging": false, "function_count": 4, "class_count": 3, "line_count": 41}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "testing_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create comprehensive test suite with fixtures and mocks with pytest testing framework\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import contextlib\nimport itertools\nimport string\nimport unittest\n\nfrom pudb.var_view import (\n    STRINGIFIERS,\n    BasicValueWalker,\n    FrameVarInfo,\n    InspectInfo,\n    PudbCollection,\n    PudbMapping,\n    PudbSequence,\n    ValueWalker,\n    get_stringifier,\n    ui_log,\n)\n\n\nclass A:\n    pass\n\n\nclass A2:\n    pass\n\n\ndef test_get_stringifier():\n    try:\n        import numpy as np\n    except ImportError:\n        numpy_values = []\n    else:\n        numpy_values = [np.float32(5), np.zeros(5)]\n\n    for value in [\n            A,\n            A2,\n            A(),\n            A2(),\n            \"lól\".encode(),\n            \"lól\",\n            1233123,\n            [\"lól\".encode(), \"lól\"],\n            *numpy_values]:\n        for display_type in STRINGIFIERS:\n            iinfo = InspectInfo()\n            iinfo.display_type = display_type\n\n            strifier = get_stringifier(iinfo)\n\n            s = strifier(value)\n            assert isinstance(s, str)\n\n\nclass FrameVarInfoForTesting(FrameVarInfo):\n    def __init__(self, paths_to_expand=None):\n        super().__init__()\n        if paths_to_expand is None:\n            paths_to_expand = set()\n        self.paths_to_expand = paths_to_expand\n\n    def get_inspect_info(self, id_path, read_only):\n        iinfo = super().get_inspect_info(\n            id_path, read_only)\n        iinfo.access_level = \"all\"\n        iinfo.display_type = \"repr\"\n        iinfo.show_methods = True\n\n        if id_path in self.paths_to_expand:\n            iinfo.show_detail = True\n        return iinfo\n\n\nclass Reasonable:\n    def __init__(self):\n        self.x = 42\n\n    def bar(self):\n        return True\n\n    @property\n    def red(self):\n        return \"red\"\n\n    @classmethod\n    def blue(cls):\n        return \"blue\"\n\n    @staticmethod\n    def green():\n        return \"green\"\n\n    def _private(self):\n        return \"shh\"\n\n    def __magicsomething__(self):\n        return \"amazing\"\n\n\ndef method_factory(method_name):\n    def method(self, *args, **kwargs):\n        func = getattr(self.__internal_dict__, method_name)\n        try:\n            return func(*args, **kwargs)\n        except KeyError:\n            # Classes without __iter__ are expected to raise IndexError in this\n            # sort of case. Frustrating, I know.\n            if (method_name == \"__getitem__\"\n                    and args and isinstance(args[0], int)):\n                raise IndexError from None\n            raise\n    return method\n\n\ndef generate_containerlike_class():\n    methods = {\n        \"__contains__\",\n        \"__getitem__\",\n        \"__iter__\",\n        \"__len__\",\n        \"__reversed__\",\n        \"count\",\n        \"get\",\n        \"index\",\n        \"items\",\n        \"keys\",\n        \"values\",\n    }\n\n    # Deliberately starting from 0\n    for r in range(0, len(methods) + 1):\n        for selected_methods in sorted(\n                map(sorted, itertools.combinations(methods, r))):\n\n            class ContainerlikeClass:\n                def __init__(self, iterable):\n                    self.__internal_dict__ = dict(iterable)\n\n                @classmethod\n                def name(cls):\n                    return \"ContainerlikeClass:{}\".format(\n                            \":\".join(selected_methods))  # noqa: B023\n\n            for method in selected_methods:\n                func = method_factory(method)\n                setattr(ContainerlikeClass, method, func)\n\n            yield ContainerlikeClass\n\n\nclass BaseValueWalkerTestCase(unittest.TestCase):\n    \"\"\"\n    There are no actual tests defined in this class, it provides utilities\n    useful for testing the variable view in various ways.\n    \"\"\"\n    EMPTY_ITEM = (ValueWalker.EMPTY_LABEL, None)\n    MOD_STR = \" [all+()]\"\n\n    def setUp(self):\n        self.values_to_expand = []\n        self.class_counts = {\n            \"mappings\": 0,\n            \"sequences\": 0,\n            \"collections\": 0,\n            \"other\": 0,\n        }\n\n    def value_string(self, obj, expand=True):\n        if expand and obj in self.values_to_expand:\n            return repr(obj) + self.MOD_STR\n        return repr(obj)\n\n    def walked_values(self):\n        return [(w.var_label, w.value_str)\n                for w in self.walker.widget_list]\n\n    def expected_attrs(self, obj):\n        \"\"\"\n        `dir()` the object and return (label, value string) pairs for each\n        attribute. Should match the order that these attributes would appear in\n        the var_view.\n        \"\"\"\n        return [(\".\" + str(label),\n                 self.value_string(getattr(obj, label), expand=False))\n                for label in sorted(dir(obj))]\n\n    @contextlib.contextmanager\n    def patched_logging(self):\n        \"\"\"\n        Context manager that patches ui_log.exception such that the test will\n        fail if it is called.\n        \"\"\"\n        def fake_exception_log(*args, **kwargs):\n            self.fail(\"ui_log.exception was unexpectedly called\")\n\n        old_logger = ui_log.exception\n        ui_log.exception = fake_exception_log\n        try:\n            yield\n        finally:\n            ui_log.exception = old_logger\n\n    def assert_walks_contents(self, container, label=\"xs\"):\n        expand_paths = {label}\n        self.values_to_expand = [container]\n        self.walker = BasicValueWalker(FrameVarInfoForTesting(expand_paths))\n\n        # Build out list of expected view contents according to container type.\n        expected = [(label, self.value_string(container))]\n        if isinstance(container, PudbMapping):\n            expected.extend([(f\"[{key!r}]\", repr(container[key]))\n                             for key in container.keys()]\n                            or [self.EMPTY_ITEM])\n            self.class_counts[\"mappings\"] += 1\n        elif isinstance(container, PudbSequence):\n            expected.extend([(f\"[{index!r}]\", repr(entry))\n                             for index, entry in enumerate(container)]\n                            or [self.EMPTY_ITEM])\n            self.class_counts[\"sequences\"] += 1\n        elif isinstance(container, PudbCollection):\n            expected.extend([(\"[]\", repr(entry))\n                             for entry in container]\n                            or [self.EMPTY_ITEM])\n            self.class_counts[\"collections\"] += 1\n        else:\n            self.class_counts[\"other\"] += 1\n        expected.extend(self.expected_attrs(container))\n\n        with self.patched_logging():\n            self.walker.walk_value(parent=None, label=label, value=container)\n\n        received = self.walked_values()\n        self.assertListEqual(expected, received)\n\n    def assert_class_counts_equal(self, seen=None):\n        \"\"\"\n        This is kinda weird since at first it looks like its testing the test\n        code, but really it's testing the `isinstance` checks. But it is also\n        true that it tests the test code, kind of like a sanity check.\n        \"\"\"\n        expected = {\n            \"mappings\": 0,\n            \"sequences\": 0,\n            \"collections\": 0,\n            \"other\": 0,\n        }\n        if seen is not None:\n            expected.update(seen)\n        self.assertDictEqual(expected, self.class_counts)\n\n\nclass ValueWalkerTest(BaseValueWalkerTestCase):\n    def test_simple_values(self):\n        self.walker = BasicValueWalker(FrameVarInfoForTesting())\n        values = [\n            # numbers\n            0,\n            1,\n            -1234567890412345243,\n            4.2,\n            float(\"inf\"),\n            complex(1.3, -1),\n\n            # strings\n            \"\",\n            \"a\",\n            \"foo bar\",\n            \"  lots\\tof\\nspaces\\r \",\n            \"♫\",\n\n            # other\n            False,\n            True,\n            None,\n        ]\n        for label, value in enumerate(values):\n            with self.patched_logging():\n                self.walker.walk_value(parent=None,\n                                       label=str(label),\n                                       value=value)\n\n        expected = [(str(_label), repr(x)) for _label, x in enumerate(values)]\n        received = self.walked_values()\n        self.assertListEqual(expected, received)\n\n    def test_simple_values_expandable(self):\n        \"\"\"\n        Simple values like numbers and strings are now expandable so we can\n        peak under the hood and take a look at their attributes. Make sure\n        that's working properly.\n        \"\"\"\n        values = [\n            # numbers\n            0,\n            1,\n            -1234567890412345243,\n            4.2,\n            float(\"inf\"),\n            complex(1.3, -1),\n\n            # strings\n            \"\",\n            \"a\",\n            \"foo bar\",\n            # \"  lots\\tof\\nspaces\\r \",  # long, hits continuation item\n            \"♫\",\n\n            # other\n            False,\n            True,\n            None,\n        ]\n        for value in values:\n            self.assert_walks_contents(value)\n\n    def test_set(self):\n        self.assert_walks_contents({\n            42, \"foo\", None, False, (), (\"a\", \"tuple\")\n        })\n        self.assert_class_counts_equal({\"collections\": 1})\n\n    def test_frozenset(self):\n        self.assert_walks_contents(frozenset([\n            42, \"foo\", None, False, (), (\"a\", \"tuple\")\n        ]))\n        self.assert_class_counts_equal({\"collections\": 1})\n\n    def test_dict(self):\n        self.assert_walks_contents({\n            0:                   42,\n            \"a\":                 \"foo\",\n            \"\": None,\n            True: False,\n            frozenset(range(3)): \"abc\",\n            ():                  \"empty tuple\",\n            (1, 2, \"c\", ()):     \"tuple\",\n        })\n        self.assert_class_counts_equal({\"mappings\": 1})\n\n    def test_list(self):\n        self.assert_walks_contents([\n            42, \"foo\", None, False, (), (\"a\", \"tuple\")\n        ])\n        self.assert_class_counts_equal({\"sequences\": 1})\n\n    def test_tuple(self):\n        self.assert_walks_contents((\n            42, \"foo\", None, False, (), (\"a\", \"tuple\")\n        ))\n        self.assert_class_counts_equal({\"sequences\": 1})\n\n    def test_containerlike_classes(self):\n        class_count = 0\n        for cls_idx, containerlike_class in enumerate(\n                generate_containerlike_class()):\n            label = containerlike_class.name()\n            value = containerlike_class(zip(string.ascii_lowercase,\n                                            range(3, 10)))\n            self.assert_walks_contents(container=value, label=label)\n            class_count = cls_idx + 1\n\n        self.assert_class_counts_equal({\n            \"mappings\": 256,\n            \"sequences\": 256,\n            \"collections\": 256,\n            \"other\": 1280,\n        })\n\n        walked_total = (self.class_counts[\"mappings\"]\n                        + self.class_counts[\"sequences\"]\n                        + self.class_counts[\"collections\"]\n                        + self.class_counts[\"other\"])\n\n        # +1 here because enumerate starts from 0, not 1\n        self.assertEqual(class_count, walked_total)\n\n    def test_empty_frozenset(self):\n        self.assert_walks_contents(frozenset())\n\n    def test_empty_set(self):\n        self.assert_walks_contents(set())\n\n    def test_empty_dict(self):\n        self.assert_walks_contents({})\n\n    def test_empty_list(self):\n        self.assert_walks_contents([])\n\n    def test_reasonable_class(self):\n        \"\"\"\n        Are the class objects themselves expandable?\n        \"\"\"\n        self.assert_walks_contents(Reasonable, label=\"Reasonable\")\n        self.assert_class_counts_equal({\"other\": 1})\n\n    def test_maybe_unreasonable_classes(self):\n        \"\"\"\n        Are class objects, that might look like containers if we're not\n        careful, reasonably expandable?\n        \"\"\"\n        for containerlike_class in generate_containerlike_class():\n            self.assert_walks_contents(\n                container=containerlike_class,\n                label=containerlike_class.name()\n            )\n\n        # This effectively makes sure that class definitions aren't considered\n        # containers.\n        self.assert_class_counts_equal({\"other\": 2048})\n", "metadata": {"source": "github.com/inducer/pudb", "file_path": "pudb/test/test_var_view.py", "language": "python", "complexity": "complex", "star_count": 3122, "quality_score": 0.8999999999999999, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": true, "has_documentation": true, "has_type_hints": true, "has_testing": true, "has_logging": true, "function_count": 37, "class_count": 12, "line_count": 411}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "testing_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create comprehensive test suite with fixtures and mocks\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from __future__ import annotations\n\nimport urwid\nfrom urwid import calc_text_pos, calc_width\n\n\n# generic urwid helpers -------------------------------------------------------\n\ndef text_width(txt):\n    \"\"\"Return the width of the text in the terminal.\n\n    :arg txt: A Unicode text object.\n\n    Use this instead of len() whenever txt could contain double- or zero-width\n    Unicode characters.\n\n    \"\"\"\n    return calc_width(txt, 0, len(txt))\n\n\ndef encode_like_urwid(s):\n    from urwid.display import escape\n    from urwid.util import _target_encoding\n\n    # Consistent with\n    # https://github.com/urwid/urwid/blob/2cc54891965283faf9113da72202f5d405f90fa3/urwid/util.py#L126-L128\n\n    s = s.replace(escape.SI+escape.SO, \"\")  # remove redundant shifts\n    s = s.encode(_target_encoding, \"replace\")\n    return s\n\n\ndef make_canvas(txt, attr, maxcol, fill_attr=None):\n    processed_txt = []\n    processed_attr = []\n    processed_cs = []\n\n    for line, line_attr in zip(txt, attr):\n        # filter out zero-length attrs\n        line_attr = [(aname, la) for aname, la in line_attr if la > 0]\n\n        diff = maxcol - text_width(line)\n        if diff > 0:\n            line += \" \"*diff\n            line_attr.append((fill_attr, diff))\n        else:\n            from urwid.util import rle_subseg\n            line = line[:calc_text_pos(line, 0, len(line), maxcol)[0]]\n            line_attr = rle_subseg(line_attr, 0, maxcol)\n\n        from urwid.util import apply_target_encoding\n        encoded_line, line_cs = apply_target_encoding(line)\n\n        # line_cs contains byte counts as requested by TextCanvas, but\n        # line_attr still contains column counts at this point: let's fix this.\n        def get_byte_line_attr(line, line_attr):\n            i = 0\n            for label, column_count in line_attr:\n                byte_count = len(encode_like_urwid(line[i:i+column_count]))\n                i += column_count\n                yield label, byte_count\n\n        line_attr = list(get_byte_line_attr(line, line_attr))\n\n        processed_txt.append(encoded_line)\n        processed_attr.append(line_attr)\n        processed_cs.append(line_cs)\n\n    return urwid.TextCanvas(\n            processed_txt,\n            processed_attr,\n            processed_cs,\n            maxcol=maxcol)\n\n\ndef make_hotkey_markup(s):\n    import re\n    match = re.match(r\"^([^_]*)_(.)(.*)$\", s)\n    assert match is not None\n\n    return [\n            (None, match.group(1)),\n            (\"hotkey\", match.group(2)),\n            (None, match.group(3)),\n            ]\n\n\ndef labelled_value(label, value):\n    return urwid.AttrMap(urwid.Text([\n        (\"label\", label), str(value)]),\n        \"fixed value\", \"fixed value\")\n\n\ndef find_widget_in_container(container, widget) -> int:\n    for i, (w, _) in enumerate(container.contents):\n        if w == widget:\n            return i\n\n    raise ValueError(f\"Widget not found in '{type(container).__name__}': {widget!r}\")\n\n\ndef focus_widget_in_container(container, widget) -> None:\n    container.focus_position = find_widget_in_container(container, widget)\n\n\nclass SelectableText(urwid.Text):\n    def selectable(self):\n        return True\n\n    def keypress(self, size, key):\n        return key\n\n\nclass SignalWrap(urwid.WidgetWrap):\n    def __init__(self, w, is_preemptive=False):\n        urwid.WidgetWrap.__init__(self, w)\n        self.event_listeners = []\n        self.is_preemptive = is_preemptive\n\n    def listen(self, mask, handler):\n        self.event_listeners.append((mask, handler))\n\n    def keypress(self, size, key):\n        result = key\n\n        if self.is_preemptive:\n            for mask, handler in self.event_listeners:\n                if mask is None or mask == key:\n                    result = handler(self, size, key)\n                    break\n\n        if result is not None:\n            result = self._w.keypress(size, key)\n\n        if result is not None and not self.is_preemptive:\n            for mask, handler in self.event_listeners:\n                if mask is None or mask == key:\n                    return handler(self, size, key)\n\n        return result\n\n\n# {{{ debugger-specific stuff\n\nclass StackFrame(urwid.Widget):\n    _sizing = frozenset([urwid.Sizing.FLOW])\n\n    def __init__(self, is_current, name, class_name, filename, line):\n        super().__init__()\n\n        self.is_current = is_current\n        self.name = name\n        self.class_name = class_name\n        self.filename = filename\n        self.line = line\n\n    def selectable(self):\n        return True\n\n    def rows(self, size, focus=False):\n        return 1\n\n    def render(self, size, focus=False):\n        maxcol = size[0]\n        if focus:\n            apfx = \"focused \"\n        else:\n            apfx = \"\"\n\n        if self.is_current:\n            apfx += \"current \"\n            crnt_pfx = \">> \"\n        else:\n            crnt_pfx = \"   \"\n\n        text = crnt_pfx+self.name\n        attr = [(apfx+\"frame name\", 4+len(self.name))]\n\n        if self.class_name is not None:\n            text += f\" [{self.class_name}]\"\n            attr.append((apfx+\"frame class\", len(self.class_name)+2))\n\n        loc = \" %s:%d\" % (self.filename, self.line)\n        text += loc\n        attr.append((apfx+\"frame location\", len(loc)))\n\n        return make_canvas([text], [attr], maxcol, apfx+\"frame location\")\n\n    def keypress(self, size, key):\n        return key\n\n\nclass BreakpointFrame(urwid.Widget):\n    _sizing = frozenset([urwid.Sizing.FLOW])\n\n    def __init__(self, is_current, filename, breakpoint):\n        super().__init__()\n\n        self.is_current = is_current\n        self.filename = filename\n        self.breakpoint = breakpoint\n        self.line = breakpoint.line  # Starts at 1\n        self.enabled = breakpoint.enabled\n        self.hits = breakpoint.hits\n\n    def selectable(self):\n        return True\n\n    def rows(self, size, focus=False):\n        return 1\n\n    def render(self, size, focus=False):\n        maxcol = size[0]\n        if focus:\n            apfx = \"focused \"\n        else:\n            apfx = \"\"\n\n        bp_pfx = \"\"\n        if not self.enabled:\n            apfx += \"disabled \"\n            bp_pfx += \"X\"\n        if self.is_current:\n            apfx += \"current \"\n            bp_pfx += \">>\"\n        bp_pfx = bp_pfx.ljust(3)\n\n        hits_label = \"hits\" if self.hits != 1 else \"hit\"\n        loc = f\" {self.filename}:{self.line} ({self.hits} {hits_label})\"\n        text = bp_pfx+loc\n        attr = [(apfx+\"breakpoint\", len(text))]\n\n        return make_canvas([text], [attr], maxcol, apfx+\"breakpoint\")\n\n    def keypress(self, size, key):\n        return key\n\n\nclass SearchController:\n    def __init__(self, ui):\n        self.ui = ui\n        self.highlight_line = None\n\n        self.search_box = None\n        self.last_search_string = None\n\n    def cancel_highlight(self):\n        if self.highlight_line is not None:\n            self.highlight_line.set_highlight(False)\n            self.highlight_line = None\n\n    def cancel_search(self):\n        self.cancel_highlight()\n        self.hide_search_ui()\n\n    def hide_search_ui(self):\n        self.search_box = None\n        del self.ui.lhs_col.contents[0]\n        self.ui.lhs_col.set_focus(self.ui.lhs_col.widget_list[0])\n\n    def open_search_ui(self):\n        lhs_col = self.ui.lhs_col\n\n        if self.search_box is None:\n            _, self.search_start = self.ui.source.get_focus()\n\n            self.search_box = SearchBox(self)\n            self.search_AttrMap = urwid.AttrMap(\n                    self.search_box, \"search box\")\n\n            lhs_col.item_types.insert(\n                    0, (\"flow\", None))\n            lhs_col.widget_list.insert(0, self.search_AttrMap)\n            self.ui.reset_cmdline_size()\n\n        self.ui.columns.set_focus(lhs_col)\n        lhs_col.set_focus(self.search_AttrMap)\n\n    def perform_search(self, dir, s=None, start=None, update_search_start=False):\n        self.cancel_highlight()\n\n        # self.ui.lhs_col.set_focus(self.ui.lhs_col.widget_list[1])\n\n        if s is None:\n            s = self.last_search_string\n\n            if s is None:\n                self.ui.message(\"No previous search term.\")\n                return False\n        else:\n            self.last_search_string = s\n\n        if start is None:\n            start = self.search_start\n\n        case_insensitive = s.lower() == s\n\n        if start > len(self.ui.source):\n            start = 0\n\n        i = (start+dir) % len(self.ui.source)\n\n        if i >= len(self.ui.source):\n            i = 0\n\n        while i != start:\n            sline = self.ui.source[i].text\n            if case_insensitive:\n                sline = sline.lower()\n\n            if s in sline:\n                sl = self.ui.source[i]\n                sl.set_highlight(True)\n                self.highlight_line = sl\n                self.ui.source.set_focus(i)\n\n                if update_search_start:\n                    self.search_start = i\n\n                return True\n\n            i = (i+dir) % len(self.ui.source)\n\n        return False\n\n\nclass SearchBox(urwid.Edit):\n    def __init__(self, controller):\n        urwid.Edit.__init__(self, [(\"label\", \"Search: \")], \"\")\n        self.controller = controller\n\n    def keypress(self, size, key):\n        result = urwid.Edit.keypress(self, size, key)\n        txt = self.get_edit_text()\n\n        if result is not None:\n            if key == \"esc\":\n                self.controller.cancel_search()\n                return None\n            elif key == \"enter\":\n                if txt:\n                    self.controller.hide_search_ui()\n                    self.controller.perform_search(dir=1, s=txt,\n                            update_search_start=True)\n                else:\n                    self.controller.cancel_search()\n                return None\n        else:\n            if self.controller.perform_search(dir=1, s=txt):\n                self.controller.search_AttrMap.set_attr_map({None: \"search box\"})\n            else:\n                self.controller.search_AttrMap.set_attr_map(\n                        {None: \"search not found\"})\n\n        return result\n\n# }}}\n", "metadata": {"source": "github.com/inducer/pudb", "file_path": "pudb/ui_tools.py", "language": "python", "complexity": "complex", "star_count": 3122, "quality_score": 0.78, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": true, "has_type_hints": true, "has_testing": true, "has_logging": false, "function_count": 31, "class_count": 6, "line_count": 358}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "testing_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create comprehensive test suite with fixtures and mocks\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from __future__ import annotations\n\n\n__copyright__ = \"\"\"\nCopyright (C) 2009-2017 Andreas Kloeckner\nCopyright (C) 2014-2017 Aaron Meurer\n\"\"\"\n\n__license__ = \"\"\"\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in\nall copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\nTHE SOFTWARE.\n\"\"\"\n\n\n# {{{ constants and imports\n\nimport inspect\nimport warnings\nfrom abc import ABC, abstractmethod\nfrom collections.abc import Callable, Sized\n\nimport urwid\n\nfrom pudb.lowlevel import ui_log\nfrom pudb.ui_tools import text_width\n\n\ntry:\n    import numpy\n    HAVE_NUMPY = 1\nexcept ImportError:\n    HAVE_NUMPY = 0\n\n# }}}\n\n\n# {{{ abstract base classes for containers\n\nclass PudbCollection(ABC):  # noqa: B024\n    @classmethod\n    def __subclasshook__(cls, c):\n        if cls is PudbCollection:\n            try:\n                return all([\n                    any(\"__contains__\" in b.__dict__ for b in c.__mro__),\n                    any(\"__iter__\" in b.__dict__ for b in c.__mro__),\n                ])\n            except Exception:\n                pass\n        return NotImplemented\n\n    @classmethod\n    def entries(cls, collection, label: str):\n        \"\"\"\n        :yield: ``(label, entry, id_path_ext)`` tuples for each entry in the\n        collection.\n        \"\"\"\n        assert isinstance(collection, cls)\n        try:\n            for count, entry in enumerate(collection):\n                yield None, entry, f\"[{count:d}]\"\n        except Exception as error:\n            ui_log.error(f\"Object {label!r} appears to be a collection, but does \"\n                         f\"not behave like one: {error}\")\n\n    @classmethod\n    def length(cls, collection):\n        return len(collection)\n\n\nclass PudbSequence(ABC):  # noqa: B024\n    @classmethod\n    def __subclasshook__(cls, c):\n        if cls is PudbSequence:\n            try:\n                return all([\n                    any(\"__getitem__\" in b.__dict__ for b in c.__mro__),\n                    any(\"__iter__\" in b.__dict__ for b in c.__mro__),\n                ])\n            except Exception:\n                pass\n        return NotImplemented\n\n    @classmethod\n    def entries(cls, sequence, label: str):\n        \"\"\"\n        :yield: ``(label, entry, id_path_ext)`` tuples for each entry in the\n        sequence.\n        \"\"\"\n        assert isinstance(sequence, cls)\n        try:\n            for count, entry in enumerate(sequence):\n                yield str(count), entry, f\"[{count:d}]\"\n        except Exception as error:\n            ui_log.error(f\"Object {label!r} appears to be a sequence, but does \"\n                         f\"not behave like one: {error}\")\n\n    @classmethod\n    def length(cls, sequence):\n        return len(sequence)\n\n\nclass PudbMapping(ABC):  # noqa: B024\n    @classmethod\n    def __subclasshook__(cls, c):\n        if cls is PudbMapping:\n            try:\n                return all([\n                    any(\"__getitem__\" in b.__dict__ for b in c.__mro__),\n                    any(\"__iter__\" in b.__dict__ for b in c.__mro__),\n                    any(\"keys\" in b.__dict__ for b in c.__mro__),\n                ])\n            except Exception:\n                pass\n        return NotImplemented\n\n    @classmethod\n    def _safe_key_repr(cls, key):\n        try:\n            return repr(key)\n        except Exception:\n            return f\"!! repr error on key with id: {id(key):#x} !!\"\n\n    @classmethod\n    def entries(cls, mapping, label: str):\n        \"\"\"\n        :yield: ``(label, entry, id_path_ext)`` tuples for each entry in the\n        mapping.\n        \"\"\"\n        assert isinstance(mapping, cls)\n        try:\n            for key in mapping.keys():\n                key_repr = cls._safe_key_repr(key)\n                yield (key_repr, mapping[key], f\"[{key_repr}]\")\n        except Exception as error:\n            ui_log.error(f\"Object {label!r} appears to be a mapping, but does \"\n                         f\"not behave like one: {error}\")\n\n    @classmethod\n    def length(cls, mapping):\n        return len(mapping.keys())\n\n\n# Order is important here- A mapping without keys could be viewed as a\n# sequence, and they're both collections.\nCONTAINER_CLASSES = (\n    PudbMapping,\n    PudbSequence,\n    PudbCollection,\n)\n\n# }}}\n\n\n# {{{ data\n\nclass FrameVarInfo:\n    def __init__(self):\n        self.id_path_to_iinfo = {}\n        self.watches = []\n\n    def get_inspect_info(self, id_path, read_only):\n        if read_only:\n            return self.id_path_to_iinfo.get(\n                    id_path, InspectInfo())\n        else:\n            return self.id_path_to_iinfo.setdefault(\n                    id_path, InspectInfo())\n\n\nclass InspectInfo:\n    def __init__(self):\n        # Do not globalize: cyclic import\n        from pudb.debugger import CONFIG\n\n        self.show_detail = False\n        self.display_type = CONFIG[\"stringifier\"]\n        self.highlighted = False\n        self.repeated_at_top = False\n        self.access_level = CONFIG[\"default_variables_access_level\"]\n        self.show_methods = False\n        self.wrap = CONFIG[\"wrap_variables\"]\n\n\nclass WatchExpression:\n    def __init__(self, expression):\n        self.expression = expression\n\n\nclass WatchEvalError:\n    def __str__(self):\n        return \"<error>\"\n\n# }}}\n\n\n# {{{ widget\n\nclass VariableWidget(urwid.Widget):\n    _sizing = frozenset([urwid.Sizing.FLOW])\n\n    PREFIX = \"| \"\n\n    def __init__(self, parent, var_label, value_str, id_path,\n            attr_prefix=None, watch_expr=None, iinfo=None):\n        super().__init__()\n\n        assert isinstance(id_path, str)\n        self.parent = parent\n        self.nesting_level = 0 if parent is None else parent.nesting_level + 1\n        self.prefix = self.PREFIX * self.nesting_level\n        self.var_label = var_label\n        self.value_str = value_str\n        self.id_path = id_path\n        self.attr_prefix = attr_prefix or \"var\"\n        self.watch_expr = watch_expr\n        if iinfo is None:\n            # Do not globalize: cyclic import\n            from pudb.debugger import CONFIG\n\n            self.wrap = CONFIG[\"wrap_variables\"]\n        else:\n            self.wrap = iinfo.wrap\n\n    def __str__(self):\n        return (f\"VariableWidget: {self.value_str}, level {self.nesting_level}, \"\n            f\"at {self.id_path}\")\n\n    def selectable(self):\n        return True\n\n    def _get_wrapped_lines(self, maxcol: int) -> list[str]:\n        \"\"\"\n        :param maxcol: the number of columns available to this widget\n        :return: list of string lines, including prefixes, wrapped to fit in\n            the available space\n        \"\"\"\n        maxcol -= len(self.prefix)  # self.prefix is padding\n        var_label = self.var_label or \"\"\n        value_str = self.value_str or \"\"\n        alltext = var_label + \": \" + value_str\n        # The first line is not indented\n        firstline = self.prefix + alltext[:maxcol]\n        if not alltext[maxcol:]:\n            return [firstline]\n        fulllines, rest = divmod(text_width(alltext) - maxcol, maxcol - 2)\n        restlines = [alltext[(maxcol - 2)*i + maxcol:(maxcol - 2)*i + 2*maxcol - 2]\n            for i in range(fulllines + bool(rest))]\n        return [firstline] + [self.prefix + \"  \" + i for i in restlines]\n\n    def rows(self, size: tuple[int], focus: bool = False) -> int:\n        \"\"\"\n        :param size: (maxcol,) the number of columns available to this widget\n        :param focus: True if this widget or one of its children is in focus\n        :return: The number of rows required for this widget\n        \"\"\"\n        if self.wrap:\n            return len(self._get_wrapped_lines(size[0]))\n\n        if len(self._get_wrapped_lines(size[0])) > 1:\n            return 2\n        else:\n            return 1\n\n    def render(self, size: tuple[int], focus: bool = False) -> urwid.Canvas:\n        \"\"\"\n        :param size: (maxcol,) the number of columns available to this widget\n        :param focus: True if this widget or one of its children is in focus\n        :return: A Canvas subclass instance containing the rendered content of\n            this widget\n        \"\"\"\n        from pudb.ui_tools import make_canvas\n\n        maxcol = size[0]\n        if focus:\n            apfx = \"focused \"+self.attr_prefix+\" \"\n        else:\n            apfx = self.attr_prefix+\" \"\n\n        var_label = self.var_label or \"\"\n\n        if self.wrap:\n            text = self._get_wrapped_lines(maxcol)\n\n            extralabel_full, extralabel_rem = divmod(\n                    text_width(var_label[maxcol:]), maxcol)\n            totallen = sum(text_width(i) for i in text)\n            labellen = (\n                    len(self.prefix)  # Padding of first line\n\n                    + (len(self.prefix) + 2)  # Padding of subsequent lines\n                    * (extralabel_full + bool(extralabel_rem))\n\n                    + text_width(var_label)\n\n                    + 1  # for \":\"\n                    )\n\n            attr_ = [(apfx+\"label\", labellen), (apfx+\"value\", totallen - labellen)]\n            from urwid.util import rle_subseg\n\n            fullcols, rem = divmod(totallen, maxcol)\n\n            attr = [rle_subseg(attr_, i*maxcol, (i + 1)*maxcol)\n                for i in range(fullcols + bool(rem))]\n\n            return make_canvas(text, attr, maxcol, apfx+\"value\")\n\n        lprefix = len(self.prefix)\n\n        if self.value_str is not None:\n            if self.var_label is not None:\n                if len(self._get_wrapped_lines(maxcol)) > 1:\n                    # label too long? generate separate value line\n                    text = [self.prefix + self.var_label + \":\",\n                            self.prefix+\"  \" + self.value_str]\n\n                    attr = [\n                        [(apfx+\"label\", lprefix+text_width(self.var_label))],\n                        [(apfx+\"value\", lprefix+3+text_width(self.value_str))]\n                        ]\n                else:\n                    text = [self.prefix + self.var_label + \": \" + self.value_str]\n\n                    attr = [[\n                            (apfx+\"label\", lprefix+text_width(self.var_label)+1),\n                            (apfx+\"value\", text_width(self.value_str)+1),\n                            ]]\n            else:\n                text = [self.prefix + self.value_str]\n\n                attr = [[\n                        (apfx+\"label\", len(self.prefix)),\n                        (apfx+\"value\", text_width(self.value_str)),\n                        ]]\n        else:\n            text = [self.prefix + self.var_label]\n\n            attr = [[(apfx+\"label\", lprefix + text_width(self.var_label)), ]]\n\n        # Ellipses to show text was cut off\n        # encoding = urwid.util.detected_encoding\n\n        for i in range(len(text)):\n            if text_width(text[i]) > maxcol:\n                text[i] = text[i][:maxcol-3] + \"...\"\n\n        return make_canvas(text, attr, maxcol, apfx+\"value\")\n\n    def keypress(self, size, key):\n        return key\n\n# }}}\n\n\n# {{{ stringifiers\n\nBASIC_TYPES = (\n    type(None),\n    int,\n    str,\n    float,\n    complex,\n)\n\n\n# {{{ safe types\n\ndef get_str_safe_types():\n    import types\n\n    return (\n        *[\n            getattr(types, s)\n            for s in (\"BuiltinFunctionType BuiltinMethodType  ClassType \"\n                 \"CodeType FileType FrameType FunctionType GetSetDescriptorType \"\n                 \"LambdaType MemberDescriptorType MethodType ModuleType \"\n                 \"SliceType TypeType TracebackType UnboundMethodType XRangeType\"\n            ).split() if hasattr(types, s)],\n         WatchEvalError)\n\n\nSTR_SAFE_TYPES = get_str_safe_types()\n\n# }}}\n\n\ndef default_stringifier(value):\n    if isinstance(value, BASIC_TYPES):\n        return repr(value)\n\n    if HAVE_NUMPY and isinstance(value, numpy.ndarray):\n        return f\"{type(value).__name__}({value.dtype}) {value.shape}\"\n\n    elif HAVE_NUMPY and isinstance(value, numpy.number):\n        return str(f\"{value} ({value.dtype})\")\n\n    elif isinstance(value, STR_SAFE_TYPES):\n        try:\n            return str(value)\n        except Exception:\n            message = \"string safe type stringifier failed\"\n            ui_log.exception(message)\n            return f\"!! {message} !!\"\n\n    elif hasattr(type(value), \"safely_stringify_for_pudb\"):\n        try:\n            # (E.g.) Mock objects will pretend to have this\n            # and return nonsense.\n            result = value.safely_stringify_for_pudb()\n        except Exception:\n            message = \"safely_stringify_for_pudb call failed\"\n            ui_log.exception(message)\n            result = f\"!! {message} !!\"\n\n        if isinstance(result, str):\n            return str(result)\n\n    elif isinstance(value, Sized):\n        try:\n            # Example: numpy arrays with shape == () raise on len()\n            obj_len = len(value)\n        except TypeError:\n            pass\n        else:\n            return f\"{type(value).__name__} ({obj_len})\"\n\n    return str(type(value).__name__)\n\n\ndef type_stringifier(value):\n    return str(type(value).__name__)\n\n\ndef id_stringifier(obj):\n    return f\"{id(obj):#x}\"\n\n\ndef error_stringifier(_):\n    return \"ERROR: Invalid custom stringifier file.\"\n\n\ncustom_stringifier_dict = {}\n\nSTRINGIFIERS = {\n    \"default\": default_stringifier,\n    \"type\": type_stringifier,\n    \"repr\": repr,\n    \"str\": str,\n    \"id\": id_stringifier,\n}\n\n\ndef get_stringifier(iinfo: InspectInfo) -> Callable:\n    \"\"\"\n    :return: a function that turns an object into a Unicode text object.\n    \"\"\"\n    try:\n        return STRINGIFIERS[iinfo.display_type]\n    except KeyError:\n        if \"\" == iinfo.display_type.strip():\n            return lambda _: \"ERROR: custom stringifier is not set\"\n        try:\n            if not custom_stringifier_dict:  # Only execfile once\n                from os.path import expanduser, expandvars\n                custom_stringifier_fname = expanduser(expandvars(iinfo.display_type))\n                with open(custom_stringifier_fname) as inf:\n                    exec(compile(inf.read(), custom_stringifier_fname, \"exec\"),\n                            custom_stringifier_dict,\n                            custom_stringifier_dict)\n        except FileNotFoundError:\n            ui_log.error(\n                         \"Unable to locate custom stringifier file \"\n                         f\"{iinfo.display_type!r}\"\n                         )\n            return error_stringifier\n        except Exception:\n            ui_log.exception(\"Error when importing custom stringifier\")\n            return error_stringifier\n        else:\n            if \"pudb_stringifier\" not in custom_stringifier_dict:\n                ui_log.error(f\"{iinfo.display_type} does not contain a function \"\n                             \"named pudb_stringifier at the module level.\")\n                return lambda value: str(\n                        \"ERROR: Invalid custom stringifier file: \"\n                        \"pudb_stringifier not defined.\")\n            else:\n                return (lambda value:\n                    str(custom_stringifier_dict[\"pudb_stringifier\"](value)))\n\n# }}}\n\n\n# {{{ tree walking\n\nclass ValueWalker(ABC):\n    EMPTY_LABEL = \"<empty>\"\n    CONTINUATION_LABEL = \"[...]\"\n\n    def __init__(self, frame_var_info):\n        self.frame_var_info = frame_var_info\n\n    @abstractmethod\n    def add_item(self, parent, var_label, value_str, id_path, attr_prefix=None):\n        pass\n\n    def add_continuation_item(self, parent: VariableWidget, id_path: str,\n                              count: int, length: int) -> bool:\n        \"\"\"\n        :arg length: the total length of the container. Negative if not known.\n        :returns: True if a continuation item (\"[...]\") was added, else False.\n            If a continuation item was added, no further entries in the\n            container should be added. If no continuation item was added,\n            continue adding entries from the container.\n        \"\"\"\n        cont_id_path = \"%s.cont-%d\" % (id_path, count)\n        if not self.frame_var_info.get_inspect_info(\n                cont_id_path, read_only=True).show_detail:\n            if length > 0:\n                omitted = f\"{length - count}\"\n            else:\n                omitted = \"some\"\n            self.add_item(parent, self.CONTINUATION_LABEL,\n                          f\"<{omitted} items omitted, expand to see more>\",\n                          id_path=cont_id_path)\n            return True\n        return False\n\n    def walk_container(self, parent: VariableWidget, label: str,\n                       value, id_path: str | None = None):\n        try:\n            container_cls = next(cls for cls in CONTAINER_CLASSES\n                                 if isinstance(value, cls))\n        except StopIteration:\n            # Not recognized as a container\n            return False\n\n        is_empty = True\n        for count, (entry_label, entry, id_path_ext) in enumerate(\n                container_cls.entries(value, label)):\n            is_empty = False\n            if count > 0 and count % 10 == 0:\n                try:\n                    length = container_cls.length(value)\n                except Exception:\n                    length = -1\n                if self.add_continuation_item(parent, id_path, count, length):\n                    return True\n\n            entry_id_path = f\"{id_path}{id_path_ext}\"\n            self.walk_value(parent,\n                            \"[{}]\".format(entry_label if entry_label else \"\"),\n                            entry, entry_id_path)\n\n        if is_empty:\n            self.add_item(parent, self.EMPTY_LABEL, None,\n                          id_path=f\"{id_path}{self.EMPTY_LABEL}\")\n\n        return True\n\n    def walk_attributes(self, parent, label, value, id_path, iinfo):\n        try:\n            keys = dir(value)\n        except Exception:\n            ui_log.exception(f\"Failed to look up attributes on {label}\")\n            return\n\n        for key in sorted(keys):\n            if iinfo.access_level == \"public\":\n                if key.startswith(\"_\"):\n                    continue\n            elif iinfo.access_level == \"private\":\n                if key.startswith(\"__\") and key.endswith(\"__\"):\n                    continue\n\n            try:\n                with warnings.catch_warnings():\n                    warnings.simplefilter(\"ignore\")\n                    attr_value = getattr(value, key)\n                if inspect.isroutine(attr_value) and not iinfo.show_methods:\n                    continue\n            except Exception:\n                attr_value = WatchEvalError()\n\n            self.walk_value(parent,\n                    f\".{key}\", attr_value,\n                    f\"{id_path}.{key}\")\n\n    def walk_value(self, parent, label, value, id_path=None, attr_prefix=None):\n        if id_path is None:\n            id_path = label\n\n        assert isinstance(id_path, str)\n        iinfo = self.frame_var_info.get_inspect_info(id_path, read_only=True)\n\n        try:\n            displayed_value = get_stringifier(iinfo)(value)\n        except Exception:\n            # Unfortunately, anything can happen when calling str() or\n            # repr() on a random object.\n            displayed_value = type_stringifier(value) \\\n                            + f\" (!! {iinfo.display_type} error !!)\"\n            ui_log.exception(\"stringifier failed\")\n\n        if iinfo.show_detail:\n            marker = iinfo.access_level[:3]\n            if iinfo.show_methods:\n                marker += \"+()\"\n            displayed_value += f\" [{marker}]\"\n\n        new_parent_item = self.add_item(parent, label, displayed_value,\n            id_path, attr_prefix)\n\n        if iinfo.show_detail:\n            if isinstance(value, CONTAINER_CLASSES):\n                self.walk_container(new_parent_item, label, value, id_path)\n            self.walk_attributes(new_parent_item, label, value, id_path, iinfo)\n\n\nclass BasicValueWalker(ValueWalker):\n    def __init__(self, frame_var_info):\n        ValueWalker.__init__(self, frame_var_info)\n\n        self.widget_list = []\n\n    def add_item(self, parent, var_label, value_str, id_path, attr_prefix=None):\n        iinfo = self.frame_var_info.get_inspect_info(id_path, read_only=True)\n        if iinfo.highlighted:\n            attr_prefix = \"highlighted var\"\n\n        new_item = VariableWidget(parent, var_label, value_str, id_path,\n            attr_prefix, iinfo=iinfo)\n        self.widget_list.append(new_item)\n        return new_item\n\n\nclass WatchValueWalker(ValueWalker):\n    def __init__(self, frame_var_info, widget_list, watch_expr):\n        ValueWalker.__init__(self, frame_var_info)\n        self.widget_list = widget_list\n        self.watch_expr = watch_expr\n\n    def add_item(self, parent, var_label, value_str, id_path, attr_prefix=None):\n        iinfo = self.frame_var_info.get_inspect_info(id_path, read_only=True)\n        if iinfo.highlighted:\n            attr_prefix = \"highlighted var\"\n\n        new_item = VariableWidget(parent, var_label, value_str, id_path,\n            attr_prefix, watch_expr=self.watch_expr, iinfo=iinfo)\n        self.widget_list.append(new_item)\n        return new_item\n\n\nclass TopAndMainVariableWalker(ValueWalker):\n    def __init__(self, frame_var_info):\n        ValueWalker.__init__(self, frame_var_info)\n\n        self.main_widget_list = []\n        self.top_widget_list = []\n\n        self.top_id_path_prefixes = []\n\n    @staticmethod\n    def _should_repeat_at_top(id_path, tipp) -> bool:\n        \"\"\"\n        :return: True if the id_path is a child path of tipp\n        \"\"\"\n        if id_path is None:\n            return False\n        if id_path == tipp:\n            return True\n\n        # Perhaps it's a child of the top-level path\n        before, sep, after = id_path.partition(tipp)\n        return (before == \"\"\n                and sep == tipp\n                and len(after) > 0\n                and after[0] in \".<[\")\n\n    def add_item(self, parent, var_label, value_str, id_path, attr_prefix=None):\n        iinfo = self.frame_var_info.get_inspect_info(id_path, read_only=True)\n        if iinfo.highlighted:\n            attr_prefix = \"highlighted var\"\n\n        repeated_at_top = iinfo.repeated_at_top\n        if repeated_at_top and id_path is not None:\n            self.top_id_path_prefixes.append(id_path)\n\n        for tipp in self.top_id_path_prefixes:\n            if self._should_repeat_at_top(id_path, tipp):\n                repeated_at_top = True\n\n        if repeated_at_top:\n            self.top_widget_list.append(VariableWidget(parent, var_label,\n                value_str, id_path, attr_prefix, iinfo=iinfo))\n\n        new_item = VariableWidget(parent, var_label, value_str, id_path,\n            attr_prefix, iinfo=iinfo)\n        self.main_widget_list.append(new_item)\n        return new_item\n\n# }}}\n\n\n# {{{ top level\n\nSEPARATOR = urwid.AttrMap(urwid.Text(\"\"), \"variable separator\")\n\n\ndef make_var_view(frame_var_info, locals, globals):\n    vars = list(locals.keys())\n    vars.sort(key=str.lower)\n\n    tmv_walker = TopAndMainVariableWalker(frame_var_info)\n    ret_walker = BasicValueWalker(frame_var_info)\n    watch_widget_list = []\n\n    for watch_expr in frame_var_info.watches:\n        try:\n            value = eval(watch_expr.expression, globals, locals)\n        except Exception:\n            value = WatchEvalError()\n\n        WatchValueWalker(frame_var_info, watch_widget_list, watch_expr) \\\n                .walk_value(None, watch_expr.expression, value)\n\n    if \"__return__\" in vars:\n        ret_walker.walk_value(None, \"Return\", locals[\"__return__\"],\n                attr_prefix=\"return\")\n\n    for var in vars:\n        if not (var.startswith(\"__\") and var.endswith(\"__\")):\n            tmv_walker.walk_value(None, var, locals[var])\n\n    result = tmv_walker.main_widget_list\n\n    if watch_widget_list:\n        result = ([*watch_widget_list, SEPARATOR, *result])\n\n    if tmv_walker.top_widget_list:\n        result = ([*tmv_walker.top_widget_list, SEPARATOR, *result])\n\n    if ret_walker.widget_list:\n        result = (ret_walker.widget_list + result)\n\n    return result\n\n\nclass FrameVarInfoKeeper:\n    def __init__(self):\n        self.frame_var_info = {}\n\n    def get_frame_var_info(self, read_only, ssid=None):\n        if ssid is None:\n            # self.debugger set by subclass\n            ssid = self.debugger.get_stack_situation_id()  # pylint: disable=no-member\n        if read_only:\n            return self.frame_var_info.get(ssid, FrameVarInfo())\n        else:\n            return self.frame_var_info.setdefault(ssid, FrameVarInfo())\n\n# }}}\n\n# vim: foldmethod=marker\n", "metadata": {"source": "github.com/inducer/pudb", "file_path": "pudb/var_view.py", "language": "python", "complexity": "complex", "star_count": 3122, "quality_score": 0.8099999999999999, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": true, "has_documentation": true, "has_type_hints": true, "has_testing": true, "has_logging": true, "function_count": 44, "class_count": 14, "line_count": 780}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "testing_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create comprehensive test suite with fixtures and mocks\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import sys\n\nimport pytest\n\npytest_plugins = [\"pytester\"]\noriginal = list(sys.modules)\n\n\n@pytest.mark.benchmark\ndef test_cli_startup(testdir):\n    # Measure the import time because running via subprocess does not give proper benchmark results under codspeed\n\n    import schemathesis.cli\n\n    for key in list(sys.modules):\n        # PyO3 modules can't be initialized multiple times\n        if key != \"rpds\" and key not in original:\n            del sys.modules[key]\n    del schemathesis\n", "metadata": {"source": "github.com/schemathesis/schemathesis", "file_path": "benches/cli.py", "language": "python", "complexity": "simple", "star_count": 2635, "quality_score": 0.69, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": false, "has_testing": true, "has_logging": false, "function_count": 1, "class_count": 0, "line_count": 20}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "testing_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create comprehensive test suite with fixtures and mocks\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"Benchmarks for the coverage phase.\"\"\"\n\nimport pytest\nfrom jsonschema import Draft202012Validator\n\nfrom schemathesis.generation.coverage import CoverageContext, cover_schema_iter\nfrom schemathesis.generation.hypothesis import setup\n\nsetup()\n\nCONTEXTS = [\n    CoverageContext(\n        location=\"body\", is_required=True, custom_formats={}, validator_cls=Draft202012Validator\n    ).with_positive(),\n    CoverageContext(\n        location=\"body\", is_required=True, custom_formats={}, validator_cls=Draft202012Validator\n    ).with_negative(),\n]\nCONTEXT_NAMES = [\",\".join([m.value for m in ctx.generation_modes]) for ctx in CONTEXTS]\n\nBASIC_TYPES = [\n    {\"type\": \"string\"},\n    {\"type\": \"integer\"},\n    {\"type\": \"number\"},\n    {\"type\": \"boolean\"},\n    {\"type\": \"null\"},\n]\n\n\n@pytest.mark.benchmark\n@pytest.mark.parametrize(\"ctx\", CONTEXTS, ids=CONTEXT_NAMES)\n@pytest.mark.parametrize(\"schema\", BASIC_TYPES, ids=lambda x: f\"basic-{x['type']}\")\ndef test_basic_types(benchmark, ctx, schema):\n    benchmark(lambda: list(cover_schema_iter(ctx, schema)))\n\n\nSTRING_CONSTRAINTS = [\n    {\"type\": \"string\", \"minLength\": 5},\n    {\"type\": \"string\", \"maxLength\": 10},\n    {\"type\": \"string\", \"pattern\": \"^[a-z]+$\"},\n    {\"type\": \"string\", \"format\": \"email\"},\n]\n\n\n@pytest.mark.benchmark\n@pytest.mark.parametrize(\"ctx\", CONTEXTS, ids=CONTEXT_NAMES)\n@pytest.mark.parametrize(\"schema\", STRING_CONSTRAINTS, ids=lambda x: f\"string-{list(x.keys())[1]}\")\ndef test_string_constraints(benchmark, ctx, schema):\n    benchmark(lambda: list(cover_schema_iter(ctx, schema)))\n\n\nNUMBER_CONSTRAINTS = [\n    {\"type\": \"number\", \"minimum\": 0, \"maximum\": 100},\n    {\"type\": \"integer\", \"multipleOf\": 5},\n    {\"type\": \"number\", \"exclusiveMinimum\": 0, \"exclusiveMaximum\": 1},\n]\n\n\n@pytest.mark.benchmark\n@pytest.mark.parametrize(\"ctx\", CONTEXTS, ids=CONTEXT_NAMES)\n@pytest.mark.parametrize(\"schema\", NUMBER_CONSTRAINTS, ids=lambda x: f\"number-{list(x.keys())[1]}\")\ndef test_number_constraints(benchmark, ctx, schema):\n    benchmark(lambda: list(cover_schema_iter(ctx, schema)))\n\n\nARRAY_CONSTRAINTS = [\n    {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n    {\"type\": \"array\", \"minItems\": 2, \"maxItems\": 5},\n    {\"type\": \"array\", \"uniqueItems\": True},\n]\n\n\n@pytest.mark.benchmark\n@pytest.mark.parametrize(\"ctx\", CONTEXTS, ids=CONTEXT_NAMES)\n@pytest.mark.parametrize(\"schema\", ARRAY_CONSTRAINTS, ids=lambda x: f\"array-{list(x.keys())[1]}\")\ndef test_array_constraints(benchmark, ctx, schema):\n    benchmark(lambda: list(cover_schema_iter(ctx, schema)))\n\n\nOBJECT_CONSTRAINTS = [\n    {\"type\": \"object\", \"properties\": {\"name\": {\"type\": \"string\"}, \"age\": {\"type\": \"integer\"}}},\n    {\"type\": \"object\", \"required\": [\"id\"], \"properties\": {\"id\": {\"type\": \"string\"}}},\n    {\"type\": \"object\", \"additionalProperties\": False, \"properties\": {\"key\": {\"type\": \"string\"}}},\n]\n\n\n@pytest.mark.benchmark\n@pytest.mark.parametrize(\"ctx\", CONTEXTS, ids=CONTEXT_NAMES)\n@pytest.mark.parametrize(\"schema\", OBJECT_CONSTRAINTS, ids=lambda x: f\"object-{list(x.keys())[1]}\")\ndef test_object_constraints(benchmark, ctx, schema):\n    benchmark(lambda: list(cover_schema_iter(ctx, schema)))\n\n\n# Combined schemas\nCOMBINED_SCHEMAS = [\n    {\"anyOf\": [{\"type\": \"string\"}, {\"type\": \"integer\"}]},\n    {\"oneOf\": [{\"type\": \"number\", \"multipleOf\": 5}, {\"type\": \"number\", \"multipleOf\": 3}]},\n    {\"allOf\": [{\"type\": \"string\"}, {\"minLength\": 5}]},\n]\n\n\n@pytest.mark.benchmark\n@pytest.mark.parametrize(\"ctx\", CONTEXTS, ids=CONTEXT_NAMES)\n@pytest.mark.parametrize(\"schema\", COMBINED_SCHEMAS, ids=lambda x: f\"combined-{list(x.keys())[0]}\")\ndef test_combined_schemas(benchmark, ctx, schema):\n    benchmark(lambda: list(cover_schema_iter(ctx, schema)))\n\n\nCOMPLEX_NESTED_SCHEMAS = [\n    {\n        \"type\": \"object\",\n        \"properties\": {\n            \"user\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"name\": {\"type\": \"string\"},\n                    \"age\": {\"type\": \"integer\", \"minimum\": 18},\n                    \"emails\": {\"type\": \"array\", \"items\": {\"type\": \"string\", \"format\": \"email\"}},\n                },\n                \"required\": [\"name\", \"age\"],\n            }\n        },\n    },\n    {\n        \"type\": \"array\",\n        \"items\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"id\": {\"type\": \"integer\"},\n                \"tags\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}, \"uniqueItems\": True},\n            },\n            \"required\": [\"id\"],\n        },\n        \"minItems\": 1,\n        \"maxItems\": 10,\n    },\n]\n\n\n@pytest.mark.benchmark\n@pytest.mark.parametrize(\"ctx\", CONTEXTS, ids=CONTEXT_NAMES)\n@pytest.mark.parametrize(\"schema\", COMPLEX_NESTED_SCHEMAS, ids=lambda x: f\"complex-{x['type']}\")\ndef test_complex_nested_schemas(benchmark, ctx, schema):\n    benchmark(lambda: list(cover_schema_iter(ctx, schema)))\n", "metadata": {"source": "github.com/schemathesis/schemathesis", "file_path": "benches/coverage_phase.py", "language": "python", "complexity": "medium", "star_count": 2635, "quality_score": 0.75, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": true, "has_type_hints": true, "has_testing": true, "has_logging": false, "function_count": 7, "class_count": 0, "line_count": 145}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "testing_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create comprehensive test suite with fixtures and mocks\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import pytest\n\nimport schemathesis\nfrom schemathesis.core.transforms import resolve_pointer\n\nRECURSIVE_SCHEMA = {\n    \"openapi\": \"3.0.0\",\n    \"info\": {\"title\": \"Example API\", \"description\": \"An API to test Schemathesis\", \"version\": \"1.0.0\"},\n    \"components\": {\n        \"schemas\": {\n            \"Node\": {\n                \"type\": \"object\",\n                \"required\": [\"child\"],\n                \"properties\": {\"child\": {\"$ref\": \"#/components/schemas/Node\"}},\n            }\n        }\n    },\n    \"paths\": {\n        \"/foo\": {\n            \"post\": {\n                \"summary\": \"Test\",\n                \"description\": \"\",\n                \"requestBody\": {\n                    \"required\": True,\n                    \"content\": {\"application/json\": {\"schema\": {\"$ref\": \"#/components/schemas/Node\"}}},\n                },\n                \"responses\": {\n                    \"200\": {\n                        \"description\": \"OK\",\n                        \"content\": {},\n                    }\n                },\n            }\n        }\n    },\n}\n\n\n@pytest.mark.benchmark\ndef test_inlining_during_resolution():\n    schema = schemathesis.openapi.from_dict(RECURSIVE_SCHEMA)\n    schema.resolver.resolve_all(\n        RECURSIVE_SCHEMA[\"paths\"][\"/foo\"][\"post\"][\"requestBody\"][\"content\"][\"application/json\"][\"schema\"]\n    )\n\n\n@pytest.mark.benchmark\ndef test_resolve_pointer():\n    resolve_pointer(RECURSIVE_SCHEMA, \"/paths/~1foo/post/requestBody/content/application~1json/schema\")\n", "metadata": {"source": "github.com/schemathesis/schemathesis", "file_path": "benches/references.py", "language": "python", "complexity": "simple", "star_count": 2635, "quality_score": 0.72, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": true, "has_logging": false, "function_count": 2, "class_count": 0, "line_count": 50}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "testing_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create comprehensive test suite with fixtures and mocks\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import pathlib\nimport sys\nfrom unittest.mock import patch\n\nimport pytest\nimport requests\n\nimport schemathesis\nfrom schemathesis.config import SchemathesisConfig\nfrom schemathesis.core.transforms import deepclone\nfrom schemathesis.core.transport import Response\nfrom schemathesis.engine import from_schema\n\nCURRENT_DIR = pathlib.Path(__file__).parent.absolute()\nsys.path.append(str(CURRENT_DIR.parent))\nCATALOG_DIR = CURRENT_DIR / \"data\"\n\nfrom corpus.tools import load_from_corpus, read_corpus_file  # noqa: E402\n\nCONFIG = SchemathesisConfig()\n\nCORPUS_OPENAPI_30 = read_corpus_file(\"openapi-3.0\")\nCORPUS_SWAGGER_20 = read_corpus_file(\"swagger-2.0\")\n# Small size (~2k lines in YAML)\nBBCI = load_from_corpus(\"bbci.co.uk/1.0.json\", CORPUS_OPENAPI_30)\nBBCI_SCHEMA = schemathesis.openapi.from_dict(BBCI)\nBBCI_OPERATIONS = list(BBCI_SCHEMA.get_all_operations())\n# Medium size (~8k lines in YAML)\nVMWARE = load_from_corpus(\"vmware.local/vrni/1.0.0.json\", CORPUS_OPENAPI_30)\nVMWARE_SCHEMA = schemathesis.openapi.from_dict(VMWARE)\nVMWARE_OPERATIONS = list(VMWARE_SCHEMA.get_all_operations())\n# Large size (~92k lines in YAML)\nSTRIPE = load_from_corpus(\"stripe.com/2022-11-15.json\", CORPUS_OPENAPI_30)\nSTRIPE_SCHEMA = schemathesis.openapi.from_dict(STRIPE)\n# Medium GraphQL schema (~6k lines)\nUNIVERSE = load_from_corpus(\"universe.json\", \"graphql\")\nUNIVERSE_SCHEMA = schemathesis.graphql.from_dict(UNIVERSE)\n\nAPPVEYOR = load_from_corpus(\"appveyor.com/1.0.0.json\", CORPUS_SWAGGER_20)\nEVETECH = load_from_corpus(\"evetech.net/0.8.6.json\", CORPUS_SWAGGER_20)\nOSISOFT = load_from_corpus(\"osisoft.com/1.11.1.5383.json\", CORPUS_SWAGGER_20)\nML_WEBSERVICES = load_from_corpus(\"azure.com/machinelearning-webservices/2017-01-01.json\", CORPUS_SWAGGER_20)\nAZURE_NETWORK = load_from_corpus(\"azure.com/network/2016-03-30.json\", CORPUS_SWAGGER_20)\n\nRESPONSE = Response(\n    status_code=200,\n    headers={},\n    content=b\"\",\n    request=requests.Request(method=\"GET\", url=\"http://127.0.0.1/test\").prepare(),\n    elapsed=0.1,\n    verify=False,\n)\npatch(\"schemathesis.Case.call\", return_value=RESPONSE).start()\n\n\n@pytest.mark.benchmark\n@pytest.mark.parametrize(\n    \"raw_schema, loader\",\n    [\n        (BBCI, schemathesis.openapi.from_dict),\n        (VMWARE, schemathesis.openapi.from_dict),\n        (UNIVERSE, schemathesis.graphql.from_dict),\n        (APPVEYOR, schemathesis.openapi.from_dict),\n        (EVETECH, schemathesis.openapi.from_dict),\n        (OSISOFT, schemathesis.openapi.from_dict),\n        (ML_WEBSERVICES, schemathesis.openapi.from_dict),\n        (AZURE_NETWORK, schemathesis.openapi.from_dict),\n    ],\n    ids=(\"bbci\", \"vmware\", \"universe\", \"appveyor\", \"evetech\", \"osisoft\", \"ml_webservices\", \"azure_network\"),\n)\ndef test_iter_operations(raw_schema, loader):\n    schema = loader(raw_schema, config=CONFIG)\n\n    for _ in schema.get_all_operations():\n        pass\n\n\n@pytest.mark.benchmark\n@pytest.mark.parametrize(\n    \"raw_schema, loader\",\n    [\n        (BBCI, schemathesis.openapi.from_dict),\n        (VMWARE, schemathesis.openapi.from_dict),\n        (STRIPE, schemathesis.openapi.from_dict),\n        (UNIVERSE, schemathesis.graphql.from_dict),\n        (APPVEYOR, schemathesis.openapi.from_dict),\n        (EVETECH, schemathesis.openapi.from_dict),\n        (OSISOFT, schemathesis.openapi.from_dict),\n        (ML_WEBSERVICES, schemathesis.openapi.from_dict),\n        (AZURE_NETWORK, schemathesis.openapi.from_dict),\n    ],\n    ids=(\"bbci\", \"vmware\", \"stripe\", \"universe\", \"appveyor\", \"evetech\", \"osisoft\", \"ml_webservices\", \"azure_network\"),\n)\ndef test_length(raw_schema, loader):\n    schema = loader(raw_schema, config=CONFIG)\n    _ = len(schema)\n\n\n# Schemas with pre-populated cache\nBBCI_OPERATION_ID = \"Get_Categories_\"\nBBCI_OPERATION_KEY = (\"/categories\", \"get\")\nBBCI_SCHEMA_WITH_OPERATIONS_CACHE = schemathesis.openapi.from_dict(BBCI)\nBBCI_SCHEMA_WITH_OPERATIONS_CACHE.get_operation_by_id(BBCI_OPERATION_ID)\nVMWARE_OPERATION_ID = \"listProblemEvents\"\nVMWARE_OPERATION_KEY = (\"/entities/problems\", \"get\")\nVMWARE_SCHEMA_WITH_OPERATIONS_CACHE = schemathesis.openapi.from_dict(VMWARE)\nVMWARE_SCHEMA_WITH_OPERATIONS_CACHE.get_operation_by_id(VMWARE_OPERATION_ID)\nUNIVERSE_OPERATION_KEY = (\"Query\", \"manageTickets\")\nUNIVERSE_SCHEMA_WITH_OPERATIONS_CACHE = schemathesis.graphql.from_dict(UNIVERSE)\nUNIVERSE_SCHEMA_WITH_OPERATIONS_CACHE[UNIVERSE_OPERATION_KEY[0]][UNIVERSE_OPERATION_KEY[1]]\n\n\nBBCI_SCHEMA.config.generation.update(max_examples=1)\nBBCI_SCHEMA.config.phases.update(phases=[\"examples\", \"fuzzing\"])\n\n\n@pytest.mark.benchmark\n@pytest.mark.parametrize(\n    \"raw_schema, key, loader\",\n    [\n        (BBCI, BBCI_OPERATION_KEY, schemathesis.openapi.from_dict),\n        (VMWARE, VMWARE_OPERATION_KEY, schemathesis.openapi.from_dict),\n        (UNIVERSE, UNIVERSE_OPERATION_KEY, schemathesis.graphql.from_dict),\n    ],\n    ids=(\"bbci\", \"vmware\", \"universe\"),\n)\ndef test_get_operation_single(raw_schema, key, loader):\n    schema = loader(raw_schema, config=CONFIG)\n    current = schema\n    for segment in key:\n        current = current[segment]\n\n\n@pytest.mark.benchmark\n@pytest.mark.parametrize(\n    \"schema, key\",\n    [\n        (BBCI_SCHEMA_WITH_OPERATIONS_CACHE, BBCI_OPERATION_KEY),\n        (VMWARE_SCHEMA_WITH_OPERATIONS_CACHE, VMWARE_OPERATION_KEY),\n        (UNIVERSE_SCHEMA_WITH_OPERATIONS_CACHE, UNIVERSE_OPERATION_KEY),\n    ],\n    ids=(\"bbci\", \"vmware\", \"universe\"),\n)\ndef test_get_operation_repeatedly(schema, key):\n    current = schema\n    for segment in key:\n        current = current[segment]\n\n\n@pytest.mark.benchmark\n@pytest.mark.parametrize(\n    \"raw_schema, key\",\n    [(BBCI, BBCI_OPERATION_ID), (VMWARE, VMWARE_OPERATION_ID)],\n    ids=(\"bbci\", \"vmware\"),\n)\ndef test_get_operation_by_id_single(raw_schema, key):\n    schema = schemathesis.openapi.from_dict(raw_schema, config=CONFIG)\n    _ = schema.get_operation_by_id(key)\n\n\n@pytest.mark.benchmark\n@pytest.mark.parametrize(\n    \"schema, key\",\n    [\n        (BBCI_SCHEMA_WITH_OPERATIONS_CACHE, BBCI_OPERATION_ID),\n        (VMWARE_SCHEMA_WITH_OPERATIONS_CACHE, VMWARE_OPERATION_ID),\n    ],\n    ids=(\"bbci\", \"vmware\"),\n)\ndef test_get_operation_by_id_repeatedly(schema, key):\n    _ = schema.get_operation_by_id(key)\n\n\n@pytest.mark.benchmark\n@pytest.mark.parametrize(\n    \"raw_schema, key\",\n    [\n        (BBCI, \"#/paths/~1categories/get\"),\n        (VMWARE, \"#/paths/~1entities~1problems/get\"),\n    ],\n    ids=(\"bbci\", \"vmware\"),\n)\ndef test_get_operation_by_reference_single(raw_schema, key):\n    schema = schemathesis.openapi.from_dict(raw_schema, config=CONFIG)\n    _ = schema.get_operation_by_reference(key)\n\n\n@pytest.mark.benchmark\n@pytest.mark.parametrize(\n    \"schema, key\",\n    [\n        (BBCI_SCHEMA_WITH_OPERATIONS_CACHE, \"#/paths/~1categories/get\"),\n        (VMWARE_SCHEMA_WITH_OPERATIONS_CACHE, \"#/paths/~1entities~1problems/get\"),\n    ],\n    ids=(\"bbci\", \"vmware\"),\n)\ndef test_get_operation_by_reference_repeatedly(schema, key):\n    _ = schema.get_operation_by_reference(key)\n\n\n@pytest.mark.benchmark\n@pytest.mark.parametrize(\"operations\", [BBCI_OPERATIONS, VMWARE_OPERATIONS], ids=(\"bbci\", \"vmware\"))\ndef test_as_json_schema(operations):\n    for operation in operations:\n        for parameter in operation.ok().iter_parameters():\n            _ = parameter.as_json_schema(operation)\n\n\n@pytest.mark.benchmark\ndef test_events():\n    engine = from_schema(BBCI_SCHEMA)\n    for _ in engine.execute():\n        pass\n\n\n@pytest.mark.benchmark\n@pytest.mark.parametrize(\"raw_schema\", [BBCI, VMWARE, STRIPE], ids=(\"bbci\", \"vmware\", \"stripe\"))\ndef test_rewritten_components(raw_schema):\n    schema = schemathesis.openapi.from_dict(raw_schema, config=CONFIG)\n\n    _ = schema.rewritten_components\n\n\n@pytest.mark.benchmark\n@pytest.mark.parametrize(\"raw_schema\", [BBCI, VMWARE, STRIPE], ids=(\"bbci\", \"vmware\", \"stripe\"))\ndef test_links_count(raw_schema):\n    schema = schemathesis.openapi.from_dict(raw_schema, config=CONFIG)\n\n    _ = schema.statistic.links.total\n\n\n@pytest.mark.benchmark\n@pytest.mark.parametrize(\n    \"schema\",\n    [\n        BBCI,\n        VMWARE,\n        STRIPE,\n        UNIVERSE,\n        APPVEYOR,\n        EVETECH,\n        OSISOFT,\n        ML_WEBSERVICES,\n        AZURE_NETWORK,\n    ],\n    ids=(\"bbci\", \"vmware\", \"stripe\", \"universe\", \"appveyor\", \"evetech\", \"osisoft\", \"ml_webservices\", \"azure_network\"),\n)\ndef test_deepclone(schema):\n    deepclone(schema)\n", "metadata": {"source": "github.com/schemathesis/schemathesis", "file_path": "benches/schema.py", "language": "python", "complexity": "complex", "star_count": 2635, "quality_score": 0.75, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": true, "has_logging": true, "function_count": 13, "class_count": 0, "line_count": 250}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "testing_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate module integration for Create comprehensive test suite with fixtures and mocks\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from __future__ import annotations\n\nfrom schemathesis import errors, graphql, openapi, pytest\nfrom schemathesis.auths import AuthContext, AuthProvider, auth\nfrom schemathesis.checks import CheckContext, CheckFunction, check\nfrom schemathesis.config import SchemathesisConfig as Config\nfrom schemathesis.core.transport import Response\nfrom schemathesis.core.version import SCHEMATHESIS_VERSION\nfrom schemathesis.generation import GenerationMode, stateful\nfrom schemathesis.generation.case import Case\nfrom schemathesis.generation.metrics import MetricContext, MetricFunction, metric\nfrom schemathesis.hooks import HookContext, hook\nfrom schemathesis.schemas import APIOperation, BaseSchema\nfrom schemathesis.transport import SerializationContext, serializer\n\n__version__ = SCHEMATHESIS_VERSION\n\n__all__ = [\n    \"__version__\",\n    # Core data structures\n    \"Case\",\n    \"Response\",\n    \"APIOperation\",\n    \"BaseSchema\",\n    \"Config\",\n    \"GenerationMode\",\n    \"stateful\",\n    # Public errors\n    \"errors\",\n    # Spec or usage specific namespaces\n    \"openapi\",\n    \"graphql\",\n    \"pytest\",\n    # Hooks\n    \"hook\",\n    \"HookContext\",\n    # Checks\n    \"check\",\n    \"CheckContext\",\n    \"CheckFunction\",\n    # Auth\n    \"auth\",\n    \"AuthContext\",\n    \"AuthProvider\",\n    # Targeted Property-based Testing\n    \"metric\",\n    \"MetricContext\",\n    \"MetricFunction\",\n    # Serialization\n    \"serializer\",\n    \"SerializationContext\",\n]\n", "metadata": {"source": "github.com/schemathesis/schemathesis", "file_path": "src/schemathesis/__init__.py", "language": "python", "complexity": "simple", "star_count": 2635, "quality_score": 0.69, "features": {"has_classes": false, "has_functions": false, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": false, "has_testing": false, "has_logging": false, "function_count": 0, "class_count": 0, "line_count": 53}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "testing_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create comprehensive test suite with fixtures and mocks\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from __future__ import annotations\n\nimport json\nfrom typing import TYPE_CHECKING, Any, Callable, Iterable, Iterator, Optional\n\nfrom schemathesis.config import ChecksConfig\nfrom schemathesis.core.failures import (\n    CustomFailure,\n    Failure,\n    FailureGroup,\n    MalformedJson,\n    ResponseTimeExceeded,\n    ServerError,\n)\nfrom schemathesis.core.registries import Registry\nfrom schemathesis.core.transport import Response\nfrom schemathesis.generation.overrides import Override\n\nif TYPE_CHECKING:\n    from requests.models import CaseInsensitiveDict\n\n    from schemathesis.engine.recorder import ScenarioRecorder\n    from schemathesis.generation.case import Case\n\nCheckFunction = Callable[[\"CheckContext\", \"Response\", \"Case\"], Optional[bool]]\n\n\nclass CheckContext:\n    \"\"\"Runtime context passed to validation check functions during API testing.\n\n    Provides access to configuration for currently checked endpoint.\n    \"\"\"\n\n    _override: Override | None\n    _auth: tuple[str, str] | None\n    _headers: CaseInsensitiveDict | None\n    config: ChecksConfig\n    \"\"\"Configuration settings for validation checks.\"\"\"\n    _transport_kwargs: dict[str, Any] | None\n    _recorder: ScenarioRecorder | None\n    _checks: list[CheckFunction]\n\n    __slots__ = (\"_override\", \"_auth\", \"_headers\", \"config\", \"_transport_kwargs\", \"_recorder\", \"_checks\")\n\n    def __init__(\n        self,\n        override: Override | None,\n        auth: tuple[str, str] | None,\n        headers: CaseInsensitiveDict | None,\n        config: ChecksConfig,\n        transport_kwargs: dict[str, Any] | None,\n        recorder: ScenarioRecorder | None = None,\n    ) -> None:\n        self._override = override\n        self._auth = auth\n        self._headers = headers\n        self.config = config\n        self._transport_kwargs = transport_kwargs\n        self._recorder = recorder\n        self._checks = []\n        for check in CHECKS.get_all():\n            name = check.__name__\n            if self.config.get_by_name(name=name).enabled:\n                self._checks.append(check)\n        if self.config.max_response_time.enabled:\n            self._checks.append(max_response_time)\n\n    def _find_parent(self, *, case_id: str) -> Case | None:\n        if self._recorder is not None:\n            return self._recorder.find_parent(case_id=case_id)\n        return None\n\n    def _find_related(self, *, case_id: str) -> Iterator[Case]:\n        if self._recorder is not None:\n            yield from self._recorder.find_related(case_id=case_id)\n\n    def _find_response(self, *, case_id: str) -> Response | None:\n        if self._recorder is not None:\n            return self._recorder.find_response(case_id=case_id)\n        return None\n\n    def _record_case(self, *, parent_id: str, case: Case) -> None:\n        if self._recorder is not None:\n            self._recorder.record_case(parent_id=parent_id, transition=None, case=case)\n\n    def _record_response(self, *, case_id: str, response: Response) -> None:\n        if self._recorder is not None:\n            self._recorder.record_response(case_id=case_id, response=response)\n\n\nCHECKS = Registry[CheckFunction]()\n\n\ndef load_all_checks() -> None:\n    # NOTE: Trigger registering all Open API checks\n    from schemathesis.specs.openapi.checks import status_code_conformance  # noqa: F401, F403\n\n\ndef check(func: CheckFunction) -> CheckFunction:\n    \"\"\"Register a custom validation check to run against API responses.\n\n    Args:\n        func: Function that takes `(ctx: CheckContext, response: Response, case: Case)` and raises `AssertionError` on validation failure\n\n    Example:\n        ```python\n        import schemathesis\n\n        @schemathesis.check\n        def check_cors_headers(ctx, response, case):\n            \\\"\\\"\\\"Verify CORS headers are present\\\"\\\"\\\"\n            if \"Access-Control-Allow-Origin\" not in response.headers:\n                raise AssertionError(\"Missing CORS headers\")\n        ```\n\n    \"\"\"\n    return CHECKS.register(func)\n\n\n@check\ndef not_a_server_error(ctx: CheckContext, response: Response, case: Case) -> bool | None:\n    \"\"\"A check to verify that the response is not a server-side error.\"\"\"\n    from schemathesis.specs.graphql.schemas import GraphQLSchema\n    from schemathesis.specs.graphql.validation import validate_graphql_response\n    from schemathesis.specs.openapi.utils import expand_status_codes\n\n    expected_statuses = expand_status_codes(ctx.config.not_a_server_error.expected_statuses or [])\n\n    status_code = response.status_code\n    if status_code not in expected_statuses:\n        raise ServerError(operation=case.operation.label, status_code=status_code)\n    if isinstance(case.operation.schema, GraphQLSchema):\n        try:\n            data = response.json()\n            validate_graphql_response(case, data)\n        except json.JSONDecodeError as exc:\n            raise MalformedJson.from_exception(operation=case.operation.label, exc=exc) from None\n    return None\n\n\nDEFAULT_MAX_RESPONSE_TIME = 10.0\n\n\ndef max_response_time(ctx: CheckContext, response: Response, case: Case) -> bool | None:\n    limit = ctx.config.max_response_time.limit or DEFAULT_MAX_RESPONSE_TIME\n    elapsed = response.elapsed\n    if elapsed > limit:\n        raise ResponseTimeExceeded(\n            operation=case.operation.label,\n            message=f\"Actual: {elapsed:.2f}ms\\nLimit: {limit * 1000:.2f}ms\",\n            elapsed=elapsed,\n            deadline=limit,\n        )\n    return None\n\n\ndef run_checks(\n    *,\n    case: Case,\n    response: Response,\n    ctx: CheckContext,\n    checks: Iterable[CheckFunction],\n    on_failure: Callable[[str, set[Failure], Failure], None],\n    on_success: Callable[[str, Case], None] | None = None,\n) -> set[Failure]:\n    \"\"\"Run a set of checks against a response.\"\"\"\n    collected: set[Failure] = set()\n\n    for check in checks:\n        name = check.__name__\n        try:\n            skip_check = check(ctx, response, case)\n            if not skip_check and on_success:\n                on_success(name, case)\n        except Failure as failure:\n            on_failure(name, collected, failure.with_traceback(None))\n        except AssertionError as exc:\n            custom_failure = CustomFailure(\n                operation=case.operation.label,\n                title=f\"Custom check failed: `{name}`\",\n                message=str(exc),\n                exception=exc,\n            )\n            on_failure(name, collected, custom_failure)\n        except FailureGroup as group:\n            for sub_failure in group.exceptions:\n                on_failure(name, collected, sub_failure)\n\n    return collected\n", "metadata": {"source": "github.com/schemathesis/schemathesis", "file_path": "src/schemathesis/checks.py", "language": "python", "complexity": "complex", "star_count": 2635, "quality_score": 0.84, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": true, "has_documentation": true, "has_type_hints": true, "has_testing": true, "has_logging": false, "function_count": 12, "class_count": 1, "line_count": 190}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "testing_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create comprehensive test suite with fixtures and mocks\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from __future__ import annotations\n\nfrom dataclasses import dataclass\nfrom typing import Any\n\nimport click\nfrom tomli import TOMLDecodeError\n\nfrom schemathesis.cli.commands.data import Data\nfrom schemathesis.cli.commands.run import run as run_command\nfrom schemathesis.cli.commands.run.handlers.output import display_header\nfrom schemathesis.cli.constants import EXTENSIONS_DOCUMENTATION_URL\nfrom schemathesis.cli.core import get_terminal_width\nfrom schemathesis.cli.ext.groups import CommandWithGroupedOptions, GroupedOption\nfrom schemathesis.config import ConfigError, SchemathesisConfig\nfrom schemathesis.core.errors import HookError, format_exception\nfrom schemathesis.core.version import SCHEMATHESIS_VERSION\n\nCONTEXT_SETTINGS = {\"help_option_names\": [\"-h\", \"--help\"]}\n\n\n@click.group(context_settings=CONTEXT_SETTINGS)  # type: ignore[misc]\n@click.option(  # type: ignore[misc]\n    \"--config-file\",\n    \"config_file\",\n    help=\"The path to `schemathesis.toml` file to use for configuration\",\n    metavar=\"PATH\",\n    type=str,\n)\n@click.pass_context  # type: ignore[misc]\n@click.version_option()  # type: ignore[misc]\ndef schemathesis(ctx: click.Context, config_file: str | None) -> None:\n    \"\"\"Property-based API testing for OpenAPI and GraphQL.\"\"\"\n    try:\n        if config_file is not None:\n            config = SchemathesisConfig.from_path(config_file)\n        else:\n            config = SchemathesisConfig.discover()\n    except FileNotFoundError:\n        display_header(SCHEMATHESIS_VERSION)\n        click.secho(\n            f\"❌  Failed to load configuration file from {config_file}\",\n            fg=\"red\",\n            bold=True,\n        )\n        click.echo(\"\\nThe configuration file does not exist\")\n        ctx.exit(1)\n    except (TOMLDecodeError, ConfigError) as exc:\n        display_header(SCHEMATHESIS_VERSION)\n        click.secho(\n            f\"❌  Failed to load configuration file{f' from {config_file}' if config_file else ''}\",\n            fg=\"red\",\n            bold=True,\n        )\n        if isinstance(exc, TOMLDecodeError):\n            detail = \"The configuration file content is not valid TOML\"\n        else:\n            detail = \"The loaded configuration is incorrect\"\n        click.echo(f\"\\n{detail}\\n\\n{exc}\")\n        ctx.exit(1)\n    except HookError as exc:\n        click.secho(\"Unable to load Schemathesis extension hooks\", fg=\"red\", bold=True)\n        formatted_module_name = click.style(f\"'{exc.module_path}'\", bold=True)\n        cause = exc.__cause__\n        assert isinstance(cause, Exception)\n        if isinstance(cause, ModuleNotFoundError) and cause.name == exc.module_path:\n            click.echo(\n                f\"\\nAn attempt to import the module {formatted_module_name} failed because it could not be found.\"\n            )\n            click.echo(\"\\nEnsure the module name is correctly spelled and reachable from the current directory.\")\n        else:\n            click.echo(f\"\\nAn error occurred while importing the module {formatted_module_name}. Traceback:\")\n            message = format_exception(cause, with_traceback=True, skip_frames=1)\n            click.secho(f\"\\n{message}\", fg=\"red\")\n        click.echo(f\"\\nFor more information on how to work with hooks, visit {EXTENSIONS_DOCUMENTATION_URL}\")\n        ctx.exit(1)\n    ctx.obj = Data(config=config)\n\n\n@dataclass\nclass Group:\n    name: str\n\n    __slots__ = (\"name\",)\n\n    def add_option(self, *args: Any, **kwargs: Any) -> None:\n        run.params.append(GroupedOption(args, group=self.name, **kwargs))\n\n\nrun = schemathesis.command(\n    short_help=\"Execute automated tests based on API specifications\",\n    cls=CommandWithGroupedOptions,\n    context_settings={\"terminal_width\": get_terminal_width(), **CONTEXT_SETTINGS},\n)(run_command)\n", "metadata": {"source": "github.com/schemathesis/schemathesis", "file_path": "src/schemathesis/cli/commands/__init__.py", "language": "python", "complexity": "medium", "star_count": 2635, "quality_score": 0.87, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": true, "has_documentation": true, "has_type_hints": true, "has_testing": true, "has_logging": false, "function_count": 2, "class_count": 1, "line_count": 95}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "testing_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create comprehensive test suite with fixtures and mocks\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from __future__ import annotations\n\nfrom dataclasses import dataclass, field\nfrom typing import TYPE_CHECKING, Callable, Generator\n\nfrom schemathesis.cli.commands.run.events import LoadingFinished\nfrom schemathesis.config import ProjectConfig\nfrom schemathesis.core.failures import Failure\nfrom schemathesis.core.result import Err, Ok\nfrom schemathesis.core.transforms import UNRESOLVABLE\nfrom schemathesis.core.transport import Response\nfrom schemathesis.engine import Status, events\nfrom schemathesis.engine.recorder import CaseNode, ScenarioRecorder\nfrom schemathesis.generation.case import Case\nfrom schemathesis.schemas import APIOperation\n\nif TYPE_CHECKING:\n    from schemathesis.generation.stateful.state_machine import ExtractionFailure\n\n\n@dataclass\nclass Statistic:\n    \"\"\"Running statistics about test execution.\"\"\"\n\n    failures: dict[str, dict[str, GroupedFailures]]\n    # Track first case_id where each unique failure was found\n    unique_failures_map: dict[Failure, str]\n\n    extraction_failures: set[ExtractionFailure]\n\n    tested_operations: set[str]\n\n    total_cases: int\n    cases_with_failures: int\n    cases_without_checks: int\n\n    __slots__ = (\n        \"failures\",\n        \"unique_failures_map\",\n        \"extraction_failures\",\n        \"tested_operations\",\n        \"total_cases\",\n        \"cases_with_failures\",\n        \"cases_without_checks\",\n    )\n\n    def __init__(self) -> None:\n        self.failures = {}\n        self.unique_failures_map = {}\n        self.extraction_failures = set()\n        self.tested_operations = set()\n        self.total_cases = 0\n        self.cases_with_failures = 0\n        self.cases_without_checks = 0\n\n    def on_scenario_finished(self, recorder: ScenarioRecorder) -> None:\n        \"\"\"Update statistics and store failures from a new batch of checks.\"\"\"\n        from schemathesis.generation.stateful.state_machine import ExtractionFailure\n\n        failures = self.failures.get(recorder.label, {})\n\n        self.total_cases += len(recorder.cases)\n\n        extraction_failures = set()\n\n        def collect_history(node: CaseNode, response: Response) -> list[tuple[Case, Response]]:\n            history = [(node.value, response)]\n            current = node\n            while current.parent_id is not None:\n                current_response = recorder.find_response(case_id=current.parent_id)\n                # We need a response to get there, so it should be present\n                assert current_response is not None\n                current = recorder.cases[current.parent_id]\n                history.append((current.value, current_response))\n            return history\n\n        for case_id, case in recorder.cases.items():\n            checks = recorder.checks.get(case_id, [])\n\n            if not checks:\n                self.cases_without_checks += 1\n                continue\n\n            self.tested_operations.add(case.value.operation.label)\n            has_failures = False\n            current_case_failures = []\n            last_failure_info = None\n\n            for check in checks:\n                if check.failure_info is not None:\n                    failure = check.failure_info.failure\n\n                    # Check if this is a new unique failure\n                    if failure not in self.unique_failures_map:\n                        last_failure_info = check.failure_info\n                        self.unique_failures_map[failure] = case_id\n                        current_case_failures.append(failure)\n                        has_failures = True\n                    else:\n                        # This failure was already seen - skip it\n                        continue\n\n            if current_case_failures:\n                assert last_failure_info is not None\n                failures[case_id] = GroupedFailures(\n                    case_id=case_id,\n                    code_sample=last_failure_info.code_sample,\n                    failures=current_case_failures,\n                    response=recorder.interactions[case_id].response,\n                )\n\n            if has_failures:\n                self.cases_with_failures += 1\n\n            if case.transition is None:\n                continue\n            transition = case.transition\n            parent = recorder.cases[transition.parent_id]\n            response = recorder.find_response(case_id=parent.value.id)\n            # We need a response to get there, so it should be present\n            assert response is not None\n\n            for params in transition.parameters.values():\n                for parameter, extracted in params.items():\n                    if isinstance(extracted.value, Ok) and extracted.value.ok() is UNRESOLVABLE:\n                        history = collect_history(parent, response)\n                        extraction_failures.add(\n                            ExtractionFailure(\n                                id=transition.id,\n                                case_id=case_id,\n                                source=parent.value.operation.label,\n                                target=case.value.operation.label,\n                                parameter_name=parameter,\n                                expression=extracted.definition,\n                                history=history,\n                                response=response,\n                                error=None,\n                            )\n                        )\n                    elif isinstance(extracted.value, Err):\n                        history = collect_history(parent, response)\n                        extraction_failures.add(\n                            ExtractionFailure(\n                                id=transition.id,\n                                case_id=case_id,\n                                source=parent.value.operation.label,\n                                target=case.value.operation.label,\n                                parameter_name=parameter,\n                                expression=extracted.definition,\n                                history=history,\n                                response=response,\n                                error=extracted.value.err(),\n                            )\n                        )\n\n        if failures:\n            for group in failures.values():\n                group.failures = sorted(set(group.failures))\n            self.failures[recorder.label] = failures\n\n        if extraction_failures:\n            self.extraction_failures.update(extraction_failures)\n\n\n@dataclass\nclass GroupedFailures:\n    \"\"\"Represents failures grouped by case ID.\"\"\"\n\n    case_id: str\n    code_sample: str\n    failures: list[Failure]\n    response: Response | None\n\n    __slots__ = (\"case_id\", \"code_sample\", \"failures\", \"response\")\n\n\n@dataclass\nclass ExecutionContext:\n    \"\"\"Storage for the current context of the execution.\"\"\"\n\n    config: ProjectConfig\n    find_operation_by_label: Callable[[str], APIOperation | None] | None = None\n    statistic: Statistic = field(default_factory=Statistic)\n    exit_code: int = 0\n    initialization_lines: list[str | Generator[str, None, None]] = field(default_factory=list)\n    summary_lines: list[str | Generator[str, None, None]] = field(default_factory=list)\n\n    def add_initialization_line(self, line: str | Generator[str, None, None]) -> None:\n        self.initialization_lines.append(line)\n\n    def add_summary_line(self, line: str | Generator[str, None, None]) -> None:\n        self.summary_lines.append(line)\n\n    def on_event(self, event: events.EngineEvent) -> None:\n        if isinstance(event, LoadingFinished):\n            self.find_operation_by_label = event.find_operation_by_label\n        if isinstance(event, events.ScenarioFinished):\n            self.statistic.on_scenario_finished(event.recorder)\n        elif isinstance(event, events.NonFatalError) or (\n            isinstance(event, events.PhaseFinished)\n            and event.phase.is_enabled\n            and event.status in (Status.FAILURE, Status.ERROR)\n        ):\n            self.exit_code = 1\n", "metadata": {"source": "github.com/schemathesis/schemathesis", "file_path": "src/schemathesis/cli/commands/run/context.py", "language": "python", "complexity": "complex", "star_count": 2635, "quality_score": 0.8099999999999999, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": true, "has_type_hints": true, "has_testing": true, "has_logging": false, "function_count": 6, "class_count": 3, "line_count": 205}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "testing_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create comprehensive test suite with fixtures and mocks\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from __future__ import annotations\n\nimport platform\nfrom dataclasses import dataclass\nfrom pathlib import Path\nfrom typing import Iterable\n\nfrom junit_xml import TestCase, TestSuite, to_xml_report_file\n\nfrom schemathesis.cli.commands.run.context import ExecutionContext, GroupedFailures\nfrom schemathesis.cli.commands.run.handlers.base import EventHandler\nfrom schemathesis.core.failures import format_failures\nfrom schemathesis.engine import Status, events\n\n\n@dataclass\nclass JunitXMLHandler(EventHandler):\n    path: Path\n    test_cases: dict\n\n    __slots__ = (\"path\", \"test_cases\")\n\n    def __init__(self, path: Path, test_cases: dict | None = None) -> None:\n        self.path = path\n        self.test_cases = test_cases or {}\n\n    def handle_event(self, ctx: ExecutionContext, event: events.EngineEvent) -> None:\n        if isinstance(event, events.ScenarioFinished):\n            label = event.recorder.label\n            test_case = self.get_or_create_test_case(label)\n            test_case.elapsed_sec += event.elapsed_time\n            if event.status == Status.FAILURE and label in ctx.statistic.failures:\n                add_failure(test_case, ctx.statistic.failures[label].values(), ctx)\n            elif event.status == Status.SKIP and event.skip_reason is not None:\n                test_case.add_skipped_info(output=event.skip_reason)\n        elif isinstance(event, events.NonFatalError):\n            test_case = self.get_or_create_test_case(event.label)\n            test_case.add_error_info(output=event.info.format())\n        elif isinstance(event, events.EngineFinished):\n            test_suites = [\n                TestSuite(\"schemathesis\", test_cases=list(self.test_cases.values()), hostname=platform.node())\n            ]\n            with open(self.path, \"w\", encoding=\"utf-8\") as fd:\n                to_xml_report_file(file_descriptor=fd, test_suites=test_suites, prettyprint=True, encoding=\"utf-8\")\n\n    def get_or_create_test_case(self, label: str) -> TestCase:\n        return self.test_cases.setdefault(label, TestCase(label, elapsed_sec=0.0, allow_multiple_subelements=True))\n\n\ndef add_failure(test_case: TestCase, checks: Iterable[GroupedFailures], context: ExecutionContext) -> None:\n    messages = [\n        format_failures(\n            case_id=f\"{idx}. Test Case ID: {group.case_id}\",\n            response=group.response,\n            failures=group.failures,\n            curl=group.code_sample,\n            config=context.config.output,\n        )\n        for idx, group in enumerate(checks, 1)\n    ]\n    test_case.add_failure_info(message=\"\\n\\n\".join(messages))\n", "metadata": {"source": "github.com/schemathesis/schemathesis", "file_path": "src/schemathesis/cli/commands/run/handlers/junitxml.py", "language": "python", "complexity": "simple", "star_count": 2635, "quality_score": 0.78, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": true, "has_logging": false, "function_count": 4, "class_count": 1, "line_count": 62}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "testing_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create comprehensive test suite with fixtures and mocks\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from __future__ import annotations\n\nfrom dataclasses import dataclass\nfrom typing import Any\n\nfrom schemathesis.config._diff_base import DiffBase\nfrom schemathesis.config._env import resolve\nfrom schemathesis.config._error import ConfigError\nfrom schemathesis.core.validation import is_latin_1_encodable\n\n\n@dataclass(repr=False)\nclass AuthConfig(DiffBase):\n    basic: tuple[str, str] | None\n\n    __slots__ = (\"basic\",)\n\n    def __init__(\n        self,\n        *,\n        basic: dict[str, str] | None = None,\n    ) -> None:\n        if basic is not None:\n            assert \"username\" in basic\n            username = resolve(basic[\"username\"])\n            assert \"password\" in basic\n            password = resolve(basic[\"password\"])\n            _validate_basic(username, password)\n            self.basic = (username, password)\n        else:\n            self.basic = None\n\n    def update(self, *, basic: tuple[str, str] | None = None) -> None:\n        if basic is not None:\n            _validate_basic(*basic)\n            self.basic = basic\n\n    @classmethod\n    def from_dict(cls, data: dict[str, Any]) -> AuthConfig:\n        return cls(basic=data.get(\"basic\"))\n\n    @property\n    def is_defined(self) -> bool:\n        return self.basic is not None\n\n\ndef _validate_basic(username: str, password: str) -> None:\n    if not is_latin_1_encodable(username):\n        raise ConfigError(\"Username should be latin-1 encodable.\")\n    if not is_latin_1_encodable(password):\n        raise ConfigError(\"Password should be latin-1 encodable.\")\n", "metadata": {"source": "github.com/schemathesis/schemathesis", "file_path": "src/schemathesis/config/_auth.py", "language": "python", "complexity": "simple", "star_count": 2635, "quality_score": 0.78, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": true, "has_logging": false, "function_count": 5, "class_count": 1, "line_count": 52}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "testing_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create comprehensive test suite with fixtures and mocks\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from __future__ import annotations\n\nfrom dataclasses import dataclass, fields, is_dataclass\nfrom typing import TypeVar\n\nT = TypeVar(\"T\", bound=\"DiffBase\")\n\n\n@dataclass\nclass DiffBase:\n    def __repr__(self) -> str:\n        \"\"\"Show only the fields that differ from the default.\"\"\"\n        assert is_dataclass(self)\n        default = self.__class__()\n        diffs = []\n        for field in fields(self):\n            name = field.name\n            if name.startswith(\"_\") and name not in (\"_seed\", \"_filter_set\"):\n                continue\n            current_value = getattr(self, name)\n            default_value = getattr(default, name)\n            if name == \"_seed\":\n                name = \"seed\"\n            if name == \"_filter_set\":\n                name = \"filter_set\"\n            if name == \"rate_limit\" and current_value is not None:\n                assert hasattr(self, \"_rate_limit\")\n                current_value = self._rate_limit\n            if self._has_diff(current_value, default_value):\n                diffs.append(f\"{name}={self._diff_repr(current_value, default_value)}\")\n        return f\"{self.__class__.__name__}({', '.join(diffs)})\"\n\n    def _has_diff(self, value: object, default: object) -> bool:\n        if is_dataclass(value):\n            return repr(value) != repr(default)\n        if isinstance(value, list) and isinstance(default, list):\n            if len(value) != len(default):\n                return True\n            return any(self._has_diff(v, d) for v, d in zip(value, default))\n        if isinstance(value, dict) and isinstance(default, dict):\n            if set(value.keys()) != set(default.keys()):\n                return True\n            return any(self._has_diff(value[k], default[k]) for k in value)\n        return value != default\n\n    def _diff_repr(self, value: object, default: object) -> str:\n        if is_dataclass(value):\n            # If the nested object is a dataclass, recursively show its diff.\n            return repr(value)\n        if isinstance(value, list) and isinstance(default, list):\n            diff_items = []\n            # Compare items pairwise.\n            for v, d in zip(value, default):\n                if self._has_diff(v, d):\n                    diff_items.append(self._diff_repr(v, d))\n            # Include any extra items in value.\n            if len(value) > len(default):\n                diff_items.extend(_repr(item) for item in value[len(default) :])\n            return f\"[{', '.join(_repr(item) for item in value)}]\"\n        if isinstance(value, dict) and isinstance(default, dict):\n            diff_items = []\n            for k, v in value.items():\n                d = default.get(k)\n                if self._has_diff(v, d):\n                    diff_items.append(f\"{k!r}: {self._diff_repr(v, d)}\")\n            return f\"{{{', '.join(diff_items)}}}\"\n        return repr(value)\n\n    @classmethod\n    def from_hierarchy(cls, configs: list[T]) -> T:\n        # This config will accumulate \"merged\" config options\n        output = cls()\n        for option in cls.__slots__:  # type: ignore\n            if option.startswith(\"_\"):\n                continue\n            default = getattr(output, option)\n            if is_dataclass(default):\n                # Sub-configs require merging of nested config options\n                sub_configs = [getattr(config, option) for config in configs]\n                merged = type(default).from_hierarchy(sub_configs)  # type: ignore[union-attr]\n                setattr(output, option, merged)\n            else:\n                # Primitive config options can be compared directly and do not\n                # require merging of nested options\n                for config in configs:\n                    current = getattr(config, option)\n                    if current != default:\n                        setattr(output, option, current)\n                        # As we go from the highest priority to the lowest one,\n                        # we can just stop on the first non-default value\n                        break\n        return output  # type: ignore\n\n\ndef _repr(item: object) -> str:\n    if callable(item) and hasattr(item, \"__name__\"):\n        return f\"<function {item.__name__}>\"\n\n    return repr(item)\n", "metadata": {"source": "github.com/schemathesis/schemathesis", "file_path": "src/schemathesis/config/_diff_base.py", "language": "python", "complexity": "medium", "star_count": 2635, "quality_score": 0.8099999999999999, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": true, "has_type_hints": true, "has_testing": true, "has_logging": false, "function_count": 5, "class_count": 1, "line_count": 100}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "testing_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create comprehensive test suite with fixtures and mocks\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from __future__ import annotations\n\nimport difflib\nfrom typing import TYPE_CHECKING\n\nfrom schemathesis.core.errors import SchemathesisError\n\nif TYPE_CHECKING:\n    from jsonschema import ValidationError\n\n\nclass ConfigError(SchemathesisError):\n    \"\"\"Invalid configuration.\"\"\"\n\n    @classmethod\n    def from_validation_error(cls, error: ValidationError) -> ConfigError:\n        message = error.message\n        if error.validator == \"enum\":\n            message = _format_enum_error(error)\n        elif error.validator == \"minimum\":\n            message = _format_minimum_error(error)\n        elif error.validator == \"required\":\n            message = _format_required_error(error)\n        elif error.validator == \"type\":\n            message = _format_type_error(error)\n        elif error.validator == \"additionalProperties\":\n            message = _format_additional_properties_error(error)\n        elif error.validator == \"anyOf\":\n            message = _format_anyof_error(error)\n        return cls(message)\n\n\ndef _format_minimum_error(error: ValidationError) -> str:\n    assert isinstance(error.validator_value, (int, float))\n    section = path_to_section_name(list(error.path)[:-1] if error.path else [])\n    assert error.path\n\n    prop_name = error.path[-1]\n    min_value = error.validator_value\n    actual_value = error.instance\n\n    return (\n        f\"Error in {section} section:\\n  Value too low:\\n\\n\"\n        f\"  - '{prop_name}' → Must be at least {min_value}, but got {actual_value}.\"\n    )\n\n\ndef _format_required_error(error: ValidationError) -> str:\n    assert isinstance(error.validator_value, list)\n    missing_keys = sorted(set(error.validator_value) - set(error.instance))\n\n    section = path_to_section_name(list(error.path))\n\n    details = \"\\n\".join(f\"  - '{key}'\" for key in missing_keys)\n    return f\"Error in {section} section:\\n  Missing required properties:\\n\\n{details}\\n\\n\"\n\n\ndef _format_enum_error(error: ValidationError) -> str:\n    assert isinstance(error.validator_value, list)\n    valid_values = sorted(error.validator_value)\n\n    path = list(error.path)\n\n    if path and isinstance(path[-1], int):\n        idx = path[-1]\n        prop_name = path[-2]\n        section_path = path[:-2]\n        description = f\"Item #{idx} in the '{prop_name}' array\"\n    else:\n        prop_name = path[-1] if path else \"value\"\n        section_path = path[:-1]\n        description = f\"'{prop_name}'\"\n\n    suggestion = \"\"\n    if isinstance(error.instance, str) and all(isinstance(v, str) for v in valid_values):\n        match = _find_closest_match(error.instance, valid_values)\n        if match:\n            suggestion = f\" Did you mean '{match}'?\"\n\n    section = path_to_section_name(section_path)\n    valid_values_str = \", \".join(repr(v) for v in valid_values)\n    return (\n        f\"Error in {section} section:\\n  Invalid value:\\n\\n\"\n        f\"  - {description} → '{error.instance}' is not a valid value.{suggestion}\\n\\n\"\n        f\"Valid values are: {valid_values_str}.\"\n    )\n\n\ndef _format_type_error(error: ValidationError) -> str:\n    expected = error.validator_value\n    assert isinstance(expected, (str, list))\n    section = path_to_section_name(list(error.path)[:-1] if error.path else [])\n    assert error.path\n\n    type_phrases = {\n        \"object\": \"an object\",\n        \"array\": \"an array\",\n        \"number\": \"a number\",\n        \"boolean\": \"a boolean\",\n        \"string\": \"a string\",\n        \"integer\": \"an integer\",\n        \"null\": \"null\",\n    }\n    message = f\"Error in {section} section:\\n  Type error:\\n\\n  - '{error.path[-1]}' → Must be \"\n\n    if isinstance(expected, list):\n        message += f\"one of: {' or '.join(expected)}\"\n    else:\n        message += type_phrases[expected]\n    actual = type(error.instance).__name__\n    message += f\", but got {actual}: {error.instance}\"\n    return message\n\n\ndef _format_additional_properties_error(error: ValidationError) -> str:\n    valid = list(error.schema.get(\"properties\", {}))\n    unknown = sorted(set(error.instance) - set(valid))\n    valid_list = \", \".join(f\"'{prop}'\" for prop in valid)\n    section = path_to_section_name(list(error.path))\n\n    details = []\n    for prop in unknown:\n        match = _find_closest_match(prop, valid)\n        if match:\n            details.append(f\"- '{prop}' → Did you mean '{match}'?\")\n        else:\n            details.append(f\"- '{prop}'\")\n\n    return (\n        f\"Error in {section} section:\\n  Unknown properties:\\n\\n\"\n        + \"\\n\".join(f\"  {detail}\" for detail in details)\n        + f\"\\n\\nValid properties for {section} are: {valid_list}.\"\n    )\n\n\ndef _format_anyof_error(error: ValidationError) -> str:\n    if list(error.schema_path) == [\"properties\", \"operations\", \"items\", \"anyOf\"]:\n        section = path_to_section_name(list(error.path))\n        return (\n            f\"Error in {section} section:\\n  At least one filter is required when defining [[operations]].\\n\\n\"\n            \"Please specify at least one include or exclude filter property (e.g., include-path, exclude-tag, etc.).\"\n        )\n    return error.message\n\n\ndef path_to_section_name(path: list[int | str]) -> str:\n    \"\"\"Convert a JSON path to a TOML-like section name.\"\"\"\n    if not path:\n        return \"root\"\n\n    return f\"[{'.'.join(str(p) for p in path)}]\"\n\n\ndef _find_closest_match(value: str, variants: list[str]) -> str | None:\n    matches = difflib.get_close_matches(value, variants, n=1, cutoff=0.6)\n    return matches[0] if matches else None\n", "metadata": {"source": "github.com/schemathesis/schemathesis", "file_path": "src/schemathesis/config/_error.py", "language": "python", "complexity": "complex", "star_count": 2635, "quality_score": 0.8099999999999999, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": true, "has_type_hints": true, "has_testing": true, "has_logging": false, "function_count": 9, "class_count": 1, "line_count": 157}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "testing_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create comprehensive test suite with fixtures and mocks\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from __future__ import annotations\n\nimport enum\n\nfrom schemathesis.config._env import resolve\n\n\nclass SchemathesisWarning(str, enum.Enum):\n    MISSING_AUTH = \"missing_auth\"\n    MISSING_TEST_DATA = \"missing_test_data\"\n    VALIDATION_MISMATCH = \"validation_mismatch\"\n\n    @classmethod\n    def from_str(cls, value: str) -> SchemathesisWarning:\n        return {\n            \"missing_auth\": cls.MISSING_AUTH,\n            \"missing_test_data\": cls.MISSING_TEST_DATA,\n            \"validation_mismatch\": cls.VALIDATION_MISMATCH,\n        }[value.lower()]\n\n\ndef resolve_warnings(value: bool | list[str] | None) -> bool | list[SchemathesisWarning] | None:\n    if isinstance(value, list):\n        return [SchemathesisWarning.from_str(resolve(item)) for item in value]\n    return value\n", "metadata": {"source": "github.com/schemathesis/schemathesis", "file_path": "src/schemathesis/config/_warnings.py", "language": "python", "complexity": "simple", "star_count": 2635, "quality_score": 0.78, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": true, "has_logging": false, "function_count": 2, "class_count": 1, "line_count": 26}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "testing_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create comprehensive test suite with fixtures and mocks\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from __future__ import annotations\n\nimport enum\nfrom dataclasses import dataclass\n\nSCHEMATHESIS_TEST_CASE_HEADER = \"X-Schemathesis-TestCaseId\"\nHYPOTHESIS_IN_MEMORY_DATABASE_IDENTIFIER = \":memory:\"\nINTERNAL_BUFFER_SIZE = 32 * 1024\nDEFAULT_STATEFUL_STEP_COUNT = 6\n\n\nclass NotSet: ...\n\n\nNOT_SET = NotSet()\n\n\nclass SpecificationFeature(str, enum.Enum):\n    \"\"\"Features that Schemathesis can provide for different specifications.\"\"\"\n\n    STATEFUL_TESTING = \"stateful_testing\"\n    COVERAGE = \"coverage_tests\"\n    EXAMPLES = \"example_tests\"\n\n\n@dataclass\nclass Specification:\n    kind: SpecificationKind\n    version: str\n\n    __slots__ = (\"kind\", \"version\")\n\n    @classmethod\n    def openapi(cls, version: str) -> Specification:\n        return cls(kind=SpecificationKind.OPENAPI, version=version)\n\n    @classmethod\n    def graphql(cls, version: str) -> Specification:\n        return cls(kind=SpecificationKind.GRAPHQL, version=version)\n\n    @property\n    def name(self) -> str:\n        name = {SpecificationKind.GRAPHQL: \"GraphQL\", SpecificationKind.OPENAPI: \"Open API\"}[self.kind]\n        return f\"{name} {self.version}\".strip()\n\n    def supports_feature(self, feature: SpecificationFeature) -> bool:\n        \"\"\"Check if Schemathesis supports a given feature for this specification.\"\"\"\n        if self.kind == SpecificationKind.OPENAPI:\n            return feature in {\n                SpecificationFeature.STATEFUL_TESTING,\n                SpecificationFeature.COVERAGE,\n                SpecificationFeature.EXAMPLES,\n            }\n        return False\n\n\nclass SpecificationKind(str, enum.Enum):\n    \"\"\"Specification of the given schema.\"\"\"\n\n    OPENAPI = \"openapi\"\n    GRAPHQL = \"graphql\"\n\n\ndef string_to_boolean(value: str) -> str | bool:\n    if value.lower() in (\"y\", \"yes\", \"t\", \"true\", \"on\", \"1\"):\n        return True\n    if value.lower() in (\"n\", \"no\", \"f\", \"false\", \"off\", \"0\"):\n        return False\n    return value\n", "metadata": {"source": "github.com/schemathesis/schemathesis", "file_path": "src/schemathesis/core/__init__.py", "language": "python", "complexity": "medium", "star_count": 2635, "quality_score": 0.75, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": true, "has_type_hints": true, "has_testing": true, "has_logging": false, "function_count": 5, "class_count": 4, "line_count": 70}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "testing_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create comprehensive test suite with fixtures and mocks\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from __future__ import annotations\n\nfrom functools import lru_cache\nfrom shlex import quote\nfrom typing import TYPE_CHECKING, Any\n\nfrom schemathesis.core import SCHEMATHESIS_TEST_CASE_HEADER\n\nif TYPE_CHECKING:\n    from requests.models import CaseInsensitiveDict\n\n\ndef generate(\n    *,\n    method: str,\n    url: str,\n    body: str | bytes | None,\n    verify: bool,\n    headers: dict[str, Any],\n    known_generated_headers: dict[str, Any] | None,\n) -> str:\n    \"\"\"Generate a code snippet for making HTTP requests.\"\"\"\n    _filter_headers(headers, known_generated_headers or {})\n    command = f\"curl -X {method}\"\n    for key, value in headers.items():\n        header = f\"{key}: {value}\"\n        command += f\" -H {quote(header)}\"\n    if body:\n        if isinstance(body, bytes):\n            body = body.decode(\"utf-8\", errors=\"replace\")\n        command += f\" -d {quote(body)}\"\n    if not verify:\n        command += \" --insecure\"\n    return f\"{command} {quote(url)}\"\n\n\ndef _filter_headers(headers: dict[str, Any], known_generated_headers: dict[str, Any]) -> None:\n    for key in list(headers):\n        if key not in known_generated_headers and key in get_excluded_headers():\n            del headers[key]\n\n\n@lru_cache\ndef get_excluded_headers() -> CaseInsensitiveDict:\n    from requests.structures import CaseInsensitiveDict\n    from requests.utils import default_headers\n\n    # These headers are added automatically by Schemathesis or `requests`.\n    # Do not show them in code samples to make them more readable\n\n    return CaseInsensitiveDict(\n        {\n            \"Content-Length\": None,\n            \"Transfer-Encoding\": None,\n            SCHEMATHESIS_TEST_CASE_HEADER: None,\n            **default_headers(),\n        }\n    )\n", "metadata": {"source": "github.com/schemathesis/schemathesis", "file_path": "src/schemathesis/core/curl.py", "language": "python", "complexity": "simple", "star_count": 2635, "quality_score": 0.72, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": true, "has_type_hints": true, "has_testing": true, "has_logging": false, "function_count": 3, "class_count": 0, "line_count": 59}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "testing_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create comprehensive test suite with fixtures and mocks\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n# testinfra documentation build configuration file\n#\n# This file is execfile()d with the current directory set to its\n# containing dir.\n#\n# Note that not all possible configuration values are present in this\n# autogenerated file.\n#\n# All configuration values have a default; values that are commented out\n# serve to show the default.\n\nimport datetime\nimport importlib.metadata\nimport os\nimport sys\n\nimport alabaster\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\nsys.path.insert(0, os.path.abspath(\"../..\"))\n\n# -- General configuration ------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n# needs_sphinx = '1.0'\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named 'sphinx.ext.*') or your custom\n# ones.\nextensions = [\n    \"sphinx.ext.autodoc\",\n    \"alabaster\",\n]\nautodoc_member_order = \"bysource\"\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [\"_templates\"]\n\n# The suffix of source filenames.\nsource_suffix = \".rst\"\n\n# The encoding of source files.\n# source_encoding = 'utf-8-sig'\n\n# The master toctree document.\nmaster_doc = \"index\"\n\n# General information about the project.\nproject = \"testinfra\"\ncopyright = f\"{datetime.date.today().year}, Philippe Pepiot\"\n\n# The version info for the project you're documenting, acts as replacement for\n# |version| and |release|, also used in various other places throughout the\n# built documents.\n#\n# The short X.Y version.\nversion = importlib.metadata.version(\"pytest-testinfra\")\n# The full version, including alpha/beta/rc tags.\nrelease = version\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n# language = None\n\n# There are two options for replacing |today|: either, you set today to some\n# non-false value, then it is used:\n# today = ''\n# Else, today_fmt is used as the format for a strftime call.\n# today_fmt = '%B %d, %Y'\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\nexclude_patterns: list[str] = []\n\n# The reST default role (used for this markup: `text`) to use for all\n# documents.\n# default_role = None\n\n# If true, '()' will be appended to :func: etc. cross-reference text.\n# add_function_parentheses = True\n\n# If true, the current module name will be prepended to all description\n# unit titles (such as .. function::).\nadd_module_names = False\n\n# If true, sectionauthor and moduleauthor directives will be shown in the\n# output. They are ignored by default.\n# show_authors = False\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = \"sphinx\"\n\n# A list of ignored prefixes for module index sorting.\n# modindex_common_prefix = []\n\n# If true, keep warnings as \"system message\" paragraphs in the built documents.\n# keep_warnings = False\n\n\n# -- Options for HTML output ----------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\nhtml_theme = \"alabaster\"\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\nhtml_theme_options = {\n    \"logo\": \"logo.svg\",\n    \"github_user\": \"pytest-dev\",\n    \"github_repo\": \"pytest-testinfra\",\n    \"github_button\": True,\n    \"extra_nav_links\": {\n        \"View on github\": \"https://github.com/pytest-dev/pytest-testinfra\",\n    },\n}\n\n# Add any paths that contain custom themes here, relative to this directory.\nhtml_theme_path = [alabaster.get_path()]\n\n# The name for this set of Sphinx documents.  If None, it defaults to\n# \"<project> v<release> documentation\".\n# html_title = None\n\n# A shorter title for the navigation bar.  Default is the same as html_title.\n# html_short_title = None\n\n# The name of an image file (relative to this directory) to place at the top\n# of the sidebar.\n# html_logo = None\n\n# The name of an image file (within the static path) to use as favicon of the\n# docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32\n# pixels large.\n# html_favicon = None\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named \"default.css\" will overwrite the builtin \"default.css\".\nhtml_static_path = [\"_static\"]\n\n# Add any extra paths that contain custom files (such as robots.txt or\n# .htaccess) here, relative to this directory. These files are copied\n# directly to the root of the documentation.\n# html_extra_path = []\n\n# If not '', a 'Last updated on:' timestamp is inserted at every page bottom,\n# using the given strftime format.\n# html_last_updated_fmt = '%b %d, %Y'\n\n# If true, SmartyPants will be used to convert quotes and dashes to\n# typographically correct entities.\n# html_use_smartypants = True\n\n# Custom sidebar templates, maps document names to template names.\nhtml_sidebars = {\n    \"**\": [\n        \"about.html\",\n        \"navigation.html\",\n        \"searchbox.html\",\n        \"piwik.html\",\n    ],\n}\n\n# Additional templates that should be rendered to pages, maps page names to\n# template names.\n# html_additional_pages = {}\n\n# If false, no module index is generated.\n# html_domain_indices = True\n\n# If false, no index is generated.\n# html_use_index = True\n\n# If true, the index is split into individual pages for each letter.\n# html_split_index = False\n\n# If true, links to the reST sources are added to the pages.\nhtml_show_sourcelink = False\n\n# If true, \"Created using Sphinx\" is shown in the HTML footer. Default is True.\n# html_show_sphinx = True\n\n# If true, \"(C) Copyright ...\" is shown in the HTML footer. Default is True.\n# html_show_copyright = True\n\n# If true, an OpenSearch description file will be output, and all pages will\n# contain a <link> tag referring to it.  The value of this option must be the\n# base URL from which the finished HTML is served.\n# html_use_opensearch = ''\n\n# This is the file name suffix for HTML files (e.g. \".xhtml\").\n# html_file_suffix = None\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = \"testinfradoc\"\n\n\n# -- Options for LaTeX output ---------------------------------------------\n\nlatex_elements: dict[str, tuple[str, ...]] = {\n    # The paper size ('letterpaper' or 'a4paper')\n    # 'papersize': 'letterpaper',\n    # The font size ('10pt', '11pt' or '12pt').\n    # 'pointsize': '10pt',\n    # Additional stuff for the LaTeX preamble.\n    # 'preamble': '',\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title,\n#  author, documentclass [howto, manual, or own class]).\nlatex_documents = [\n    (\"index\", \"testinfra.tex\", \"testinfra Documentation\", \"Philippe Pepiot\", \"manual\"),\n]\n\n# The name of an image file (relative to this directory) to place at the top of\n# the title page.\n# latex_logo = None\n\n# For \"manual\" documents, if this is true, then toplevel headings are parts,\n# not chapters.\n# latex_use_parts = False\n\n# If true, show page references after internal links.\n# latex_show_pagerefs = False\n\n# If true, show URL addresses after external links.\n# latex_show_urls = False\n\n# Documents to append as an appendix to all manuals.\n# latex_appendices = []\n\n# If false, no module index is generated.\n# latex_domain_indices = True\n\n\n# -- Options for manual page output ---------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [(\"index\", \"testinfra\", \"testinfra Documentation\", [\"Philippe Pepiot\"], 1)]\n\n# If true, show URL addresses after external links.\n# man_show_urls = False\n\n\n# -- Options for Texinfo output -------------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n    (\n        \"index\",\n        \"testinfra\",\n        \"testinfra Documentation\",\n        \"Philippe Pepiot\",\n        \"testinfra\",\n        \"One line description of project.\",\n        \"Miscellaneous\",\n    ),\n]\n\n# Documents to append as an appendix to all manuals.\n# texinfo_appendices = []\n\n# If false, no module index is generated.\n# texinfo_domain_indices = True\n\n# How to display URL addresses: 'footnote', 'no', or 'inline'.\n# texinfo_show_urls = 'footnote'\n\n# If true, do not generate a @detailmenu in the \"Top\" node's menu.\n# texinfo_no_detailmenu = False\n", "metadata": {"source": "github.com/pytest-dev/pytest-testinfra", "file_path": "doc/source/conf.py", "language": "python", "complexity": "complex", "star_count": 2433, "quality_score": 0.72, "features": {"has_classes": true, "has_functions": false, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": false, "has_logging": true, "function_count": 0, "class_count": 1, "line_count": 291}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "testing_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create comprehensive test suite with fixtures and mocks\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport itertools\nimport os\nimport subprocess\nimport sys\nimport threading\nimport time\nimport urllib.parse\n\nimport pytest\n\nimport testinfra\nfrom testinfra.backend import parse_hostspec\nfrom testinfra.backend.base import BaseBackend\n\nBASETESTDIR = os.path.abspath(os.path.dirname(__file__))\nBASEDIR = os.path.abspath(os.path.join(BASETESTDIR, os.pardir))\n_HAS_DOCKER = None\n\n# Use testinfra to get a handy function to run commands locally\nlocal_host = testinfra.get_host(\"local://\")\ncheck_output = local_host.check_output\n\n\ndef has_docker():\n    global _HAS_DOCKER\n    if _HAS_DOCKER is None:\n        _HAS_DOCKER = local_host.exists(\"docker\")\n    return _HAS_DOCKER\n\n\n# Generated with\n# $ echo myhostvar: bar > hostvars.yml\n# $ echo polichinelle > vault-pass.txt\n# $ ansible-vault encrypt --vault-password-file vault-pass.txt hostvars.yml\n# $ cat hostvars.yml\nANSIBLE_HOSTVARS = \"\"\"$ANSIBLE_VAULT;1.1;AES256\n39396233323131393835363638373764336364323036313434306134636633353932623363646233\n6436653132383662623364313438376662666135346266370a343934663431363661393363386633\n64656261336662623036373036363535313964313538366533313334366363613435303066316639\n3235393661656230350a326264356530326432393832353064363439393330616634633761393838\n3261\n\"\"\"\n\nDOCKER_IMAGES = [\n    \"rockylinux9\",\n    \"debian_bookworm\",\n]\n\n\ndef setup_ansible_config(tmpdir, name, host, user, port, key):\n    items = [\n        name,\n        f\"ansible_ssh_private_key_file={key}\",\n        'ansible_ssh_common_args=\"-o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no -o LogLevel=FATAL\"',\n        \"myvar=foo\",\n        f\"ansible_host={host}\",\n        f\"ansible_user={user}\",\n        f\"ansible_port={port}\",\n    ]\n    tmpdir.join(\"inventory\").write(\"[testgroup]\\n\" + \" \".join(items) + \"\\n\")\n    tmpdir.mkdir(\"host_vars\").join(name).write(ANSIBLE_HOSTVARS)\n    tmpdir.mkdir(\"group_vars\").join(\"testgroup\").write(\n        \"---\\nmyhostvar: should_be_overriden\\nmygroupvar: qux\\n\"\n    )\n    vault_password_file = tmpdir.join(\"vault-pass.txt\")\n    vault_password_file.write(\"polichinelle\\n\")\n    ansible_cfg = tmpdir.join(\"ansible.cfg\")\n    ansible_cfg.write(\n        \"[defaults]\\n\"\n        f\"vault_password_file={str(vault_password_file)}\\n\"\n        \"host_key_checking=False\\n\\n\"\n        \"[ssh_connection]\\n\"\n        \"pipelining=True\\n\"\n    )\n\n\ndef build_docker_container_fixture(image, scope):\n    @pytest.fixture(scope=scope)\n    def func(request):\n        docker_host = os.environ.get(\"DOCKER_HOST\")\n        if docker_host is not None:\n            docker_host = urllib.parse.urlparse(docker_host).hostname or \"localhost\"\n        else:\n            docker_host = \"localhost\"\n\n        cmd = [\"docker\", \"run\", \"-d\", \"-P\"]\n        if image in DOCKER_IMAGES:\n            cmd.append(\"--privileged\")\n\n        cmd.append(\"testinfra:\" + image)\n        docker_id = check_output(\" \".join(cmd))\n\n        def teardown():\n            check_output(\"docker rm -f %s\", docker_id)\n\n        request.addfinalizer(teardown)\n\n        port = check_output(\"docker port %s 22\", docker_id)\n        # IPv4 addresses seem to be reported consistently in the first line\n        # of the output. To workaround https://github.com/moby/moby/issues/42442\n        # use only the first line of the command output.\n        port = int(port.splitlines()[0].rsplit(\":\", 1)[-1])\n\n        return docker_id, docker_host, port\n\n    fname = f\"_docker_container_{image}_{scope}\"\n    mod = sys.modules[__name__]\n    setattr(mod, fname, func)\n\n\ndef initialize_container_fixtures():\n    for image, scope in itertools.product(DOCKER_IMAGES, [\"function\", \"session\"]):\n        build_docker_container_fixture(image, scope)\n\n\ninitialize_container_fixtures()\n\n\n@pytest.fixture\ndef host(request, tmpdir_factory):\n    if not has_docker():\n        pytest.skip()\n        return\n    image, kw = parse_hostspec(request.param)\n    spec = BaseBackend.parse_hostspec(image)\n\n    for marker in getattr(request.function, \"pytestmark\", []):\n        if marker.name == \"destructive\":\n            scope = \"function\"\n            break\n    else:\n        scope = \"session\"\n\n    fname = f\"_docker_container_{spec.name}_{scope}\"\n    docker_id, docker_host, port = request.getfixturevalue(fname)\n\n    if kw[\"connection\"] == \"docker\":\n        hostname = docker_id\n    elif kw[\"connection\"] in (\"ansible\", \"ssh\", \"paramiko\", \"safe-ssh\"):\n        hostname = spec.name\n        tmpdir = tmpdir_factory.mktemp(str(id(request)))\n        key = tmpdir.join(\"ssh_key\")\n        with open(os.path.join(BASETESTDIR, \"ssh_key\")) as f:\n            key.write(f.read())\n        key.chmod(384)  # octal 600\n        if kw[\"connection\"] == \"ansible\":\n            setup_ansible_config(\n                tmpdir, hostname, docker_host, spec.user or \"root\", port, str(key)\n            )\n            os.environ[\"ANSIBLE_CONFIG\"] = str(tmpdir.join(\"ansible.cfg\"))\n            # this force backend cache reloading\n            kw[\"ansible_inventory\"] = str(tmpdir.join(\"inventory\"))\n        else:\n            ssh_config = tmpdir.join(\"ssh_config\")\n            ssh_config.write(\n                f\"Host {hostname}\\n\"\n                f\"  Hostname {docker_host}\\n\"\n                f\"  Port {port}\\n\"\n                \"  UserKnownHostsFile /dev/null\\n\"\n                \"  StrictHostKeyChecking no\\n\"\n                f\"  IdentityFile {str(key)}\\n\"\n                \"  IdentitiesOnly yes\\n\"\n                \"  LogLevel FATAL\\n\"\n            )\n            kw[\"ssh_config\"] = str(ssh_config)\n\n        # Wait ssh to be up\n        service = testinfra.get_host(docker_id, connection=\"docker\").service\n\n        service_name = \"sshd\" if image == \"rockylinux9\" else \"ssh\"\n\n        while not service(service_name).is_running:\n            time.sleep(0.5)\n\n    if kw[\"connection\"] != \"ansible\":\n        hostspec = (spec.user or \"root\") + \"@\" + hostname\n    else:\n        hostspec = spec.name\n\n    b = testinfra.host.get_host(hostspec, **kw)\n    b.backend.get_hostname = lambda: image\n    return b\n\n\n@pytest.fixture\ndef docker_image(host):\n    return host.backend.get_hostname()\n\n\ndef pytest_generate_tests(metafunc):\n    if \"host\" in metafunc.fixturenames:\n        for marker in getattr(metafunc.function, \"pytestmark\", []):\n            if marker.name == \"testinfra_hosts\":\n                hosts = marker.args\n                break\n        else:\n            # Default\n            hosts = [\"docker://debian_bookworm\"]\n        metafunc.parametrize(\"host\", hosts, indirect=True, scope=\"function\")\n\n\ndef pytest_configure(config):\n    if not has_docker():\n        return\n\n    def build_image(build_failed, dockerfile, image, image_path):\n        try:\n            subprocess.check_call(\n                [\n                    \"docker\",\n                    \"build\",\n                    \"-f\",\n                    dockerfile,\n                    \"-t\",\n                    f\"testinfra:{image}\",\n                    image_path,\n                ]\n            )\n        except Exception:\n            build_failed.set()\n            raise\n\n    threads = []\n    images_path = os.path.join(BASEDIR, \"images\")\n    build_failed = threading.Event()\n    for image in os.listdir(images_path):\n        image_path = os.path.join(images_path, image)\n        dockerfile = os.path.join(image_path, \"Dockerfile\")\n        if os.path.exists(dockerfile):\n            threads.append(\n                threading.Thread(\n                    target=build_image,\n                    args=(build_failed, dockerfile, image, image_path),\n                )\n            )\n\n    for thread in threads:\n        thread.start()\n    for thread in threads:\n        thread.join()\n    if build_failed.is_set():\n        raise RuntimeError(\"One or more docker build failed\")\n\n    config.addinivalue_line(\n        \"markers\", \"testinfra_hosts(host_selector): mark test to run on selected hosts\"\n    )\n    config.addinivalue_line(\"markers\", \"destructive: mark test as destructive\")\n    config.addinivalue_line(\"markers\", \"skip_wsl: skip test on WSL, no systemd support\")\n", "metadata": {"source": "github.com/pytest-dev/pytest-testinfra", "file_path": "test/conftest.py", "language": "python", "complexity": "complex", "star_count": 2433, "quality_score": 0.8099999999999999, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": true, "has_documentation": true, "has_type_hints": true, "has_testing": true, "has_logging": true, "function_count": 11, "class_count": 0, "line_count": 261}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "testing_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create comprehensive test suite with fixtures and mocks with pytest testing framework\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport operator\nimport os\nimport tempfile\n\nimport pytest\n\nimport testinfra\nimport testinfra.backend\nfrom testinfra.backend.base import BaseBackend, HostSpec\nfrom testinfra.backend.winrm import _quote\nfrom testinfra.utils.ansible_runner import AnsibleRunner\n\nHOSTS = [\n    \"ssh://debian_bookworm\",\n    \"safe-ssh://debian_bookworm\",\n    \"docker://debian_bookworm\",\n    \"paramiko://debian_bookworm\",\n    \"ansible://debian_bookworm\",\n    \"ansible://debian_bookworm?force_ansible=True\",\n]\nUSER_HOSTS = [\n    \"ssh://user@debian_bookworm\",\n    \"safe-ssh://user@debian_bookworm\",\n    \"docker://user@debian_bookworm\",\n    \"paramiko://user@debian_bookworm\",\n    \"ansible://user@debian_bookworm\",\n    \"ansible://user@debian_bookworm?force_ansible=True\",\n]\nSUDO_HOSTS = [\n    \"ssh://user@debian_bookworm?sudo=True\",\n    \"safe-ssh://user@debian_bookworm?sudo=True\",\n    \"docker://user@debian_bookworm?sudo=True\",\n    \"paramiko://user@debian_bookworm?sudo=True\",\n    \"ansible://user@debian_bookworm?sudo=True\",\n    \"ansible://user@debian_bookworm?force_ansible=True&sudo=True\",\n]\nSUDO_USER_HOSTS = [\n    \"ssh://debian_bookworm?sudo=True&sudo_user=user\",\n    \"safe-ssh://debian_bookworm?sudo=True&sudo_user=user\",\n    \"docker://debian_bookworm?sudo=True&sudo_user=user\",\n    \"paramiko://debian_bookworm?sudo=True&sudo_user=user\",\n    \"ansible://debian_bookworm?sudo=True&sudo_user=user\",\n    \"ansible://debian_bookworm?force_ansible=True&sudo=True&sudo_user=user\",\n]\n\n\n@pytest.mark.testinfra_hosts(*(HOSTS + USER_HOSTS + SUDO_HOSTS + SUDO_USER_HOSTS))\ndef test_command(host):\n    assert host.check_output(\"true\") == \"\"\n    # test that quotting is correct\n    assert host.run(\"echo a b | grep -q %s\", \"a c\").rc == 1\n    out = host.run(\"echo out && echo err >&2 && exit 42\")\n    assert out.rc == 42\n    if host.backend.get_connection_type() == \"ansible\" and host.backend.force_ansible:\n        assert out.stdout_bytes == b\"out\"\n        assert out.stderr_bytes == b\"err\"\n    else:\n        assert out.stdout_bytes == b\"out\\n\"\n        assert out.stderr_bytes == b\"err\\n\"\n    out = host.run(\"commandthatdoesnotexists\")\n    assert out.rc == 127\n\n\n@pytest.mark.testinfra_hosts(*HOSTS)\ndef test_encoding(host):\n    # bookworm image is fr_FR@ISO-8859-15\n    cmd = host.run(\"ls -l %s\", \"/é\")\n    if host.backend.get_connection_type() == \"docker\":\n        # docker bug ?\n        assert cmd.stderr_bytes == (\n            b\"ls: impossible d'acc\\xe9der \\xe0 '/\\xef\\xbf\\xbd': \"\n            b\"Aucun fichier ou dossier de ce type\\n\"\n        )\n    elif host.backend.get_connection_type() == \"ansible\" and host.backend.force_ansible:\n        # XXX: this encoding issue comes directly from ansible\n        # not sure how to handle this...\n        assert (\n            cmd.stderr\n            == \"ls: impossible d'acc\\udce9der \\udce0 '/é': Aucun fichier ou dossier de ce type\"\n        )\n    else:\n        assert cmd.stderr_bytes == (\n            b\"ls: impossible d'acc\\xe9der \\xe0 '/\\xe9': \"\n            b\"Aucun fichier ou dossier de ce type\\n\"\n        )\n        assert cmd.stderr == (\n            \"ls: impossible d'accéder à '/é': Aucun fichier ou dossier de ce type\\n\"\n        )\n\n\n@pytest.mark.testinfra_hosts(\"ansible://debian_bookworm?force_ansible=True\")\ndef test_ansible_any_error_fatal(host):\n    os.environ[\"ANSIBLE_ANY_ERRORS_FATAL\"] = \"True\"\n    try:\n        out = host.run(\"echo out && echo err >&2 && exit 42\")\n        assert out.rc == 42\n        assert out.stdout == \"out\"\n        assert out.stderr == \"err\"\n    finally:\n        del os.environ[\"ANSIBLE_ANY_ERRORS_FATAL\"]\n\n\n@pytest.mark.testinfra_hosts(*(USER_HOSTS + SUDO_USER_HOSTS))\ndef test_user_connection(host):\n    assert host.user().name == \"user\"\n\n\n@pytest.mark.testinfra_hosts(*SUDO_HOSTS)\ndef test_sudo(host):\n    assert host.user().name == \"root\"\n\n\ndef test_ansible_get_hosts():\n    with tempfile.NamedTemporaryFile() as f:\n        f.write(\n            b\"ungrp\\n\"\n            b\"[g1]\\n\"\n            b\"debian\\n\"\n            b\"[g2]\\n\"\n            b\"rockylinux\\n\"\n            b\"[g3:children]\\n\"\n            b\"g1\\n\"\n            b\"g2\\n\"\n            b\"[g4:children]\\n\"\n            b\"g3\"\n        )\n        f.flush()\n\n        def get_hosts(spec):\n            return AnsibleRunner(f.name).get_hosts(spec)\n\n        assert get_hosts(\"all\") == [\"debian\", \"rockylinux\", \"ungrp\"]\n        assert get_hosts(\"*\") == [\"debian\", \"rockylinux\", \"ungrp\"]\n        assert get_hosts(\"g1\") == [\"debian\"]\n        assert get_hosts(\"*2\") == [\"rockylinux\"]\n        assert get_hosts(\"*ia*\") == [\"debian\"]\n        assert get_hosts(\"*3\") == [\"debian\", \"rockylinux\"]\n        assert get_hosts(\"*4\") == [\"debian\", \"rockylinux\"]\n        assert get_hosts(\"ungrouped\") == [\"ungrp\"]\n        assert get_hosts(\"un*\") == [\"ungrp\"]\n        assert get_hosts(\"nope\") == []\n\n\ndef test_ansible_get_variables():\n    with tempfile.NamedTemporaryFile() as f:\n        f.write(\n            b\"debian a=b c=d\\n\"\n            b\"rockylinux e=f\\n\"\n            b\"[all:vars]\\n\"\n            b\"a=a\\n\"\n            b\"[g]\\n\"\n            b\"debian\\n\"\n            b\"[g:vars]\\n\"\n            b\"x=z\\n\"\n        )\n        f.flush()\n\n        def get_vars(host):\n            return AnsibleRunner(f.name).get_variables(host)\n\n        groups = {\n            \"all\": [\"debian\", \"rockylinux\"],\n            \"g\": [\"debian\"],\n            \"ungrouped\": [\"rockylinux\"],\n        }\n        assert get_vars(\"debian\") == {\n            \"a\": \"b\",\n            \"c\": \"d\",\n            \"x\": \"z\",\n            \"inventory_hostname\": \"debian\",\n            \"group_names\": [\"all\", \"g\"],\n            \"groups\": groups,\n        }\n        assert get_vars(\"rockylinux\") == {\n            \"a\": \"a\",\n            \"e\": \"f\",\n            \"inventory_hostname\": \"rockylinux\",\n            \"group_names\": [\"all\", \"ungrouped\"],\n            \"groups\": groups,\n        }\n\n\n@pytest.mark.parametrize(\n    \"kwargs,inventory,expected\",\n    [\n        (\n            {},\n            b\"host ansible_connection=local ansible_become=yes ansible_become_user=u\",\n            {\n                \"NAME\": \"local\",\n                \"sudo\": True,\n                \"sudo_user\": \"u\",\n            },\n        ),\n        (\n            {},\n            b\"host\",\n            {\n                \"NAME\": \"ssh\",\n                \"host.name\": \"host\",\n            },\n        ),\n        (\n            {},\n            b\"host ansible_connection=smart\",\n            {\n                \"NAME\": \"ssh\",\n                \"host.name\": \"host\",\n            },\n        ),\n        (\n            {},\n            b\"host ansible_host=127.0.1.1 ansible_user=u ansible_ssh_private_key_file=key ansible_port=2222 ansible_become=yes ansible_become_user=u\",\n            {\n                \"NAME\": \"ssh\",\n                \"sudo\": True,\n                \"sudo_user\": \"u\",\n                \"host.name\": \"127.0.1.1\",\n                \"host.port\": \"2222\",\n                \"ssh_identity_file\": \"key\",\n            },\n        ),\n        (\n            {},\n            b\"host ansible_host=127.0.1.1 ansible_user=u ansible_private_key_file=key ansible_port=2222 ansible_become=yes ansible_become_user=u\",\n            {\n                \"NAME\": \"ssh\",\n                \"sudo\": True,\n                \"sudo_user\": \"u\",\n                \"host.name\": \"127.0.1.1\",\n                \"host.port\": \"2222\",\n                \"ssh_identity_file\": \"key\",\n            },\n        ),\n        (\n            {},\n            b'host ansible_ssh_common_args=\"-o LogLevel=FATAL\"',\n            {\n                \"NAME\": \"ssh\",\n                \"host.name\": \"host\",\n                \"ssh_extra_args\": \"-o LogLevel=FATAL\",\n            },\n        ),\n        (\n            {},\n            b'host ansible_ssh_extra_args=\"-o LogLevel=FATAL\"',\n            {\n                \"NAME\": \"ssh\",\n                \"host.name\": \"host\",\n                \"ssh_extra_args\": \"-o LogLevel=FATAL\",\n            },\n        ),\n        (\n            {},\n            b'host ansible_ssh_common_args=\"-o StrictHostKeyChecking=no\" ansible_ssh_extra_args=\"-o LogLevel=FATAL\"',\n            {\n                \"NAME\": \"ssh\",\n                \"host.name\": \"host\",\n                \"ssh_extra_args\": \"-o StrictHostKeyChecking=no -o LogLevel=FATAL\",\n            },\n        ),\n        (\n            {},\n            b\"host ansible_connection=docker\",\n            {\n                \"NAME\": \"docker\",\n                \"name\": \"host\",\n                \"user\": None,\n            },\n        ),\n        (\n            {},\n            b\"host ansible_connection=community.docker.docker\",\n            {\n                \"NAME\": \"docker\",\n                \"name\": \"host\",\n                \"user\": None,\n            },\n        ),\n        (\n            {},\n            b\"host ansible_connection=docker ansible_become=yes ansible_become_user=u ansible_user=z ansible_host=container\",\n            {\n                \"NAME\": \"docker\",\n                \"name\": \"container\",\n                \"user\": \"z\",\n                \"sudo\": True,\n                \"sudo_user\": \"u\",\n            },\n        ),\n        (\n            {\"ssh_config\": \"/ssh_config\", \"ssh_identity_file\": \"/id_ed25519\"},\n            b\"host\",\n            {\n                \"NAME\": \"ssh\",\n                \"host.name\": \"host\",\n                \"ssh_config\": \"/ssh_config\",\n                \"ssh_identity_file\": \"/id_ed25519\",\n            },\n        ),\n    ],\n)\ndef test_ansible_get_host(kwargs, inventory, expected):\n    with tempfile.NamedTemporaryFile() as f:\n        f.write(inventory + b\"\\n\")\n        f.flush()\n        backend = AnsibleRunner(f.name).get_host(\"host\", **kwargs).backend\n        for attr, value in expected.items():\n            assert operator.attrgetter(attr)(backend) == value\n\n\n@pytest.mark.parametrize(\n    \"inventory,expected\",\n    [\n        (\n            b\"host\",\n            (\n                \"ssh -o ConnectTimeout=10 -o ControlMaster=auto \"\n                \"-o ControlPersist=60s host true\"\n            ),\n        ),\n        # password passing from inventory: user is required\n        (\n            b\"host ansible_user=user ansible_ssh_pass=password\",\n            (\n                \"sshpass -p password ssh -o User=user \"\n                \"-o ConnectTimeout=10 -o ControlMaster=auto \"\n                \"-o ControlPersist=60s host true\"\n            ),\n        ),\n        # identity_file has the highest priority\n        (\n            b\"host ansible_user=user ansible_ssh_pass=password ansible_ssh_private_key_file=some_file\",\n            (\n                \"ssh -o User=user -i some_file \"\n                \"-o ConnectTimeout=10 -o ControlMaster=auto \"\n                \"-o ControlPersist=60s host true\"\n            ),\n        ),\n        # password without usr won't work\n        (\n            b\"host ansible_ssh_pass=password\",\n            (\n                \"ssh -o ConnectTimeout=10 -o ControlMaster=auto \"\n                \"-o ControlPersist=60s host true\"\n            ),\n        ),\n        # avoid interference between our ssh backend and ansible_ssh_extra_args\n        (\n            b'host ansible_ssh_extra_args=\"-o ConnectTimeout=5 -o ControlMaster=auto '\n            b'-o ControlPersist=10s\"',\n            (\n                \"ssh -o ConnectTimeout=5 -o ControlMaster=auto -o \"\n                \"ControlPersist=10s host true\"\n            ),\n        ),\n        # escape %\n        (\n            b'host ansible_ssh_extra_args=\"-o ControlPath ~/.ssh/ansible/cp/%r@%h-%p\"',\n            (\n                \"ssh -o ControlPath ~/.ssh/ansible/cp/%r@%h-%p -o ConnectTimeout=10 \"\n                \"-o ControlMaster=auto -o ControlPersist=60s host true\"\n            ),\n        ),\n    ],\n)\ndef test_ansible_ssh_command(inventory, expected):\n    with tempfile.NamedTemporaryFile() as f:\n        f.write(inventory + b\"\\n\")\n        f.flush()\n        backend = AnsibleRunner(f.name).get_host(\"host\").backend\n        cmd, cmd_args = backend._build_ssh_command(\"true\")\n        command = backend.quote(\" \".join(cmd), *cmd_args)\n        assert command == expected\n\n\ndef test_ansible_no_host():\n    with tempfile.NamedTemporaryFile() as f:\n        f.write(b\"host\\n\")\n        f.flush()\n        assert AnsibleRunner(f.name).get_hosts() == [\"host\"]\n        hosts = testinfra.get_hosts(\n            [None], connection=\"ansible\", ansible_inventory=f.name\n        )\n        assert [h.backend.get_pytest_id() for h in hosts] == [\"ansible://host\"]\n    with tempfile.NamedTemporaryFile() as f:\n        # empty or no inventory should not return any hosts except for\n        # localhost\n        nohost = (\n            \"No inventory was parsed (missing file ?), \"\n            \"only implicit localhost is available\"\n        )\n        with pytest.raises(RuntimeError) as exc:\n            assert AnsibleRunner(f.name).get_hosts() == []\n        assert str(exc.value) == nohost\n        with pytest.raises(RuntimeError) as exc:\n            assert AnsibleRunner(f.name).get_hosts(\"local*\") == []\n        assert str(exc.value) == nohost\n        assert AnsibleRunner(f.name).get_hosts(\"localhost\") == [\"localhost\"]\n\n\ndef test_ansible_config():\n    # test testinfra use ANSIBLE_CONFIG\n    tmp = tempfile.NamedTemporaryFile\n    with tmp(suffix=\".cfg\") as cfg, tmp() as inventory:\n        cfg.write(b\"[defaults]\\ninventory=\" + inventory.name.encode() + b\"\\n\")\n        cfg.flush()\n        inventory.write(b\"h\\n\")\n        inventory.flush()\n        old = os.environ.get(\"ANSIBLE_CONFIG\")\n        os.environ[\"ANSIBLE_CONFIG\"] = cfg.name\n        try:\n            assert AnsibleRunner(None).get_hosts(\"all\") == [\"h\"]\n        finally:\n            if old is not None:\n                os.environ[\"ANSIBLE_CONFIG\"] = old\n            else:\n                del os.environ[\"ANSIBLE_CONFIG\"]\n\n\n@pytest.mark.parametrize(\n    \"options,expected_cli,expected_args\",\n    [\n        ({}, \"--check\", []),\n        ({\"become\": True}, \"--become --check\", []),\n        ({\"check\": False}, \"\", []),\n        ({\"diff\": True, \"check\": False}, \"--diff\", []),\n        ({\"one_line\": True, \"check\": False}, \"--one-line\", []),\n        (\n            {\"become_method\": \"sudo\", \"check\": False},\n            \"--become-method %s\",\n            [\"sudo\"],\n        ),\n        (\n            {\"become_user\": \"root\", \"check\": False},\n            \"--become-user %s\",\n            [\"root\"],\n        ),\n        ({\"user\": \"root\", \"check\": False}, \"--user %s\", [\"root\"]),\n        (\n            {\"extra_vars\": {\"target\": \"production\", \"foo\": 42}, \"check\": False},\n            \"--extra-vars %s\",\n            ['{\"target\": \"production\", \"foo\": 42}'],\n        ),\n        ({\"verbose\": 0, \"check\": False}, \"\", []),\n        ({\"verbose\": 1, \"check\": False}, \"-v\", []),\n        ({\"verbose\": 2, \"check\": False}, \"-vv\", []),\n        ({\"verbose\": 3, \"check\": False}, \"-vvv\", []),\n        ({\"verbose\": 4, \"check\": False}, \"-vvvv\", []),\n    ],\n)\ndef test_ansible_options(options, expected_cli, expected_args):\n    runner = AnsibleRunner()\n    cli, args = runner.options_to_cli(options)\n    assert cli == expected_cli\n    assert args == expected_args\n\n\ndef test_ansible_unknown_option():\n    runner = AnsibleRunner()\n    with pytest.raises(KeyError, match=\"^'unknown'$\"):\n        runner.options_to_cli({\"unknown\": True})\n\n\ndef test_backend_importables():\n    # check that all declared backends are importable and the backend name\n    # is set correctly\n    for connection_type in testinfra.backend.BACKENDS:\n        obj = testinfra.backend.get_backend_class(connection_type)\n        assert obj.get_connection_type() == connection_type\n\n\n@pytest.mark.testinfra_hosts(\"docker://rockylinux9\", \"ssh://rockylinux9\")\ndef test_docker_encoding(host):\n    encoding = host.check_output(\n        \"python3 -c 'import locale;print(locale.getpreferredencoding())'\"\n    )\n    assert encoding == \"UTF-8\"\n    string = \"ťēꞩƫìṇḟřặ ṧꝕèȃǩ ửƫᵮ8\"\n    assert host.check_output(\"echo %s | tee /tmp/s.txt\", string) == string\n    assert host.file(\"/tmp/s.txt\").content_string.strip() == string\n\n\n@pytest.mark.parametrize(\n    \"hostspec,expected\",\n    [\n        (\"u:P@h:p\", HostSpec(\"h\", \"p\", \"u\", \"P\")),\n        (\"u@h:p\", HostSpec(\"h\", \"p\", \"u\", None)),\n        (\"u:P@h\", HostSpec(\"h\", None, \"u\", \"P\")),\n        (\"u@h\", HostSpec(\"h\", None, \"u\", None)),\n        (\"h\", HostSpec(\"h\", None, None, None)),\n        (\"pr%C3%A9nom@h\", HostSpec(\"h\", None, \"prénom\", None)),\n        (\"pr%C3%A9nom:p%40ss%3Aw0rd@h\", HostSpec(\"h\", None, \"prénom\", \"p@ss:w0rd\")),\n        # ipv6 matching\n        (\"[2001:db8:a0b:12f0::1]\", HostSpec(\"2001:db8:a0b:12f0::1\", None, None, None)),\n        (\n            \"user:password@[2001:db8:a0b:12f0::1]\",\n            HostSpec(\"2001:db8:a0b:12f0::1\", None, \"user\", \"password\"),\n        ),\n        (\n            \"user:password@[2001:4800:7819:103:be76:4eff:fe04:9229]:22\",\n            HostSpec(\n                \"2001:4800:7819:103:be76:4eff:fe04:9229\", \"22\", \"user\", \"password\"\n            ),\n        ),\n    ],\n)\ndef test_parse_hostspec(hostspec, expected):\n    assert BaseBackend.parse_hostspec(hostspec) == expected\n\n\n@pytest.mark.parametrize(\n    \"hostspec,pod,container,namespace,kubeconfig,context\",\n    [\n        (\"kubectl://pod\", \"pod\", None, None, None, None),\n        (\"kubectl://pod?namespace=n\", \"pod\", None, \"n\", None, None),\n        (\"kubectl://pod?container=c&namespace=n\", \"pod\", \"c\", \"n\", None, None),\n        (\"kubectl://pod?namespace=n&kubeconfig=k\", \"pod\", None, \"n\", \"k\", None),\n        (\"kubectl://pod?context=ctx&container=c\", \"pod\", \"c\", None, None, \"ctx\"),\n    ],\n)\ndef test_kubectl_hostspec(\n    hostspec,\n    pod,\n    container,\n    namespace,\n    kubeconfig,\n    context,\n):\n    backend = testinfra.get_host(hostspec).backend\n    assert backend.name == pod\n    assert backend.container == container\n    assert backend.namespace == namespace\n    assert backend.kubeconfig == kubeconfig\n    assert backend.context == context\n\n\n@pytest.mark.parametrize(\n    \"hostspec,pod,container,namespace,kubeconfig\",\n    [\n        (\"openshift://pod\", \"pod\", None, None, None),\n        (\"openshift://pod?namespace=n\", \"pod\", None, \"n\", None),\n        (\"openshift://pod?container=c&namespace=n\", \"pod\", \"c\", \"n\", None),\n        (\"openshift://pod?namespace=n&kubeconfig=k\", \"pod\", None, \"n\", \"k\"),\n    ],\n)\ndef test_openshift_hostspec(hostspec, pod, container, namespace, kubeconfig):\n    backend = testinfra.get_host(hostspec).backend\n    assert backend.name == pod\n    assert backend.container == container\n    assert backend.namespace == namespace\n    assert backend.kubeconfig == kubeconfig\n\n\n@pytest.mark.parametrize(\n    \"arg_string,expected\",\n    [\n        (\"C:\\\\Users\\\\vagrant\\\\This Dir\\\\salt\", '\"C:\\\\Users\\\\vagrant\\\\This Dir\\\\salt\"'),\n        (\n            \"C:\\\\Users\\\\vagrant\\\\AppData\\\\Local\\\\Temp\\\\kitchen\\\\etc\\\\salt\",\n            '\"C:\\\\Users\\\\vagrant\\\\AppData\\\\Local\\\\Temp\\\\kitchen\\\\etc\\\\salt\"',\n        ),\n    ],\n)\ndef test_winrm_quote(arg_string, expected):\n    assert _quote(arg_string) == expected\n\n\n@pytest.mark.parametrize(\n    \"hostspec,expected\",\n    [\n        (\n            \"ssh://h\",\n            \"ssh -o ConnectTimeout=10 -o ControlMaster=auto \"\n            \"-o ControlPersist=60s h true\",\n        ),\n        (\n            \"ssh://h?timeout=1\",\n            \"ssh -o ConnectTimeout=1 -o ControlMaster=auto \"\n            \"-o ControlPersist=60s h true\",\n        ),\n        (\n            \"ssh://u@h:2222\",\n            \"ssh -o User=u -o Port=2222 -o ConnectTimeout=10 \"\n            \"-o ControlMaster=auto -o ControlPersist=60s h true\",\n        ),\n        (\n            \"ssh://h:2222?ssh_config=/f\",\n            \"ssh -F /f -o Port=2222 -o ConnectTimeout=10 \"\n            \"-o ControlMaster=auto -o ControlPersist=60s h true\",\n        ),\n        (\n            \"ssh://u@h?ssh_identity_file=/id\",\n            \"ssh -o User=u -i /id -o ConnectTimeout=10 \"\n            \"-o ControlMaster=auto -o ControlPersist=60s h true\",\n        ),\n        (\n            \"ssh://h?controlpersist=1\",\n            \"ssh -o ConnectTimeout=10 \"\n            \"-o ControlMaster=auto -o ControlPersist=1s h true\",\n        ),\n        (\"ssh://h?controlpersist=0\", \"ssh -o ConnectTimeout=10 h true\"),\n    ],\n)\ndef test_ssh_hostspec(hostspec, expected):\n    backend = testinfra.get_host(hostspec).backend\n    cmd, cmd_args = backend._build_ssh_command(\"true\")\n    command = backend.quote(\" \".join(cmd), *cmd_args)\n    assert command == expected\n\n\ndef test_get_hosts():\n    # Hosts returned by get_host must be deduplicated (by name & kwargs) and in\n    # same order as asked\n    hosts = testinfra.backend.get_backends(\n        [\n            \"ssh://foo\",\n            \"ssh://a\",\n            \"ssh://a\",\n            \"ssh://a?timeout=1\",\n        ]\n    )\n    assert [(h.host.name, h.timeout) for h in hosts] == [\n        (\"foo\", 10),\n        (\"a\", 10),\n        (\"a\", 1),\n    ]\n\n\n@pytest.mark.testinfra_hosts(*HOSTS)\ndef test_command_deadlock(host):\n    # Test for deadlock when exceeding Paramiko transport buffer (2MB)\n    # https://docs.paramiko.org/en/latest/api/channel.html#paramiko.channel.Channel.recv_exit_status\n    size = 3 * 1024 * 1024\n    output = host.check_output(f\"python3 -c 'print(\\\"a\\\" * {size})'\")\n    assert len(output) == size\n", "metadata": {"source": "github.com/pytest-dev/pytest-testinfra", "file_path": "test/test_backends.py", "language": "python", "complexity": "complex", "star_count": 2433, "quality_score": 0.84, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": true, "has_documentation": false, "has_type_hints": true, "has_testing": true, "has_logging": true, "function_count": 24, "class_count": 0, "line_count": 649}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "testing_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create comprehensive test suite with fixtures and mocks with pytest testing framework\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\npytest_plugins = [\"pytester\"]\n\n\ndef test_nagios_notest(testdir, request):\n    params = [\"--nagios\", \"-q\", \"--tb=no\"]\n    if not request.config.pluginmanager.hasplugin(\"pytest11.testinfra\"):\n        params.extend([\"-p\", \"testinfra.plugin\"])\n    result = testdir.runpytest(*params)\n    assert result.ret == 0\n    lines = result.stdout.str().splitlines()\n    assert lines[0].startswith(\"TESTINFRA OK - 0 passed, 0 failed, 0 skipped\")\n\n\ndef test_nagios_ok(testdir, request):\n    testdir.makepyfile(\"def test_ok(): pass\")\n    params = [\"--nagios\", \"-q\", \"--tb=no\"]\n    if not request.config.pluginmanager.hasplugin(\"pytest11.testinfra\"):\n        params.extend([\"-p\", \"testinfra.plugin\"])\n    result = testdir.runpytest(*params)\n    assert result.ret == 0\n    lines = result.stdout.str().splitlines()\n    assert lines[0].startswith(\"TESTINFRA OK - 1 passed, 0 failed, 0 skipped\")\n    assert lines[1][0] == \".\"\n\n\ndef test_nagios_fail(testdir, request):\n    testdir.makepyfile(\"def test_ok(): pass\\ndef test_fail(): assert False\")\n    params = [\"--nagios\", \"-q\", \"--tb=no\"]\n    if not request.config.pluginmanager.hasplugin(\"pytest11.testinfra\"):\n        params.extend([\"-p\", \"testinfra.plugin\"])\n    result = testdir.runpytest(*params)\n    assert result.ret == 2\n    lines = result.stdout.str().splitlines()\n    assert lines[0].startswith(\"TESTINFRA CRITICAL - 1 passed, 1 failed, 0 skipped\")\n    assert lines[1][:2] == \".F\"\n", "metadata": {"source": "github.com/pytest-dev/pytest-testinfra", "file_path": "test/test_invocation.py", "language": "python", "complexity": "simple", "star_count": 2433, "quality_score": 0.72, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": false, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": true, "has_logging": false, "function_count": 5, "class_count": 0, "line_count": 48}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "testing_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create comprehensive test suite with fixtures and mocks with pytest testing framework\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport datetime\nimport os\nimport re\nimport tempfile\nimport textwrap\nimport time\nfrom ipaddress import IPv4Address, IPv6Address, ip_address\n\nimport pytest\n\nfrom testinfra.modules.socket import parse_socketspec\nfrom testinfra.utils.ansible_runner import AnsibleRunner\n\nall_images = pytest.mark.testinfra_hosts(\n    *[\n        f\"docker://{image}\"\n        for image in (\n            \"rockylinux9\",\n            \"debian_bookworm\",\n        )\n    ]\n)\n\n\n@all_images\ndef test_package(host, docker_image):\n    assert not host.package(\"zsh\").is_installed\n    ssh = host.package(\"openssh-server\")\n    version = {\n        \"rockylinux9\": \"8.\",\n        \"debian_bookworm\": \"1:9.2\",\n    }[docker_image]\n    assert ssh.is_installed\n    assert ssh.version.startswith(version)\n    release = {\n        \"rockylinux9\": \".el9\",\n        \"debian_bookworm\": None,\n    }[docker_image]\n    if release is None:\n        with pytest.raises(NotImplementedError):\n            ssh.release  # noqa: B018\n    else:\n        assert release in ssh.release\n\n\ndef test_held_package(host):\n    python = host.package(\"python3\")\n    assert python.is_installed\n    assert python.version.startswith(\"3.11.\")\n\n\n@pytest.mark.destructive\n@pytest.mark.testinfra_hosts(\"docker://rockylinux9\")\ndef test_rpmdb_corrupted(host):\n    host.check_output(\"dd if=/dev/zero of=/var/lib/rpm/rpmdb.sqlite bs=1024 count=1\")\n    with pytest.raises(RuntimeError) as excinfo:\n        host.package(\"zsh\").is_installed  # noqa: B018\n    assert (\n        \"Could not check if RPM package 'zsh' is installed. error: sqlite failure:\"\n    ) in str(excinfo.value)\n\n\n@pytest.mark.testinfra_hosts(\"docker://rockylinux9\")\ndef test_non_default_package_tool(host):\n    # Make non-default pkg tool binary present\n    host.run(\"install -m a+rx /bin/true /usr/bin/dpkg-query\")\n    assert host.package(\"openssh\").is_installed\n\n\n@pytest.mark.destructive\ndef test_uninstalled_package_version(host):\n    with pytest.raises(AssertionError) as excinfo:\n        host.package(\"zsh\").version  # noqa: B018\n    assert (\n        \"The package zsh is not installed, dpkg-query output: unknown ok not-installed\"\n        in str(excinfo.value)\n    )\n    assert host.package(\"sudo\").is_installed\n    host.check_output(\"apt-get -y remove sudo\")\n    assert not host.package(\"sudo\").is_installed\n    with pytest.raises(AssertionError) as excinfo:\n        host.package(\"sudo\").version  # noqa: B018\n    assert (\n        \"The package sudo is not installed, dpkg-query output: \"\n        \"deinstall ok config-files 1.9.\"\n    ) in str(excinfo.value)\n\n\n@all_images\ndef test_systeminfo(host, docker_image):\n    assert host.system_info.type == \"linux\"\n\n    release, distribution, codename, arch = {\n        \"rockylinux9\": (r\"^9.\\d+$\", \"rocky\", None, \"x86_64\"),\n        \"debian_bookworm\": (r\"^12\", \"debian\", \"bookworm\", \"x86_64\"),\n    }[docker_image]\n\n    assert host.system_info.distribution == distribution\n    assert host.system_info.codename == codename\n    assert re.match(release, host.system_info.release)\n\n\n@all_images\ndef test_ssh_service(host, docker_image):\n    name = \"sshd\" if docker_image == \"rockylinux9\" else \"ssh\"\n    ssh = host.service(name)\n    # wait at max 10 seconds for ssh is running\n    for _ in range(10):\n        if ssh.is_running:\n            break\n        time.sleep(1)\n    else:\n        raise AssertionError(\"ssh is not running\")\n\n    assert ssh.is_enabled\n\n\ndef test_service_systemd_mask(host):\n    ssh = host.service(\"ssh\")\n    assert not ssh.is_masked\n    host.run(\"systemctl mask ssh\")\n    assert ssh.is_masked\n    host.run(\"systemctl unmask ssh\")\n    assert not ssh.is_masked\n\n\ndef test_salt(host):\n    ssh_version = host.salt(\"pkg.version\", \"openssh-server\", local=True)\n    assert ssh_version.startswith(\"1:9.2\")\n\n\ndef test_puppet_resource(host):\n    resource = host.puppet_resource(\"package\", \"openssh-server\")\n    assert resource[\"openssh-server\"][\"ensure\"].startswith(\"1:9.2\")\n\n\ndef test_facter(host):\n    assert host.facter()[\"os\"][\"distro\"][\"codename\"] == \"bookworm\"\n    assert host.facter(\"virtual\") in (\n        {\"virtual\": \"docker\"},\n        {\"virtual\": \"hyperv\"},  # GitHub action uses hyperv\n        {\"virtual\": \"physical\"},  # I've this on my machine...\n    )\n\n\ndef test_sysctl(host):\n    assert host.sysctl(\"kernel.hostname\") == host.check_output(\"hostname\")\n    assert isinstance(host.sysctl(\"kernel.panic\"), int)\n\n\ndef test_parse_socketspec():\n    assert parse_socketspec(\"tcp://22\") == (\"tcp\", None, 22)\n    assert parse_socketspec(\"tcp://:::22\") == (\"tcp\", \"::\", 22)\n    assert parse_socketspec(\"udp://0.0.0.0:22\") == (\"udp\", \"0.0.0.0\", 22)\n    assert parse_socketspec(\"unix://can:be.any/thing:22\") == (\n        \"unix\",\n        \"can:be.any/thing:22\",\n        None,\n    )\n\n\ndef test_socket(host):\n    listening = host.socket.get_listening_sockets()\n    for spec in (\n        \"tcp://0.0.0.0:22\",\n        \"tcp://:::22\",\n        \"unix:///run/systemd/private\",\n    ):\n        assert spec in listening\n    for spec in (\n        \"tcp://22\",\n        \"tcp://0.0.0.0:22\",\n        \"tcp://127.0.0.1:22\",\n        \"tcp://:::22\",\n        \"tcp://::1:22\",\n        \"unix:///run/systemd/private\",\n    ):\n        socket = host.socket(spec)\n        assert socket.is_listening\n\n    assert not host.socket(\"tcp://4242\").is_listening\n\n    if host.backend.get_connection_type() != \"docker\":\n        # FIXME\n        for spec in (\n            \"tcp://22\",\n            \"tcp://0.0.0.0:22\",\n        ):\n            assert len(host.socket(spec).clients) >= 1\n\n\n@all_images\ndef test_process(host, docker_image):\n    init = host.process.get(pid=1)\n    assert init.ppid == 0\n    assert init.euid == 0\n    assert init.user == \"root\"\n\n    args, comm = {\n        \"rockylinux9\": (\"/usr/sbin/init\", \"systemd\"),\n        \"debian_bookworm\": (\"/sbin/init\", \"systemd\"),\n    }[docker_image]\n    assert init.args == args\n    assert init.comm == comm\n\n\ndef test_user(host):\n    user = host.user(\"sshd\")\n    assert user.exists\n    assert user.name == \"sshd\"\n    assert user.uid == 100\n    assert user.gid == 65534\n    assert user.group == \"nogroup\"\n    assert user.gids == [65534]\n    assert user.groups == [\"nogroup\"]\n    assert user.shell == \"/usr/sbin/nologin\"\n    assert user.home == \"/run/sshd\"\n    assert user.password == \"!\"\n\n\ndef test_user_get_all_users(host):\n    user_list = host.user(\"root\").get_all_users\n    assert \"root\" in user_list\n    assert \"man\" in user_list\n    assert \"nobody\" in user_list\n\n\ndef test_user_password_days(host):\n    assert host.user(\"root\").password_max_days == 99999\n    assert host.user(\"root\").password_min_days == 0\n    assert host.user(\"user\").password_max_days == 90\n    assert host.user(\"user\").password_min_days == 7\n\n\ndef test_user_user(host):\n    user = host.user(\"user\")\n    assert user.exists\n    assert user.gecos == \"gecos.comment\"\n\n\ndef test_user_expiration_date(host):\n    assert host.user(\"root\").expiration_date is None\n    assert host.user(\"user\").expiration_date == datetime.datetime(2030, 1, 1)\n\n\ndef test_nonexistent_user(host):\n    assert not host.user(\"zzzzzzzzzz\").exists\n\n\ndef test_current_user(host):\n    assert host.user().name == \"root\"\n    pw = host.user().password\n    assert pw.startswith(\"$\")\n    assert len(pw) == 73\n\n\ndef test_group(host):\n    assert host.group(\"root\").exists\n    assert host.group(\"root\").gid == 0\n    group_list = host.group(\"root\").get_all_groups\n    assert \"root\" in group_list\n    assert \"bin\" in group_list\n    group_list = host.group(\"root\").get_local_groups\n    assert \"root\" in group_list\n    assert \"bin\" in group_list\n\n\ndef test_empty_command_output(host):\n    assert host.check_output(\"printf ''\") == \"\"\n\n\ndef test_local_command(host):\n    assert host.get_host(\"local://\").check_output(\"true\") == \"\"\n\n\ndef test_file(host):\n    host.check_output(\"mkdir -p /d && printf foo > /d/f && chmod 600 /d/f\")\n    host.check_output('touch \"/d/f\\nl\"')\n    host.check_output('touch \"/d/f s\"')\n    d = host.file(\"/d\")\n    assert d.is_directory\n    assert not d.is_file\n    assert d.listdir() == [\"f\", \"f?l\", \"f s\"]\n    f = host.file(\"/d/f\")\n    assert f.exists\n    assert f.is_file\n    assert f.content == b\"foo\"\n    assert f.content_string == \"foo\"\n    assert f.user == \"root\"\n    assert f.uid == 0\n    assert f.gid == 0\n    assert f.group == \"root\"\n    assert f.mode == 0o600\n    assert f.contains(\"fo\")\n    assert not f.is_directory\n    assert not f.is_symlink\n    assert not f.is_pipe\n    assert f.linked_to == \"/d/f\"\n    assert f.size == 3\n    assert f.md5sum == \"acbd18db4cc2f85cedef654fccc4a4d8\"\n    assert f.sha256sum == (\n        \"2c26b46b68ffc68ff99b453c1d30413413422d706483bfa0f98a5e886266e7ae\"\n    )\n    host.check_output(\"ln -fsn /d/f /d/l\")\n    link = host.file(\"/d/l\")\n    assert link.is_symlink\n    assert link.is_file\n    assert link.linked_to == \"/d/f\"\n    assert link.linked_to == f\n    assert f == host.file(\"/d/f\")\n    assert d != f\n\n    host.check_output(\"ln /d/f /d/h\")\n    hardlink = host.file(\"/d/h\")\n    assert hardlink.is_file\n    assert not hardlink.is_symlink\n    assert isinstance(hardlink.inode, int)\n    assert isinstance(f.inode, int)\n    assert hardlink.inode == f.inode\n    assert f == host.file(\"/d/f\")\n    assert d != f\n\n    host.check_output(\"rm -f /d/p && mkfifo /d/p\")\n    assert host.file(\"/d/p\").is_pipe\n\n    host.check_output(\"chmod 700 /d/f\")\n    assert f.is_executable\n    assert f.mode == 0o700\n\n\ndef test_ansible_unavailable(host):\n    expected = \"Ansible module is only available with ansible connection backend\"\n    with pytest.raises(RuntimeError) as excinfo:\n        host.ansible(\"setup\")\n    assert expected in str(excinfo.value)\n    with pytest.raises(RuntimeError) as excinfo:\n        host.ansible.get_variables()\n    assert expected in str(excinfo.value)\n\n\n@pytest.mark.testinfra_hosts(\"ansible://debian_bookworm\")\ndef test_ansible_module(host):\n    setup = host.ansible(\"setup\")[\"ansible_facts\"]\n    assert setup[\"ansible_lsb\"][\"codename\"] == \"bookworm\"\n    passwd = host.ansible(\"file\", \"path=/etc/passwd state=file\")\n    assert passwd[\"changed\"] is False\n    assert passwd[\"gid\"] == 0\n    assert passwd[\"group\"] == \"root\"\n    assert passwd[\"mode\"] == \"0644\"\n    assert passwd[\"owner\"] == \"root\"\n    assert isinstance(passwd[\"size\"], int)\n    assert passwd[\"path\"] == \"/etc/passwd\"\n    # seems to vary with different docker fs backend\n    assert passwd[\"state\"] in (\"file\", \"hard\")\n    assert passwd[\"uid\"] == 0\n\n    with pytest.raises(host.ansible.AnsibleException) as excinfo:\n        host.ansible(\"command\", \"zzz\")\n    assert excinfo.value.result[\"msg\"] == \"Skipped. You might want to try check=False\"\n\n    try:\n        host.ansible(\"command\", \"zzz\", check=False)\n    except host.ansible.AnsibleException as exc:\n        assert exc.result[\"rc\"] == 2\n        # notez que the debian bookworm container is set to LANG=fr_FR\n        assert exc.result[\"msg\"] == (\"[Errno 2] Aucun fichier ou dossier de ce type\")\n\n    result = host.ansible(\"command\", \"echo foo\", check=False)\n    assert result[\"stdout\"] == \"foo\"\n\n\n@pytest.mark.testinfra_hosts(\"ansible://debian_bookworm\")\ndef test_ansible_get_variables_flat_wo_child_groups(host):\n    \"\"\"Test AnsibleRunner.get_variables() with parent groups only\"\"\"\n    variables = host.ansible.get_variables()\n    assert variables[\"myvar\"] == \"foo\"\n    assert variables[\"myhostvar\"] == \"bar\"\n    assert variables[\"mygroupvar\"] == \"qux\"\n    assert variables[\"inventory_hostname\"] == \"debian_bookworm\"\n    assert variables[\"group_names\"] == [\"all\", \"testgroup\"]\n    assert variables[\"groups\"] == {\n        \"all\": [\"debian_bookworm\"],\n        \"testgroup\": [\"debian_bookworm\"],\n    }\n\n\ndef test_ansible_get_variables_w_child_groups():\n    \"\"\"Test AnsibleRunner.get_variables() with parent and child groups\"\"\"\n    inventory = \"\"\"\n        host_a\n        [toplevel1]\n        host_b\n        [toplevel2]\n        host_c\n        [toplevel3:children]\n        toplevel1\n        \"\"\"\n    with tempfile.NamedTemporaryFile(mode=\"wt\", encoding=\"ascii\") as file_inventory:\n        file_inventory.write(textwrap.dedent(inventory.strip()))\n        file_inventory.flush()\n\n        get_variables = AnsibleRunner(file_inventory.name).get_variables\n\n        assert get_variables(\"host_a\")[\"group_names\"] == [\"all\", \"ungrouped\"]\n        assert get_variables(\"host_b\")[\"group_names\"] == [\n            \"all\",\n            \"toplevel1\",\n            \"toplevel3\",\n        ]\n        assert get_variables(\"host_c\")[\"group_names\"] == [\"all\", \"toplevel2\"]\n\n\n@pytest.mark.testinfra_hosts(\n    \"ansible://debian_bookworm\", \"ansible://user@debian_bookworm\"\n)\ndef test_ansible_module_become(host):\n    user_name = host.user().name\n    assert host.ansible(\"shell\", \"echo $USER\", check=False)[\"stdout\"] == user_name\n    assert (\n        host.ansible(\"shell\", \"echo $USER\", check=False, become=True)[\"stdout\"]\n        == \"root\"\n    )\n\n    with host.sudo():\n        assert host.user().name == \"root\"\n        assert host.ansible(\"shell\", \"echo $USER\", check=False)[\"stdout\"] == user_name\n        assert (\n            host.ansible(\"shell\", \"echo $USER\", check=False, become=True)[\"stdout\"]\n            == \"root\"\n        )\n\n\n@pytest.mark.testinfra_hosts(\"ansible://debian_bookworm\")\ndef test_ansible_module_options(host):\n    assert (\n        host.ansible(\n            \"command\",\n            \"id --user --name\",\n            check=False,\n            become=True,\n            become_user=\"nobody\",\n        )[\"stdout\"]\n        == \"nobody\"\n    )\n\n\n@pytest.mark.destructive\n@pytest.mark.parametrize(\n    \"supervisorctl_path,supervisorctl_conf\",\n    [\n        (\"supervisorctl\", None),\n        (\"/usr/bin/supervisorctl\", \"/etc/supervisor/supervisord.conf\"),\n    ],\n)\ndef test_supervisor(host, supervisorctl_path, supervisorctl_conf):\n    # Wait supervisord is running\n    for _ in range(20):\n        if host.service(\"supervisor\").is_running:\n            break\n        time.sleep(0.5)\n    else:\n        raise RuntimeError(\"No running supervisor\")\n\n    for _ in range(20):\n        service = host.supervisor(\n            \"tail\",\n            supervisorctl_path=supervisorctl_path,\n            supervisorctl_conf=supervisorctl_conf,\n        )\n        if service.status == \"RUNNING\":\n            break\n        assert service.status == \"STARTING\"\n        time.sleep(0.5)\n    else:\n        raise RuntimeError(\"No running tail in supervisor\")\n\n    assert service.is_running\n    proc = host.process.get(pid=service.pid)\n    assert proc.comm == \"tail\"\n\n    services = host.supervisor.get_services(supervisorctl_path, supervisorctl_conf)\n    assert len(services) == 1\n    assert services[0].name == \"tail\"\n    assert services[0].is_running\n    assert services[0].pid == service.pid\n    # Checking if conf is propagated\n    assert services[0].supervisorctl_path == supervisorctl_path\n    assert services[0].supervisorctl_conf == supervisorctl_conf\n\n    host.check_output(\"supervisorctl stop tail\")\n    service = host.supervisor(\n        \"tail\",\n        supervisorctl_path=supervisorctl_path,\n        supervisorctl_conf=supervisorctl_conf,\n    )\n    assert not service.is_running\n    assert service.status == \"STOPPED\"\n    assert service.pid is None\n\n    host.run(\"service supervisor stop\")\n    assert not host.service(\"supervisor\").is_running\n    with pytest.raises(RuntimeError) as excinfo:\n        host.supervisor(  # noqa: B018\n            \"tail\",\n            supervisorctl_path=supervisorctl_path,\n            supervisorctl_conf=supervisorctl_conf,\n        ).is_running\n    assert \"Is supervisor running\" in str(excinfo.value)\n\n\ndef test_mountpoint(host):\n    root_mount = host.mount_point(\"/\")\n    assert root_mount.exists\n    assert repr(root_mount)\n    assert isinstance(root_mount.options, list)\n    assert \"rw\" in root_mount.options\n    assert root_mount.filesystem\n\n    fake_mount = host.mount_point(\"/fake/mount\")\n    assert not fake_mount.exists\n    assert repr(fake_mount)\n\n    mountpoints = host.mount_point.get_mountpoints()\n    assert mountpoints\n    assert all(isinstance(m, host.mount_point) for m in mountpoints)\n    assert len([m for m in mountpoints if m.path == \"/\"]) == 1\n\n\ndef test_sudo_from_root(host):\n    assert host.user().name == \"root\"\n    with host.sudo(\"user\"):\n        assert host.user().name == \"user\"\n    assert host.user().name == \"root\"\n\n\ndef test_sudo_fail_from_root(host):\n    assert host.user().name == \"root\"\n    with host.sudo(\"unprivileged\"):\n        assert host.user().name == \"unprivileged\"\n        with pytest.raises(AssertionError) as exc:\n            host.check_output(\"ls /root/invalid\")\n    assert str(exc.value).startswith(\"Unexpected exit code\")\n    with host.sudo():\n        assert host.user().name == \"root\"\n\n\n@pytest.mark.testinfra_hosts(\"docker://user@debian_bookworm\")\ndef test_sudo_to_root(host):\n    assert host.user().name == \"user\"\n    with host.sudo():\n        assert host.user().name == \"root\"\n        # Test nested sudo\n        with host.sudo(\"www-data\"):\n            assert host.user().name == \"www-data\"\n    assert host.user().name == \"user\"\n\n\ndef test_command_execution(host):\n    assert host.run(\"false\").failed\n    assert host.run(\"true\").succeeded\n\n\ndef test_pip(host):\n    # get_packages\n    assert host.pip.get_packages()[\"pip\"][\"version\"].startswith(\"23.\")\n    pkg = host.pip.get_packages(pip_path=\"/v/bin/pip\")[\"requests\"]\n    assert pkg[\"version\"] == \"2.30.0\"\n    # outdated\n    outdated = host.pip.get_outdated_packages(pip_path=\"/v/bin/pip\")[\"requests\"]\n    assert outdated[\"current\"] == pkg[\"version\"]\n    # check\n    assert host.pip.check().succeeded\n    # is_installed\n    assert host.pip(\"pip\").is_installed\n    assert not host.pip(\"does_not_exist\").is_installed\n    pkg = host.pip(\"requests\", pip_path=\"/v/bin/pip\")\n    assert pkg.is_installed\n    # version\n    assert host.pip(\"pip\").version.startswith(\"23.\")\n    assert pkg.version == \"2.30.0\"\n    assert host.pip(\"does_not_exist\").version == \"\"\n\n\ndef test_environment_home(host):\n    assert host.environment().get(\"HOME\") == \"/root\"\n\n\n@pytest.mark.skipif(\n    \"WSL_DISTRO_NAME\" in os.environ, reason=\"Skip on WSL (Windows Subsystem for Linux)\"\n)\ndef test_iptables(host):\n    cmd = host.run(\"systemctl start netfilter-persistent\")\n    assert cmd.exit_status == 0, f\"{cmd.stdout}\\n{cmd.stderr}\"\n    ssh_rule_str = \"-A INPUT -p tcp -m state --state NEW -m tcp --dport 22 -j ACCEPT\"\n    vip_redirect_rule_str = \"-A PREROUTING -d 192.168.0.1/32 -j REDIRECT\"\n    rules = host.iptables.rules()\n    input_rules = host.iptables.rules(\"filter\", \"INPUT\")\n    nat_rules = host.iptables.rules(\"nat\")\n    nat_prerouting_rules = host.iptables.rules(\"nat\", \"PREROUTING\")\n    assert ssh_rule_str in rules\n    assert ssh_rule_str in input_rules\n    assert vip_redirect_rule_str in nat_rules\n    assert vip_redirect_rule_str in nat_prerouting_rules\n    assert host.iptables._has_w_argument is True\n\n\ndef test_ip6tables(host):\n    # test ip6tables call works; ipv6 setup is a whole huge thing, but\n    # ensure we at least see the headings\n    try:\n        v6_rules = host.iptables.rules(version=6)\n    except AssertionError as exc_info:\n        if \"Perhaps ip6tables or your kernel needs to be upgraded\" in exc_info.args[0]:\n            pytest.skip(\n                f\"IPV6 does not seem to be enabled on the docker host\\n{exc_info}\"\n            )\n        else:\n            raise\n    else:\n        assert \"-P INPUT ACCEPT\" in v6_rules\n        assert \"-P FORWARD ACCEPT\" in v6_rules\n        assert \"-P OUTPUT ACCEPT\" in v6_rules\n        v6_filter_rules = host.iptables.rules(\"filter\", \"INPUT\", version=6)\n        assert \"-P INPUT ACCEPT\" in v6_filter_rules\n\n\n@all_images\ndef test_addr(host):\n    non_resolvable = host.addr(\"some_non_resolvable_host\")\n    assert not non_resolvable.is_resolvable\n    assert not non_resolvable.is_reachable\n    assert not non_resolvable.port(80).is_reachable\n\n    # Some arbitrary internal IP, hopefully non-reachable\n    # IP addresses are always resolvable no matter what\n    non_reachable_ip = host.addr(\"10.42.13.73\")\n    assert non_reachable_ip.is_resolvable\n    assert non_reachable_ip.ipv4_addresses == [\"10.42.13.73\"]\n    assert not non_reachable_ip.is_reachable\n    assert not non_reachable_ip.port(80).is_reachable\n\n    google_dns = host.addr(\"8.8.8.8\")\n    assert google_dns.is_resolvable\n    assert google_dns.ipv4_addresses == [\"8.8.8.8\"]\n    assert google_dns.port(53).is_reachable\n    assert not google_dns.port(666).is_reachable\n\n    google_addr = host.addr(\"google.com\")\n    assert google_addr.is_resolvable\n    assert google_addr.port(443).is_reachable\n    assert not google_addr.port(666).is_reachable\n\n    for ip in google_addr.ipv4_addresses:\n        assert isinstance(ip_address(ip), IPv4Address)\n\n    for ip in google_addr.ipv6_addresses:\n        assert isinstance(ip_address(ip), IPv6Address)\n\n    for ip in google_addr.ip_addresses:\n        assert isinstance(ip_address(ip), (IPv4Address, IPv6Address))\n\n\n@pytest.mark.testinfra_hosts(\"ansible://debian_bookworm\")\ndef test_addr_namespace(host):\n    namespace_lookup = host.addr(\"localhost\", \"ns1\")\n    assert not namespace_lookup.namespace_exists\n\n\n@pytest.mark.parametrize(\n    \"family\",\n    [\"inet\", None],\n)\ndef test_interface(host, family):\n    # exist\n    assert host.interface(\"eth0\", family=family).exists\n    assert not host.interface(\"does_not_exist\", family=family).exists\n    # addresses\n    addresses = host.interface.default(family).addresses\n    assert len(addresses) > 0\n    for add in addresses:\n        try:\n            ip_address(add)\n        except ValueError:\n            pytest.fail(f\"{add} is not a valid IP address\")\n    # names and default\n    assert \"eth0\" in host.interface.names()\n    default_itf = host.interface.default(family)\n    assert default_itf.name == \"eth0\"\n    assert default_itf.exists\n", "metadata": {"source": "github.com/pytest-dev/pytest-testinfra", "file_path": "test/test_modules.py", "language": "python", "complexity": "complex", "star_count": 2433, "quality_score": 0.84, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": true, "has_documentation": true, "has_type_hints": true, "has_testing": true, "has_logging": true, "function_count": 45, "class_count": 0, "line_count": 702}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "testing_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create comprehensive test suite with fixtures and mocks with pytest testing framework\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport abc\nimport dataclasses\nimport locale\nimport logging\nimport shlex\nimport subprocess\nimport urllib.parse\nfrom typing import TYPE_CHECKING, Any, Optional, Union\n\nif TYPE_CHECKING:\n    import testinfra.host\n\nlogger = logging.getLogger(\"testinfra\")\n\n\n@dataclasses.dataclass\nclass HostSpec:\n    name: str\n    port: Optional[str]\n    user: Optional[str]\n    password: Optional[str]\n\n\n@dataclasses.dataclass\nclass CommandResult:\n    \"\"\"Object that encapsulates all returned details of the command execution.\n\n    Example:\n\n    >>> cmd = host.run(\"ls -l /etc/passwd\")\n    >>> cmd.rc\n    0\n    >>> cmd.stdout\n    '-rw-r--r-- 1 root root 1790 Feb 11 00:28 /etc/passwd\\\\n'\n    >>> cmd.stderr\n    ''\n    >>> cmd.succeeded\n    True\n    >>> cmd.failed\n    False\n    \"\"\"\n\n    backend: \"BaseBackend\"\n    exit_status: int\n    command: bytes\n    _stdout: Union[str, bytes]\n    _stderr: Union[str, bytes]\n\n    @property\n    def succeeded(self) -> bool:\n        \"\"\"Returns whether the command was successful\n\n        >>> host.run(\"true\").succeeded\n        True\n        \"\"\"\n        return self.exit_status == 0\n\n    @property\n    def failed(self) -> bool:\n        \"\"\"Returns whether the command failed\n\n        >>> host.run(\"false\").failed\n        True\n        \"\"\"\n        return self.exit_status != 0\n\n    @property\n    def rc(self) -> int:\n        \"\"\"Gets the returncode of a command\n\n        >>> host.run(\"true\").rc\n        0\n        \"\"\"\n        return self.exit_status\n\n    @property\n    def stdout(self) -> str:\n        \"\"\"Gets standard output (stdout) stream of an executed command\n\n        >>> host.run(\"mkdir -v new_directory\").stdout\n        mkdir: created directory 'new_directory'\n        \"\"\"\n        if isinstance(self._stdout, bytes):\n            return self.backend.decode(self._stdout)\n        return self._stdout\n\n    @property\n    def stderr(self) -> str:\n        \"\"\"Gets standard error (stderr) stream of an executed command\n\n        >>> host.run(\"mkdir new_directory\").stderr\n        mkdir: cannot create directory 'new_directory': File exists\n        \"\"\"\n        if isinstance(self._stderr, bytes):\n            return self.backend.decode(self._stderr)\n        return self._stderr\n\n    @property\n    def stdout_bytes(self) -> bytes:\n        \"\"\"Gets standard output (stdout) stream of an executed command as bytes\n\n        >>> host.run(\"mkdir -v new_directory\").stdout_bytes\n        b\"mkdir: created directory 'new_directory'\"\n        \"\"\"\n        if isinstance(self._stdout, str):\n            return self.backend.encode(self._stdout)\n        return self._stdout\n\n    @property\n    def stderr_bytes(self) -> bytes:\n        \"\"\"Gets standard error (stderr) stream of an executed command as bytes\n\n        >>> host.run(\"mkdir new_directory\").stderr_bytes\n        b\"mkdir: cannot create directory 'new_directory': File exists\"\n        \"\"\"\n        if isinstance(self._stderr, str):\n            return self.backend.encode(self._stderr)\n        return self._stderr\n\n\nclass BaseBackend(metaclass=abc.ABCMeta):\n    \"\"\"Represent the connection to the remote or local system\"\"\"\n\n    HAS_RUN_SALT = False\n    HAS_RUN_ANSIBLE = False\n    NAME: str\n\n    def __init__(\n        self,\n        hostname: str,\n        sudo: bool = False,\n        sudo_user: Optional[str] = None,\n        *args: Any,\n        **kwargs: Any,\n    ):\n        self._encoding: Optional[str] = None\n        self._host: Optional[testinfra.host.Host] = None\n        self.hostname = hostname\n        self.sudo = sudo\n        self.sudo_user = sudo_user\n        super().__init__()\n\n    def set_host(self, host: \"testinfra.host.Host\") -> None:\n        self._host = host\n\n    @classmethod\n    def get_connection_type(cls) -> str:\n        \"\"\"Return the connection backend used as string.\n\n        Can be local, paramiko, ssh, docker, salt or ansible\n        \"\"\"\n        return cls.NAME\n\n    def get_hostname(self) -> str:\n        \"\"\"Return the hostname (for testinfra) of the remote or local system\n\n\n        Can be useful for multi-hosts tests:\n\n        Example:\n        ::\n\n            import requests\n\n\n            def test(TestinfraBackend):\n                host = TestinfraBackend.get_hostname()\n                response = requests.get(\"http://\" + host)\n                assert response.status_code == 200\n\n\n        ::\n\n            $ testinfra --hosts=server1,server2 test.py\n\n            test.py::test[paramiko://server1] PASSED\n            test.py::test[paramiko://server2] PASSED\n        \"\"\"\n        return self.hostname\n\n    def get_pytest_id(self) -> str:\n        return self.get_connection_type() + \"://\" + self.get_hostname()\n\n    @classmethod\n    def get_hosts(cls, host: str, **kwargs: Any) -> list[str]:\n        if host is None:\n            raise RuntimeError(\n                f\"One or more hosts is required with the {cls.get_connection_type()} backend\"\n            )\n        return [host]\n\n    @staticmethod\n    def quote(command: str, *args: str) -> str:\n        if args:\n            return command % tuple(shlex.quote(a) for a in args)\n        return command\n\n    def get_sudo_command(self, command: str, sudo_user: Optional[str]) -> str:\n        if sudo_user is None:\n            return self.quote(\"sudo /bin/sh -c %s\", command)\n        return self.quote(\"sudo -u %s /bin/sh -c %s\", sudo_user, command)\n\n    def get_command(self, command: str, *args: str) -> str:\n        command = self.quote(command, *args)\n        if self.sudo:\n            command = self.get_sudo_command(command, self.sudo_user)\n        return command\n\n    def run(self, command: str, *args: str, **kwargs: Any) -> CommandResult:\n        raise NotImplementedError\n\n    def run_local(self, command: str, *args: str) -> CommandResult:\n        command = self.quote(command, *args)\n        cmd = self.encode(command)\n        p = subprocess.Popen(\n            cmd,\n            shell=True,\n            stdin=subprocess.PIPE,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n        )\n        stdout, stderr = p.communicate()\n        result = self.result(p.returncode, cmd, stdout, stderr)\n        return result\n\n    @staticmethod\n    def parse_hostspec(hostspec: str) -> HostSpec:\n        name = hostspec\n        port = None\n        user = None\n        password = None\n        if \"@\" in name:\n            user, name = name.split(\"@\", 1)\n            if \":\" in user:\n                user, password = user.split(\":\", 1)\n        # A literal IPv6 address might be like\n        #  [fe80:0::a:b:c]:80\n        # thus, below in words; if this starts with a '[' assume it\n        # encloses an ipv6 address with a closing ']', with a possible\n        # trailing port after a colon\n        if name.startswith(\"[\"):\n            name, port = name.split(\"]\")\n            name = name[1:]\n            port = port[1:] if port.startswith(\":\") else None\n        else:\n            if \":\" in name:\n                name, port = name.split(\":\", 1)\n        name = urllib.parse.unquote(name)\n        if user is not None:\n            user = urllib.parse.unquote(user)\n        if password is not None:\n            password = urllib.parse.unquote(password)\n        return HostSpec(name, port, user, password)\n\n    @staticmethod\n    def parse_containerspec(containerspec: str) -> tuple[str, Optional[str]]:\n        name = containerspec\n        user = None\n        if \"@\" in name:\n            user, name = name.split(\"@\", 1)\n        return name, user\n\n    def get_encoding(self) -> str:\n        encoding = None\n        for python in (\"python3\", \"python\"):\n            cmd = self.run(\n                \"%s -c 'import locale;print(locale.getpreferredencoding())'\",\n                python,\n                encoding=None,\n            )\n            if cmd.rc == 0:\n                encoding = cmd.stdout_bytes.splitlines()[0].decode(\"ascii\")\n                break\n        # Python is not installed, we hope the encoding to be the same as\n        # local machine...\n        if not encoding:\n            encoding = locale.getpreferredencoding()\n        if encoding == \"ANSI_X3.4-1968\":\n            # Workaround default encoding ascii without LANG set\n            encoding = \"UTF-8\"\n        return encoding\n\n    @property\n    def encoding(self) -> str:\n        if self._encoding is None:\n            self._encoding = self.get_encoding()\n        return self._encoding\n\n    def decode(self, data: bytes) -> str:\n        try:\n            return data.decode(\"ascii\")\n        except UnicodeDecodeError:\n            return data.decode(self.encoding)\n\n    def encode(self, data: str) -> bytes:\n        try:\n            return data.encode(\"ascii\")\n        except UnicodeEncodeError:\n            return data.encode(self.encoding)\n\n    def result(\n        self, rc: int, cmd: bytes, stdout: Union[str, bytes], stderr: Union[str, bytes]\n    ) -> CommandResult:\n        result = CommandResult(\n            backend=self,\n            exit_status=rc,\n            command=cmd,\n            _stdout=stdout,\n            _stderr=stderr,\n        )\n        logger.debug(\"RUN %s\", result)\n        return result\n", "metadata": {"source": "github.com/pytest-dev/pytest-testinfra", "file_path": "testinfra/backend/base.py", "language": "python", "complexity": "complex", "star_count": 2433, "quality_score": 0.8999999999999999, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": true, "has_documentation": true, "has_type_hints": true, "has_testing": true, "has_logging": true, "function_count": 26, "class_count": 3, "line_count": 325}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "testing_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create comprehensive test suite with fixtures and mocks\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom typing import Any\n\nfrom testinfra.backend import base\n\n\nclass LocalBackend(base.BaseBackend):\n    NAME = \"local\"\n\n    def __init__(self, *args: Any, **kwargs: Any):\n        super().__init__(\"local\", **kwargs)\n\n    def get_pytest_id(self) -> str:\n        return \"local\"\n\n    @classmethod\n    def get_hosts(cls, host: str, **kwargs: Any) -> list[str]:\n        return [host]\n\n    def run(self, command: str, *args: str, **kwargs: Any) -> base.CommandResult:\n        return self.run_local(self.get_command(command, *args))\n", "metadata": {"source": "github.com/pytest-dev/pytest-testinfra", "file_path": "testinfra/backend/local.py", "language": "python", "complexity": "simple", "star_count": 2433, "quality_score": 0.75, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": true, "has_logging": false, "function_count": 4, "class_count": 1, "line_count": 33}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "testing_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create comprehensive test suite with fixtures and mocks\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport os\n\ntry:\n    import paramiko\nexcept ImportError:\n    raise RuntimeError(\n        \"You must install paramiko package (pip install paramiko) \"\n        \"to use the paramiko backend\"\n    ) from None\n\nimport functools\nfrom typing import Any, Optional\n\nimport paramiko.pkey\nimport paramiko.ssh_exception\n\nfrom testinfra.backend import base\n\n\nclass IgnorePolicy(paramiko.MissingHostKeyPolicy):\n    \"\"\"Policy for ignoring a missing host key.\"\"\"\n\n    def missing_host_key(\n        self, client: paramiko.SSHClient, hostname: str, key: paramiko.pkey.PKey\n    ) -> None:\n        pass\n\n\nclass ParamikoBackend(base.BaseBackend):\n    NAME = \"paramiko\"\n\n    def __init__(\n        self,\n        hostspec: str,\n        ssh_config: Optional[str] = None,\n        ssh_identity_file: Optional[str] = None,\n        timeout: int = 10,\n        *args: Any,\n        **kwargs: Any,\n    ):\n        self.host = self.parse_hostspec(hostspec)\n        self.ssh_config = ssh_config\n        self.ssh_identity_file = ssh_identity_file\n        self.get_pty = False\n        self.timeout = int(timeout)\n        super().__init__(self.host.name, *args, **kwargs)\n\n    def _load_ssh_config(\n        self,\n        client: paramiko.SSHClient,\n        cfg: dict[str, Any],\n        ssh_config: paramiko.SSHConfig,\n        ssh_config_dir: str = \"~/.ssh\",\n    ) -> None:\n        for key, value in ssh_config.lookup(self.host.name).items():\n            if key == \"hostname\":\n                cfg[key] = value\n            elif key == \"user\":\n                cfg[\"username\"] = value\n            elif key == \"port\":\n                cfg[key] = int(value)\n            elif key == \"identityfile\":\n                cfg[\"key_filename\"] = os.path.expanduser(value[0])\n            elif key == \"stricthostkeychecking\" and value == \"no\":\n                client.set_missing_host_key_policy(IgnorePolicy())\n            elif key == \"requesttty\":\n                self.get_pty = value in (\"yes\", \"force\")\n            elif key == \"gssapikeyexchange\":\n                cfg[\"gss_auth\"] = value == \"yes\"\n            elif key == \"gssapiauthentication\":\n                cfg[\"gss_kex\"] = value == \"yes\"\n            elif key == \"proxycommand\":\n                cfg[\"sock\"] = paramiko.ProxyCommand(value)\n            elif key == \"include\":\n                new_config_path = os.path.join(\n                    os.path.expanduser(ssh_config_dir), value\n                )\n                with open(new_config_path) as f:\n                    new_ssh_config = paramiko.SSHConfig()\n                    new_ssh_config.parse(f)\n                    self._load_ssh_config(client, cfg, new_ssh_config, ssh_config_dir)\n\n    @functools.cached_property\n    def client(self) -> paramiko.SSHClient:\n        client = paramiko.SSHClient()\n        client.set_missing_host_key_policy(paramiko.WarningPolicy())\n        cfg = {\n            \"hostname\": self.host.name,\n            \"port\": int(self.host.port) if self.host.port else 22,\n            \"username\": self.host.user,\n            \"timeout\": self.timeout,\n            \"password\": self.host.password,\n        }\n        if self.ssh_config:\n            ssh_config_dir = os.path.dirname(self.ssh_config)\n\n            with open(self.ssh_config) as f:\n                ssh_config = paramiko.SSHConfig()\n                ssh_config.parse(f)\n                self._load_ssh_config(client, cfg, ssh_config, ssh_config_dir)\n        else:\n            # fallback reading ~/.ssh/config\n            default_ssh_config = os.path.join(os.path.expanduser(\"~\"), \".ssh\", \"config\")\n            ssh_config_dir = os.path.dirname(default_ssh_config)\n\n            try:\n                with open(default_ssh_config) as f:\n                    ssh_config = paramiko.SSHConfig()\n                    ssh_config.parse(f)\n            except OSError:\n                pass\n            else:\n                self._load_ssh_config(client, cfg, ssh_config, ssh_config_dir)\n\n        if self.ssh_identity_file:\n            cfg[\"key_filename\"] = self.ssh_identity_file\n        client.connect(**cfg)  # type: ignore[arg-type]\n        return client\n\n    def _exec_command(self, command: bytes) -> tuple[int, bytes, bytes]:\n        transport = self.client.get_transport()\n        assert transport is not None\n        chan = transport.open_session()\n        if self.get_pty:\n            chan.get_pty()\n        chan.exec_command(command)\n        stdout = b\"\".join(chan.makefile(\"rb\"))\n        stderr = b\"\".join(chan.makefile_stderr(\"rb\"))\n        rc = chan.recv_exit_status()\n        return rc, stdout, stderr\n\n    def run(self, command: str, *args: str, **kwargs: Any) -> base.CommandResult:\n        command = self.get_command(command, *args)\n        cmd = self.encode(command)\n        try:\n            rc, stdout, stderr = self._exec_command(cmd)\n        except (paramiko.ssh_exception.SSHException, ConnectionResetError):\n            transport = self.client.get_transport()\n            assert transport is not None\n            if not transport.is_active():\n                # try to reinit connection (once)\n                del self.client\n                rc, stdout, stderr = self._exec_command(cmd)\n            else:\n                raise\n\n        return self.result(rc, cmd, stdout, stderr)\n", "metadata": {"source": "github.com/pytest-dev/pytest-testinfra", "file_path": "testinfra/backend/paramiko.py", "language": "python", "complexity": "complex", "star_count": 2433, "quality_score": 0.84, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": true, "has_documentation": true, "has_type_hints": true, "has_testing": true, "has_logging": false, "function_count": 6, "class_count": 2, "line_count": 160}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "testing_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create comprehensive test suite with fixtures and mocks with pytest testing framework\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport functools\nimport os\nfrom collections.abc import Iterable\nfrom typing import Any\n\nimport testinfra.backend\nimport testinfra.backend.base\nimport testinfra.modules\nimport testinfra.modules.base\n\n\nclass Host:\n    _host_cache: dict[tuple[str, frozenset[tuple[str, Any]]], \"Host\"] = {}\n    _hosts_cache: dict[\n        tuple[frozenset[str], frozenset[tuple[str, Any]]], list[\"Host\"]\n    ] = {}\n\n    def __init__(self, backend: testinfra.backend.base.BaseBackend):\n        self.backend = backend\n        super().__init__()\n\n    def __repr__(self) -> str:\n        return f\"<testinfra.host.Host {self.backend.get_pytest_id()}>\"\n\n    @functools.cached_property\n    def has_command_v(self) -> bool:\n        \"\"\"Return True if `command -v` is available\"\"\"\n        return self.run(\"command -v command\").rc == 0\n\n    def exists(self, command: str) -> bool:\n        \"\"\"Return True if given command exist in $PATH\"\"\"\n        if self.has_command_v:\n            out = self.run(\"command -v %s\", command)\n        else:\n            out = self.run_expect([0, 1], \"which %s\", command)\n        return out.rc == 0\n\n    def find_command(\n        self, command: str, extrapaths: Iterable[str] = (\"/sbin\", \"/usr/sbin\")\n    ) -> str:\n        \"\"\"Return the path of the given command\n\n        raise ValueError if command cannot be found\n        \"\"\"\n        if self.has_command_v:\n            out = self.run(\"command -v %s\", command)\n        else:\n            out = self.run_expect([0, 1], \"which %s\", command)\n        if out.rc == 0:\n            return out.stdout.rstrip(\"\\r\\n\")\n        for basedir in extrapaths:\n            path = os.path.join(basedir, command)\n            if self.exists(path):\n                return path\n        raise ValueError(f'cannot find \"{command}\" command')\n\n    def run(\n        self, command: str, *args: str, **kwargs: Any\n    ) -> testinfra.backend.base.CommandResult:\n        \"\"\"Run given command and return rc (exit status), stdout and stderr\n\n        >>> cmd = host.run(\"ls -l /etc/passwd\")\n        >>> cmd.rc\n        0\n        >>> cmd.stdout\n        '-rw-r--r-- 1 root root 1790 Feb 11 00:28 /etc/passwd\\\\n'\n        >>> cmd.stderr\n        ''\n        >>> cmd.succeeded\n        True\n        >>> cmd.failed\n        False\n\n\n        Good practice: always use shell arguments quoting to avoid shell\n        injection\n\n\n        >>> cmd = host.run(\"ls -l -- %s\", \"/;echo inject\")\n        CommandResult(\n            rc=2, stdout='',\n            stderr=(\n              'ls: cannot access /;echo inject: No such file or directory\\\\n'),\n            command=\"ls -l '/;echo inject'\")\n        \"\"\"\n        return self.backend.run(command, *args, **kwargs)\n\n    def run_expect(\n        self, expected: list[int], command: str, *args: str, **kwargs: Any\n    ) -> testinfra.backend.base.CommandResult:\n        \"\"\"Run command and check it return an expected exit status\n\n        :param expected: A list of expected exit status\n        :raises: AssertionError\n        \"\"\"\n        __tracebackhide__ = True\n        out = self.run(command, *args, **kwargs)\n        assert out.rc in expected, f\"Unexpected exit code {out.rc} for {out}\"\n        return out\n\n    def run_test(\n        self, command: str, *args: str, **kwargs: Any\n    ) -> testinfra.backend.base.CommandResult:\n        \"\"\"Run command and check it return an exit status of 0 or 1\n\n        :raises: AssertionError\n        \"\"\"\n        return self.run_expect([0, 1], command, *args, **kwargs)\n\n    def check_output(self, command: str, *args: str, **kwargs: Any) -> str:\n        \"\"\"Get stdout of a command which has run successfully\n\n        :returns: stdout without trailing newline\n        :raises: AssertionError\n        \"\"\"\n        __tracebackhide__ = True\n        out = self.run(command, *args, **kwargs)\n        assert out.rc == 0, f\"Unexpected exit code {out.rc} for {out}\"\n        return out.stdout.rstrip(\"\\r\\n\")\n\n    def __getattr__(self, name: str) -> type[testinfra.modules.base.Module]:\n        if name in testinfra.modules.modules:\n            module_class = testinfra.modules.get_module_class(name)\n            obj = module_class.get_module(self)\n            setattr(self, name, obj)\n            return obj\n        raise AttributeError(\n            f\"'{self.__class__.__name__}' object has no attribute '{name}'\"\n        )\n\n    @classmethod\n    def get_host(cls, hostspec: str, **kwargs: Any) -> \"Host\":\n        \"\"\"Return a Host instance from `hostspec`\n\n        `hostspec` should be like\n        `<backend_type>://<name>?param1=value1&param2=value2`\n\n        Params can also be passed in `**kwargs` (e.g. get_host(\"local://\",\n        sudo=True) is equivalent to get_host(\"local://?sudo=true\"))\n\n        Examples::\n\n        >>> get_host(\"local://\", sudo=True)\n        >>> get_host(\"paramiko://user@host\", ssh_config=\"/path/my_ssh_config\")\n        >>> get_host(\"ansible://all?ansible_inventory=/etc/ansible/inventory\")\n        \"\"\"\n        key = (hostspec, frozenset(kwargs.items()))\n        cache = cls._host_cache\n        if key not in cache:\n            backend = testinfra.backend.get_backend(hostspec, **kwargs)\n            cache[key] = host = cls(backend)\n            backend.set_host(host)\n        return cache[key]\n\n    @classmethod\n    def get_hosts(cls, hosts: Iterable[str], **kwargs: Any) -> list[\"Host\"]:\n        key = (frozenset(hosts), frozenset(kwargs.items()))\n        cache = cls._hosts_cache\n        if key not in cache:\n            cache[key] = []\n            for backend in testinfra.backend.get_backends(hosts, **kwargs):\n                host = cls(backend)\n                backend.set_host(host)\n                cache[key].append(host)\n        return cache[key]\n\n\nget_host = Host.get_host\nget_hosts = Host.get_hosts\n", "metadata": {"source": "github.com/pytest-dev/pytest-testinfra", "file_path": "testinfra/host.py", "language": "python", "complexity": "complex", "star_count": 2433, "quality_score": 0.84, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": true, "has_type_hints": true, "has_testing": true, "has_logging": false, "function_count": 12, "class_count": 2, "line_count": 182}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "testing_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create comprehensive test suite with fixtures and mocks\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport warnings\n\nimport pytest\n\n\ndef main() -> int:\n    warnings.warn(\"calling testinfra is deprecated, call pytest instead\", stacklevel=1)\n    return pytest.main()\n", "metadata": {"source": "github.com/pytest-dev/pytest-testinfra", "file_path": "testinfra/main.py", "language": "python", "complexity": "simple", "star_count": 2433, "quality_score": 0.6599999999999999, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 1, "class_count": 0, "line_count": 21}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "testing_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create comprehensive test suite with fixtures and mocks\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport functools\nimport pprint\n\nfrom testinfra.modules.base import InstanceModule\n\n\nclass AnsibleException(Exception):\n    \"\"\"Exception raised when an error occurs in an ansible call.\n\n    Result from ansible can be accessed through the ``result`` attribute.\n\n    >>> try:\n    ...     host.ansible(\"command\", \"echo foo\")\n    ... except host.ansible.AnsibleException as exc:\n    ...     assert exc.result['failed'] is True\n    ...     assert exc.result['msg'] == 'Skipped. You might want to try check=False'  # noqa\n    \"\"\"\n\n    def __init__(self, result):\n        self.result = result\n        super().__init__(f\"Unexpected error: {pprint.pformat(result)}\")\n\n\ndef need_ansible(func):\n    @functools.wraps(func)\n    def wrapper(self, *args, **kwargs):\n        if not self._host.backend.HAS_RUN_ANSIBLE:\n            raise RuntimeError(\n                \"Ansible module is only available with ansible connection backend\"\n            )\n        return func(self, *args, **kwargs)\n\n    return wrapper\n\n\nclass Ansible(InstanceModule):\n    \"\"\"Run Ansible module functions\n\n    This module is only available with the :ref:`ansible connection\n    backend` connection backend.\n\n    `Check mode\n    <https://docs.ansible.com/ansible/playbooks_checkmode.html>`_ is\n    enabled by default, you can disable it with `check=False`.\n\n    `Become\n    <https://docs.ansible.com/ansible/user_guide/become.html#id1>`_ is\n    `False` by default. You can enable it with `become=True`.\n\n    Ansible arguments that are not related to the Ansible inventory or\n    connection (both managed by testinfra) are also accepted through keyword\n    arguments:\n\n        - ``become_method`` *str* sudo, su, doas, etc.\n        - ``become_user`` *str* become this user.\n        - ``diff`` *bool*: when changing (small) files and templates, show the\n          differences in those files.\n        - ``extra_vars`` *dict* serialized to a JSON string, passed to\n          Ansible.\n        - ``one_line`` *bool*: condense output.\n        - ``user`` *str* connect as this user.\n        - ``verbose`` *int* level of verbosity\n\n    >>> host.ansible(\"apt\", \"name=nginx state=present\")[\"changed\"]\n    False\n    >>> host.ansible(\"apt\", \"name=nginx state=present\", become=True)[\"changed\"]\n    False\n    >>> host.ansible(\"command\", \"echo foo\", check=False)[\"stdout\"]\n    'foo'\n    >>> host.ansible(\"setup\")[\"ansible_facts\"][\"ansible_lsb\"][\"codename\"]\n    'jessie'\n    >>> host.ansible(\"file\", \"path=/etc/passwd\")[\"mode\"]\n    '0640'\n    >>> host.ansible(\n    ... \"command\",\n    ... \"id --user --name\",\n    ... check=False,\n    ... become=True,\n    ... become_user=\"http\",\n    ... )[\"stdout\"]\n    'http'\n    >>> host.ansible(\n    ... \"apt\",\n    ... \"name={{ packages }}\",\n    ... check=False,\n    ... extra_vars={\"packages\": [\"neovim\", \"vim\"]},\n    ... )\n    # Installs neovim and vim.\n\n    \"\"\"\n\n    AnsibleException = AnsibleException\n\n    @need_ansible\n    def __call__(\n        self, module_name, module_args=None, check=True, become=False, **kwargs\n    ):\n        result = self._host.backend.run_ansible(\n            module_name, module_args, check=check, become=become, **kwargs\n        )\n        if result.get(\"failed\", False):\n            raise AnsibleException(result)\n        return result\n\n    @need_ansible\n    def get_variables(self):\n        \"\"\"Returns a dict of ansible variables\n\n        >>> host.ansible.get_variables()\n        {\n            'inventory_hostname': 'localhost',\n            'group_names': ['ungrouped'],\n            'foo': 'bar',\n        }\n\n        \"\"\"\n        return self._host.backend.get_variables()\n\n    def __repr__(self):\n        return \"<ansible>\"\n", "metadata": {"source": "github.com/pytest-dev/pytest-testinfra", "file_path": "testinfra/modules/ansible.py", "language": "python", "complexity": "medium", "star_count": 2433, "quality_score": 0.8099999999999999, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": true, "has_documentation": true, "has_type_hints": true, "has_testing": true, "has_logging": false, "function_count": 6, "class_count": 2, "line_count": 133}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "testing_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create comprehensive test suite with fixtures and mocks\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport json\n\nfrom testinfra.modules.base import Module\n\n\nclass Package(Module):\n    \"\"\"Test packages status and version\"\"\"\n\n    def __init__(self, name):\n        self.name = name\n        super().__init__()\n\n    @property\n    def is_installed(self):\n        \"\"\"Test if the package is installed\n\n        >>> host.package(\"nginx\").is_installed\n        True\n\n        Supported package systems:\n\n        - apk (Alpine)\n        - apt (Debian, Ubuntu, ...)\n        - brew (macOS)\n        - pacman (Arch, Manjaro )\n        - pkg (FreeBSD)\n        - pkg_info (NetBSD)\n        - pkg_info (OpenBSD)\n        - rpm (RHEL, RockyLinux, Fedora, ...)\n        \"\"\"\n        raise NotImplementedError\n\n    @property\n    def release(self):\n        \"\"\"Return the release-specific info from the package version\n\n        >>> host.package(\"nginx\").release\n        '1.el6'\n        \"\"\"\n        raise NotImplementedError\n\n    @property\n    def version(self):\n        \"\"\"Return package version as returned by the package system\n\n        >>> host.package(\"nginx\").version\n        '1.2.1-2.2+wheezy3'\n        \"\"\"\n        raise NotImplementedError\n\n    def __repr__(self):\n        return f\"<package {self.name}>\"\n\n    @classmethod\n    def get_module_class(cls, host):\n        if host.system_info.type == \"windows\":\n            return ChocolateyPackage\n        if host.system_info.type == \"freebsd\":\n            return FreeBSDPackage\n        if host.system_info.type in (\"openbsd\", \"netbsd\"):\n            return OpenBSDPackage\n        if host.system_info.distribution in (\"debian\", \"ubuntu\"):\n            return DebianPackage\n        if host.system_info.distribution and (\n            host.system_info.distribution.lower()\n            in (\n                \"almalinux\",\n                \"centos\",\n                \"cloudlinux\",\n                \"fedora\",\n                \"ol\",\n                \"opensuse-leap\",\n                \"opensuse-tumbleweed\",\n                \"rhel\",\n                \"rocky\",\n            )\n        ):\n            return RpmPackage\n        if host.system_info.distribution in (\"arch\", \"manjarolinux\"):\n            return ArchPackage\n        if host.exists(\"apk\"):\n            return AlpinePackage\n        # Fallback conditions\n        if host.exists(\"dpkg-query\"):\n            return DebianPackage\n        if host.exists(\"rpm\"):\n            return RpmPackage\n        if host.exists(\"brew\"):\n            return HomebrewPackage\n        raise NotImplementedError\n\n\nclass DebianPackage(Package):\n    @property\n    def is_installed(self):\n        result = self.run_test(\"dpkg-query -f '${Status}' -W %s\", self.name)\n        if result.rc == 1:\n            return False\n        out = result.stdout.strip().split()\n        installed_status = [\"ok\", \"installed\"]\n        return out[0] in [\"install\", \"hold\"] and out[1:3] == installed_status\n\n    @property\n    def release(self):\n        raise NotImplementedError\n\n    @property\n    def version(self):\n        out = self.check_output(\"dpkg-query -f '${Status} ${Version}' -W %s\", self.name)\n        splitted = out.split()\n        assert splitted[0].lower() in (\n            \"install\",\n            \"hold\",\n        ), f\"The package {self.name} is not installed, dpkg-query output: {out}\"\n        return splitted[3]\n\n\nclass FreeBSDPackage(Package):\n    @property\n    def is_installed(self):\n        EX_UNAVAILABLE = 69\n        return (\n            self.run_expect([0, EX_UNAVAILABLE], \"pkg query %%n %s\", self.name).rc == 0\n        )\n\n    @property\n    def release(self):\n        raise NotImplementedError\n\n    @property\n    def version(self):\n        return self.check_output(\"pkg query %%v %s\", self.name)\n\n\nclass OpenBSDPackage(Package):\n    @property\n    def is_installed(self):\n        return self.run_test(\"pkg_info -e %s\", f\"{self.name}-*\").rc == 0\n\n    @property\n    def release(self):\n        raise NotImplementedError\n\n    @property\n    def version(self):\n        out = self.check_output(\"pkg_info -e %s\", f\"{self.name}-*\")\n        # OpenBSD: inst:zsh-5.0.5p0\n        # NetBSD: zsh-5.0.7nb1\n        return out.split(self.name + \"-\", 1)[1]\n\n\nclass RpmPackage(Package):\n    @property\n    def is_installed(self):\n        result = self.run_test(\"rpm -q --quiet %s 2>&1\", self.name)\n        if result.succeeded:\n            return True\n        elif result.failed and result.stdout == \"\":\n            return False\n        else:\n            raise RuntimeError(\n                f\"Could not check if RPM package '{self.name}' is installed. {result.stdout}\"\n            )\n\n    @property\n    def version(self):\n        return self.check_output('rpm -q --queryformat=\"%%{VERSION}\" %s', self.name)\n\n    @property\n    def release(self):\n        return self.check_output('rpm -q --queryformat=\"%%{RELEASE}\" %s', self.name)\n\n\nclass AlpinePackage(Package):\n    @property\n    def is_installed(self):\n        return self.run_test(\"apk -e info %s\", self.name).rc == 0\n\n    @property\n    def version(self):\n        out = self.check_output(\"apk -e -v info %s\", self.name).split(\"-\")\n        return out[-2]\n\n    @property\n    def release(self):\n        out = self.check_output(\"apk -e -v info %s\", self.name).split(\"-\")\n        return out[-1]\n\n\nclass ArchPackage(Package):\n    @property\n    def is_installed(self):\n        return self.run_test(\"pacman -Q %s\", self.name).rc == 0\n\n    @property\n    def version(self):\n        out = self.check_output(\"pacman -Q %s\", self.name).split(\" \")\n        return out[1]\n\n    @property\n    def release(self):\n        raise NotImplementedError\n\n\nclass ChocolateyPackage(Package):\n    @property\n    def is_installed(self):\n        return self.run_test(\"choco info -lo %s\", self.name).rc == 0\n\n    @property\n    def version(self):\n        _, version = self.check_output(\"choco info -lo %s -r\", self.name).split(\"|\", 1)\n        return version\n\n    @property\n    def release(self):\n        raise NotImplementedError\n\n\nclass HomebrewPackage(Package):\n    @property\n    def is_installed(self):\n        info = self.check_output(\"brew info --formula --json %s\", self.name)\n        return len(json.loads(info)[0][\"installed\"]) > 0\n\n    @property\n    def version(self):\n        info = self.check_output(\"brew info --formula --json %s\", self.name)\n        version = json.loads(info)[0][\"installed\"][0][\"version\"]\n        return version\n\n    @property\n    def release(self):\n        raise NotImplementedError\n", "metadata": {"source": "github.com/pytest-dev/pytest-testinfra", "file_path": "testinfra/modules/package.py", "language": "python", "complexity": "complex", "star_count": 2433, "quality_score": 0.78, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": true, "has_type_hints": true, "has_testing": true, "has_logging": false, "function_count": 30, "class_count": 9, "line_count": 246}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "testing_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create comprehensive test suite with fixtures and mocks\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport json\nimport re\n\nfrom testinfra.modules.base import Module\n\n\ndef _re_match(line, regexp):\n    match = regexp.match(line)\n    if match is None:\n        raise RuntimeError(f\"could not parse {line}\")\n    return match.groups()\n\n\nclass Pip(Module):\n    \"\"\"Test pip package manager and packages\"\"\"\n\n    def __init__(self, name, pip_path=\"pip\"):\n        self.name = name\n        self.pip_path = pip_path\n        super().__init__()\n\n    @property\n    def is_installed(self):\n        \"\"\"Test if the package is installed\n\n        >>> host.package(\"pip\").is_installed\n        True\n        \"\"\"\n        return self.run_test(\"%s show %s\", self.pip_path, self.name).rc == 0\n\n    @property\n    def version(self):\n        \"\"\"Return package version as returned by pip\n\n        >>> host.package(\"pip\").version\n        '18.1'\n        \"\"\"\n        return self.check_output(\n            \"%s show %s | grep Version: | cut -d' ' -f2\",\n            self.pip_path,\n            self.name,\n        )\n\n    @classmethod\n    def check(cls, pip_path=\"pip\"):\n        \"\"\"Verify installed packages have compatible dependencies.\n\n        >>> cmd = host.pip.check()\n        >>> cmd.rc\n        0\n        >>> cmd.stdout\n        No broken requirements found.\n\n        Can only be used if `pip check`_ command is available,\n        for pip versions >= 9.0.0_.\n\n        .. _pip check: https://pip.pypa.io/en/stable/reference/pip_check/\n        .. _9.0.0: https://pip.pypa.io/en/stable/news/#id526\n        \"\"\"\n        return cls.run_expect([0, 1], \"%s check\", pip_path)\n\n    @classmethod\n    def get_packages(cls, pip_path=\"pip\"):\n        \"\"\"Get all installed packages and versions returned by `pip list`:\n\n        >>> host.pip.get_packages(pip_path='~/venv/website/bin/pip')\n        {'Django': {'version': '1.10.2'},\n         'mywebsite': {'version': '1.0a3', 'path': '/srv/website'},\n         'psycopg2': {'version': '2.6.2'}}\n        \"\"\"\n        out = cls.run_expect([0, 2], \"%s list --no-index --format=json\", pip_path)\n        pkgs = {}\n        if out.rc == 0:\n            for pkg in json.loads(out.stdout):\n                # XXX: --output=json does not return install path\n                pkgs[pkg[\"name\"]] = {\"version\": pkg[\"version\"]}\n        else:\n            # pip < 9\n            output_re = re.compile(r\"^(.+) \\((.+)\\)$\")\n            for line in cls.check_output(\"%s list --no-index\", pip_path).splitlines():\n                if line.startswith(\"Warning: \"):\n                    # Warning: cannot find svn location for ...\n                    continue\n                name, version = _re_match(line, output_re)\n                if \",\" in version:\n                    version, path = version.split(\",\", 1)\n                    pkgs[name] = {\"version\": version, \"path\": path.strip()}\n                else:\n                    pkgs[name] = {\"version\": version}\n        return pkgs\n\n    @classmethod\n    def get_outdated_packages(cls, pip_path=\"pip\"):\n        \"\"\"Get all outdated packages with the current and latest version\n\n        >>> host.pip.get_outdated_packages(\n        ...     pip_path='~/venv/website/bin/pip')\n        {'Django': {'current': '1.10.2', 'latest': '1.10.3'}}\n        \"\"\"\n        out = cls.run_expect([0, 2], \"%s list -o --format=json\", pip_path)\n        pkgs = {}\n        if out.rc == 0:\n            for pkg in json.loads(out.stdout):\n                pkgs[pkg[\"name\"]] = {\n                    \"current\": pkg[\"version\"],\n                    \"latest\": pkg[\"latest_version\"],\n                }\n        else:\n            # pip < 9\n            # pip 8: pytest (3.4.2) - Latest: 3.5.0 [wheel]\n            # pip < 8: pytest (Current: 3.4.2 Latest: 3.5.0 [wheel])\n            regexpes = [\n                re.compile(r\"^(.+?) \\((.+)\\) - Latest: (.+) .*$\"),\n                re.compile(r\"^(.+?) \\(Current: (.+) Latest: (.+) .*$\"),\n            ]\n            for line in cls.check_output(\"%s list -o\", pip_path).splitlines():\n                if line.startswith(\"Warning: \"):\n                    # Warning: cannot find svn location for ...\n                    continue\n                output_re = regexpes[1] if \"Current:\" in line else regexpes[0]\n                name, current, latest = _re_match(line, output_re)\n                pkgs[name] = {\"current\": current, \"latest\": latest}\n        return pkgs\n", "metadata": {"source": "github.com/pytest-dev/pytest-testinfra", "file_path": "testinfra/modules/pip.py", "language": "python", "complexity": "medium", "star_count": 2433, "quality_score": 0.78, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": true, "has_type_hints": true, "has_testing": true, "has_logging": false, "function_count": 7, "class_count": 1, "line_count": 136}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "testing_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create comprehensive test suite with fixtures and mocks\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport functools\n\nfrom testinfra.modules.base import Module\n\n\nclass Service(Module):\n    \"\"\"Test services\n\n    Implementations:\n\n    - Linux: detect Systemd, Upstart or OpenRC, fallback to SysV\n    - FreeBSD: service(1)\n    - OpenBSD: ``/etc/rc.d/$name check`` for ``is_running``\n      ``rcctl ls on`` for ``is_enabled`` (only OpenBSD >= 5.8)\n    - NetBSD: ``/etc/rc.d/$name onestatus`` for ``is_running``\n      (``is_enabled`` is not yet implemented)\n\n    \"\"\"\n\n    def __init__(self, name):\n        self.name = name\n        super().__init__()\n\n    @property\n    def exists(self):\n        \"\"\"Test if the service exists\"\"\"\n        raise NotImplementedError\n\n    @property\n    def is_running(self):\n        \"\"\"Test if service is running\"\"\"\n        raise NotImplementedError\n\n    @property\n    def is_enabled(self):\n        \"\"\"Test if service is enabled\"\"\"\n        raise NotImplementedError\n\n    @property\n    def is_valid(self):\n        \"\"\"Test if service is valid\n\n        This method is only available in the systemd implementation,\n        it will raise ``NotImplementedError`` in other implementations.\n        \"\"\"\n        raise NotImplementedError\n\n    @property\n    def is_masked(self):\n        \"\"\"Test if service is masked\n\n        This method is only available in the systemd implementation,\n        it will raise ``NotImplementedError`` in other implementations.\n        \"\"\"\n        raise NotImplementedError\n\n    @functools.cached_property\n    def systemd_properties(self):\n        \"\"\"Properties of the service (unit).\n\n        Return service properties as a `dict`,\n        empty properties are not returned.\n\n        >>> ntp = host.service(\"ntp\")\n        >>> ntp.systemd_properties[\"FragmentPath\"]\n        '/lib/systemd/system/ntp.service'\n\n        This method is only available in the systemd implementation,\n        it will raise ``NotImplementedError`` in other implementations.\n\n        Note: based on `systemctl show`_\n\n        .. _systemctl show: https://man7.org/linux/man-pages/man1/systemctl.1.html\n        \"\"\"\n        raise NotImplementedError\n\n    @classmethod\n    def get_module_class(cls, host):\n        if host.system_info.type == \"linux\":\n            if host.file(\"/run/systemd/system/\").is_directory or (\n                host.exists(\"systemctl\")\n                and \"systemd\" in host.file(\"/sbin/init\").linked_to\n            ):\n                return SystemdService\n            if (\n                host.exists(\"initctl\")\n                and host.exists(\"status\")\n                and host.file(\"/etc/init\").is_directory\n            ):\n                return UpstartService\n            if host.exists(\"rc-service\"):\n                return OpenRCService\n            return SysvService\n        if host.system_info.type == \"freebsd\":\n            return FreeBSDService\n        if host.system_info.type == \"openbsd\":\n            return OpenBSDService\n        if host.system_info.type == \"netbsd\":\n            return NetBSDService\n        if host.system_info.type == \"windows\":\n            return WindowsService\n        raise NotImplementedError\n\n    def __repr__(self):\n        return f\"<service {self.name}>\"\n\n\nclass SysvService(Service):\n    @functools.cached_property\n    def _service_command(self):\n        return self.find_command(\"service\")\n\n    @property\n    def is_running(self):\n        # based on /lib/lsb/init-functions\n        # 0: program running\n        # 1: program is dead and pid file exists\n        # 3: not running and pid file does not exists\n        # 4: Unable to determine status\n        # 8: starting (alpine specific ?)\n        return (\n            self.run_expect(\n                [0, 1, 3, 8], \"%s %s status\", self._service_command, self.name\n            ).rc\n            == 0\n        )\n\n    @property\n    def is_enabled(self):\n        return bool(\n            self.check_output(\n                \"find -L /etc/rc?.d/ -name %s\",\n                \"S??\" + self.name,\n            )\n        )\n\n\nclass SystemdService(SysvService):\n    suffix_list = [\n        \"service\",\n        \"socket\",\n        \"device\",\n        \"mount\",\n        \"automount\",\n        \"swap\",\n        \"target\",\n        \"path\",\n        \"timer\",\n        \"slice\",\n        \"scope\",\n    ]\n    \"\"\"\n    List of valid suffixes for systemd unit files\n\n    See systemd.unit(5) for more details\n    \"\"\"\n\n    def _has_systemd_suffix(self):\n        \"\"\"\n        Check if service name has a known systemd unit suffix\n        \"\"\"\n        unit_suffix = self.name.split(\".\")[-1]\n        return unit_suffix in self.suffix_list\n\n    @property\n    def exists(self):\n        cmd = self.run_test('systemctl list-unit-files | grep -q \"^%s\"', self.name)\n        return cmd.rc == 0\n\n    @property\n    def is_running(self):\n        # based on https://man7.org/linux/man-pages/man1/systemctl.1.html\n        # 0: program running\n        # 1: program is dead and pid file exists\n        # 3: not running and pid file does not exists\n        # 4: Unable to determine status (no such unit)\n        out = self.run_expect([0, 1, 3, 4], \"systemctl is-active %s\", self.name)\n        if out.rc == 1:\n            # Failed to connect to bus: No such file or directory\n            return super().is_running\n        return out.rc == 0\n\n    @property\n    def is_enabled(self):\n        cmd = self.run_test(\"systemctl is-enabled %s\", self.name)\n        if cmd.rc == 0:\n            return True\n        if cmd.stdout.strip() == \"disabled\":\n            return False\n        # Fallback on SysV - only for non-systemd units\n        # https://bugs.debian.org/cgi-bin/bugreport.cgi?bug=760616\n        if not self._has_systemd_suffix():\n            return super().is_enabled\n        raise RuntimeError(\n            f\"Unable to determine state of {self.name}. Does this service exist?\"\n        )\n\n    @property\n    def is_valid(self):\n        # systemd-analyze requires a full unit name.\n        name = self.name if self._has_systemd_suffix() else f\"{self.name}.service\"\n        cmd = self.run(\"systemd-analyze verify %s\", name)\n        # A bad unit file still returns a rc of 0, so check the\n        # stdout for anything.  Nothing means no warns/errors.\n        # Docs at https://www.freedesktop.org/software/systemd/man/systemd\n        # -analyze.html#Examples%20for%20verify\n        assert (cmd.stdout, cmd.stderr) == (\"\", \"\")\n        return True\n\n    @property\n    def is_masked(self):\n        cmd = self.run_test(\"systemctl is-enabled %s\", self.name)\n        return cmd.stdout.strip() == \"masked\"\n\n    @functools.cached_property\n    def systemd_properties(self):\n        out = self.check_output(\"systemctl show %s\", self.name)\n        out_d = {}\n        if out:\n            # maxsplit is required because values can contain `=`\n            out_d = dict(\n                map(lambda pair: pair.split(\"=\", maxsplit=1), out.splitlines())\n            )\n        return out_d\n\n\nclass UpstartService(SysvService):\n    @property\n    def exists(self):\n        return self._host.file(f\"/etc/init/{self.name}.conf\").exists\n\n    @property\n    def is_enabled(self):\n        if (\n            self.run(\n                \"grep -q '^start on' /etc/init/%s.conf\",\n                self.name,\n            ).rc\n            == 0\n            and self.run(\n                \"grep -q '^manual' /etc/init/%s.override\",\n                self.name,\n            ).rc\n            != 0\n        ):\n            return True\n        # Fallback on SysV\n        return super().is_enabled\n\n    @property\n    def is_running(self):\n        cmd = self.run_test(\"status %s\", self.name)\n        if cmd.rc == 0 and len(cmd.stdout.split()) > 1:\n            return \"running\" in cmd.stdout.split()[1]\n        return super().is_running\n\n\nclass OpenRCService(SysvService):\n    @functools.cached_property\n    def _service_command(self):\n        return self.find_command(\"rc-service\")\n\n    @property\n    def is_enabled(self):\n        return bool(\n            self.check_output(\n                \"find /etc/runlevels/ -name %s\",\n                self.name,\n            )\n        )\n\n\nclass FreeBSDService(Service):\n    @property\n    def exists(self):\n        return self._host.file(f\"/etc/rc.d/{self.name}\").exists\n\n    @property\n    def is_running(self):\n        return self.run_test(\"service %s onestatus\", self.name).rc == 0\n\n    @property\n    def is_enabled(self):\n        # Return list of enabled services like\n        # /etc/rc.d/sshd\n        # /etc/rc.d/sendmail\n        for path in self.check_output(\"service -e\").splitlines():\n            if path and path.rsplit(\"/\", 1)[1] == self.name:\n                return True\n        return False\n\n\nclass OpenBSDService(Service):\n    @property\n    def exists(self):\n        return self._host.file(f\"/etc/rc.d/{self.name}\").exists\n\n    @property\n    def is_running(self):\n        return self.run_test(\"/etc/rc.d/%s check\", self.name).rc == 0\n\n    @property\n    def is_enabled(self):\n        if self.name in self.check_output(\"rcctl ls on\").splitlines():\n            return True\n        if self.name in self.check_output(\"rcctl ls off\").splitlines():\n            return False\n        raise RuntimeError(\n            f\"Unable to determine state of {self.name}. Does this service exist?\"\n        )\n\n\nclass NetBSDService(Service):\n    @property\n    def exists(self):\n        return self._host.file(f\"/etc/rc.d/{self.name}\").exists\n\n    @property\n    def is_running(self):\n        return self.run_test(\"/etc/rc.d/%s onestatus\", self.name).rc == 0\n\n    @property\n    def is_enabled(self):\n        raise NotImplementedError\n\n\nclass WindowsService(Service):\n    @property\n    def exists(self):\n        out = self.check_output(\n            f\"Get-Service -Name {self.name} -ErrorAction SilentlyContinue\"\n        )\n        return self.name in out\n\n    @property\n    def is_running(self):\n        return (\n            self.check_output(\n                \"Get-Service '%s' | Select -ExpandProperty Status\",\n                self.name,\n            )\n            == \"Running\"\n        )\n\n    @property\n    def is_enabled(self):\n        return (\n            self.check_output(\n                \"Get-Service '%s' | Select -ExpandProperty StartType\",\n                self.name,\n            )\n            == \"Automatic\"\n        )\n", "metadata": {"source": "github.com/pytest-dev/pytest-testinfra", "file_path": "testinfra/modules/service.py", "language": "python", "complexity": "complex", "star_count": 2433, "quality_score": 0.78, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": true, "has_type_hints": true, "has_testing": true, "has_logging": false, "function_count": 36, "class_count": 9, "line_count": 366}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "testing_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create comprehensive test suite with fixtures and mocks\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom typing import NoReturn\n\nfrom testinfra.modules.base import Module\n\nSTATUS = [\n    \"STOPPED\",\n    \"STARTING\",\n    \"RUNNING\",\n    \"BACKOFF\",\n    \"STOPPING\",\n    \"EXITED\",\n    \"FATAL\",\n    \"UNKNOWN\",\n]\n\n\ndef supervisor_not_running() -> NoReturn:\n    raise RuntimeError(\"Cannot get supervisor status. Is supervisor running ?\")\n\n\nclass Supervisor(Module):\n    \"\"\"Test supervisor managed services\n\n    >>> gunicorn = host.supervisor(\"gunicorn\")\n    >>> gunicorn.status\n    'RUNNING'\n    >>> gunicorn.is_running\n    True\n    >>> gunicorn.pid\n    4242\n\n    The path where supervisorctl and its configuration file reside can be specified.\n\n    >>> gunicorn = host.supervisor(\"gunicorn\", \"/usr/bin/supervisorctl\", \"/etc/supervisor/supervisord.conf\")\n    >>> gunicorn.status\n    'RUNNING'\n    \"\"\"\n\n    def __init__(\n        self,\n        name,\n        supervisorctl_path=\"supervisorctl\",\n        supervisorctl_conf=None,\n        _attrs_cache=None,\n    ):\n        self.name = name\n        self.supervisorctl_path = supervisorctl_path\n        self.supervisorctl_conf = supervisorctl_conf\n        self._attrs_cache = _attrs_cache\n        super().__init__()\n\n    @staticmethod\n    def _parse_status(line):\n        splitted = line.split()\n        name = splitted[0]\n        status = splitted[1]\n        # Some old supervisorctl versions exit status is 0 even if it cannot\n        # connect to supervisord socket and output the error to stdout. So we\n        # check that parsed status is a known status.\n        if status not in STATUS:\n            supervisor_not_running()\n        pid = int(splitted[3].removesuffix(\",\")) if status == \"RUNNING\" else None\n        return {\"name\": name, \"status\": status, \"pid\": pid}\n\n    @property\n    def _attrs(self):\n        if self._attrs_cache is None:\n            if self.supervisorctl_conf:\n                out = self.run_expect(\n                    [0, 3, 4],\n                    \"%s -c %s status %s\",\n                    self.supervisorctl_path,\n                    self.supervisorctl_conf,\n                    self.name,\n                )\n            else:\n                out = self.run_expect(\n                    [0, 3, 4], \"%s status %s\", self.supervisorctl_path, self.name\n                )\n            if out.rc == 4:\n                supervisor_not_running()\n            line = out.stdout.rstrip(\"\\r\\n\")\n            attrs = self._parse_status(line)\n            assert attrs[\"name\"] == self.name\n            self._attrs_cache = attrs\n        return self._attrs_cache\n\n    @property\n    def is_running(self):\n        \"\"\"Return True if managed service is in status RUNNING\"\"\"\n        return self.status == \"RUNNING\"\n\n    @property\n    def status(self):\n        \"\"\"Return the status of the managed service\n\n        Status can be STOPPED, STARTING, RUNNING, BACKOFF, STOPPING,\n        EXITED, FATAL, UNKNOWN.\n\n        See http://supervisord.org/subprocess.html#process-states\n        \"\"\"\n        return self._attrs[\"status\"]\n\n    @property\n    def pid(self):\n        \"\"\"Return the pid (as int) of the managed service\"\"\"\n        return self._attrs[\"pid\"]\n\n    @classmethod\n    def get_services(\n        cls,\n        supervisorctl_path=\"supervisorctl\",\n        supervisorctl_conf=None,\n    ):\n        \"\"\"Get a list of services running under supervisor\n\n        >>> host.supervisor.get_services()\n        [<Supervisor(name=\"gunicorn\", status=\"RUNNING\", pid=4232)>\n         <Supervisor(name=\"celery\", status=\"FATAL\", pid=None)>]\n\n        The path where supervisorctl and its configuration file reside can be specified.\n\n        >>> host.supervisor.get_services(\"/usr/bin/supervisorctl\", \"/etc/supervisor/supervisord.conf\")\n        [<Supervisor(name=\"gunicorn\", status=\"RUNNING\", pid=4232)>\n         <Supervisor(name=\"celery\", status=\"FATAL\", pid=None)>]\n        \"\"\"\n        services = []\n        if supervisorctl_conf:\n            out = cls.check_output(\n                \"%s -c %s status\", supervisorctl_path, supervisorctl_conf\n            )\n        else:\n            out = cls.check_output(\"%s status\", supervisorctl_path)\n        for line in out.splitlines():\n            attrs = cls._parse_status(line)\n            service = cls(\n                attrs[\"name\"],\n                supervisorctl_path=supervisorctl_path,\n                supervisorctl_conf=supervisorctl_conf,\n                _attrs_cache=attrs,\n            )\n            services.append(service)\n        return services\n\n    def __repr__(self):\n        return f\"<Supervisor(name={self.name}, status={self.status}, pid={self.pid})>\"\n", "metadata": {"source": "github.com/pytest-dev/pytest-testinfra", "file_path": "testinfra/modules/supervisor.py", "language": "python", "complexity": "complex", "star_count": 2433, "quality_score": 0.8099999999999999, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": true, "has_type_hints": true, "has_testing": true, "has_logging": false, "function_count": 9, "class_count": 1, "line_count": 159}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "testing_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create comprehensive test suite with fixtures and mocks\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import pytest\n\n\n@pytest.fixture\ndef context() -> dict[str, str]:\n    \"\"\"Creates default prompt values.\"\"\"\n    return {\n        'project_name': 'test-project',\n        'project_verbose_name': 'Test Project',\n        'project_domain': 'myapp.com',\n        'organization': 'wemake.services',\n    }\n", "metadata": {"source": "github.com/wemake-services/wemake-django-template", "file_path": "tests/conftest.py", "language": "python", "complexity": "simple", "star_count": 2112, "quality_score": 0.69, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": true, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 1, "class_count": 0, "line_count": 13}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "testing_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create comprehensive test suite with fixtures and mocks with pytest testing framework\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"\nDoes some basic tests on the generated project.\n\nAlmost completely taken from (you guys rock!):\nhttps://github.com/pydanny/cookiecutter-django/blob/master/tests\n\"\"\"\n\nimport os\nimport re\nfrom pathlib import Path\nfrom typing import Final\n\nimport pytest\nimport tomli\nfrom binaryornot.check import is_binary\nfrom cookiecutter.exceptions import FailedHookException\nfrom pytest_cookies.plugin import Cookies\n\n_RE_OBJ: Final = re.compile(r'{{(\\s?cookiecutter)[.](.*?)}}')\n\n\ndef _build_files_list(root_dir: Path) -> list[Path]:\n    \"\"\"Build a list containing absolute paths to the generated files.\"\"\"\n    return [\n        Path(dirpath) / file_path\n        for dirpath, _subdirs, files in os.walk(str(root_dir))\n        for file_path in files\n    ]\n\n\ndef assert_variables_replaced(paths: list[Path]) -> None:\n    \"\"\"Method to check that all paths have correct substitutions.\"\"\"\n    assert paths, 'No files are generated'\n\n    for path in paths:\n        if is_binary(str(path)):\n            continue\n\n        file_contents = path.read_text()\n\n        match = _RE_OBJ.search(file_contents)\n        msg = 'cookiecutter variable not replaced in {0} at {1}'\n\n        # Assert that no match is found:\n        assert match is None, msg.format(path, match.start())\n\n\ndef test_with_default_configuration(\n    cookies: Cookies,\n    context: dict[str, str],\n) -> None:\n    \"\"\"Tests project structure with default prompt values.\"\"\"\n    baked_project = cookies.bake(extra_context=context)\n\n    assert baked_project.exit_code == 0\n    assert baked_project.exception is None\n    assert baked_project.project_path.name == context['project_name']\n    assert baked_project.project_path.is_dir()\n\n\ndef test_variables_replaced(\n    cookies: Cookies,\n    context: dict[str, str],\n) -> None:\n    \"\"\"Ensures that all variables are replaced inside project files.\"\"\"\n    baked_project = cookies.bake(extra_context=context)\n    paths = _build_files_list(baked_project.project_path)\n\n    assert_variables_replaced(paths)\n\n\ndef test_pyproject_toml(\n    cookies: Cookies,\n    context: dict[str, str],\n) -> None:\n    \"\"\"Ensures that all variables are replaced inside project files.\"\"\"\n    baked_project = cookies.bake(extra_context=context)\n    path = baked_project.project_path / 'pyproject.toml'\n\n    pyproject = tomli.loads(path.read_text())\n    project = pyproject['project']\n    poetry = pyproject['tool']['poetry']\n\n    assert project['name'] == context['project_name']\n    assert project['description'] == context['project_verbose_name']\n    assert poetry\n\n\n@pytest.mark.parametrize(\n    ('prompt', 'entered_value'),\n    [\n        ('project_name', 'myProject'),\n        ('project_name', 'my_project'),\n        ('project_name', '43project'),\n        ('project_name', '_test'),\n        ('project_name', 'test_'),\n        ('project_name', '1_test'),\n        ('project_name', 'test@'),\n        ('project_name', '0123456'),\n        ('project_domain', 'https://wemake.services'),\n        ('project_domain', 'wemake.services?search=python'),\n        ('project_domain', ''),\n    ],\n)\ndef test_validators_work(\n    prompt: str,\n    entered_value: str,\n    cookies: Cookies,\n    context: dict[str, str],\n) -> None:\n    \"\"\"Ensures that project can not be created with invalid name.\"\"\"\n    context.update({prompt: entered_value})\n    baked_project = cookies.bake(extra_context=context)\n\n    assert isinstance(baked_project.exception, FailedHookException)\n    assert baked_project.exit_code == -1\n", "metadata": {"source": "github.com/wemake-services/wemake-django-template", "file_path": "tests/test_project_generation.py", "language": "python", "complexity": "medium", "star_count": 2112, "quality_score": 0.8099999999999999, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": true, "has_type_hints": true, "has_testing": true, "has_logging": false, "function_count": 6, "class_count": 0, "line_count": 117}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "testing_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create comprehensive test suite with fixtures and mocks\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# wemake-django-template documentation build configuration file, created by\n# sphinx-quickstart on Sat Sep 30 12:42:34 2017.\n#\n# This file is execfile()d with the current directory set to its\n# containing dir.\n#\n# Note that not all possible configuration values are present in this\n# autogenerated file.\n#\n# All configuration values have a default; values that are commented out\n# serve to show the default.\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n\nimport os\nimport sys\nfrom pathlib import Path\nfrom typing import cast\n\nimport django\nimport tomli\n\n# We need `server` to be importable from here:\n_ROOT = Path('..').resolve(strict=True)\nsys.path.insert(0, str(_ROOT))\n\n# Django setup, all deps must be present to succeed:\nos.environ.setdefault('DJANGO_ENV', 'production')\nos.environ.setdefault('DJANGO_SETTINGS_MODULE', 'server.settings')\ndjango.setup()\n\n\n# -- Project information -----------------------------------------------------\n\n\ndef _get_project_meta() -> dict[str, str]:  # lying about return type\n    pyproject = _ROOT / 'pyproject.toml'\n    return cast(\n        dict[str, str],\n        tomli.loads(pyproject.read_text())['project'],\n    )\n\n\npkg_meta = _get_project_meta()\nproject = pkg_meta['name']\n\n# The short X.Y version\nversion = pkg_meta['version']\n# The full version, including alpha/beta/rc tags\nrelease = version\n\n\n# -- General configuration ------------------------------------------------\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named 'sphinx.ext.*') or your custom\n# ones.\nextensions = [\n    'sphinx.ext.autodoc',\n    'sphinx.ext.doctest',\n    'sphinx.ext.todo',\n    'sphinx.ext.coverage',\n    'sphinx.ext.viewcode',\n    'sphinx.ext.githubpages',\n    'sphinx.ext.napoleon',\n    # https://github.com/executablebooks/MyST-Parser\n    'myst_parser',\n    # 3rd party, order matters:\n    # https://github.com/wemake-services/wemake-django-template/issues/159\n    'sphinx_autodoc_typehints',\n]\n\n# If true, Sphinx will warn about all references\n# where the target cannot be found. Default is `False``.\n# You can activate this mode temporarily using the `-n` command-line switch.\nnitpicky = True\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = ['_templates']\n\n# The suffix(es) of source filenames.\n# You can specify multiple suffix as a list of string:\nsource_suffix = ['.rst', '.md']\n\n# The master toctree document.\nmaster_doc = 'index'\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n#\n# This is also used if you do content translation via gettext catalogs.\n# Usually you set \"language\" from the command line for these cases.\nlanguage = 'en'\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This patterns also effect to html_static_path and html_extra_path\nexclude_patterns = ['_build', 'Thumbs.db', '.DS_Store', 'README.md']\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = 'sphinx'\n\n# If true, `todo` and `todoList` produce output, else they produce nothing.\ntodo_include_todos = True\n\n\n# -- Options for HTML output ----------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\nhtml_theme = 'furo'\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named \"default.css\" will overwrite the builtin \"default.css\".\nhtml_static_path = ['_static']\n", "metadata": {"source": "github.com/wemake-services/wemake-django-template", "file_path": "{{cookiecutter.project_name}}/docs/conf.py", "language": "python", "complexity": "medium", "star_count": 2112, "quality_score": 0.75, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": false, "has_logging": true, "function_count": 1, "class_count": 0, "line_count": 119}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "testing_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create comprehensive test suite with fixtures and mocks with SQLAlchemy/Django ORM patterns\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import textwrap\nfrom typing import Final, final, override\n\nfrom django.db import models\n\n#: That's how constants should be defined.\n_POST_TITLE_MAX_LENGTH: Final = 80\n\n\n@final\nclass BlogPost(models.Model):\n    \"\"\"\n    This model is used just as an example.\n\n    With it we show how one can:\n    - Use fixtures and factories\n    - Use migrations testing\n\n    \"\"\"\n\n    title = models.CharField(max_length=_POST_TITLE_MAX_LENGTH)\n    body = models.TextField()\n\n    created_at = models.DateTimeField(auto_now_add=True)\n    updated_at = models.DateTimeField(auto_now=True)\n\n    class Meta:\n        verbose_name = 'BlogPost'  # You can probably use `gettext` for this\n        verbose_name_plural = 'BlogPosts'\n\n    @override\n    def __str__(self) -> str:\n        \"\"\"All django models should have this method.\"\"\"\n        return textwrap.wrap(self.title, _POST_TITLE_MAX_LENGTH // 4)[0]\n", "metadata": {"source": "github.com/wemake-services/wemake-django-template", "file_path": "{{cookiecutter.project_name}}/server/apps/main/models.py", "language": "python", "complexity": "simple", "star_count": 2112, "quality_score": 0.75, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": true, "has_type_hints": true, "has_testing": false, "has_logging": true, "function_count": 1, "class_count": 2, "line_count": 35}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "testing_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create comprehensive test suite with fixtures and mocks\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"\nThis file contains all the settings that defines the development server.\n\nSECURITY WARNING: don't run with debug turned on in production!\n\"\"\"\n\nfrom __future__ import annotations\n\nimport logging\nimport socket\nfrom typing import TYPE_CHECKING\n\nfrom server.settings.components import config\nfrom server.settings.components.common import (\n    DATABASES,\n    INSTALLED_APPS,\n    MIDDLEWARE,\n)\nfrom server.settings.components.csp import (\n    CSP_CONNECT_SRC,\n    CSP_IMG_SRC,\n    CSP_SCRIPT_SRC,\n)\n\nif TYPE_CHECKING:\n    from django.http import HttpRequest\n\n# Setting the development status:\n\nDEBUG = True\n\nALLOWED_HOSTS = [\n    config('DOMAIN_NAME'),\n    'localhost',\n    '0.0.0.0',  # noqa: S104\n    '127.0.0.1',\n    '[::1]',\n]\n\n\n# Installed apps for development only:\n\nINSTALLED_APPS += (\n    # Better debug:\n    'debug_toolbar',\n    'zeal',\n    # Linting migrations:\n    'django_migration_linter',\n    # django-test-migrations:\n    'django_test_migrations.contrib.django_checks.AutoNames',\n    # This check might be useful in production as well,\n    # so it might be a good idea to move `django-test-migrations`\n    # to prod dependencies and use this check in the main `settings.py`.\n    # This will check that your database is configured properly,\n    # when you run `python manage.py check` before deploy.\n    'django_test_migrations.contrib.django_checks.DatabaseConfiguration',\n    # django-extra-checks:\n    'extra_checks',\n    # django-query-counter:\n    'query_counter',\n    # django-drifter:\n    'drifter',\n)\n\n\n# Django debug toolbar:\n# https://django-debug-toolbar.readthedocs.io\n\nMIDDLEWARE += (\n    'debug_toolbar.middleware.DebugToolbarMiddleware',\n    # https://github.com/conformist-mw/django-query-counter\n    # Prints how many queries were executed, useful for the APIs.\n    'query_counter.middleware.DjangoQueryCounterMiddleware',\n)\n\n# https://django-debug-toolbar.readthedocs.io/en/stable/installation.html#configure-internal-ips\ntry:  # This might fail on some OS\n    INTERNAL_IPS = [\n        '{}.1'.format(ip[: ip.rfind('.')])\n        for ip in socket.gethostbyname_ex(socket.gethostname())[2]\n    ]\nexcept OSError:  # pragma: no cover\n    INTERNAL_IPS = []\nINTERNAL_IPS += ['127.0.0.1', '10.0.2.2']\n\n\ndef _custom_show_toolbar(request: HttpRequest) -> bool:\n    \"\"\"Only show the debug toolbar to users with the superuser flag.\"\"\"\n    return DEBUG and request.user.is_superuser\n\n\nDEBUG_TOOLBAR_CONFIG = {\n    'SHOW_TOOLBAR_CALLBACK': (\n        'server.settings.environments.development._custom_show_toolbar'\n    ),\n}\n\n# This will make debug toolbar to work with django-csp,\n# since `ddt` loads some scripts from `ajax.googleapis.com`:\nCSP_SCRIPT_SRC += ('ajax.googleapis.com',)\nCSP_IMG_SRC += ('data:',)\nCSP_CONNECT_SRC += (\"'self'\",)\n\n\n# django-zeal\n# https://github.com/taobojlen/django-zeal\n\n# Should be the first in line:\nMIDDLEWARE = ('zeal.middleware.zeal_middleware', *MIDDLEWARE)\n\n# Logging N+1 requests:\nZEAL_RAISE = True  # comment out if you want to allow N+1 requests\nZEAL_SHOW_ALL_CALLERS = True\nZEAL_LOGGER = logging.getLogger('django')\nZEAL_ALLOWLIST = [\n    {'model': 'admin.*'},\n]\n\n\n# django-test-migrations\n# https://github.com/wemake-services/django-test-migrations\n\n# Set of badly named migrations to ignore:\nDTM_IGNORED_MIGRATIONS = frozenset((('axes', '*'),))\n\n\n# django-migration-linter\n# https://github.com/3YOURMIND/django-migration-linter\n\nMIGRATION_LINTER_OPTIONS = {\n    'exclude_apps': ['axes'],\n    'exclude_migration_tests': ['CREATE_INDEX', 'CREATE_INDEX_EXCLUSIVE'],\n    'warnings_as_errors': True,\n}\n\n\n# django-extra-checks\n# https://github.com/kalekseev/django-extra-checks\n\nEXTRA_CHECKS = {\n    'checks': [\n        # Forbid `unique_together`:\n        'no-unique-together',\n        # Each model must be registered in admin:\n        'model-admin',\n        # FileField/ImageField must have non empty `upload_to` argument:\n        'field-file-upload-to',\n        # Text fields shouldn't use `null=True`:\n        'field-text-null',\n        # Don't pass `null=False` to model fields (this is django default)\n        'field-null',\n        # ForeignKey fields must specify db_index explicitly if used in\n        # other indexes:\n        {'id': 'field-foreign-key-db-index', 'when': 'indexes'},\n        # If field nullable `(null=True)`,\n        # then default=None argument is redundant and should be removed:\n        'field-default-null',\n        # Fields with choices must have companion CheckConstraint\n        # to enforce choices on database level\n        'field-choices-constraint',\n    ],\n}\n\n# Disable persistent DB connections\n# https://docs.djangoproject.com/en/5.2/ref/databases/#caveats\nDATABASES['default']['CONN_MAX_AGE'] = 0\n", "metadata": {"source": "github.com/wemake-services/wemake-django-template", "file_path": "{{cookiecutter.project_name}}/server/settings/environments/development.py", "language": "python", "complexity": "complex", "star_count": 2112, "quality_score": 0.87, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": true, "has_documentation": true, "has_type_hints": true, "has_testing": true, "has_logging": true, "function_count": 1, "class_count": 0, "line_count": 167}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "testing_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate configuration for Create comprehensive test suite with fixtures and mocks\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"\nThis module is used to provide configuration, fixtures, and plugins for pytest.\n\nIt may be also used for extending doctest's context:\n1. https://docs.python.org/3/library/doctest.html\n2. https://docs.pytest.org/en/latest/doctest.html\n\"\"\"\n\npytest_plugins = [\n    # Should be the first custom one:\n    'plugins.django_settings',\n    # TODO: add your own plugins here!\n    'plugins.main.main_templates',\n]\n", "metadata": {"source": "github.com/wemake-services/wemake-django-template", "file_path": "{{cookiecutter.project_name}}/tests/conftest.py", "language": "python", "complexity": "simple", "star_count": 2112, "quality_score": 0.72, "features": {"has_classes": false, "has_functions": false, "has_async": false, "has_decorators": false, "has_imports": false, "has_error_handling": false, "has_documentation": true, "has_type_hints": true, "has_testing": true, "has_logging": false, "function_count": 0, "class_count": 0, "line_count": 15}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "testing_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create comprehensive test suite with fixtures and mocks\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import pytest\nfrom django.conf import LazySettings\n\n\n@pytest.fixture(autouse=True)\ndef _media_root(\n    settings: LazySettings,\n    tmpdir_factory: pytest.TempPathFactory,\n) -> None:\n    \"\"\"Forces django to save media files into temp folder.\"\"\"\n    settings.MEDIA_ROOT = tmpdir_factory.mktemp('media', numbered=True)\n\n\n@pytest.fixture(autouse=True)\ndef _password_hashers(settings: LazySettings) -> None:\n    \"\"\"Forces django to use fast password hashers for tests.\"\"\"\n    settings.PASSWORD_HASHERS = [\n        'django.contrib.auth.hashers.MD5PasswordHasher',\n    ]\n\n\n@pytest.fixture(autouse=True)\ndef _auth_backends(settings: LazySettings) -> None:\n    \"\"\"Deactivates security backend from Axes app.\"\"\"\n    settings.AUTHENTICATION_BACKENDS = (\n        'django.contrib.auth.backends.ModelBackend',\n    )\n\n\n@pytest.fixture(autouse=True)\ndef _debug(settings: LazySettings) -> None:\n    \"\"\"Sets proper DEBUG and TEMPLATE debug mode for coverage.\"\"\"\n    settings.DEBUG = False\n    for template in settings.TEMPLATES:\n        template['OPTIONS']['debug'] = True\n", "metadata": {"source": "github.com/wemake-services/wemake-django-template", "file_path": "{{cookiecutter.project_name}}/tests/plugins/django_settings.py", "language": "python", "complexity": "simple", "star_count": 2112, "quality_score": 0.75, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": true, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 4, "class_count": 0, "line_count": 36}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "testing_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create comprehensive test suite with fixtures and mocks\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import pytest\n\n\n@pytest.fixture\ndef main_heading() -> str:\n    \"\"\"An example fixture containing some html fragment.\"\"\"\n    return 'data-testid=\"wemake-django-template\"'\n", "metadata": {"source": "github.com/wemake-services/wemake-django-template", "file_path": "{{cookiecutter.project_name}}/tests/plugins/main/main_templates.py", "language": "python", "complexity": "simple", "star_count": 2112, "quality_score": 0.48999999999999994, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": true, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 1, "class_count": 0, "line_count": 8}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "testing_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create comprehensive test suite with fixtures and mocks with pytest testing framework\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from typing import final\n\nfrom hypothesis import given\nfrom hypothesis.extra import django\n\nfrom server.apps.main.models import BlogPost\n\n\n@final\nclass TestBlogPost(django.TestCase):\n    \"\"\"This is a property-based test that ensures model correctness.\"\"\"\n\n    @given(django.from_model(BlogPost))\n    def test_model_properties(self, instance: BlogPost) -> None:\n        \"\"\"Tests that instance can be saved and has correct representation.\"\"\"\n        instance.save()\n\n        assert instance.id > 0\n        assert len(str(instance)) <= 20\n", "metadata": {"source": "github.com/wemake-services/wemake-django-template", "file_path": "{{cookiecutter.project_name}}/tests/test_apps/test_main/test_models/test_blog_post_model.py", "language": "python", "complexity": "simple", "star_count": 2112, "quality_score": 0.8099999999999999, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": true, "has_type_hints": true, "has_testing": true, "has_logging": true, "function_count": 1, "class_count": 1, "line_count": 20}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "testing_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create comprehensive test suite with fixtures and mocks with pytest testing framework\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import pytest\nfrom django_test_migrations.migrator import Migrator\n\nfrom server.apps.main.urls import app_name\n\n\ndef test_initial0001(migrator: Migrator) -> None:\n    \"\"\"Tests the initial migration forward application.\"\"\"\n    old_state = migrator.apply_initial_migration((app_name, None))\n    with pytest.raises(LookupError):\n        # This model does not exist before this migration:\n        old_state.apps.get_model(app_name, 'BlogPost')\n\n    new_state = migrator.apply_tested_migration((app_name, '0001_initial'))\n    model = new_state.apps.get_model(app_name, 'BlogPost')\n\n    assert model.objects.create(title='test', body='some body')\n", "metadata": {"source": "github.com/wemake-services/wemake-django-template", "file_path": "{{cookiecutter.project_name}}/tests/test_apps/test_main/test_models/test_migrations/test_blog_post_migrations.py", "language": "python", "complexity": "simple", "star_count": 2112, "quality_score": 0.78, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": true, "has_type_hints": true, "has_testing": true, "has_logging": true, "function_count": 1, "class_count": 0, "line_count": 18}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "testing_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create comprehensive test suite with fixtures and mocks with pytest testing framework\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from http import HTTPStatus\n\nfrom django.test import Client\nfrom django.urls import reverse\n\n\ndef test_main_page(client: Client, main_heading: str) -> None:\n    \"\"\"Ensures that main page works.\"\"\"\n    response = client.get(reverse('index'))\n\n    assert response.status_code == HTTPStatus.OK\n    assert main_heading in str(response.content)\n\n\ndef test_hello_page(client: Client, main_heading: str) -> None:\n    \"\"\"Ensures that hello page works.\"\"\"\n    response = client.get(reverse('main:hello'))\n\n    assert response.status_code == HTTPStatus.OK\n    assert main_heading in str(response.content)\n", "metadata": {"source": "github.com/wemake-services/wemake-django-template", "file_path": "{{cookiecutter.project_name}}/tests/test_apps/test_main/test_views/test_index_view.py", "language": "python", "complexity": "simple", "star_count": 2112, "quality_score": 0.78, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": true, "has_type_hints": true, "has_testing": true, "has_logging": false, "function_count": 2, "class_count": 0, "line_count": 21}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "testing_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create comprehensive test suite with fixtures and mocks with pytest testing framework\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from http import HTTPStatus\n\nimport pytest\nfrom axes.models import AccessAttempt, AccessFailureLog, AccessLog\nfrom django.contrib.admin import AdminSite, ModelAdmin\nfrom django.contrib.admin.sites import all_sites\nfrom django.db.models import Model\nfrom django.test import Client\nfrom django.urls import reverse\n\n# Models that should have restricted (FORBIDDEN) admin add pages\n_RESTRICTED_ADMIN_ADD_MODELS = frozenset((\n    AccessAttempt,\n    AccessLog,\n    AccessFailureLog,\n))\n\n# Creates a list of tuples containing all registered admin sites,\n# their associated models, and corresponding model admin classes\n# from the admin site's registry.\n_MODEL_ADMIN_PARAMS = tuple(\n    (site, model, model_admin)\n    for site in all_sites\n    for model, model_admin in site._registry.items()  # noqa: SLF001\n)\n\n\ndef _make_url(site: AdminSite, model: type[Model], page: str) -> str:\n    \"\"\"Generates a URL for the given admin site, model, and page.\"\"\"\n    app_label = model._meta.app_label  # noqa: SLF001\n    model_name = model._meta.model_name  # noqa: SLF001\n    return reverse(f'{site.name}:{app_label}_{model_name}_{page}')  # noqa: WPS221\n\n\n@pytest.mark.django_db\n@pytest.mark.parametrize(\n    ('site', 'model', 'model_admin'),\n    _MODEL_ADMIN_PARAMS,\n)\ndef test_admin_changelist(\n    admin_client: Client,\n    site: AdminSite,\n    model: type[Model],\n    model_admin: type[ModelAdmin[Model]],\n) -> None:\n    \"\"\"Ensures that admin changelist pages are accessible.\"\"\"\n    url = _make_url(site, model, 'changelist')\n    response = admin_client.get(url, {'q': 'something'})\n\n    assert response.status_code == HTTPStatus.OK\n\n\n@pytest.mark.django_db\n@pytest.mark.parametrize(\n    ('site', 'model', 'model_admin'),\n    _MODEL_ADMIN_PARAMS,\n)\ndef test_admin_add(\n    admin_client: Client,\n    site: AdminSite,\n    model: type[Model],\n    model_admin: type[ModelAdmin[Model]],\n) -> None:\n    \"\"\"Ensures that admin add pages are accessible or restricted.\"\"\"\n    url = _make_url(site, model, 'add')\n    response = admin_client.get(url)\n    expected_status = (\n        HTTPStatus.FORBIDDEN\n        if model in _RESTRICTED_ADMIN_ADD_MODELS\n        else HTTPStatus.OK\n    )\n\n    assert response.status_code == expected_status\n", "metadata": {"source": "github.com/wemake-services/wemake-django-template", "file_path": "{{cookiecutter.project_name}}/tests/test_server/test_admin.py", "language": "python", "complexity": "simple", "star_count": 2112, "quality_score": 0.78, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": true, "has_type_hints": true, "has_testing": true, "has_logging": true, "function_count": 3, "class_count": 0, "line_count": 74}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "testing_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create comprehensive test suite with fixtures and mocks with pytest testing framework\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import logging\nimport re\nfrom typing import Final\n\nimport pytest\n\n_LOGGING_FORMAT_RE: Final = re.compile(\n    r\"timestamp='.+' level='error' event='Test message' logger='django'\",\n)\n\n\n@pytest.fixture(name='logger')\ndef logger_fixture() -> logging.Logger:\n    \"\"\"Returns the current logger instance.\"\"\"\n    return logging.getLogger('django')\n\n\n@pytest.fixture(autouse=True)\ndef _redact_caplog_handlers(\n    caplog: pytest.LogCaptureFixture,\n    logger: logging.Logger,\n) -> None:\n    \"\"\"Pytest inserts custom formatter, we need to reset it back.\"\"\"\n    caplog.handler.setFormatter(logger.handlers[0].formatter)\n\n\ndef test_logging_format(\n    caplog: pytest.LogCaptureFixture,\n    logger: logging.Logger,\n) -> None:\n    \"\"\"Ensures logging is done correctly.\"\"\"\n    message = 'Test message'\n\n    logger.error(message)\n\n    assert _LOGGING_FORMAT_RE.match(caplog.text)\n", "metadata": {"source": "github.com/wemake-services/wemake-django-template", "file_path": "{{cookiecutter.project_name}}/tests/test_server/test_logging.py", "language": "python", "complexity": "simple", "star_count": 2112, "quality_score": 0.84, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": true, "has_type_hints": true, "has_testing": true, "has_logging": true, "function_count": 3, "class_count": 0, "line_count": 37}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "testing_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create comprehensive test suite with fixtures and mocks with pytest testing framework\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from http import HTTPStatus\n\nimport pytest\nfrom django.test import Client\n\n\n@pytest.mark.django_db\ndef test_health_check(client: Client) -> None:\n    \"\"\"Ensures that health check is accessible.\"\"\"\n    response = client.get('/health/')\n\n    assert response.status_code == HTTPStatus.OK\n\n\ndef test_admin_unauthorized(client: Client) -> None:\n    \"\"\"Ensures that admin panel requires auth.\"\"\"\n    response = client.get('/admin/')\n\n    assert response.status_code == HTTPStatus.FOUND\n\n\ndef test_admin_authorized(admin_client: Client) -> None:\n    \"\"\"Ensures that admin panel is accessible.\"\"\"\n    response = admin_client.get('/admin/')\n\n    assert response.status_code == HTTPStatus.OK\n\n\ndef test_admin_docs_unauthorized(client: Client) -> None:\n    \"\"\"Ensures that admin panel docs requires auth.\"\"\"\n    response = client.get('/admin/doc/')\n\n    assert response.status_code == HTTPStatus.FOUND\n\n\ndef test_admin_docs_authorized(admin_client: Client) -> None:\n    \"\"\"Ensures that admin panel docs are accessible.\"\"\"\n    response = admin_client.get('/admin/doc/')\n\n    assert response.status_code == HTTPStatus.OK\n    assert b'docutils' not in response.content\n\n\n@pytest.mark.parametrize(\n    'page',\n    [\n        '/robots.txt',\n        '/humans.txt',\n    ],\n)\ndef test_specials_txt(client: Client, page: str) -> None:\n    \"\"\"Ensures that special `txt` files are accessible.\"\"\"\n    response = client.get(page)\n\n    assert response.status_code == HTTPStatus.OK\n    assert response.get('Content-Type') == 'text/plain'\n", "metadata": {"source": "github.com/wemake-services/wemake-django-template", "file_path": "{{cookiecutter.project_name}}/tests/test_server/test_urls.py", "language": "python", "complexity": "medium", "star_count": 2112, "quality_score": 0.78, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": true, "has_type_hints": true, "has_testing": true, "has_logging": false, "function_count": 6, "class_count": 0, "line_count": 57}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "testing_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate module integration for Create comprehensive test suite with fixtures and mocks\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Configuration file for the Sphinx documentation builder.\n#\n# This file only contains a selection of the most common options. For a full\n# list see the documentation:\n# https://www.sphinx-doc.org/en/master/usage/configuration.html\n# -- Path setup --------------------------------------------------------------\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n#\n# import os\n# import sys\n# sys.path.insert(0, os.path.abspath('.'))\n# -- Project information -----------------------------------------------------\n\nproject = \"pytest-mock\"\ncopyright = \"2022, Bruno Oliveira\"\nauthor = \"Bruno Oliveira\"\n\n\n# -- General configuration ---------------------------------------------------\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named 'sphinx.ext.*') or your custom\n# ones.\nextensions = [\n    \"sphinx.ext.autodoc\",\n    \"sphinx_copybutton\",\n]\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [\"_templates\"]\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This pattern also affects html_static_path and html_extra_path.\nexclude_patterns = [\"_build\", \"Thumbs.db\", \".DS_Store\"]\n\n\n# -- Options for HTML output -------------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\nhtml_theme = \"furo\"\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named \"default.css\" will overwrite the builtin \"default.css\".\nhtml_static_path = [\"_static\"]\n", "metadata": {"source": "github.com/pytest-dev/pytest-mock", "file_path": "docs/conf.py", "language": "python", "complexity": "simple", "star_count": 1969, "quality_score": 0.6599999999999999, "features": {"has_classes": false, "has_functions": false, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": false, "has_testing": true, "has_logging": false, "function_count": 0, "class_count": 0, "line_count": 51}, "authority_level": "medium", "authentic": true}}
{"domain": "code_generation", "category": "testing_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate module integration for Create comprehensive test suite with fixtures and mocks\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from pytest_mock.plugin import AsyncMockType\nfrom pytest_mock.plugin import MockerFixture\nfrom pytest_mock.plugin import MockType\nfrom pytest_mock.plugin import PytestMockWarning\nfrom pytest_mock.plugin import class_mocker\nfrom pytest_mock.plugin import mocker\nfrom pytest_mock.plugin import module_mocker\nfrom pytest_mock.plugin import package_mocker\nfrom pytest_mock.plugin import pytest_addoption\nfrom pytest_mock.plugin import pytest_configure\nfrom pytest_mock.plugin import session_mocker\n\nMockFixture = MockerFixture  # backward-compatibility only (#204)\n\n__all__ = [\n    \"AsyncMockType\",\n    \"MockerFixture\",\n    \"MockFixture\",\n    \"MockType\",\n    \"PytestMockWarning\",\n    \"pytest_addoption\",\n    \"pytest_configure\",\n    \"session_mocker\",\n    \"package_mocker\",\n    \"module_mocker\",\n    \"class_mocker\",\n    \"mocker\",\n]\n", "metadata": {"source": "github.com/pytest-dev/pytest-mock", "file_path": "src/pytest_mock/__init__.py", "language": "python", "complexity": "simple", "star_count": 1969, "quality_score": 0.72, "features": {"has_classes": false, "has_functions": false, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": false, "has_testing": true, "has_logging": false, "function_count": 0, "class_count": 0, "line_count": 29}, "authority_level": "medium", "authentic": true}}
{"domain": "code_generation", "category": "testing_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create comprehensive test suite with fixtures and mocks\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from typing import Union\n\n_mock_module = None\n\n\ndef get_mock_module(config):\n    \"\"\"\n    Import and return the actual \"mock\" module. By default this is\n    \"unittest.mock\", but the user can force to always use \"mock\" using\n    the mock_use_standalone_module ini option.\n    \"\"\"\n    global _mock_module\n    if _mock_module is None:\n        use_standalone_module = parse_ini_boolean(\n            config.getini(\"mock_use_standalone_module\")\n        )\n        if use_standalone_module:\n            import mock\n\n            _mock_module = mock\n        else:\n            import unittest.mock\n\n            _mock_module = unittest.mock\n\n    return _mock_module\n\n\ndef parse_ini_boolean(value: Union[bool, str]) -> bool:\n    if isinstance(value, bool):\n        return value\n    if value.lower() == \"true\":\n        return True\n    if value.lower() == \"false\":\n        return False\n    raise ValueError(\"unknown string for bool: %r\" % value)\n", "metadata": {"source": "github.com/pytest-dev/pytest-mock", "file_path": "src/pytest_mock/_util.py", "language": "python", "complexity": "simple", "star_count": 1969, "quality_score": 0.75, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": true, "has_type_hints": true, "has_testing": true, "has_logging": false, "function_count": 2, "class_count": 0, "line_count": 37}, "authority_level": "medium", "authentic": true}}
{"domain": "code_generation", "category": "testing_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create comprehensive test suite with fixtures and mocks with asyncio coroutines\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import builtins\nimport functools\nimport inspect\nimport unittest.mock\nimport warnings\nfrom dataclasses import dataclass\nfrom dataclasses import field\nfrom typing import Any\nfrom typing import Callable\nfrom typing import Dict\nfrom typing import Generator\nfrom typing import Iterable\nfrom typing import Iterator\nfrom typing import List\nfrom typing import Mapping\nfrom typing import Optional\nfrom typing import Tuple\nfrom typing import Type\nfrom typing import TypeVar\nfrom typing import Union\nfrom typing import cast\nfrom typing import overload\n\nimport pytest\n\nfrom ._util import get_mock_module\nfrom ._util import parse_ini_boolean\n\n_T = TypeVar(\"_T\")\n\nAsyncMockType = unittest.mock.AsyncMock\nMockType = Union[\n    unittest.mock.MagicMock,\n    unittest.mock.AsyncMock,\n    unittest.mock.NonCallableMagicMock,\n]\n\n\nclass PytestMockWarning(UserWarning):\n    \"\"\"Base class for all warnings emitted by pytest-mock.\"\"\"\n\n\n@dataclass\nclass MockCacheItem:\n    mock: MockType\n    patch: Optional[Any] = None\n\n\n@dataclass\nclass MockCache:\n    \"\"\"\n    Cache MagicMock and Patcher instances so we can undo them later.\n    \"\"\"\n\n    cache: List[MockCacheItem] = field(default_factory=list)\n\n    def _find(self, mock: MockType) -> MockCacheItem:\n        for mock_item in self.cache:\n            if mock_item.mock is mock:\n                return mock_item\n        raise ValueError(\"This mock object is not registered\")\n\n    def add(self, mock: MockType, **kwargs: Any) -> MockCacheItem:\n        self.cache.append(MockCacheItem(mock=mock, **kwargs))\n        return self.cache[-1]\n\n    def remove(self, mock: MockType) -> None:\n        mock_item = self._find(mock)\n        if mock_item.patch:\n            mock_item.patch.stop()\n        self.cache.remove(mock_item)\n\n    def clear(self) -> None:\n        for mock_item in reversed(self.cache):\n            if mock_item.patch is not None:\n                mock_item.patch.stop()\n        self.cache.clear()\n\n    def __iter__(self) -> Iterator[MockCacheItem]:\n        return iter(self.cache)\n\n\nclass MockerFixture:\n    \"\"\"\n    Fixture that provides the same interface to functions in the mock module,\n    ensuring that they are uninstalled at the end of each test.\n    \"\"\"\n\n    def __init__(self, config: Any) -> None:\n        self._mock_cache: MockCache = MockCache()\n        self.mock_module = mock_module = get_mock_module(config)\n        self.patch = self._Patcher(self._mock_cache, mock_module)  # type: MockerFixture._Patcher\n        # aliases for convenience\n        self.Mock = mock_module.Mock\n        self.MagicMock = mock_module.MagicMock\n        self.NonCallableMock = mock_module.NonCallableMock\n        self.NonCallableMagicMock = mock_module.NonCallableMagicMock\n        self.PropertyMock = mock_module.PropertyMock\n        if hasattr(mock_module, \"AsyncMock\"):\n            self.AsyncMock = mock_module.AsyncMock\n        self.call = mock_module.call\n        self.ANY = mock_module.ANY\n        self.DEFAULT = mock_module.DEFAULT\n        self.sentinel = mock_module.sentinel\n        self.mock_open = mock_module.mock_open\n        if hasattr(mock_module, \"seal\"):\n            self.seal = mock_module.seal\n\n    def create_autospec(\n        self, spec: Any, spec_set: bool = False, instance: bool = False, **kwargs: Any\n    ) -> MockType:\n        m: MockType = self.mock_module.create_autospec(\n            spec, spec_set, instance, **kwargs\n        )\n        self._mock_cache.add(m)\n        return m\n\n    def resetall(\n        self, *, return_value: bool = False, side_effect: bool = False\n    ) -> None:\n        \"\"\"\n        Call reset_mock() on all patchers started by this fixture.\n\n        :param bool return_value: Reset the return_value of mocks.\n        :param bool side_effect: Reset the side_effect of mocks.\n        \"\"\"\n        supports_reset_mock_with_args: Tuple[Type[Any], ...]\n        if hasattr(self, \"AsyncMock\"):\n            supports_reset_mock_with_args = (self.Mock, self.AsyncMock)\n        else:\n            supports_reset_mock_with_args = (self.Mock,)\n\n        for mock_item in self._mock_cache:\n            # See issue #237.\n            if not hasattr(mock_item.mock, \"reset_mock\"):\n                continue\n            # NOTE: The mock may be a dictionary\n            if hasattr(mock_item.mock, \"spy_return_list\"):\n                mock_item.mock.spy_return_list = []\n            if isinstance(mock_item.mock, supports_reset_mock_with_args):\n                mock_item.mock.reset_mock(\n                    return_value=return_value, side_effect=side_effect\n                )\n            else:\n                mock_item.mock.reset_mock()\n\n    def stopall(self) -> None:\n        \"\"\"\n        Stop all patchers started by this fixture. Can be safely called multiple\n        times.\n        \"\"\"\n        self._mock_cache.clear()\n\n    def stop(self, mock: unittest.mock.MagicMock) -> None:\n        \"\"\"\n        Stops a previous patch or spy call by passing the ``MagicMock`` object\n        returned by it.\n        \"\"\"\n        self._mock_cache.remove(mock)\n\n    def spy(self, obj: object, name: str) -> MockType:\n        \"\"\"\n        Create a spy of method. It will run method normally, but it is now\n        possible to use `mock` call features with it, like call count.\n\n        :param obj: An object.\n        :param name: A method in object.\n        :return: Spy object.\n        \"\"\"\n        method = getattr(obj, name)\n\n        def wrapper(*args, **kwargs):\n            spy_obj.spy_return = None\n            spy_obj.spy_exception = None\n            try:\n                r = method(*args, **kwargs)\n            except BaseException as e:\n                spy_obj.spy_exception = e\n                raise\n            else:\n                spy_obj.spy_return = r\n                spy_obj.spy_return_list.append(r)\n            return r\n\n        async def async_wrapper(*args, **kwargs):\n            spy_obj.spy_return = None\n            spy_obj.spy_exception = None\n            try:\n                r = await method(*args, **kwargs)\n            except BaseException as e:\n                spy_obj.spy_exception = e\n                raise\n            else:\n                spy_obj.spy_return = r\n                spy_obj.spy_return_list.append(r)\n            return r\n\n        if inspect.iscoroutinefunction(method):\n            wrapped = functools.update_wrapper(async_wrapper, method)\n        else:\n            wrapped = functools.update_wrapper(wrapper, method)\n\n        autospec = inspect.ismethod(method) or inspect.isfunction(method)\n\n        spy_obj = self.patch.object(obj, name, side_effect=wrapped, autospec=autospec)\n        spy_obj.spy_return = None\n        spy_obj.spy_return_list = []\n        spy_obj.spy_exception = None\n        return spy_obj\n\n    def stub(self, name: Optional[str] = None) -> unittest.mock.MagicMock:\n        \"\"\"\n        Create a stub method. It accepts any arguments. Ideal to register to\n        callbacks in tests.\n\n        :param name: the constructed stub's name as used in repr\n        :return: Stub object.\n        \"\"\"\n        return cast(\n            unittest.mock.MagicMock,\n            self.mock_module.MagicMock(spec=lambda *args, **kwargs: None, name=name),\n        )\n\n    def async_stub(self, name: Optional[str] = None) -> AsyncMockType:\n        \"\"\"\n        Create a async stub method. It accepts any arguments. Ideal to register to\n        callbacks in tests.\n\n        :param name: the constructed stub's name as used in repr\n        :return: Stub object.\n        \"\"\"\n        return cast(\n            AsyncMockType,\n            self.mock_module.AsyncMock(spec=lambda *args, **kwargs: None, name=name),\n        )\n\n    class _Patcher:\n        \"\"\"\n        Object to provide the same interface as mock.patch, mock.patch.object,\n        etc. We need this indirection to keep the same API of the mock package.\n        \"\"\"\n\n        DEFAULT = object()\n\n        def __init__(self, mock_cache, mock_module):\n            self.__mock_cache = mock_cache\n            self.mock_module = mock_module\n\n        def _start_patch(\n            self, mock_func: Any, warn_on_mock_enter: bool, *args: Any, **kwargs: Any\n        ) -> MockType:\n            \"\"\"Patches something by calling the given function from the mock\n            module, registering the patch to stop it later and returns the\n            mock object resulting from the mock call.\n            \"\"\"\n            p = mock_func(*args, **kwargs)\n            mocked: MockType = p.start()\n            self.__mock_cache.add(mock=mocked, patch=p)\n            if hasattr(mocked, \"reset_mock\"):\n                # check if `mocked` is actually a mock object, as depending on autospec or target\n                # parameters `mocked` can be anything\n                if hasattr(mocked, \"__enter__\") and warn_on_mock_enter:\n                    mocked.__enter__.side_effect = lambda: warnings.warn(\n                        \"Mocks returned by pytest-mock do not need to be used as context managers. \"\n                        \"The mocker fixture automatically undoes mocking at the end of a test. \"\n                        \"This warning can be ignored if it was triggered by mocking a context manager. \"\n                        \"https://pytest-mock.readthedocs.io/en/latest/usage.html#usage-as-context-manager\",\n                        PytestMockWarning,\n                        stacklevel=5,\n                    )\n            return mocked\n\n        def object(\n            self,\n            target: object,\n            attribute: str,\n            new: object = DEFAULT,\n            spec: Optional[object] = None,\n            create: bool = False,\n            spec_set: Optional[object] = None,\n            autospec: Optional[object] = None,\n            new_callable: object = None,\n            **kwargs: Any,\n        ) -> MockType:\n            \"\"\"API to mock.patch.object\"\"\"\n            if new is self.DEFAULT:\n                new = self.mock_module.DEFAULT\n            return self._start_patch(\n                self.mock_module.patch.object,\n                True,\n                target,\n                attribute,\n                new=new,\n                spec=spec,\n                create=create,\n                spec_set=spec_set,\n                autospec=autospec,\n                new_callable=new_callable,\n                **kwargs,\n            )\n\n        def context_manager(\n            self,\n            target: builtins.object,\n            attribute: str,\n            new: builtins.object = DEFAULT,\n            spec: Optional[builtins.object] = None,\n            create: bool = False,\n            spec_set: Optional[builtins.object] = None,\n            autospec: Optional[builtins.object] = None,\n            new_callable: builtins.object = None,\n            **kwargs: Any,\n        ) -> MockType:\n            \"\"\"This is equivalent to mock.patch.object except that the returned mock\n            does not issue a warning when used as a context manager.\"\"\"\n            if new is self.DEFAULT:\n                new = self.mock_module.DEFAULT\n            return self._start_patch(\n                self.mock_module.patch.object,\n                False,\n                target,\n                attribute,\n                new=new,\n                spec=spec,\n                create=create,\n                spec_set=spec_set,\n                autospec=autospec,\n                new_callable=new_callable,\n                **kwargs,\n            )\n\n        def multiple(\n            self,\n            target: builtins.object,\n            spec: Optional[builtins.object] = None,\n            create: bool = False,\n            spec_set: Optional[builtins.object] = None,\n            autospec: Optional[builtins.object] = None,\n            new_callable: Optional[builtins.object] = None,\n            **kwargs: Any,\n        ) -> Dict[str, MockType]:\n            \"\"\"API to mock.patch.multiple\"\"\"\n            return self._start_patch(\n                self.mock_module.patch.multiple,\n                True,\n                target,\n                spec=spec,\n                create=create,\n                spec_set=spec_set,\n                autospec=autospec,\n                new_callable=new_callable,\n                **kwargs,\n            )\n\n        def dict(\n            self,\n            in_dict: Union[Mapping[Any, Any], str],\n            values: Union[Mapping[Any, Any], Iterable[Tuple[Any, Any]]] = (),\n            clear: bool = False,\n            **kwargs: Any,\n        ) -> Any:\n            \"\"\"API to mock.patch.dict\"\"\"\n            return self._start_patch(\n                self.mock_module.patch.dict,\n                True,\n                in_dict,\n                values=values,\n                clear=clear,\n                **kwargs,\n            )\n\n        @overload\n        def __call__(\n            self,\n            target: str,\n            new: None = ...,\n            spec: Optional[builtins.object] = ...,\n            create: bool = ...,\n            spec_set: Optional[builtins.object] = ...,\n            autospec: Optional[builtins.object] = ...,\n            new_callable: None = ...,\n            **kwargs: Any,\n        ) -> MockType: ...\n\n        @overload\n        def __call__(\n            self,\n            target: str,\n            new: _T,\n            spec: Optional[builtins.object] = ...,\n            create: bool = ...,\n            spec_set: Optional[builtins.object] = ...,\n            autospec: Optional[builtins.object] = ...,\n            new_callable: None = ...,\n            **kwargs: Any,\n        ) -> _T: ...\n\n        @overload\n        def __call__(\n            self,\n            target: str,\n            new: None,\n            spec: Optional[builtins.object],\n            create: bool,\n            spec_set: Optional[builtins.object],\n            autospec: Optional[builtins.object],\n            new_callable: Callable[[], _T],\n            **kwargs: Any,\n        ) -> _T: ...\n\n        @overload\n        def __call__(\n            self,\n            target: str,\n            new: None = ...,\n            spec: Optional[builtins.object] = ...,\n            create: bool = ...,\n            spec_set: Optional[builtins.object] = ...,\n            autospec: Optional[builtins.object] = ...,\n            *,\n            new_callable: Callable[[], _T],\n            **kwargs: Any,\n        ) -> _T: ...\n\n        def __call__(\n            self,\n            target: str,\n            new: builtins.object = DEFAULT,\n            spec: Optional[builtins.object] = None,\n            create: bool = False,\n            spec_set: Optional[builtins.object] = None,\n            autospec: Optional[builtins.object] = None,\n            new_callable: Optional[Callable[[], Any]] = None,\n            **kwargs: Any,\n        ) -> Any:\n            \"\"\"API to mock.patch\"\"\"\n            if new is self.DEFAULT:\n                new = self.mock_module.DEFAULT\n            return self._start_patch(\n                self.mock_module.patch,\n                True,\n                target,\n                new=new,\n                spec=spec,\n                create=create,\n                spec_set=spec_set,\n                autospec=autospec,\n                new_callable=new_callable,\n                **kwargs,\n            )\n\n\ndef _mocker(pytestconfig: Any) -> Generator[MockerFixture, None, None]:\n    \"\"\"\n    Return an object that has the same interface to the `mock` module, but\n    takes care of automatically undoing all patches after each test method.\n    \"\"\"\n    result = MockerFixture(pytestconfig)\n    yield result\n    result.stopall()\n\n\nmocker = pytest.fixture()(_mocker)  # default scope is function\nclass_mocker = pytest.fixture(scope=\"class\")(_mocker)\nmodule_mocker = pytest.fixture(scope=\"module\")(_mocker)\npackage_mocker = pytest.fixture(scope=\"package\")(_mocker)\nsession_mocker = pytest.fixture(scope=\"session\")(_mocker)\n\n\n_mock_module_patches = []  # type: List[Any]\n_mock_module_originals = {}  # type: Dict[str, Any]\n\n\ndef assert_wrapper(\n    __wrapped_mock_method__: Callable[..., Any], *args: Any, **kwargs: Any\n) -> None:\n    __tracebackhide__ = True\n    try:\n        __wrapped_mock_method__(*args, **kwargs)\n        return\n    except AssertionError as e:\n        if getattr(e, \"_mock_introspection_applied\", 0):\n            msg = str(e)\n        else:\n            __mock_self = args[0]\n            msg = str(e)\n            if __mock_self.call_args is not None:\n                actual_args, actual_kwargs = __mock_self.call_args\n                introspection = \"\"\n                try:\n                    assert actual_args == args[1:]\n                except AssertionError as e_args:\n                    introspection += \"\\nArgs:\\n\" + str(e_args)\n                try:\n                    assert actual_kwargs == kwargs\n                except AssertionError as e_kwargs:\n                    introspection += \"\\nKwargs:\\n\" + str(e_kwargs)\n                if introspection:\n                    msg += \"\\n\\npytest introspection follows:\\n\" + introspection\n        e = AssertionError(msg)\n        e._mock_introspection_applied = True  # type:ignore[attr-defined]\n        raise e\n\n\ndef assert_has_calls_wrapper(\n    __wrapped_mock_method__: Callable[..., Any], *args: Any, **kwargs: Any\n) -> None:\n    __tracebackhide__ = True\n    try:\n        __wrapped_mock_method__(*args, **kwargs)\n        return\n    except AssertionError as e:\n        any_order = kwargs.get(\"any_order\", False)\n        if getattr(e, \"_mock_introspection_applied\", 0) or any_order:\n            msg = str(e)\n        else:\n            __mock_self = args[0]\n            msg = str(e)\n            if __mock_self.call_args_list is not None:\n                actual_calls = list(__mock_self.call_args_list)\n                expect_calls = args[1]\n                introspection = \"\"\n                from itertools import zip_longest\n\n                for actual_call, expect_call in zip_longest(actual_calls, expect_calls):\n                    if actual_call is not None:\n                        actual_args, actual_kwargs = actual_call\n                    else:\n                        actual_args = tuple()\n                        actual_kwargs = {}\n\n                    if expect_call is not None:\n                        _, expect_args, expect_kwargs = expect_call\n                    else:\n                        expect_args = tuple()\n                        expect_kwargs = {}\n\n                    try:\n                        assert actual_args == expect_args\n                    except AssertionError as e_args:\n                        introspection += \"\\nArgs:\\n\" + str(e_args)\n                    try:\n                        assert actual_kwargs == expect_kwargs\n                    except AssertionError as e_kwargs:\n                        introspection += \"\\nKwargs:\\n\" + str(e_kwargs)\n                if introspection:\n                    msg += \"\\n\\npytest introspection follows:\\n\" + introspection\n        e = AssertionError(msg)\n        e._mock_introspection_applied = True  # type:ignore[attr-defined]\n        raise e\n\n\ndef wrap_assert_not_called(*args: Any, **kwargs: Any) -> None:\n    __tracebackhide__ = True\n    assert_wrapper(_mock_module_originals[\"assert_not_called\"], *args, **kwargs)\n\n\ndef wrap_assert_called_with(*args: Any, **kwargs: Any) -> None:\n    __tracebackhide__ = True\n    assert_wrapper(_mock_module_originals[\"assert_called_with\"], *args, **kwargs)\n\n\ndef wrap_assert_called_once(*args: Any, **kwargs: Any) -> None:\n    __tracebackhide__ = True\n    assert_wrapper(_mock_module_originals[\"assert_called_once\"], *args, **kwargs)\n\n\ndef wrap_assert_called_once_with(*args: Any, **kwargs: Any) -> None:\n    __tracebackhide__ = True\n    assert_wrapper(_mock_module_originals[\"assert_called_once_with\"], *args, **kwargs)\n\n\ndef wrap_assert_has_calls(*args: Any, **kwargs: Any) -> None:\n    __tracebackhide__ = True\n    assert_has_calls_wrapper(\n        _mock_module_originals[\"assert_has_calls\"], *args, **kwargs\n    )\n\n\ndef wrap_assert_any_call(*args: Any, **kwargs: Any) -> None:\n    __tracebackhide__ = True\n    assert_wrapper(_mock_module_originals[\"assert_any_call\"], *args, **kwargs)\n\n\ndef wrap_assert_called(*args: Any, **kwargs: Any) -> None:\n    __tracebackhide__ = True\n    assert_wrapper(_mock_module_originals[\"assert_called\"], *args, **kwargs)\n\n\ndef wrap_assert_not_awaited(*args: Any, **kwargs: Any) -> None:\n    __tracebackhide__ = True\n    assert_wrapper(_mock_module_originals[\"assert_not_awaited\"], *args, **kwargs)\n\n\ndef wrap_assert_awaited_with(*args: Any, **kwargs: Any) -> None:\n    __tracebackhide__ = True\n    assert_wrapper(_mock_module_originals[\"assert_awaited_with\"], *args, **kwargs)\n\n\ndef wrap_assert_awaited_once(*args: Any, **kwargs: Any) -> None:\n    __tracebackhide__ = True\n    assert_wrapper(_mock_module_originals[\"assert_awaited_once\"], *args, **kwargs)\n\n\ndef wrap_assert_awaited_once_with(*args: Any, **kwargs: Any) -> None:\n    __tracebackhide__ = True\n    assert_wrapper(_mock_module_originals[\"assert_awaited_once_with\"], *args, **kwargs)\n\n\ndef wrap_assert_has_awaits(*args: Any, **kwargs: Any) -> None:\n    __tracebackhide__ = True\n    assert_wrapper(_mock_module_originals[\"assert_has_awaits\"], *args, **kwargs)\n\n\ndef wrap_assert_any_await(*args: Any, **kwargs: Any) -> None:\n    __tracebackhide__ = True\n    assert_wrapper(_mock_module_originals[\"assert_any_await\"], *args, **kwargs)\n\n\ndef wrap_assert_awaited(*args: Any, **kwargs: Any) -> None:\n    __tracebackhide__ = True\n    assert_wrapper(_mock_module_originals[\"assert_awaited\"], *args, **kwargs)\n\n\ndef wrap_assert_methods(config: Any) -> None:\n    \"\"\"\n    Wrap assert methods of mock module so we can hide their traceback and\n    add introspection information to specified argument asserts.\n    \"\"\"\n    # Make sure we only do this once\n    if _mock_module_originals:\n        return\n\n    mock_module = get_mock_module(config)\n\n    wrappers = {\n        \"assert_called\": wrap_assert_called,\n        \"assert_called_once\": wrap_assert_called_once,\n        \"assert_called_with\": wrap_assert_called_with,\n        \"assert_called_once_with\": wrap_assert_called_once_with,\n        \"assert_any_call\": wrap_assert_any_call,\n        \"assert_has_calls\": wrap_assert_has_calls,\n        \"assert_not_called\": wrap_assert_not_called,\n    }\n    for method, wrapper in wrappers.items():\n        try:\n            original = getattr(mock_module.NonCallableMock, method)\n        except AttributeError:  # pragma: no cover\n            continue\n        _mock_module_originals[method] = original\n        patcher = mock_module.patch.object(mock_module.NonCallableMock, method, wrapper)\n        patcher.start()\n        _mock_module_patches.append(patcher)\n\n    if hasattr(mock_module, \"AsyncMock\"):\n        async_wrappers = {\n            \"assert_awaited\": wrap_assert_awaited,\n            \"assert_awaited_once\": wrap_assert_awaited_once,\n            \"assert_awaited_with\": wrap_assert_awaited_with,\n            \"assert_awaited_once_with\": wrap_assert_awaited_once_with,\n            \"assert_any_await\": wrap_assert_any_await,\n            \"assert_has_awaits\": wrap_assert_has_awaits,\n            \"assert_not_awaited\": wrap_assert_not_awaited,\n        }\n        for method, wrapper in async_wrappers.items():\n            try:\n                original = getattr(mock_module.AsyncMock, method)\n            except AttributeError:  # pragma: no cover\n                continue\n            _mock_module_originals[method] = original\n            patcher = mock_module.patch.object(mock_module.AsyncMock, method, wrapper)\n            patcher.start()\n            _mock_module_patches.append(patcher)\n\n    config.add_cleanup(unwrap_assert_methods)\n\n\ndef unwrap_assert_methods() -> None:\n    for patcher in _mock_module_patches:\n        try:\n            patcher.stop()\n        except RuntimeError as e:\n            # a patcher might have been stopped by user code (#137)\n            # so we need to catch this error here and ignore it;\n            # unfortunately there's no public API to check if a patch\n            # has been started, so catching the error it is\n            if str(e) == \"stop called on unstarted patcher\":\n                pass\n            else:\n                raise\n    _mock_module_patches[:] = []\n    _mock_module_originals.clear()\n\n\ndef pytest_addoption(parser: Any) -> None:\n    parser.addini(\n        \"mock_traceback_monkeypatch\",\n        \"Monkeypatch the mock library to improve reporting of the \"\n        \"assert_called_... methods\",\n        default=True,\n    )\n    parser.addini(\n        \"mock_use_standalone_module\",\n        'Use standalone \"mock\" (from PyPI) instead of builtin \"unittest.mock\" '\n        \"on Python 3\",\n        default=False,\n    )\n\n\ndef pytest_configure(config: Any) -> None:\n    tb = config.getoption(\"--tb\", default=\"auto\")\n    if (\n        parse_ini_boolean(config.getini(\"mock_traceback_monkeypatch\"))\n        and tb != \"native\"\n    ):\n        wrap_assert_methods(config)\n", "metadata": {"source": "github.com/pytest-dev/pytest-mock", "file_path": "src/pytest_mock/plugin.py", "language": "python", "complexity": "complex", "star_count": 1969, "quality_score": 0.87, "features": {"has_classes": true, "has_functions": true, "has_async": true, "has_decorators": true, "has_imports": true, "has_error_handling": true, "has_documentation": true, "has_type_hints": true, "has_testing": true, "has_logging": false, "function_count": 47, "class_count": 6, "line_count": 717}, "authority_level": "medium", "authentic": true}}
{"domain": "code_generation", "category": "testing_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create comprehensive test suite with fixtures and mocks with asyncio coroutines\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import os\nimport platform\nimport re\nimport sys\nimport warnings\nfrom contextlib import contextmanager\nfrom typing import Any\nfrom typing import Callable\nfrom typing import Generator\nfrom typing import Tuple\nfrom typing import Type\nfrom unittest.mock import AsyncMock\nfrom unittest.mock import MagicMock\n\nimport pytest\n\nfrom pytest_mock import MockerFixture\nfrom pytest_mock import PytestMockWarning\n\npytest_plugins = \"pytester\"\n\n# could not make some of the tests work on PyPy, patches are welcome!\nskip_pypy = pytest.mark.skipif(\n    platform.python_implementation() == \"PyPy\", reason=\"could not make it work on pypy\"\n)\n\n# Python 3.11.7 changed the output formatting, https://github.com/python/cpython/issues/111019\nNEWEST_FORMATTING = sys.version_info >= (3, 11, 7)\n\n\n@pytest.fixture\ndef needs_assert_rewrite(pytestconfig):\n    \"\"\"\n    Fixture which skips requesting test if assertion rewrite is disabled (#102)\n\n    Making this a fixture to avoid accessing pytest's config in the global context.\n    \"\"\"\n    option = pytestconfig.getoption(\"assertmode\")\n    if option != \"rewrite\":\n        pytest.skip(\n            \"this test needs assertion rewrite to work but current option \"\n            'is \"{}\"'.format(option)\n        )\n\n\nclass UnixFS:\n    \"\"\"\n    Wrapper to os functions to simulate a Unix file system, used for testing\n    the mock fixture.\n    \"\"\"\n\n    @classmethod\n    def rm(cls, filename):\n        os.remove(filename)\n\n    @classmethod\n    def ls(cls, path):\n        return os.listdir(path)\n\n\nclass TestObject:\n    \"\"\"\n    Class that is used for testing create_autospec with child mocks\n    \"\"\"\n\n    def run(self) -> str:\n        return \"not mocked\"\n\n\n@pytest.fixture\ndef check_unix_fs_mocked(\n    tmpdir: Any, mocker: MockerFixture\n) -> Callable[[Any, Any], None]:\n    \"\"\"\n    performs a standard test in a UnixFS, assuming that both `os.remove` and\n    `os.listdir` have been mocked previously.\n    \"\"\"\n\n    def check(mocked_rm, mocked_ls):\n        assert mocked_rm is os.remove\n        assert mocked_ls is os.listdir\n\n        file_name = tmpdir / \"foo.txt\"\n        file_name.ensure()\n\n        UnixFS.rm(str(file_name))\n        mocked_rm.assert_called_once_with(str(file_name))\n        assert os.path.isfile(str(file_name))\n\n        mocked_ls.return_value = [\"bar.txt\"]\n        assert UnixFS.ls(str(tmpdir)) == [\"bar.txt\"]\n        mocked_ls.assert_called_once_with(str(tmpdir))\n\n        mocker.stopall()\n\n        assert UnixFS.ls(str(tmpdir)) == [\"foo.txt\"]\n        UnixFS.rm(str(file_name))\n        assert not os.path.isfile(str(file_name))\n\n    return check\n\n\ndef mock_using_patch_object(mocker: MockerFixture) -> Tuple[MagicMock, MagicMock]:\n    return mocker.patch.object(os, \"remove\"), mocker.patch.object(os, \"listdir\")\n\n\ndef mock_using_patch(mocker: MockerFixture) -> Tuple[MagicMock, MagicMock]:\n    return mocker.patch(\"os.remove\"), mocker.patch(\"os.listdir\")\n\n\ndef mock_using_patch_multiple(mocker: MockerFixture) -> Tuple[MagicMock, MagicMock]:\n    r = mocker.patch.multiple(\"os\", remove=mocker.DEFAULT, listdir=mocker.DEFAULT)\n    return r[\"remove\"], r[\"listdir\"]\n\n\n@pytest.mark.parametrize(\n    \"mock_fs\", [mock_using_patch_object, mock_using_patch, mock_using_patch_multiple]\n)\ndef test_mock_patches(\n    mock_fs: Any,\n    mocker: MockerFixture,\n    check_unix_fs_mocked: Callable[[Any, Any], None],\n) -> None:\n    \"\"\"\n    Installs mocks into `os` functions and performs a standard testing of\n    mock functionality. We parametrize different mock methods to ensure\n    all (intended, at least) mock API is covered.\n    \"\"\"\n    # mock it twice on purpose to ensure we unmock it correctly later\n    mock_fs(mocker)\n    mocked_rm, mocked_ls = mock_fs(mocker)\n    check_unix_fs_mocked(mocked_rm, mocked_ls)\n    mocker.resetall()\n    mocker.stopall()\n\n\ndef test_mock_patch_dict(mocker: MockerFixture) -> None:\n    \"\"\"\n    Testing\n    :param mock:\n    \"\"\"\n    x = {\"original\": 1}\n    mocker.patch.dict(x, values=[(\"new\", 10)], clear=True)\n    assert x == {\"new\": 10}\n    mocker.stopall()\n    assert x == {\"original\": 1}\n\n\ndef test_mock_patch_dict_resetall(mocker: MockerFixture) -> None:\n    \"\"\"\n    We can call resetall after patching a dict.\n    :param mock:\n    \"\"\"\n    x = {\"original\": 1}\n    mocker.patch.dict(x, values=[(\"new\", 10)], clear=True)\n    assert x == {\"new\": 10}\n    mocker.resetall()\n    assert x == {\"new\": 10}\n\n\n@pytest.mark.parametrize(\n    \"name\",\n    [\n        \"ANY\",\n        \"call\",\n        \"MagicMock\",\n        \"Mock\",\n        \"mock_open\",\n        \"NonCallableMagicMock\",\n        \"NonCallableMock\",\n        \"PropertyMock\",\n        \"sentinel\",\n        \"seal\",\n    ],\n)\ndef test_mocker_aliases(name: str, pytestconfig: Any) -> None:\n    from pytest_mock._util import get_mock_module\n\n    mock_module = get_mock_module(pytestconfig)\n\n    mocker = MockerFixture(pytestconfig)\n    assert getattr(mocker, name) is getattr(mock_module, name)\n\n\ndef test_mocker_resetall(mocker: MockerFixture) -> None:\n    listdir = mocker.patch(\"os.listdir\", return_value=\"foo\")\n    open = mocker.patch(\"os.open\", side_effect=[\"bar\", \"baz\"])\n\n    mocked_object = mocker.create_autospec(TestObject)\n    mocked_object.run.return_value = \"mocked\"\n\n    assert listdir(\"/tmp\") == \"foo\"\n    assert open(\"/tmp/foo.txt\") == \"bar\"\n    assert mocked_object.run() == \"mocked\"\n    listdir.assert_called_once_with(\"/tmp\")\n    open.assert_called_once_with(\"/tmp/foo.txt\")\n    mocked_object.run.assert_called_once()\n\n    mocker.resetall()\n\n    assert not listdir.called\n    assert not open.called\n    assert not mocked_object.called\n    assert listdir.return_value == \"foo\"\n    assert list(open.side_effect) == [\"baz\"]\n    assert mocked_object.run.return_value == \"mocked\"\n\n    mocker.resetall(return_value=True, side_effect=True)\n\n    assert isinstance(listdir.return_value, mocker.Mock)\n    assert open.side_effect is None\n\n    if sys.version_info >= (3, 9):\n        # The reset on child mocks have been implemented in 3.9\n        # https://bugs.python.org/issue38932\n        assert mocked_object.run.return_value != \"mocked\"\n\n\nclass TestMockerStub:\n    def test_call(self, mocker: MockerFixture) -> None:\n        stub = mocker.stub()\n        stub(\"foo\", \"bar\")\n        stub.assert_called_once_with(\"foo\", \"bar\")\n\n    def test_repr_with_no_name(self, mocker: MockerFixture) -> None:\n        stub = mocker.stub()\n        assert \"name\" not in repr(stub)\n\n    def test_repr_with_name(self, mocker: MockerFixture) -> None:\n        test_name = \"funny walk\"\n        stub = mocker.stub(name=test_name)\n        assert f\"name={test_name!r}\" in repr(stub)\n\n    def __test_failure_message(self, mocker: MockerFixture, **kwargs: Any) -> None:\n        expected_name = kwargs.get(\"name\") or \"mock\"\n        if NEWEST_FORMATTING:\n            msg = \"expected call not found.\\nExpected: {0}()\\n  Actual: not called.\"\n        else:\n            msg = \"expected call not found.\\nExpected: {0}()\\nActual: not called.\"\n        expected_message = msg.format(expected_name)\n        stub = mocker.stub(**kwargs)\n        with pytest.raises(AssertionError, match=re.escape(expected_message)):\n            stub.assert_called_with()\n\n    def test_failure_message_with_no_name(self, mocker: MagicMock) -> None:\n        self.__test_failure_message(mocker)\n\n    @pytest.mark.parametrize(\"name\", (None, \"\", \"f\", \"The Castle of aaarrrrggh\"))\n    def test_failure_message_with_name(self, mocker: MagicMock, name: str) -> None:\n        self.__test_failure_message(mocker, name=name)\n\n    def test_async_stub_type(self, mocker: MockerFixture) -> None:\n        assert isinstance(mocker.async_stub(), AsyncMock)\n\n\ndef test_instance_method_spy(mocker: MockerFixture) -> None:\n    class Foo:\n        def bar(self, arg):\n            return arg * 2\n\n    foo = Foo()\n    other = Foo()\n    spy = mocker.spy(foo, \"bar\")\n    assert foo.bar(arg=10) == 20\n    assert other.bar(arg=10) == 20\n    foo.bar.assert_called_once_with(arg=10)  # type:ignore[attr-defined]\n    assert foo.bar.spy_return == 20  # type:ignore[attr-defined]\n    assert foo.bar.spy_return_list == [20]  # type:ignore[attr-defined]\n    spy.assert_called_once_with(arg=10)\n    assert spy.spy_return == 20\n    assert foo.bar(arg=11) == 22\n    assert foo.bar(arg=12) == 24\n    assert spy.spy_return == 24\n    assert spy.spy_return_list == [20, 22, 24]\n\n\n# Ref: https://docs.python.org/3/library/exceptions.html#exception-hierarchy\n@pytest.mark.parametrize(\n    \"exc_cls\",\n    (\n        BaseException,\n        Exception,\n        GeneratorExit,  # BaseException\n        KeyboardInterrupt,  # BaseException\n        RuntimeError,  # regular Exception\n        SystemExit,  # BaseException\n    ),\n)\ndef test_instance_method_spy_exception(\n    exc_cls: Type[BaseException],\n    mocker: MockerFixture,\n) -> None:\n    class Foo:\n        def bar(self, arg):\n            raise exc_cls(f\"Error with {arg}\")\n\n    foo = Foo()\n    spy = mocker.spy(foo, \"bar\")\n\n    expected_calls = []\n    for i, v in enumerate([10, 20]):\n        with pytest.raises(exc_cls, match=f\"Error with {v}\"):\n            foo.bar(arg=v)\n\n        expected_calls.append(mocker.call(arg=v))\n        assert foo.bar.call_args_list == expected_calls  # type:ignore[attr-defined]\n        assert str(spy.spy_exception) == f\"Error with {v}\"\n\n\ndef test_instance_class_static_method_spy_autospec_true(mocker: MockerFixture) -> None:\n    class Foo:\n        def bar(self, arg):\n            return arg * 2\n\n        @classmethod\n        def baz(cls, arg):\n            return arg * 2\n\n        @staticmethod\n        def qux(arg):\n            return arg * 2\n\n    foo = Foo()\n    instance_method_spy = mocker.spy(foo, \"bar\")\n    with pytest.raises(\n        AttributeError, match=\"'function' object has no attribute 'fake_assert_method'\"\n    ):\n        instance_method_spy.fake_assert_method(arg=5)\n\n    class_method_spy = mocker.spy(Foo, \"baz\")\n    with pytest.raises(\n        AttributeError, match=\"Mock object has no attribute 'fake_assert_method'\"\n    ):\n        class_method_spy.fake_assert_method(arg=5)\n\n    static_method_spy = mocker.spy(Foo, \"qux\")\n    with pytest.raises(\n        AttributeError, match=\"Mock object has no attribute 'fake_assert_method'\"\n    ):\n        static_method_spy.fake_assert_method(arg=5)\n\n\ndef test_spy_reset(mocker: MockerFixture) -> None:\n    class Foo:\n        def bar(self, x):\n            if x == 0:\n                raise ValueError(\"invalid x\")\n            return x * 3\n\n    spy = mocker.spy(Foo, \"bar\")\n    assert spy.spy_return is None\n    assert spy.spy_return_list == []\n    assert spy.spy_exception is None\n\n    Foo().bar(10)\n    assert spy.spy_return == 30\n    assert spy.spy_return_list == [30]\n    assert spy.spy_exception is None\n\n    # Testing spy can still be reset (#237).\n    mocker.resetall()\n\n    with pytest.raises(ValueError):\n        Foo().bar(0)\n    assert spy.spy_return is None\n    assert spy.spy_return_list == []\n    assert str(spy.spy_exception) == \"invalid x\"\n\n    Foo().bar(15)\n    assert spy.spy_return == 45\n    assert spy.spy_return_list == [45]\n    assert spy.spy_exception is None\n\n\n@skip_pypy\ndef test_instance_method_by_class_spy(mocker: MockerFixture) -> None:\n    class Foo:\n        def bar(self, arg):\n            return arg * 2\n\n    spy = mocker.spy(Foo, \"bar\")\n    foo = Foo()\n    other = Foo()\n    assert foo.bar(arg=10) == 20\n    assert other.bar(arg=10) == 20\n    calls = [mocker.call(foo, arg=10), mocker.call(other, arg=10)]\n    assert spy.call_args_list == calls\n\n\n@skip_pypy\ndef test_instance_method_by_subclass_spy(mocker: MockerFixture) -> None:\n    class Base:\n        def bar(self, arg):\n            return arg * 2\n\n    class Foo(Base):\n        pass\n\n    spy = mocker.spy(Foo, \"bar\")\n    foo = Foo()\n    other = Foo()\n    assert foo.bar(arg=10) == 20\n    assert other.bar(arg=10) == 20\n    calls = [mocker.call(foo, arg=10), mocker.call(other, arg=10)]\n    assert spy.call_args_list == calls\n    assert spy.spy_return == 20\n    assert spy.spy_return_list == [20, 20]\n\n\n@skip_pypy\ndef test_class_method_spy(mocker: MockerFixture) -> None:\n    class Foo:\n        @classmethod\n        def bar(cls, arg):\n            return arg * 2\n\n    spy = mocker.spy(Foo, \"bar\")\n    assert Foo.bar(arg=10) == 20\n    Foo.bar.assert_called_once_with(arg=10)  # type:ignore[attr-defined]\n    assert Foo.bar.spy_return == 20  # type:ignore[attr-defined]\n    assert Foo.bar.spy_return_list == [20]  # type:ignore[attr-defined]\n    spy.assert_called_once_with(arg=10)\n    assert spy.spy_return == 20\n    assert spy.spy_return_list == [20]\n\n\n@skip_pypy\ndef test_class_method_subclass_spy(mocker: MockerFixture) -> None:\n    class Base:\n        @classmethod\n        def bar(self, arg):\n            return arg * 2\n\n    class Foo(Base):\n        pass\n\n    spy = mocker.spy(Foo, \"bar\")\n    assert Foo.bar(arg=10) == 20\n    Foo.bar.assert_called_once_with(arg=10)  # type:ignore[attr-defined]\n    assert Foo.bar.spy_return == 20  # type:ignore[attr-defined]\n    assert Foo.bar.spy_return_list == [20]  # type:ignore[attr-defined]\n    spy.assert_called_once_with(arg=10)\n    assert spy.spy_return == 20\n    assert spy.spy_return_list == [20]\n\n\n@skip_pypy\ndef test_class_method_with_metaclass_spy(mocker: MockerFixture) -> None:\n    class MetaFoo(type):\n        pass\n\n    class Foo:\n        __metaclass__ = MetaFoo\n\n        @classmethod\n        def bar(cls, arg):\n            return arg * 2\n\n    spy = mocker.spy(Foo, \"bar\")\n    assert Foo.bar(arg=10) == 20\n    Foo.bar.assert_called_once_with(arg=10)  # type:ignore[attr-defined]\n    assert Foo.bar.spy_return == 20  # type:ignore[attr-defined]\n    assert Foo.bar.spy_return_list == [20]  # type:ignore[attr-defined]\n    spy.assert_called_once_with(arg=10)\n    assert spy.spy_return == 20\n    assert spy.spy_return_list == [20]\n\n\n@skip_pypy\ndef test_static_method_spy(mocker: MockerFixture) -> None:\n    class Foo:\n        @staticmethod\n        def bar(arg):\n            return arg * 2\n\n    spy = mocker.spy(Foo, \"bar\")\n    assert Foo.bar(arg=10) == 20\n    Foo.bar.assert_called_once_with(arg=10)  # type:ignore[attr-defined]\n    assert Foo.bar.spy_return == 20  # type:ignore[attr-defined]\n    assert Foo.bar.spy_return_list == [20]  # type:ignore[attr-defined]\n    spy.assert_called_once_with(arg=10)\n    assert spy.spy_return == 20\n    assert spy.spy_return_list == [20]\n\n\n@skip_pypy\ndef test_static_method_subclass_spy(mocker: MockerFixture) -> None:\n    class Base:\n        @staticmethod\n        def bar(arg):\n            return arg * 2\n\n    class Foo(Base):\n        pass\n\n    spy = mocker.spy(Foo, \"bar\")\n    assert Foo.bar(arg=10) == 20\n    Foo.bar.assert_called_once_with(arg=10)  # type:ignore[attr-defined]\n    assert Foo.bar.spy_return == 20  # type:ignore[attr-defined]\n    assert Foo.bar.spy_return_list == [20]  # type:ignore[attr-defined]\n    spy.assert_called_once_with(arg=10)\n    assert spy.spy_return == 20\n    assert spy.spy_return_list == [20]\n\n\ndef test_callable_like_spy(testdir: Any, mocker: MockerFixture) -> None:\n    testdir.makepyfile(\n        uut=\"\"\"\n        class CallLike(object):\n            def __call__(self, x):\n                return x * 2\n\n        call_like = CallLike()\n    \"\"\"\n    )\n    testdir.syspathinsert()\n\n    uut = __import__(\"uut\")\n\n    spy = mocker.spy(uut, \"call_like\")\n    uut.call_like(10)\n    spy.assert_called_once_with(10)\n    assert spy.spy_return == 20\n    assert spy.spy_return_list == [20]\n\n\nasync def test_instance_async_method_spy(mocker: MockerFixture) -> None:\n    class Foo:\n        async def bar(self, arg):\n            return arg * 2\n\n    foo = Foo()\n    spy = mocker.spy(foo, \"bar\")\n\n    result = await foo.bar(10)\n\n    spy.assert_called_once_with(10)\n    assert result == 20\n\n\n@contextmanager\ndef assert_traceback() -> Generator[None, None, None]:\n    \"\"\"\n    Assert that this file is at the top of the filtered traceback\n    \"\"\"\n    try:\n        yield\n    except AssertionError as e:\n        assert e.__traceback__.tb_frame.f_code.co_filename == __file__  # type:ignore\n    else:\n        raise AssertionError(\"DID NOT RAISE\")\n\n\n@contextmanager\ndef assert_argument_introspection(left: Any, right: Any) -> Generator[None, None, None]:\n    \"\"\"\n    Assert detailed argument introspection is used\n    \"\"\"\n    try:\n        yield\n    except AssertionError as e:\n        # this may be a bit too assuming, but seems nicer then hard-coding\n        import _pytest.assertion.util as util\n\n        # NOTE: we assert with either verbose or not, depending on how our own\n        #       test was run by examining sys.argv\n        verbose = any(a.startswith(\"-v\") for a in sys.argv)\n        if int(pytest.__version__.split(\".\")[0]) < 8:\n            expected = \"\\n  \".join(util._compare_eq_iterable(left, right, verbose))  # type:ignore[arg-type]\n        else:\n            expected = \"\\n  \".join(\n                util._compare_eq_iterable(left, right, lambda t, *_: t, verbose)\n            )\n        assert expected in str(e)\n    else:\n        raise AssertionError(\"DID NOT RAISE\")\n\n\ndef test_assert_not_called_wrapper(mocker: MockerFixture) -> None:\n    stub = mocker.stub()\n    stub.assert_not_called()\n    stub()\n    with assert_traceback():\n        stub.assert_not_called()\n\n\ndef test_assert_called_with_wrapper(mocker: MockerFixture) -> None:\n    stub = mocker.stub()\n    stub(\"foo\")\n    stub.assert_called_with(\"foo\")\n    with assert_traceback():\n        stub.assert_called_with(\"bar\")\n\n\ndef test_assert_called_once_with_wrapper(mocker: MockerFixture) -> None:\n    stub = mocker.stub()\n    stub(\"foo\")\n    stub.assert_called_once_with(\"foo\")\n    stub(\"foo\")\n    with assert_traceback():\n        stub.assert_called_once_with(\"foo\")\n\n\ndef test_assert_called_once_wrapper(mocker: MockerFixture) -> None:\n    stub = mocker.stub()\n    if not hasattr(stub, \"assert_called_once\"):\n        pytest.skip(\"assert_called_once not available\")\n    stub(\"foo\")\n    stub.assert_called_once()\n    stub(\"foo\")\n    with assert_traceback():\n        stub.assert_called_once()\n\n\ndef test_assert_called_wrapper(mocker: MockerFixture) -> None:\n    stub = mocker.stub()\n    if not hasattr(stub, \"assert_called\"):\n        pytest.skip(\"assert_called_once not available\")\n    with assert_traceback():\n        stub.assert_called()\n    stub(\"foo\")\n    stub.assert_called()\n    stub(\"foo\")\n    stub.assert_called()\n\n\n@pytest.mark.usefixtures(\"needs_assert_rewrite\")\ndef test_assert_called_args_with_introspection(mocker: MockerFixture) -> None:\n    stub = mocker.stub()\n\n    complex_args = (\"a\", 1, {\"test\"})\n    wrong_args = (\"b\", 2, {\"jest\"})\n\n    stub(*complex_args)\n    stub.assert_called_with(*complex_args)\n    stub.assert_called_once_with(*complex_args)\n\n    with assert_argument_introspection(complex_args, wrong_args):\n        stub.assert_called_with(*wrong_args)\n        stub.assert_called_once_with(*wrong_args)\n\n\n@pytest.mark.usefixtures(\"needs_assert_rewrite\")\ndef test_assert_called_kwargs_with_introspection(mocker: MockerFixture) -> None:\n    stub = mocker.stub()\n\n    complex_kwargs = dict(foo={\"bar\": 1, \"baz\": \"spam\"})\n    wrong_kwargs = dict(foo={\"goo\": 1, \"baz\": \"bran\"})\n\n    stub(**complex_kwargs)\n    stub.assert_called_with(**complex_kwargs)\n    stub.assert_called_once_with(**complex_kwargs)\n\n    with assert_argument_introspection(complex_kwargs, wrong_kwargs):\n        stub.assert_called_with(**wrong_kwargs)\n        stub.assert_called_once_with(**wrong_kwargs)\n\n\ndef test_assert_any_call_wrapper(mocker: MockerFixture) -> None:\n    stub = mocker.stub()\n    stub(\"foo\")\n    stub(\"foo\")\n    stub.assert_any_call(\"foo\")\n    with assert_traceback():\n        stub.assert_any_call(\"bar\")\n\n\ndef test_assert_has_calls(mocker: MockerFixture) -> None:\n    stub = mocker.stub()\n    stub(\"foo\")\n    stub.assert_has_calls([mocker.call(\"foo\")])\n    with assert_traceback():\n        stub.assert_has_calls([mocker.call(\"bar\")])\n\n\ndef test_assert_has_calls_multiple_calls(mocker: MockerFixture) -> None:\n    stub = mocker.stub()\n    stub(\"foo\")\n    stub(\"bar\")\n    stub(\"baz\")\n    stub.assert_has_calls([mocker.call(\"foo\"), mocker.call(\"bar\"), mocker.call(\"baz\")])\n    with assert_traceback():\n        stub.assert_has_calls(\n            [\n                mocker.call(\"foo\"),\n                mocker.call(\"bar\"),\n                mocker.call(\"baz\"),\n                mocker.call(\"bat\"),\n            ]\n        )\n    with assert_traceback():\n        stub.assert_has_calls(\n            [mocker.call(\"foo\"), mocker.call(\"baz\"), mocker.call(\"bar\")]\n        )\n\n\ndef test_assert_has_calls_multiple_calls_subset(mocker: MockerFixture) -> None:\n    stub = mocker.stub()\n    stub(\"foo\")\n    stub(\"bar\")\n    stub(\"baz\")\n    stub.assert_has_calls([mocker.call(\"bar\"), mocker.call(\"baz\")])\n    with assert_traceback():\n        stub.assert_has_calls([mocker.call(\"foo\"), mocker.call(\"baz\")])\n    with assert_traceback():\n        stub.assert_has_calls(\n            [mocker.call(\"foo\"), mocker.call(\"bar\"), mocker.call(\"bat\")]\n        )\n    with assert_traceback():\n        stub.assert_has_calls([mocker.call(\"baz\"), mocker.call(\"bar\")])\n\n\ndef test_assert_has_calls_multiple_calls_any_order(mocker: MockerFixture) -> None:\n    stub = mocker.stub()\n    stub(\"foo\")\n    stub(\"bar\")\n    stub(\"baz\")\n    stub.assert_has_calls(\n        [mocker.call(\"foo\"), mocker.call(\"baz\"), mocker.call(\"bar\")], any_order=True\n    )\n    with assert_traceback():\n        stub.assert_has_calls(\n            [\n                mocker.call(\"foo\"),\n                mocker.call(\"baz\"),\n                mocker.call(\"bar\"),\n                mocker.call(\"bat\"),\n            ],\n            any_order=True,\n        )\n\n\ndef test_assert_has_calls_multiple_calls_any_order_subset(\n    mocker: MockerFixture,\n) -> None:\n    stub = mocker.stub()\n    stub(\"foo\")\n    stub(\"bar\")\n    stub(\"baz\")\n    stub.assert_has_calls([mocker.call(\"baz\"), mocker.call(\"foo\")], any_order=True)\n    with assert_traceback():\n        stub.assert_has_calls(\n            [mocker.call(\"baz\"), mocker.call(\"foo\"), mocker.call(\"bat\")], any_order=True\n        )\n\n\ndef test_assert_has_calls_no_calls(\n    mocker: MockerFixture,\n) -> None:\n    stub = mocker.stub()\n    stub.assert_has_calls([])\n    with assert_traceback():\n        stub.assert_has_calls([mocker.call(\"foo\")])\n\n\ndef test_monkeypatch_ini(testdir: Any, mocker: MockerFixture) -> None:\n    # Make sure the following function actually tests something\n    stub = mocker.stub()\n    assert stub.assert_called_with.__module__ != stub.__module__\n\n    testdir.makepyfile(\n        \"\"\"\n        def test_foo(mocker):\n            stub = mocker.stub()\n            assert stub.assert_called_with.__module__ == stub.__module__\n    \"\"\"\n    )\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        mock_traceback_monkeypatch = false\n    \"\"\"\n    )\n    result = testdir.runpytest_subprocess()\n    assert result.ret == 0\n\n\ndef test_parse_ini_boolean() -> None:\n    from pytest_mock._util import parse_ini_boolean\n\n    assert parse_ini_boolean(\"True\") is True\n    assert parse_ini_boolean(\"false\") is False\n    with pytest.raises(ValueError):\n        parse_ini_boolean(\"foo\")\n\n\ndef test_patched_method_parameter_name(mocker: MockerFixture) -> None:\n    \"\"\"Test that our internal code uses uncommon names when wrapping other\n    \"mock\" methods to avoid conflicts with user code (#31).\n    \"\"\"\n\n    class Request:\n        @classmethod\n        def request(cls, method, args):\n            pass\n\n    m = mocker.patch.object(Request, \"request\")\n    Request.request(method=\"get\", args={\"type\": \"application/json\"})\n    m.assert_called_once_with(method=\"get\", args={\"type\": \"application/json\"})\n\n\ndef test_monkeypatch_native(testdir: Any) -> None:\n    \"\"\"Automatically disable monkeypatching when --tb=native.\"\"\"\n    testdir.makepyfile(\n        \"\"\"\n        def test_foo(mocker):\n            stub = mocker.stub()\n            stub(1, greet='hello')\n            stub.assert_called_once_with(1, greet='hey')\n    \"\"\"\n    )\n    result = testdir.runpytest_subprocess(\"--tb=native\")\n    assert result.ret == 1\n    assert \"During handling of the above exception\" not in result.stdout.str()\n    assert \"Differing items:\" not in result.stdout.str()\n    traceback_lines = [\n        x\n        for x in result.stdout.str().splitlines()\n        if \"Traceback (most recent call last)\" in x\n    ]\n    assert (\n        len(traceback_lines) == 1\n    )  # make sure there are no duplicated tracebacks (#44)\n\n\ndef test_monkeypatch_no_terminal(testdir: Any) -> None:\n    \"\"\"Don't crash without 'terminal' plugin.\"\"\"\n    testdir.makepyfile(\n        \"\"\"\n        def test_foo(mocker):\n            stub = mocker.stub()\n            stub(1, greet='hello')\n            stub.assert_called_once_with(1, greet='hey')\n        \"\"\"\n    )\n    result = testdir.runpytest_subprocess(\"-p\", \"no:terminal\", \"-s\")\n    assert result.ret == 1\n    assert result.stdout.lines == []\n\n\ndef test_standalone_mock(testdir: Any) -> None:\n    \"\"\"Check that the \"mock_use_standalone\" is being used.\"\"\"\n    pytest.importorskip(\"mock\")\n\n    testdir.makepyfile(\n        \"\"\"\n        import mock\n\n        def test_foo(mocker):\n            assert mock.MagicMock is mocker.MagicMock\n    \"\"\"\n    )\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        mock_use_standalone_module = true\n    \"\"\"\n    )\n    result = testdir.runpytest_subprocess()\n    assert result.ret == 0\n\n\n@pytest.mark.usefixtures(\"needs_assert_rewrite\")\ndef test_detailed_introspection(testdir: Any) -> None:\n    \"\"\"Check that the \"mock_use_standalone\" is being used.\"\"\"\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        asyncio_mode=auto\n        \"\"\"\n    )\n    testdir.makepyfile(\n        \"\"\"\n        def test(mocker):\n            m = mocker.Mock()\n            m('fo')\n            m.assert_called_once_with('', bar=4)\n    \"\"\"\n    )\n    result = testdir.runpytest(\"-s\")\n    expected_lines = [\n        \"*AssertionError: expected call not found.\",\n        \"*Expected: mock('', bar=4)\",\n        \"*Actual: mock('fo')\",\n    ]\n    expected_lines += [\n        \"*pytest introspection follows:*\",\n        \"*Args:\",\n        \"*assert ('fo',) == ('',)\",\n        \"*At index 0 diff: 'fo' != ''*\",\n        \"*Use -v to*\",\n        \"*Kwargs:*\",\n        \"*assert {} == {'bar': 4}*\",\n        \"*Right contains* more item*\",\n        \"*{'bar': 4}*\",\n        \"*Use -v to*\",\n    ]\n    result.stdout.fnmatch_lines(expected_lines)\n\n\n@pytest.mark.usefixtures(\"needs_assert_rewrite\")\ndef test_detailed_introspection_async(testdir: Any) -> None:\n    \"\"\"Check that the \"mock_use_standalone\" is being used.\"\"\"\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        asyncio_mode=auto\n        \"\"\"\n    )\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n\n        async def test(mocker):\n            m = mocker.AsyncMock()\n            await m('fo')\n            m.assert_awaited_once_with('', bar=4)\n    \"\"\"\n    )\n    result = testdir.runpytest(\"-s\")\n    expected_lines = [\n        \"*AssertionError: expected await not found.\",\n        \"*Expected: mock('', bar=4)\",\n        \"*Actual: mock('fo')\",\n        \"*pytest introspection follows:*\",\n        \"*Args:\",\n        \"*assert ('fo',) == ('',)\",\n        \"*At index 0 diff: 'fo' != ''*\",\n        \"*Use -v to*\",\n        \"*Kwargs:*\",\n        \"*assert {} == {'bar': 4}*\",\n        \"*Right contains* more item*\",\n        \"*{'bar': 4}*\",\n        \"*Use -v to*\",\n    ]\n    result.stdout.fnmatch_lines(expected_lines)\n\n\ndef test_missing_introspection(testdir: Any) -> None:\n    testdir.makepyfile(\n        \"\"\"\n        def test_foo(mocker):\n            mock = mocker.Mock()\n            mock('foo')\n            mock('test')\n            mock.assert_called_once_with('test')\n    \"\"\"\n    )\n    result = testdir.runpytest()\n    assert \"pytest introspection follows:\" not in result.stdout.str()\n\n\ndef test_assert_called_with_unicode_arguments(mocker: MockerFixture) -> None:\n    \"\"\"Test bug in assert_call_with called with non-ascii unicode string (#91)\"\"\"\n    stub = mocker.stub()\n    stub(b\"l\\xc3\\xb6k\".decode(\"UTF-8\"))\n\n    with pytest.raises(AssertionError):\n        stub.assert_called_with(\"lak\")\n\n\ndef test_plain_stopall(testdir: Any) -> None:\n    \"\"\"patch.stopall() in a test should not cause an error during unconfigure (#137)\"\"\"\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        asyncio_mode=auto\n        \"\"\"\n    )\n    testdir.makepyfile(\n        \"\"\"\n        import random\n\n        def get_random_number():\n            return random.randint(0, 100)\n\n        def test_get_random_number(mocker):\n            patcher = mocker.mock_module.patch(\"random.randint\", lambda x, y: 5)\n            patcher.start()\n            assert get_random_number() == 5\n            mocker.mock_module.patch.stopall()\n    \"\"\"\n    )\n    result = testdir.runpytest_subprocess()\n    result.stdout.fnmatch_lines(\"* 1 passed in *\")\n    assert \"RuntimeError\" not in result.stderr.str()\n\n\ndef test_warn_patch_object_context_manager(mocker: MockerFixture) -> None:\n    class A:\n        def doIt(self):\n            return False\n\n    a = A()\n\n    expected_warning_msg = (\n        \"Mocks returned by pytest-mock do not need to be used as context managers. \"\n        \"The mocker fixture automatically undoes mocking at the end of a test. \"\n        \"This warning can be ignored if it was triggered by mocking a context manager. \"\n        \"https://pytest-mock.readthedocs.io/en/latest/usage.html#usage-as-context-manager\"\n    )\n\n    with pytest.warns(\n        PytestMockWarning, match=re.escape(expected_warning_msg)\n    ) as warn_record:\n        with mocker.patch.object(a, \"doIt\", return_value=True):\n            assert a.doIt() is True\n\n    assert warn_record[0].filename == __file__\n\n\ndef test_warn_patch_context_manager(mocker: MockerFixture) -> None:\n    expected_warning_msg = (\n        \"Mocks returned by pytest-mock do not need to be used as context managers. \"\n        \"The mocker fixture automatically undoes mocking at the end of a test. \"\n        \"This warning can be ignored if it was triggered by mocking a context manager. \"\n        \"https://pytest-mock.readthedocs.io/en/latest/usage.html#usage-as-context-manager\"\n    )\n\n    with pytest.warns(\n        PytestMockWarning, match=re.escape(expected_warning_msg)\n    ) as warn_record:\n        with mocker.patch(\"json.loads\"):\n            pass\n\n    assert warn_record[0].filename == __file__\n\n\ndef test_context_manager_patch_example(mocker: MockerFixture) -> None:\n    \"\"\"Our message about misusing mocker as a context manager should not affect mocking\n    context managers (see #192)\"\"\"\n\n    class dummy_module:\n        class MyContext:\n            def __enter__(self, *args, **kwargs):\n                return 10\n\n            def __exit__(self, *args, **kwargs):\n                pass\n\n    def my_func():\n        with dummy_module.MyContext() as v:\n            return v\n\n    mocker.patch.object(dummy_module, \"MyContext\")\n    assert isinstance(my_func(), mocker.MagicMock)\n\n\ndef test_patch_context_manager_with_context_manager(mocker: MockerFixture) -> None:\n    \"\"\"Test that no warnings are issued when an object patched with\n    patch.context_manager is used as a context manager (#221)\"\"\"\n\n    class A:\n        def doIt(self):\n            return False\n\n    a = A()\n\n    with warnings.catch_warnings(record=True) as warn_record:\n        with mocker.patch.context_manager(a, \"doIt\", return_value=True):\n            assert a.doIt() is True\n\n    assert len(warn_record) == 0\n\n\ndef test_abort_patch_context_manager_with_stale_pyc(testdir: Any) -> None:\n    \"\"\"Ensure we don't trigger an error in case the frame where mocker.patch is being\n    used doesn't have a 'context' (#169)\"\"\"\n    import compileall\n\n    py_fn = testdir.makepyfile(\n        c=\"\"\"\n        class C:\n            x = 1\n\n        def check(mocker):\n            mocker.patch.object(C, \"x\", 2)\n            assert C.x == 2\n    \"\"\"\n    )\n    testdir.syspathinsert()\n\n    testdir.makepyfile(\n        \"\"\"\n        from c import check\n        def test_foo(mocker):\n            check(mocker)\n    \"\"\"\n    )\n    result = testdir.runpytest()\n    result.assert_outcomes(passed=1)\n\n    assert compileall.compile_file(str(py_fn), legacy=True)\n\n    pyc_fn = str(py_fn) + \"c\"\n    assert os.path.isfile(pyc_fn)\n\n    py_fn.remove()\n    result = testdir.runpytest()\n    result.assert_outcomes(passed=1)\n\n\ndef test_used_with_class_scope(testdir: Any) -> None:\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        asyncio_mode=auto\n        \"\"\"\n    )\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        import random\n        import unittest\n\n        def get_random_number():\n            return random.randint(0, 1)\n\n        @pytest.fixture(autouse=True, scope=\"class\")\n        def randint_mock(class_mocker):\n            return class_mocker.patch(\"random.randint\", lambda x, y: 5)\n\n        class TestGetRandomNumber(unittest.TestCase):\n            def test_get_random_number(self):\n                assert get_random_number() == 5\n    \"\"\"\n    )\n    result = testdir.runpytest_subprocess()\n    assert \"AssertionError\" not in result.stderr.str()\n    result.stdout.fnmatch_lines(\"* 1 passed in *\")\n\n\ndef test_used_with_module_scope(testdir: Any) -> None:\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        asyncio_mode=auto\n        \"\"\"\n    )\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        import random\n\n        def get_random_number():\n            return random.randint(0, 1)\n\n        @pytest.fixture(autouse=True, scope=\"module\")\n        def randint_mock(module_mocker):\n            return module_mocker.patch(\"random.randint\", lambda x, y: 5)\n\n        def test_get_random_number():\n            assert get_random_number() == 5\n    \"\"\"\n    )\n    result = testdir.runpytest_subprocess()\n    assert \"AssertionError\" not in result.stderr.str()\n    result.stdout.fnmatch_lines(\"* 1 passed in *\")\n\n\ndef test_used_with_package_scope(testdir: Any) -> None:\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        asyncio_mode=auto\n        \"\"\"\n    )\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        import random\n\n        def get_random_number():\n            return random.randint(0, 1)\n\n        @pytest.fixture(autouse=True, scope=\"package\")\n        def randint_mock(package_mocker):\n            return package_mocker.patch(\"random.randint\", lambda x, y: 5)\n\n        def test_get_random_number():\n            assert get_random_number() == 5\n    \"\"\"\n    )\n    result = testdir.runpytest_subprocess()\n    assert \"AssertionError\" not in result.stderr.str()\n    result.stdout.fnmatch_lines(\"* 1 passed in *\")\n\n\ndef test_used_with_session_scope(testdir: Any) -> None:\n    testdir.makeini(\n        \"\"\"\n        [pytest]\n        asyncio_mode=auto\n        \"\"\"\n    )\n    testdir.makepyfile(\n        \"\"\"\n        import pytest\n        import random\n\n        def get_random_number():\n            return random.randint(0, 1)\n\n        @pytest.fixture(autouse=True, scope=\"session\")\n        def randint_mock(session_mocker):\n            return session_mocker.patch(\"random.randint\", lambda x, y: 5)\n\n        def test_get_random_number():\n            assert get_random_number() == 5\n    \"\"\"\n    )\n    result = testdir.runpytest_subprocess()\n    assert \"AssertionError\" not in result.stderr.str()\n    result.stdout.fnmatch_lines(\"* 1 passed in *\")\n\n\ndef test_stop_patch(mocker):\n    class UnSpy:\n        def foo(self):\n            return 42\n\n    m = mocker.patch.object(UnSpy, \"foo\", return_value=0)\n    assert UnSpy().foo() == 0\n    mocker.stop(m)\n    assert UnSpy().foo() == 42\n\n    with pytest.raises(ValueError):\n        mocker.stop(m)\n\n\ndef test_stop_instance_patch(mocker):\n    class UnSpy:\n        def foo(self):\n            return 42\n\n    m = mocker.patch.object(UnSpy, \"foo\", return_value=0)\n    un_spy = UnSpy()\n    assert un_spy.foo() == 0\n    mocker.stop(m)\n    assert un_spy.foo() == 42\n\n\ndef test_stop_spy(mocker):\n    class UnSpy:\n        def foo(self):\n            return 42\n\n    spy = mocker.spy(UnSpy, \"foo\")\n    assert UnSpy().foo() == 42\n    assert spy.call_count == 1\n    mocker.stop(spy)\n    assert UnSpy().foo() == 42\n    assert spy.call_count == 1\n\n\ndef test_stop_instance_spy(mocker):\n    class UnSpy:\n        def foo(self):\n            return 42\n\n    spy = mocker.spy(UnSpy, \"foo\")\n    un_spy = UnSpy()\n    assert un_spy.foo() == 42\n    assert spy.call_count == 1\n    mocker.stop(spy)\n    assert un_spy.foo() == 42\n    assert spy.call_count == 1\n\n\ndef test_stop_multiple_patches(mocker: MockerFixture) -> None:\n    \"\"\"Regression for #420.\"\"\"\n\n    class Class1:\n        @staticmethod\n        def get():\n            return 1\n\n    class Class2:\n        @staticmethod\n        def get():\n            return 2\n\n    def handle_get():\n        return 3\n\n    mocker.patch.object(Class1, \"get\", handle_get)\n    mocker.patch.object(Class2, \"get\", handle_get)\n\n    mocker.stopall()\n\n    assert Class1.get() == 1\n    assert Class2.get() == 2\n", "metadata": {"source": "github.com/pytest-dev/pytest-mock", "file_path": "tests/test_pytest_mock.py", "language": "python", "complexity": "complex", "star_count": 1969, "quality_score": 0.77, "features": {"has_classes": true, "has_functions": true, "has_async": true, "has_decorators": true, "has_imports": true, "has_error_handling": true, "has_documentation": true, "has_type_hints": true, "has_testing": true, "has_logging": false, "function_count": 126, "class_count": 33, "line_count": 1293}, "authority_level": "medium", "authentic": true}}
{"domain": "code_generation", "category": "testing_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create comprehensive test suite with fixtures and mocks\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import logging\nimport random\nimport uuid\nimport os\nfrom flask import Blueprint, jsonify, session, request, current_app\nfrom datetime import datetime, timedelta\nfrom decimal import Decimal\n\nfrom app.api.tree import Tree\nfrom app.utils.code import ResponseCode\nfrom app.utils.response import ResMsg\nfrom app.utils.util import route, Redis, CaptchaTool, PhoneTool\nfrom app.utils.auth import Auth, login_required\nfrom app.api.report import excel_write, word_write, pdf_write\nfrom app.api.wx_login_or_register import get_access_code, get_wx_user_info, wx_login_or_register\nfrom app.api.phone_login_or_register import SendSms, phone_login_or_register\nfrom app.celery import add, flask_app_context\n\nbp = Blueprint(\"test\", __name__, url_prefix='/')\n\nlogger = logging.getLogger(__name__)\n\n\n# -----------------原生蓝图路由---------------#\n\n\n@bp.route('/logs', methods=[\"GET\"])\ndef test_logger():\n    \"\"\"\n    测试自定义logger\n    :return:\n    \"\"\"\n    logger.info(\"this is info\")\n    logger.debug(\"this is debug\")\n    logger.warning(\"this is warning\")\n    logger.error(\"this is error\")\n    logger.critical(\"this is critical\")\n    return \"ok\"\n\n\n@bp.route(\"/unifiedResponse\", methods=[\"GET\"])\ndef test_unified_response():\n    \"\"\"\n    测试统一返回消息\n    :return:\n    \"\"\"\n    res = ResMsg()\n    test_dict = dict(name=\"zhang\", age=18)\n    res.update(code=ResponseCode.Success, data=test_dict)\n    return jsonify(res.data)\n\n\n# --------------使用自定义封装蓝图路由--------------------#\n\n\n@route(bp, '/packedResponse', methods=[\"GET\"])\ndef test_packed_response():\n    \"\"\"\n    测试响应封装\n    :return:\n    \"\"\"\n    res = ResMsg()\n    test_dict = dict(name=\"zhang\", age=18)\n    # 此处只需要填入响应状态码,即可获取到对应的响应消息\n    res.update(code=ResponseCode.Success, data=test_dict)\n    # 此处不再需要用jsonify，如果需要定制返回头或者http响应如下所示\n    # return res.data,200,{\"token\":\"111\"}\n    return res.data\n\n\n@route(bp, '/typeResponse', methods=[\"GET\"])\ndef test_type_response():\n    \"\"\"\n    测试返回不同的类型\n    :return:\n    \"\"\"\n    res = ResMsg()\n    now = datetime.now()\n    date = datetime.now().date()\n    num = Decimal(11.11)\n    test_dict = dict(now=now, date=date, num=num)\n    # 此处只需要填入响应状态码,即可获取到对应的响应消息\n    res.update(code=ResponseCode.Success, data=test_dict)\n    # 此处不再需要用jsonify，如果需要定制返回头或者http响应如下所示\n    # return res.data,200,{\"token\":\"111\"}\n    return res.data\n\n\n# --------------Redis测试封装--------------------#\n\n@route(bp, '/testRedisWrite', methods=['GET'])\ndef test_redis_write():\n    \"\"\"\n    测试redis写入\n    \"\"\"\n    # 写入\n    Redis.write(\"test_key\", \"test_value\", 60)\n    return \"ok\"\n\n\n@route(bp, '/testRedisRead', methods=['GET'])\ndef test_redis_read():\n    \"\"\"\n    测试redis获取\n    \"\"\"\n    data = Redis.read(\"test_key\")\n    return data\n\n\n# -----------------图形验证码测试---------------------------#\n\n@route(bp, '/testGetCaptcha', methods=[\"GET\"])\ndef test_get_captcha():\n    \"\"\"\n    获取图形验证码\n    :return:\n    \"\"\"\n    res = ResMsg()\n    new_captcha = CaptchaTool()\n    img, code = new_captcha.get_verify_code()\n    res.update(data=img)\n    session[\"code\"] = code\n    return res.data\n\n\n@route(bp, '/testVerifyCaptcha', methods=[\"POST\"])\ndef test_verify_captcha():\n    \"\"\"\n    验证图形验证码\n    :return:\n    \"\"\"\n    res = ResMsg()\n    obj = request.get_json(force=True)\n    code = obj.get('code', None)\n    s_code = session.get(\"code\", None)\n    print(code, s_code)\n    if not all([code, s_code]):\n        res.update(code=ResponseCode.InvalidParameter)\n        return res.data\n    if code != s_code:\n        res.update(code=ResponseCode.VerificationCodeError)\n        return res.data\n    return res.data\n\n\n# --------------------JWT测试-----------------------------------------#\n\n@route(bp, '/testLogin', methods=[\"POST\"])\ndef test_login():\n    \"\"\"\n    登陆成功获取到数据获取token和刷新token\n    :return:\n    \"\"\"\n    res = ResMsg()\n    obj = request.get_json(force=True)\n    user_name = obj.get(\"name\")\n    # 未获取到参数或参数不存在\n    if not obj or not user_name:\n        res.update(code=ResponseCode.InvalidParameter)\n        return res.data\n\n    if user_name == \"qin\":\n        # 生成数据获取token和刷新token\n        access_token, refresh_token = Auth.encode_auth_token(user_id=user_name)\n\n        data = {\"access_token\": access_token.decode(\"utf-8\"),\n                \"refresh_token\": refresh_token.decode(\"utf-8\")\n                }\n        res.update(data=data)\n        return res.data\n    else:\n        res.update(code=ResponseCode.AccountOrPassWordErr)\n        return res.data\n\n\n@route(bp, '/testGetData', methods=[\"GET\"])\n@login_required\ndef test_get_data():\n    \"\"\"\n    测试登陆保护下获取数据\n    :return:\n    \"\"\"\n    res = ResMsg()\n    name = session.get(\"user_name\")\n    data = \"{}，你好！！\".format(name)\n    res.update(data=data)\n    return res.data\n\n\n@route(bp, '/testRefreshToken', methods=[\"GET\"])\ndef test_refresh_token():\n    \"\"\"\n    刷新token，获取新的数据获取token\n    :return:\n    \"\"\"\n    res = ResMsg()\n    refresh_token = request.args.get(\"refresh_token\")\n    if not refresh_token:\n        res.update(code=ResponseCode.InvalidParameter)\n        return res.data\n    payload = Auth.decode_auth_token(refresh_token)\n    # token被串改或过期\n    if not payload:\n        res.update(code=ResponseCode.PleaseSignIn)\n        return res.data\n\n    # 判断token正确性\n    if \"user_id\" not in payload:\n        res.update(code=ResponseCode.PleaseSignIn)\n        return res.data\n    # 获取新的token\n    access_token = Auth.generate_access_token(user_id=payload[\"user_id\"])\n    data = {\"access_token\": access_token.decode(\"utf-8\"), \"refresh_token\": refresh_token}\n    res.update(data=data)\n    return res.data\n\n\n# --------------------测试Excel报表输出-------------------------------#\n\n@route(bp, '/testExcel', methods=[\"GET\"])\ndef test_excel():\n    \"\"\"\n    测试excel报表输出\n    :return:\n    \"\"\"\n    res = ResMsg()\n    report_path = current_app.config.get(\"REPORT_PATH\", \"./report\")\n    file_name = \"{}.xlsx\".format(uuid.uuid4().hex)\n    path = os.path.join(report_path, file_name)\n    path = excel_write(path)\n    path = path.lstrip(\".\")\n    res.update(data=path)\n    return res.data\n\n\n# --------------------测试Word报表输出-------------------------------#\n\n@route(bp, '/testWord', methods=[\"GET\"])\ndef test_word():\n    \"\"\"\n    测试word报表输出\n    :return:\n    \"\"\"\n    res = ResMsg()\n    report_path = current_app.config.get(\"REPORT_PATH\", \"./report\")\n    file_name = \"{}.docx\".format(uuid.uuid4().hex)\n    path = os.path.join(report_path, file_name)\n    path = word_write(path)\n    path = path.lstrip(\".\")\n    res.update(data=path)\n    return res.data\n\n\n# --------------------测试无限层级目录树-------------------------------#\n\n@route(bp, '/testTree', methods=[\"GET\"])\ndef test_tree():\n    \"\"\"\n    测试无限层级目录树\n    :return:\n    \"\"\"\n    res = ResMsg()\n    data = [\n        {\"id\": 1, \"father_id\": None, \"name\": \"01\"},\n        {\"id\": 2, \"father_id\": 1, \"name\": \"0101\"},\n        {\"id\": 3, \"father_id\": 1, \"name\": \"0102\"},\n        {\"id\": 4, \"father_id\": 1, \"name\": \"0103\"},\n        {\"id\": 5, \"father_id\": 2, \"name\": \"010101\"},\n        {\"id\": 6, \"father_id\": 2, \"name\": \"010102\"},\n        {\"id\": 7, \"father_id\": 2, \"name\": \"010103\"},\n        {\"id\": 8, \"father_id\": 3, \"name\": \"010201\"},\n        {\"id\": 9, \"father_id\": 4, \"name\": \"010301\"},\n        {\"id\": 10, \"father_id\": 9, \"name\": \"01030101\"},\n        {\"id\": 11, \"father_id\": 9, \"name\": \"01030102\"},\n    ]\n\n    new_tree = Tree(data=data)\n\n    data = new_tree.build_tree()\n\n    res.update(data=data)\n    return res.data\n\n\n# --------------------测试微信登陆注册-------------------------------#\n\n@route(bp, '/testWXLoginOrRegister', methods=[\"GET\"])\ndef test_wx_login_or_register():\n    \"\"\"\n    测试微信登陆注册\n    :return:\n    \"\"\"\n    res = ResMsg()\n    code = request.args.get(\"code\")\n    flag = request.args.get(\"flag\")\n    # 参数错误\n    if code is None or flag is None:\n        res.update(code=ResponseCode.InvalidParameter)\n        return res.data\n    # 获取微信用户授权码\n    access_code = get_access_code(code=code, flag=flag)\n    if access_code is None:\n        res.update(code=ResponseCode.WeChatAuthorizationFailure)\n        return res.data\n    # 获取微信用户信息\n    wx_user_info = get_wx_user_info(access_data=access_code)\n    if wx_user_info is None:\n        res.update(code=ResponseCode.WeChatAuthorizationFailure)\n        return res.data\n\n    # 验证微信用户信息本平台是否有，\n    data = wx_login_or_register(wx_user_info=wx_user_info)\n    if data is None:\n        res.update(code=ResponseCode.Fail)\n        return res.data\n    res.update(data=data)\n    return res.data\n\n\n# --------------------测试手机短信验证码登陆注册-------------------------------#\n\n@route(bp, '/testGetVerificationCode', methods=[\"GET\"])\ndef test_get_verification_code():\n    \"\"\"\n    获取手机验证码\n    :return:\n    \"\"\"\n    now = datetime.now()\n    res = ResMsg()\n\n    category = request.args.get(\"category\", None)\n    # category 参数如下：\n    # authentication: 身份验证\n    # login_confirmation: 登陆验证\n    # login_exception: 登陆异常\n    # user_registration: 用户注册\n    # change_password: 修改密码\n    # information_change: 信息修改\n\n    phone = request.args.get('phone', None)\n\n    # 验证手机号码正确性\n    re_phone = PhoneTool.check_phone(phone)\n    if phone is None or re_phone is None:\n        res.update(code=ResponseCode.MobileNumberError)\n        return res.data\n    if category is None:\n        res.update(code=ResponseCode.InvalidParameter)\n        return res.data\n\n    try:\n        # 获取手机验证码设置时间\n        flag = Redis.hget(re_phone, 'expire_time')\n        if flag is not None:\n            flag = datetime.strptime(flag, '%Y-%m-%d %H:%M:%S')\n            # 判断是否重复操作\n            if (flag - now).total_seconds() < 60:\n                res.update(code=ResponseCode.FrequentOperation)\n                return res.data\n\n        # 获取随机验证码\n        code = \"\".join([str(random.randint(0, 9)) for _ in range(6)])\n        template_param = {\"code\": code}\n        # 发送验证码\n        sms = SendSms(phone=re_phone, category=category, template_param=template_param)\n        sms.send_sms()\n        # 将验证码存入redis，方便接下来的验证\n        Redis.hset(re_phone, \"code\", code)\n        # 设置重复操作屏障\n        Redis.hset(re_phone, \"expire_time\", (now + timedelta(minutes=1)).strftime('%Y-%m-%d %H:%M:%S'))\n        # 设置验证码过去时间\n        Redis.expire(re_phone, 60 * 3)\n        return res.data\n    except Exception as e:\n        logger.exception(e)\n        res.update(code=ResponseCode.Fail)\n        return res.data\n\n\n@route(bp, '/testPhoneLoginOrRegister', methods=[\"POST\"])\ndef test_phone_login_or_register():\n    \"\"\"\n    用户验证码登录或注册\n    :return:\n    \"\"\"\n    res = ResMsg()\n\n    obj = request.get_json(force=True)\n    phone = obj.get('account', None)\n    code = obj.get('code', None)\n    if phone is None or code is None:\n        res.update(code=ResponseCode.InvalidParameter)\n        return res.data\n    # 验证手机号和验证码是否正确\n    flag = PhoneTool.check_phone_code(phone, code)\n    if not flag:\n        res.update(code=ResponseCode.InvalidOrExpired)\n        return res.data\n\n    # 登陆或注册\n    data = phone_login_or_register(phone)\n\n    if data is None:\n        res.update(code=ResponseCode.Fail)\n        return res.data\n    res.update(data=data)\n    return res.data\n\n\n# --------------------测试PDF报表输出-------------------------------#\n\n@route(bp, '/testPDF', methods=[\"GET\"])\ndef test_pdf():\n    \"\"\"\n    测试pdf报表输出\n    :return:\n    \"\"\"\n    res = ResMsg()\n    report_path = current_app.config.get(\"REPORT_PATH\", \"./report\")\n    file_name = \"{}.pdf\".format(uuid.uuid4().hex)\n    path = os.path.join(report_path, file_name)\n    path = pdf_write(path)\n    path = path.lstrip(\".\")\n    res.update(data=path)\n    return res.data\n\n\n# --------------------测试Celery-------------------------------#\n\n\n@route(bp, '/testCeleryAdd', methods=[\"GET\"])\ndef test_add():\n    \"\"\"\n    测试相加\n    :return:\n    \"\"\"\n    result = add.delay(1, 2)\n    return result.get(timeout=1)\n\n\n@route(bp, '/testCeleryFlaskAppContext', methods=[\"GET\"])\ndef test_flask_app_context():\n    \"\"\"\n    测试获取flask上下文\n    :return:\n    \"\"\"\n    result = flask_app_context.delay()\n    return result.get(timeout=1)\n", "metadata": {"source": "github.com/qzq1111/flask-restful-example", "file_path": "app/api/api_test.py", "language": "python", "complexity": "complex", "star_count": 1004, "quality_score": 0.87, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": true, "has_documentation": true, "has_type_hints": true, "has_testing": true, "has_logging": true, "function_count": 20, "class_count": 0, "line_count": 449}, "authority_level": "medium", "authentic": true}}
{"domain": "code_generation", "category": "testing_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create comprehensive test suite with fixtures and mocks\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"\nA TestRunner for use with the Python unit testing framework. It\ngenerates a HTML report to show the result at a glance.\n\nThe simplest way to use this is to invoke its main method. E.g.\n\n    import unittest\n    import HTMLTestRunner\n\n    ... define your tests ...\n\n    if __name__ == '__main__':\n        HTMLTestRunner.main()\n\n\nFor more customization options, instantiates a HTMLTestRunner object.\nHTMLTestRunner is a counterpart to unittest's TextTestRunner. E.g.\n\n    # output to a file\n    fp = file('my_report.html', 'wb')\n    runner = HTMLTestRunner.HTMLTestRunner(\n                stream=fp,\n                title='My unit test',\n                description='This demonstrates the report output by HTMLTestRunner.'\n                )\n\n    # Use an external stylesheet.\n    # See the Template_mixin class for more customizable options\n    runner.STYLESHEET_TMPL = '<link rel=\"stylesheet\" href=\"my_stylesheet.css\" type=\"text/css\">'\n\n    # run the test\n    runner.run(my_test_suite)\n\n\n------------------------------------------------------------------------\nCopyright (c) 2004-2007, Wai Yip Tung\nAll rights reserved.\n\nRedistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are\nmet:\n\n* Redistributions of source code must retain the above copyright notice,\n  this list of conditions and the following disclaimer.\n* Redistributions in binary form must reproduce the above copyright\n  notice, this list of conditions and the following disclaimer in the\n  documentation and/or other materials provided with the distribution.\n* Neither the name Wai Yip Tung nor the names of its contributors may be\n  used to endorse or promote products derived from this software without\n  specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS\nIS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED\nTO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A\nPARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER\nOR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\nEXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\nPROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR\nPROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF\nLIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING\nNEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\nSOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\"\"\"\n\n# URL: http://tungwaiyip.info/software/HTMLTestRunner.html\n\n__author__ = \"Wai Yip Tung\"\n__version__ = \"0.8.2\"\n\n\"\"\"\nChange History\n\nVersion 0.8.2\n* Show output inline instead of popup window (Viorel Lupu).\n\nVersion in 0.8.1\n* Validated XHTML (Wolfgang Borgert).\n* Added description of test classes and test cases.\n\nVersion in 0.8.0\n* Define Template_mixin class for customization.\n* Workaround a IE 6 bug that it does not treat <script> block as CDATA.\n\nVersion in 0.7.1\n* Back port to Python 2.3 (Frank Horowitz).\n* Fix missing scroll bars in detail log (Podi).\n\"\"\"\n\n# TODO: color stderr\n# TODO: simplify javascript using ,ore than 1 class in the class attribute?\n\nimport datetime\nimport io\nimport sys\nimport time\nimport unittest\nfrom xml.sax import saxutils\n\n\n# ------------------------------------------------------------------------\n# The redirectors below are used to capture output during testing. Output\n# sent to sys.stdout and sys.stderr are automatically captured. However\n# in some cases sys.stdout is already cached before HTMLTestRunner is\n# invoked (e.g. calling logging.basicConfig). In order to capture those\n# output, use the redirectors for the cached stream.\n#\n# e.g.\n#   >>> logging.basicConfig(stream=HTMLTestRunner.stdout_redirector)\n#   >>>\n\nclass OutputRedirector(object):\n    \"\"\" Wrapper to redirect stdout or stderr \"\"\"\n\n    def __init__(self, fp):\n        self.fp = fp\n\n    def write(self, s):\n        self.fp.write(s)\n\n    def writelines(self, lines):\n        self.fp.writelines(lines)\n\n    def flush(self):\n        self.fp.flush()\n\n\nstdout_redirector = OutputRedirector(sys.stdout)\nstderr_redirector = OutputRedirector(sys.stderr)\n\n\n# ----------------------------------------------------------------------\n# Template\n\nclass Template_mixin(object):\n    \"\"\"\n    Define a HTML template for report customerization and generation.\n\n    Overall structure of an HTML report\n\n    HTML\n    +------------------------+\n    |<html>                  |\n    |  <head>                |\n    |                        |\n    |   STYLESHEET           |\n    |   +----------------+   |\n    |   |                |   |\n    |   +----------------+   |\n    |                        |\n    |  </head>               |\n    |                        |\n    |  <body>                |\n    |                        |\n    |   HEADING              |\n    |   +----------------+   |\n    |   |                |   |\n    |   +----------------+   |\n    |                        |\n    |   REPORT               |\n    |   +----------------+   |\n    |   |                |   |\n    |   +----------------+   |\n    |                        |\n    |   ENDING               |\n    |   +----------------+   |\n    |   |                |   |\n    |   +----------------+   |\n    |                        |\n    |  </body>               |\n    |</html>                 |\n    +------------------------+\n    \"\"\"\n\n    STATUS = {\n        0: 'pass',\n        1: 'fail',\n        2: 'error',\n    }\n\n    DEFAULT_TITLE = 'Unit Test Report'\n    DEFAULT_DESCRIPTION = ''\n\n    # ------------------------------------------------------------------------\n    # HTML Template\n\n    HTML_TMPL = r\"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Strict//EN\" \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd\">\n<html xmlns=\"http://www.w3.org/1999/xhtml\">\n<head>\n    <title>%(title)s</title>\n    <meta name=\"generator\" content=\"%(generator)s\"/>\n    <meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8\"/>\n    %(stylesheet)s\n</head>\n<body>\n<script language=\"javascript\" type=\"text/javascript\"><!--\noutput_list = Array();\n\n/* level - 0:Summary; 1:Failed; 2:All */\nfunction showCase(level) {\n    trs = document.getElementsByTagName(\"tr\");\n    for (var i = 0; i < trs.length; i++) {\n        tr = trs[i];\n        id = tr.id;\n        if (id.substr(0,2) == 'ft') {\n            if (level < 1) {\n                tr.className = 'hiddenRow';\n            }\n            else {\n                tr.className = '';\n            }\n        }\n        if (id.substr(0,2) == 'pt') {\n            if (level > 1) {\n                tr.className = '';\n            }\n            else {\n                tr.className = 'hiddenRow';\n            }\n        }\n    }\n}\n\n\nfunction showClassDetail(cid, count) {\n    var id_list = Array(count);\n    var toHide = 1;\n    for (var i = 0; i < count; i++) {\n        tid0 = 't' + cid.substr(1) + '.' + (i+1);\n        tid = 'f' + tid0;\n        tr = document.getElementById(tid);\n        if (!tr) {\n            tid = 'p' + tid0;\n            tr = document.getElementById(tid);\n        }\n        id_list[i] = tid;\n        if (tr.className) {\n            toHide = 0;\n        }\n    }\n    for (var i = 0; i < count; i++) {\n        tid = id_list[i];\n        if (toHide) {\n            document.getElementById('div_'+tid).style.display = 'none'\n            document.getElementById(tid).className = 'hiddenRow';\n        }\n        else {\n            document.getElementById(tid).className = '';\n        }\n    }\n}\n\n\nfunction showTestDetail(div_id){\n    var details_div = document.getElementById(div_id)\n    var displayState = details_div.style.display\n    // alert(displayState)\n    if (displayState != 'block' ) {\n        displayState = 'block'\n        details_div.style.display = 'block'\n    }\n    else {\n        details_div.style.display = 'none'\n    }\n}\n\n\nfunction html_escape(s) {\n    s = s.replace(/&/g,'&amp;');\n    s = s.replace(/</g,'&lt;');\n    s = s.replace(/>/g,'&gt;');\n    return s;\n}\n\n/* obsoleted by detail in <div>\nfunction showOutput(id, name) {\n    var w = window.open(\"\", //url\n                    name,\n                    \"resizable,scrollbars,status,width=800,height=450\");\n    d = w.document;\n    d.write(\"<pre>\");\n    d.write(html_escape(output_list[id]));\n    d.write(\"\\n\");\n    d.write(\"<a href='javascript:window.close()'>close</a>\\n\");\n    d.write(\"</pre>\\n\");\n    d.close();\n}\n*/\n--></script>\n\n%(heading)s\n%(report)s\n%(ending)s\n\n</body>\n</html>\n\"\"\"\n    # variables: (title, generator, stylesheet, heading, report, ending)\n\n    # ------------------------------------------------------------------------\n    # Stylesheet\n    #\n    # alternatively use a <link> for external style sheet, e.g.\n    #   <link rel=\"stylesheet\" href=\"$url\" type=\"text/css\">\n\n    STYLESHEET_TMPL = \"\"\"\n<style type=\"text/css\" media=\"screen\">\nbody        { font-family: verdana, arial, helvetica, sans-serif; font-size: 80%; }\ntable       { font-size: 100%; }\npre         { }\n\n/* -- heading ---------------------------------------------------------------------- */\nh1 {\n\tfont-size: 16pt;\n\tcolor: gray;\n}\n.heading {\n    margin-top: 0ex;\n    margin-bottom: 1ex;\n}\n\n.heading .attribute {\n    margin-top: 1ex;\n    margin-bottom: 0;\n}\n\n.heading .description {\n    margin-top: 4ex;\n    margin-bottom: 6ex;\n}\n\n/* -- css div popup ------------------------------------------------------------------------ */\na.popup_link {\n}\n\na.popup_link:hover {\n    color: red;\n}\n\n.popup_window {\n    display: none;\n    position: relative;\n    left: 0px;\n    top: 0px;\n    /*border: solid #627173 1px; */\n    padding: 10px;\n    background-color: #E6E6D6;\n    font-family: \"Lucida Console\", \"Courier New\", Courier, monospace;\n    text-align: left;\n    font-size: 8pt;\n    width: 500px;\n}\n\n}\n/* -- report ------------------------------------------------------------------------ */\n#show_detail_line {\n    margin-top: 3ex;\n    margin-bottom: 1ex;\n}\n#result_table {\n    width: 80%;\n    border-collapse: collapse;\n    border: 1px solid #777;\n}\n#header_row {\n    font-weight: bold;\n    color: white;\n    background-color: #777;\n}\n#result_table td {\n    border: 1px solid #777;\n    padding: 2px;\n}\n#total_row  { font-weight: bold; }\n.passClass  { background-color: #6c6; }\n.failClass  { background-color: #c60; }\n.errorClass { background-color: #c00; }\n.passCase   { color: #6c6; }\n.failCase   { color: #c60; font-weight: bold; }\n.errorCase  { color: #c00; font-weight: bold; }\n.hiddenRow  { display: none; }\n.testcase   { margin-left: 2em; }\n\n\n/* -- ending ---------------------------------------------------------------------- */\n#ending {\n}\n\n</style>\n\"\"\"\n\n    # ------------------------------------------------------------------------\n    # Heading\n    #\n\n    HEADING_TMPL = \"\"\"<div class='heading'>\n<h1>%(title)s</h1>\n%(parameters)s\n<p class='description'>%(description)s</p>\n</div>\n\n\"\"\"  # variables: (title, parameters, description)\n\n    HEADING_ATTRIBUTE_TMPL = \"\"\"<p class='attribute'><strong>%(name)s:</strong> %(value)s</p>\n\"\"\"  # variables: (name, value)\n\n    # ------------------------------------------------------------------------\n    # Report\n    #\n\n    REPORT_TMPL = \"\"\"\n<p id='show_detail_line'>Show\n<a href='javascript:showCase(0)'>Summary</a>\n<a href='javascript:showCase(1)'>Failed</a>\n<a href='javascript:showCase(2)'>All</a>\n</p>\n<table id='result_table'>\n<colgroup>\n<col align='left' />\n<col align='right' />\n<col align='right' />\n<col align='right' />\n<col align='right' />\n<col align='right' />\n</colgroup>\n<tr id='header_row'>\n    <td>Test Group/Test case</td>\n    <td>Count</td>\n    <td>Pass</td>\n    <td>Fail</td>\n    <td>Error</td>\n    <td>View</td>\n</tr>\n%(test_list)s\n<tr id='total_row'>\n    <td>Total</td>\n    <td>%(count)s</td>\n    <td>%(Pass)s</td>\n    <td>%(fail)s</td>\n    <td>%(error)s</td>\n    <td>&nbsp;</td>\n</tr>\n</table>\n\"\"\"  # variables: (test_list, count, Pass, fail, error)\n\n    REPORT_CLASS_TMPL = r\"\"\"\n<tr class='%(style)s'>\n    <td>%(desc)s</td>\n    <td>%(count)s</td>\n    <td>%(Pass)s</td>\n    <td>%(fail)s</td>\n    <td>%(error)s</td>\n    <td><a href=\"javascript:showClassDetail('%(cid)s',%(count)s)\">Detail</a></td>\n</tr>\n\"\"\"  # variables: (style, desc, count, Pass, fail, error, cid)\n\n    REPORT_TEST_WITH_OUTPUT_TMPL = r\"\"\"\n<tr id='%(tid)s' class='%(Class)s'>\n    <td class='%(style)s'><div class='testcase'>%(desc)s</div></td>\n    <td colspan='5' align='center'>\n\n    <!--css div popup start-->\n    <a class=\"popup_link\" onfocus='this.blur();' href=\"javascript:showTestDetail('div_%(tid)s')\" >\n        %(status)s</a>\n\n    <div id='div_%(tid)s' class=\"popup_window\">\n        <div style='text-align: right; color:red;cursor:pointer'>\n        <a onfocus='this.blur();' onclick=\"document.getElementById('div_%(tid)s').style.display = 'none' \" >\n           [x]</a>\n        </div>\n        <pre>\n        %(script)s\n        </pre>\n    </div>\n    <!--css div popup end-->\n\n    </td>\n</tr>\n\"\"\"  # variables: (tid, Class, style, desc, status)\n\n    REPORT_TEST_NO_OUTPUT_TMPL = r\"\"\"\n<tr id='%(tid)s' class='%(Class)s'>\n    <td class='%(style)s'><div class='testcase'>%(desc)s</div></td>\n    <td colspan='5' align='center'>%(status)s</td>\n</tr>\n\"\"\"  # variables: (tid, Class, style, desc, status)\n\n    REPORT_TEST_OUTPUT_TMPL = r\"\"\"\n%(id)s: %(output)s\n\"\"\"  # variables: (id, output)\n\n    # ------------------------------------------------------------------------\n    # ENDING\n    #\n\n    ENDING_TMPL = \"\"\"<div id='ending'>&nbsp;</div>\"\"\"\n\n\n# -------------------- The end of the Template class -------------------\n\n\nTestResult = unittest.TestResult\n\n\nclass _TestResult(TestResult):\n    # note: _TestResult is a pure representation of results.\n    # It lacks the output and reporting ability compares to unittest._TextTestResult.\n\n    def __init__(self, verbosity=1):\n        TestResult.__init__(self)\n        self.stdout0 = None\n        self.stderr0 = None\n        self.success_count = 0\n        self.failure_count = 0\n        self.error_count = 0\n        self.verbosity = verbosity\n\n        # result is a list of result in 4 tuple\n        # (\n        #   result code (0: success; 1: fail; 2: error),\n        #   TestCase object,\n        #   Test output (byte string),\n        #   stack trace,\n        # )\n        self.result = []\n\n    def startTest(self, test):\n        TestResult.startTest(self, test)\n        # just one buffer for both stdout and stderr\n        self.outputBuffer = io.StringIO()\n        stdout_redirector.fp = self.outputBuffer\n        stderr_redirector.fp = self.outputBuffer\n        self.stdout0 = sys.stdout\n        self.stderr0 = sys.stderr\n        sys.stdout = stdout_redirector\n        sys.stderr = stderr_redirector\n\n    def complete_output(self):\n        \"\"\"\n        Disconnect output redirection and return buffer.\n        Safe to call multiple times.\n        \"\"\"\n        if self.stdout0:\n            sys.stdout = self.stdout0\n            sys.stderr = self.stderr0\n            self.stdout0 = None\n            self.stderr0 = None\n        return self.outputBuffer.getvalue()\n\n    def stopTest(self, test):\n        # Usually one of addSuccess, addError or addFailure would have been called.\n        # But there are some path in unittest that would bypass this.\n        # We must disconnect stdout in stopTest(), which is guaranteed to be called.\n        self.complete_output()\n\n    def addSuccess(self, test):\n        self.success_count += 1\n        TestResult.addSuccess(self, test)\n        output = self.complete_output()\n        self.result.append((0, test, output, ''))\n        if self.verbosity > 1:\n            sys.stderr.write('ok ')\n            sys.stderr.write(str(test))\n            sys.stderr.write('\\n')\n        else:\n            sys.stderr.write('.')\n\n    def addError(self, test, err):\n        self.error_count += 1\n        TestResult.addError(self, test, err)\n        _, _exc_str = self.errors[-1]\n        output = self.complete_output()\n        self.result.append((2, test, output, _exc_str))\n        if self.verbosity > 1:\n            sys.stderr.write('E  ')\n            sys.stderr.write(str(test))\n            sys.stderr.write('\\n')\n        else:\n            sys.stderr.write('E')\n\n    def addFailure(self, test, err):\n        self.failure_count += 1\n        TestResult.addFailure(self, test, err)\n        _, _exc_str = self.failures[-1]\n        output = self.complete_output()\n        self.result.append((1, test, output, _exc_str))\n        if self.verbosity > 1:\n            sys.stderr.write('F  ')\n            sys.stderr.write(str(test))\n            sys.stderr.write('\\n')\n        else:\n            sys.stderr.write('F')\n\n\nclass HTMLTestRunner(Template_mixin):\n    \"\"\"\n    \"\"\"\n\n    def __init__(self, stream=sys.stdout, verbosity=1, title=None, description=None):\n        self.stream = stream\n        self.verbosity = verbosity\n        if title is None:\n            self.title = self.DEFAULT_TITLE\n        else:\n            self.title = title\n        if description is None:\n            self.description = self.DEFAULT_DESCRIPTION\n        else:\n            self.description = description\n\n        self.startTime = datetime.datetime.now()\n\n    def run(self, test):\n        \"Run the given test case or test suite.\"\n        result = _TestResult(self.verbosity)\n        test(result)\n        self.stopTime = datetime.datetime.now()\n        self.generateReport(test, result)\n        # print >> sys.stderr, '\\nTime Elapsed: %s' % (self.stopTime-self.startTime)\n        print(sys.stderr, '\\nTime Elapsed: %s' % (self.stopTime - self.startTime))\n        return result\n\n    def sortResult(self, result_list):\n        # unittest does not seems to run in any particular order.\n        # Here at least we want to group them together by class.\n        rmap = {}\n        classes = []\n        for n, t, o, e in result_list:\n            cls = t.__class__\n            if not cls in rmap:\n                rmap[cls] = []\n                classes.append(cls)\n            rmap[cls].append((n, t, o, e))\n        r = [(cls, rmap[cls]) for cls in classes]\n        return r\n\n    def getReportAttributes(self, result):\n        \"\"\"\n        Return report attributes as a list of (name, value).\n        Override this to add custom attributes.\n        \"\"\"\n        startTime = str(self.startTime)[:19]\n        duration = str(self.stopTime - self.startTime)\n        status = []\n        if result.success_count: status.append('Pass %s' % result.success_count)\n        if result.failure_count: status.append('Failure %s' % result.failure_count)\n        if result.error_count:   status.append('Error %s' % result.error_count)\n        if status:\n            status = ' '.join(status)\n        else:\n            status = 'none'\n        return [\n            ('Start Time', startTime),\n            ('Duration', duration),\n            ('Status', status),\n        ]\n\n    def generateReport(self, test, result):\n        report_attrs = self.getReportAttributes(result)\n        generator = 'HTMLTestRunner %s' % __version__\n        stylesheet = self._generate_stylesheet()\n        heading = self._generate_heading(report_attrs)\n        report = self._generate_report(result)\n        ending = self._generate_ending()\n        output = self.HTML_TMPL % dict(\n            title=saxutils.escape(self.title),\n            generator=generator,\n            stylesheet=stylesheet,\n            heading=heading,\n            report=report,\n            ending=ending,\n        )\n        self.stream.write(output.encode('utf8'))\n\n    def _generate_stylesheet(self):\n        return self.STYLESHEET_TMPL\n\n    def _generate_heading(self, report_attrs):\n        a_lines = []\n        for name, value in report_attrs:\n            line = self.HEADING_ATTRIBUTE_TMPL % dict(\n                name=saxutils.escape(name),\n                value=saxutils.escape(value),\n            )\n            a_lines.append(line)\n        heading = self.HEADING_TMPL % dict(\n            title=saxutils.escape(self.title),\n            parameters=''.join(a_lines),\n            description=saxutils.escape(self.description),\n        )\n        return heading\n\n    def _generate_report(self, result):\n        rows = []\n        sortedResult = self.sortResult(result.result)\n        for cid, (cls, cls_results) in enumerate(sortedResult):\n            # subtotal for a class\n            np = nf = ne = 0\n            for n, t, o, e in cls_results:\n                if n == 0:\n                    np += 1\n                elif n == 1:\n                    nf += 1\n                else:\n                    ne += 1\n\n            # format class description\n            if cls.__module__ == \"__main__\":\n                name = cls.__name__\n            else:\n                name = \"%s.%s\" % (cls.__module__, cls.__name__)\n            doc = cls.__doc__ and cls.__doc__.split(\"\\n\")[0] or \"\"\n            desc = doc and '%s: %s' % (name, doc) or name\n\n            row = self.REPORT_CLASS_TMPL % dict(\n                style=ne > 0 and 'errorClass' or nf > 0 and 'failClass' or 'passClass',\n                desc=desc,\n                count=np + nf + ne,\n                Pass=np,\n                fail=nf,\n                error=ne,\n                cid='c%s' % (cid + 1),\n            )\n            rows.append(row)\n\n            for tid, (n, t, o, e) in enumerate(cls_results):\n                self._generate_report_test(rows, cid, tid, n, t, o, e)\n\n        report = self.REPORT_TMPL % dict(\n            test_list=''.join(rows),\n            count=str(result.success_count + result.failure_count + result.error_count),\n            Pass=str(result.success_count),\n            fail=str(result.failure_count),\n            error=str(result.error_count),\n        )\n        return report\n\n    def _generate_report_test(self, rows, cid, tid, n, t, o, e):\n        # e.g. 'pt1.1', 'ft1.1', etc\n        has_output = bool(o or e)\n        tid = (n == 0 and 'p' or 'f') + 't%s.%s' % (cid + 1, tid + 1)\n        name = t.id().split('.')[-1]\n        doc = t.shortDescription() or \"\"\n        desc = doc and ('%s: %s' % (name, doc)) or name\n        tmpl = has_output and self.REPORT_TEST_WITH_OUTPUT_TMPL or self.REPORT_TEST_NO_OUTPUT_TMPL\n\n        # o and e should be byte string because they are collected from stdout and stderr?\n        if isinstance(o, str):\n            # TODO: some problem with 'string_escape': it escape \\n and mess up formating\n            # uo = unicode(o.encode('string_escape'))\n            # uo = o.decode('latin-1')\n            uo = e\n        else:\n            uo = o\n        if isinstance(e, str):\n            # TODO: some problem with 'string_escape': it escape \\n and mess up formating\n            # ue = unicode(e.encode('string_escape'))\n            # ue = e.decode('latin-1')\n            ue = e\n        else:\n            ue = e\n\n        script = self.REPORT_TEST_OUTPUT_TMPL % dict(\n            id=tid,\n            output=saxutils.escape(str(uo) + ue),\n        )\n\n        row = tmpl % dict(\n            tid=tid,\n            Class=(n == 0 and 'hiddenRow' or 'none'),\n            style=n == 2 and 'errorCase' or (n == 1 and 'failCase' or 'none'),\n            desc=desc,\n            script=script,\n            status=self.STATUS[n],\n        )\n        rows.append(row)\n        if not has_output:\n            return\n\n    def _generate_ending(self):\n        return self.ENDING_TMPL\n\n\n##############################################################################\n# Facilities for running tests from the command line\n##############################################################################\n\n# Note: Reuse unittest.TestProgram to launch test. In the future we may\n# build our own launcher to support more specific command line\n# parameters like test title, CSS, etc.\nclass TestProgram(unittest.TestProgram):\n    \"\"\"\n    A variation of the unittest.TestProgram. Please refer to the base\n    class for command line parameters.\n    \"\"\"\n\n    def runTests(self):\n        # Pick HTMLTestRunner as the default test runner.\n        # base class's testRunner parameter is not useful because it means\n        # we have to instantiate HTMLTestRunner before we know self.verbosity.\n        if self.testRunner is None:\n            self.testRunner = HTMLTestRunner(verbosity=self.verbosity)\n        unittest.TestProgram.runTests(self)\n\n\nmain = TestProgram\n\n##############################################################################\n# Executing this module from the command line\n##############################################################################\n\nif __name__ == \"__main__\":\n    main(module=None)\n", "metadata": {"source": "github.com/qzq1111/flask-restful-example", "file_path": "tests/HTMLTestRunner.py", "language": "python", "complexity": "complex", "star_count": 1004, "quality_score": 0.8099999999999999, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": true, "has_type_hints": true, "has_testing": true, "has_logging": true, "function_count": 22, "class_count": 11, "line_count": 814}, "authority_level": "medium", "authentic": true}}
{"domain": "code_generation", "category": "testing_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate module integration for Create comprehensive test suite with fixtures and mocks\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import unittest\nfrom tests.HTMLTestRunner import HTMLTestRunner\n\ntest_dir = './'\ndiscover = unittest.defaultTestLoader.discover(test_dir, pattern='*test_*.py')\n\nif __name__ == '__main__':\n    with open('HtmlReport.html', 'wb') as f:\n        runner = HTMLTestRunner(stream=f, title='test report', description='', verbosity=2)\n        runner.run(discover)\n\n    # with open('UnittestTextReport.txt', 'a') as f:\n    #     runner = unittest.TextTestRunner(stream=f, verbosity=2)\n    #     runner.run(discover)\n", "metadata": {"source": "github.com/qzq1111/flask-restful-example", "file_path": "tests/run_test.py", "language": "python", "complexity": "simple", "star_count": 1004, "quality_score": 0.69, "features": {"has_classes": false, "has_functions": false, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": false, "has_testing": true, "has_logging": false, "function_count": 0, "class_count": 0, "line_count": 15}, "authority_level": "medium", "authentic": true}}
{"domain": "code_generation", "category": "testing_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create comprehensive test suite with fixtures and mocks with pytest testing framework\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import unittest\nimport os\nfrom app.factory import create_app\n\n\nclass TestAPI(unittest.TestCase):\n\n    def setUp(self):\n        pwd = os.path.abspath(os.path.dirname(os.getcwd()))\n        config_path = os.path.join(pwd, 'config/config.yaml')\n        app = create_app(config_name=\"DEVELOPMENT\", config_path=config_path)\n        app.config['TESTING'] = True\n        self.app = app.test_client()\n\n    def tearDown(self):\n        pass\n\n    def test_logger(self):\n        rv = self.app.get('/logs')\n        self.assertEqual(rv.data.decode(\"utf-8\"), \"ok\")\n\n    def test_unified_response(self):\n        rv = self.app.get('/unifiedResponse')\n        data = rv.get_json()\n        self.assertIsInstance(data, dict)\n        self.assertEqual(data[\"code\"], 0)\n        self.assertEqual(data[\"msg\"], '成功')\n        self.assertDictEqual(data[\"data\"], {'age': 18, 'name': 'zhang'})\n\n    def test_packed_response(self):\n        rv = self.app.get('/packedResponse')\n        data = rv.get_json()\n        self.assertIsInstance(data, dict)\n        self.assertEqual(data[\"code\"], 0)\n        self.assertEqual(data[\"msg\"], '成功')\n        self.assertDictEqual(data[\"data\"], dict(name=\"zhang\", age=18))\n\n    def test_type_response(self):\n        rv = self.app.get('/typeResponse')\n        data = rv.get_json()\n\n        self.assertIsInstance(data, dict)\n        self.assertEqual(data[\"code\"], 0)\n        self.assertEqual(data[\"msg\"], '成功')\n        data = data[\"data\"]\n        self.assertIsInstance(data[\"now\"], str)\n        self.assertIsInstance(data[\"date\"], str)\n        self.assertIsInstance(data[\"num\"], str)\n        self.assertEqual(data[\"num\"], '11.11')\n\n", "metadata": {"source": "github.com/qzq1111/flask-restful-example", "file_path": "tests/test_api.py", "language": "python", "complexity": "medium", "star_count": 1004, "quality_score": 0.84, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": true, "has_logging": true, "function_count": 6, "class_count": 1, "line_count": 51}, "authority_level": "medium", "authentic": true}}
{"domain": "code_generation", "category": "testing_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create comprehensive test suite with fixtures and mocks\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# -*- coding: utf-8 -*-\nimport sys\nimport time\n\nfrom httpdbg import __version__\nfrom httpdbg.args import read_args\nfrom httpdbg.hooks.all import httprecord\nfrom httpdbg.log import set_env_for_logging\nfrom httpdbg.server import httpdbg_srv\nfrom httpdbg.mode_console import run_console\nfrom httpdbg.mode_module import run_module\nfrom httpdbg.mode_script import run_script\n\n\ndef print_msg(msg):\n    sep = \".... - - .--. -.. -... --. .... - - .--. -.. -... --. .... - - .--. -.. -... --.\"\n    msg = f\"{sep}\\n{msg}\\n{sep}\"\n    print(msg)\n\n\ndef pyhttpdbg(params, subparams, test_mode=False):\n\n    set_env_for_logging(params.log_level, params.log)\n\n    url = f\"http://{params.host}:{params.port}/{'?hi=on' if params.console else ''}\"\n\n    print_msg(f\"  httpdbg - HTTP(S) requests available at {url}\")\n\n    sys.path.insert(0, \"\")  # to mimic the default python command behavior\n\n    with httpdbg_srv(params.host, params.port) as records:\n        records.server = not params.only_client\n        with httprecord(records, params.initiator, server=records.server):\n            if params.module:\n                run_module(subparams)\n            elif params.script:\n                run_script(subparams)\n            else:\n                run_console(records, test_mode)\n\n        if not (params.force_quit or test_mode):\n            print_msg(f\"  httpdbg - HTTP(S) requests available at {url}\")\n\n            if params.keep_up:\n                input(\"Press enter to quit\")\n            else:\n                # we keep the server up until all the requests have been loaded in the web interface\n                print(\n                    \"Waiting until all the requests have been loaded in the web interface.\"\n                )\n                print(\"Press Ctrl+C to quit.\")\n                try:\n                    while records.unread:\n                        time.sleep(0.5)\n                except KeyboardInterrupt:  # pragma: no cover\n                    pass\n\n\ndef pyhttpdbg_entry_point(test_mode=False):\n    params, subparams = read_args(sys.argv[1:])\n    if params.version:\n        print(__version__)\n    else:\n        pyhttpdbg(params, subparams, test_mode=test_mode)\n\n\nif __name__ == \"__main__\":\n    pyhttpdbg_entry_point()\n", "metadata": {"source": "github.com/cle-b/httpdbg", "file_path": "httpdbg/__main__.py", "language": "python", "complexity": "simple", "star_count": 874, "quality_score": 0.6799999999999999, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": true, "has_documentation": false, "has_type_hints": true, "has_testing": true, "has_logging": true, "function_count": 3, "class_count": 0, "line_count": 69}, "authority_level": "medium", "authentic": true}}
{"domain": "code_generation", "category": "testing_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create comprehensive test suite with fixtures and mocks\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# -*- coding: utf-8 -*-\nimport argparse\nfrom pathlib import Path\nfrom typing import List, Tuple\n\nfrom httpdbg.log import LogLevel\n\n\ndef read_args(args: List[str]) -> Tuple[argparse.Namespace, List[str]]:\n    httpdbg_args = args\n    client_args = []\n    for action in [\"--console\", \"--module\", \"-m\", \"--script\"]:\n        if action in args:\n            httpdbg_args = args[: args.index(action) + 2]\n            client_args = args[args.index(action) + 1 :]\n            break\n\n    parser = argparse.ArgumentParser(\n        description=\"httpdbg - easily debug HTTP client and server requests\"\n    )\n    parser.add_argument(\n        \"--host\",\n        type=str,\n        default=\"localhost\",\n        help=\"the web interface host IP address\",\n    )\n    parser.add_argument(\n        \"--port\", \"-p\", type=int, default=4909, help=\"the web interface port\"\n    )\n\n    parser.add_argument(\n        \"--version\", \"-v\", action=\"store_true\", help=\"print the httpdbg version\"\n    )\n\n    parser.add_argument(\n        \"--initiator\", \"-i\", action=\"append\", help=\"add a new initiator (package)\"\n    )\n\n    parser.add_argument(\n        \"--only-client\",\n        action=\"store_true\",\n        help=\"record only HTTP client requests\",\n    )\n\n    server_state = parser.add_mutually_exclusive_group()\n\n    server_state.add_argument(\n        \"--keep-up\",\n        \"-k\",\n        action=\"store_true\",\n        help=\"keep the server up even if the requests have been read\",\n    )\n\n    server_state.add_argument(\n        \"--force-quit\",\n        \"-q\",\n        action=\"store_true\",\n        help=\"stop the server even if the requests have not been read\",\n    )\n\n    parser.add_argument(\n        \"--log-level\",\n        type=LogLevel,\n        choices=LogLevel,\n        default=LogLevel.WARNING,\n        help=\"The log level\",\n    )\n\n    parser.add_argument(\"--log\", type=Path, help=\"Path to the log file.\")\n\n    actions = parser.add_mutually_exclusive_group()\n\n    actions.add_argument(\n        \"--console\", action=\"store_true\", help=\"run a python console (default)\"\n    )\n\n    actions.add_argument(\n        \"--module\",\n        \"-m\",\n        type=str,\n        help=\"run library module as a script (the next args are passed to pytest as is)\",\n    )\n\n    actions.add_argument(\n        \"--script\",\n        type=str,\n        help=\"run a script (the next args are passed to the script as is)\",\n    )\n\n    return parser.parse_args(httpdbg_args), client_args\n", "metadata": {"source": "github.com/cle-b/httpdbg", "file_path": "httpdbg/args.py", "language": "python", "complexity": "medium", "star_count": 874, "quality_score": 0.59, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": false, "has_logging": true, "function_count": 1, "class_count": 0, "line_count": 91}, "authority_level": "medium", "authentic": true}}
{"domain": "code_generation", "category": "testing_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create comprehensive test suite with fixtures and mocks\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# -*- coding: utf-8 -*-\nfrom contextlib import contextmanager\nfrom typing import Generator\nfrom typing import List\nfrom typing import Tuple\nfrom typing import Union\n\nfrom httpdbg.hooks.aiohttp import hook_aiohttp\nfrom httpdbg.hooks.external import watcher_external\nfrom httpdbg.hooks.fastapi import hook_fastapi\nfrom httpdbg.hooks.flask import hook_flask\nfrom httpdbg.hooks.generic import hook_generic\nfrom httpdbg.hooks.http import hook_http\nfrom httpdbg.hooks.httpx import hook_httpx\nfrom httpdbg.hooks.pytest import hook_pytest\nfrom httpdbg.hooks.requests import hook_requests\nfrom httpdbg.hooks.socket import hook_socket\nfrom httpdbg.hooks.starlette import hook_starlette\nfrom httpdbg.hooks.unittest import hook_unittest\nfrom httpdbg.hooks.urllib3 import hook_urllib3\nfrom httpdbg.hooks.uvicorn import hook_uvicorn\n\nfrom httpdbg.records import HTTPRecords\n\n\n@contextmanager\ndef httprecord(\n    records: HTTPRecords = None,\n    initiators: Union[List[str], None] = None,\n    client: bool = True,\n    server: bool = False,\n    ignore: Union[List[Tuple[str, int]], None] = None,\n    multiprocess: bool = True,\n) -> Generator[HTTPRecords, None, None]:\n    if records is None:\n        records = HTTPRecords(client=client, server=server, ignore=ignore)\n\n    with hook_flask(records):\n        with hook_socket(records):\n            with hook_fastapi(records):\n                with hook_starlette(records):\n                    with hook_uvicorn(records):\n                        with hook_http(records):\n                            with hook_httpx(records):\n                                with hook_requests(records):\n                                    with hook_urllib3(records):\n                                        with hook_aiohttp(records):\n                                            with hook_pytest(records):\n                                                with hook_unittest(records):\n                                                    with hook_generic(\n                                                        records, initiators\n                                                    ):\n                                                        if multiprocess:\n                                                            with watcher_external(\n                                                                records,\n                                                                initiators,\n                                                                server,\n                                                            ):\n                                                                yield records\n                                                        else:\n                                                            yield records\n", "metadata": {"source": "github.com/cle-b/httpdbg", "file_path": "httpdbg/hooks/all.py", "language": "python", "complexity": "simple", "star_count": 874, "quality_score": 0.59, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 1, "class_count": 0, "line_count": 62}, "authority_level": "medium", "authentic": true}}
{"domain": "code_generation", "category": "testing_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create comprehensive test suite with fixtures and mocks\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# -*- coding: utf-8 -*-\nfrom collections.abc import Callable\nfrom contextlib import contextmanager\nimport functools\nfrom functools import wraps\nfrom typing import Dict\nfrom typing import Generator\nfrom typing import Union\n\nfrom httpdbg.hooks.utils import getcallargs\nfrom httpdbg.hooks.utils import decorate\nfrom httpdbg.hooks.utils import undecorate\nfrom httpdbg.initiator import httpdbg_endpoint\nfrom httpdbg.records import HTTPRecords\n\n\ndef set_hook_flask_endpoint(records: HTTPRecords, method: Callable):\n\n    @wraps(\n        method\n    )  # to avoid AssertionError: View function mapping is overwriting an existing endpoint function: hook\n    def hook(*args, **kwargs):\n\n        with httpdbg_endpoint(\n            records,\n            method,\n            *args,\n            **kwargs,\n        ):\n            ret = method(*args, **kwargs)\n        return ret\n\n    return hook\n\n\ndef set_hook_flask_add_url_rule(\n    records: HTTPRecords,\n    method: Callable,\n    already_mapped: Union[Dict[Callable, Callable], None] = None,\n):\n\n    def hook(*args, **kwargs):\n\n        callargs = getcallargs(method, *args, **kwargs)\n\n        param = \"view_func\"\n\n        if param in callargs:\n            original_view_func = callargs[param]\n            if original_view_func not in already_mapped:\n                callargs[param] = decorate(\n                    records, callargs[param], set_hook_flask_endpoint\n                )\n                already_mapped[original_view_func] = callargs[param]\n            else:\n                callargs[param] = already_mapped[original_view_func]\n\n            args = [callargs[param] if x == original_view_func else x for x in args]\n            if param in kwargs:\n                kwargs[param] = callargs[param]\n\n        return method(*args, **kwargs)\n\n    return hook\n\n\ndef set_hook_flask_register_error_handler(\n    records: HTTPRecords,\n    method: Callable,\n    already_mapped: Union[Dict[Callable, Callable], None] = None,\n):\n\n    def hook(*args, **kwargs):\n\n        callargs = getcallargs(method, *args, **kwargs)\n\n        param = \"f\"\n\n        if param in callargs:\n            original_view_func = callargs[param]\n            if original_view_func not in already_mapped:\n                callargs[param] = decorate(\n                    records, callargs[param], set_hook_flask_endpoint\n                )\n                already_mapped[original_view_func] = callargs[param]\n            else:\n                callargs[param] = already_mapped[original_view_func]\n\n            args = [callargs[param] if x == original_view_func else x for x in args]\n            if param in kwargs:\n                kwargs[param] = callargs[param]\n\n        return method(*args, **kwargs)\n\n    return hook\n\n\n@contextmanager\ndef hook_flask(records: HTTPRecords) -> Generator[None, None, None]:\n    hooks = False\n    try:\n        import flask\n\n        # we must not apply the hook more than once on a mapped endpoint function\n        # AssertionError: View function mapping is overwriting an existing endpoint function: xxxx\n        already_mapped: Dict[Callable, Callable] = {}\n\n        set_hook_flask_add_url_rule_with_already_mapped = functools.partial(\n            set_hook_flask_add_url_rule, already_mapped=already_mapped\n        )\n\n        flask.Flask.add_url_rule = decorate(\n            records,\n            flask.Flask.add_url_rule,\n            set_hook_flask_add_url_rule_with_already_mapped,\n        )\n\n        set_hook_flask_register_error_handler_with_already_mapped = functools.partial(\n            set_hook_flask_register_error_handler, already_mapped=already_mapped\n        )\n\n        flask.Flask.register_error_handler = decorate(\n            records,\n            flask.Flask.register_error_handler,\n            set_hook_flask_register_error_handler_with_already_mapped,\n        )\n        flask.Flask.handle_exception = decorate(\n            records, flask.Flask.handle_exception, set_hook_flask_endpoint\n        )\n        flask.Flask.handle_http_exception = decorate(\n            records, flask.Flask.handle_http_exception, set_hook_flask_endpoint\n        )\n        flask.Flask.handle_user_exception = decorate(\n            records, flask.Flask.handle_user_exception, set_hook_flask_endpoint\n        )\n\n        hooks = True\n    except ImportError:\n        pass\n\n    yield\n\n    if hooks:\n        flask.Flask.add_url_rule = undecorate(flask.Flask.add_url_rule)\n        flask.Flask.register_error_handler = undecorate(\n            flask.Flask.register_error_handler\n        )\n        flask.Flask.handle_exception = undecorate(flask.Flask.handle_exception)\n        flask.Flask.handle_http_exception = undecorate(\n            flask.Flask.handle_http_exception\n        )\n        flask.Flask.handle_user_exception = undecorate(\n            flask.Flask.handle_user_exception\n        )\n", "metadata": {"source": "github.com/cle-b/httpdbg", "file_path": "httpdbg/hooks/flask.py", "language": "python", "complexity": "complex", "star_count": 874, "quality_score": 0.62, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": true, "has_documentation": false, "has_type_hints": true, "has_testing": true, "has_logging": false, "function_count": 7, "class_count": 0, "line_count": 155}, "authority_level": "medium", "authentic": true}}
{"domain": "code_generation", "category": "async_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create asynchronous code with proper concurrency handling using Flask/FastAPI decorators\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from fastapi import FastAPI\nfrom fastapi.responses import JSONResponse\nfrom pydantic import BaseModel\n\n\nclass Item(BaseModel):\n    id: str\n    value: str\n\n\nclass Message(BaseModel):\n    message: str\n\n\napp = FastAPI()\n\n\n@app.get(\"/items/{item_id}\", response_model=Item, responses={404: {\"model\": Message}})\nasync def read_item(item_id: str):\n    if item_id == \"foo\":\n        return {\"id\": \"foo\", \"value\": \"there goes my hero\"}\n    return JSONResponse(status_code=404, content={\"message\": \"Item not found\"})\n", "metadata": {"source": "github.com/fastapi/fastapi", "file_path": "docs_src/additional_responses/tutorial001.py", "language": "python", "complexity": "simple", "star_count": 87656, "quality_score": 0.92, "features": {"has_classes": true, "has_functions": true, "has_async": true, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 1, "class_count": 2, "line_count": 23}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "async_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create asynchronous code with proper concurrency handling using Flask/FastAPI decorators\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from typing import Union\n\nfrom fastapi import FastAPI\nfrom fastapi.responses import FileResponse\nfrom pydantic import BaseModel\n\n\nclass Item(BaseModel):\n    id: str\n    value: str\n\n\napp = FastAPI()\n\n\n@app.get(\n    \"/items/{item_id}\",\n    response_model=Item,\n    responses={\n        200: {\n            \"content\": {\"image/png\": {}},\n            \"description\": \"Return the JSON item or an image.\",\n        }\n    },\n)\nasync def read_item(item_id: str, img: Union[bool, None] = None):\n    if img:\n        return FileResponse(\"image.png\", media_type=\"image/png\")\n    else:\n        return {\"id\": \"foo\", \"value\": \"there goes my hero\"}\n", "metadata": {"source": "github.com/fastapi/fastapi", "file_path": "docs_src/additional_responses/tutorial002.py", "language": "python", "complexity": "simple", "star_count": 87656, "quality_score": 0.92, "features": {"has_classes": true, "has_functions": true, "has_async": true, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 1, "class_count": 1, "line_count": 31}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "async_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create asynchronous code with proper concurrency handling using Flask/FastAPI decorators\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from fastapi import FastAPI\nfrom fastapi.responses import JSONResponse\nfrom pydantic import BaseModel\n\n\nclass Item(BaseModel):\n    id: str\n    value: str\n\n\nclass Message(BaseModel):\n    message: str\n\n\napp = FastAPI()\n\n\n@app.get(\n    \"/items/{item_id}\",\n    response_model=Item,\n    responses={\n        404: {\"model\": Message, \"description\": \"The item was not found\"},\n        200: {\n            \"description\": \"Item requested by ID\",\n            \"content\": {\n                \"application/json\": {\n                    \"example\": {\"id\": \"bar\", \"value\": \"The bar tenders\"}\n                }\n            },\n        },\n    },\n)\nasync def read_item(item_id: str):\n    if item_id == \"foo\":\n        return {\"id\": \"foo\", \"value\": \"there goes my hero\"}\n    else:\n        return JSONResponse(status_code=404, content={\"message\": \"Item not found\"})\n", "metadata": {"source": "github.com/fastapi/fastapi", "file_path": "docs_src/additional_responses/tutorial003.py", "language": "python", "complexity": "simple", "star_count": 87656, "quality_score": 0.92, "features": {"has_classes": true, "has_functions": true, "has_async": true, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 1, "class_count": 2, "line_count": 38}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "async_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create asynchronous code with proper concurrency handling using Flask/FastAPI decorators\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from typing import Union\n\nfrom fastapi import FastAPI\nfrom fastapi.responses import FileResponse\nfrom pydantic import BaseModel\n\n\nclass Item(BaseModel):\n    id: str\n    value: str\n\n\nresponses = {\n    404: {\"description\": \"Item not found\"},\n    302: {\"description\": \"The item was moved\"},\n    403: {\"description\": \"Not enough privileges\"},\n}\n\n\napp = FastAPI()\n\n\n@app.get(\n    \"/items/{item_id}\",\n    response_model=Item,\n    responses={**responses, 200: {\"content\": {\"image/png\": {}}}},\n)\nasync def read_item(item_id: str, img: Union[bool, None] = None):\n    if img:\n        return FileResponse(\"image.png\", media_type=\"image/png\")\n    else:\n        return {\"id\": \"foo\", \"value\": \"there goes my hero\"}\n", "metadata": {"source": "github.com/fastapi/fastapi", "file_path": "docs_src/additional_responses/tutorial004.py", "language": "python", "complexity": "simple", "star_count": 87656, "quality_score": 0.92, "features": {"has_classes": true, "has_functions": true, "has_async": true, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 1, "class_count": 1, "line_count": 33}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "async_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create asynchronous code with proper concurrency handling with asyncio coroutines\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from typing import Union\n\nfrom fastapi import Body, FastAPI, status\nfrom fastapi.responses import JSONResponse\n\napp = FastAPI()\n\nitems = {\"foo\": {\"name\": \"Fighters\", \"size\": 6}, \"bar\": {\"name\": \"Tenders\", \"size\": 3}}\n\n\n@app.put(\"/items/{item_id}\")\nasync def upsert_item(\n    item_id: str,\n    name: Union[str, None] = Body(default=None),\n    size: Union[int, None] = Body(default=None),\n):\n    if item_id in items:\n        item = items[item_id]\n        item[\"name\"] = name\n        item[\"size\"] = size\n        return item\n    else:\n        item = {\"name\": name, \"size\": size}\n        items[item_id] = item\n        return JSONResponse(status_code=status.HTTP_201_CREATED, content=item)\n", "metadata": {"source": "github.com/fastapi/fastapi", "file_path": "docs_src/additional_status_codes/tutorial001.py", "language": "python", "complexity": "simple", "star_count": 87656, "quality_score": 0.89, "features": {"has_classes": false, "has_functions": true, "has_async": true, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 1, "class_count": 0, "line_count": 26}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "async_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create asynchronous code with proper concurrency handling with asyncio coroutines\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from typing import Union\n\nfrom fastapi import Body, FastAPI, status\nfrom fastapi.responses import JSONResponse\nfrom typing_extensions import Annotated\n\napp = FastAPI()\n\nitems = {\"foo\": {\"name\": \"Fighters\", \"size\": 6}, \"bar\": {\"name\": \"Tenders\", \"size\": 3}}\n\n\n@app.put(\"/items/{item_id}\")\nasync def upsert_item(\n    item_id: str,\n    name: Annotated[Union[str, None], Body()] = None,\n    size: Annotated[Union[int, None], Body()] = None,\n):\n    if item_id in items:\n        item = items[item_id]\n        item[\"name\"] = name\n        item[\"size\"] = size\n        return item\n    else:\n        item = {\"name\": name, \"size\": size}\n        items[item_id] = item\n        return JSONResponse(status_code=status.HTTP_201_CREATED, content=item)\n", "metadata": {"source": "github.com/fastapi/fastapi", "file_path": "docs_src/additional_status_codes/tutorial001_an.py", "language": "python", "complexity": "simple", "star_count": 87656, "quality_score": 0.89, "features": {"has_classes": false, "has_functions": true, "has_async": true, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 1, "class_count": 0, "line_count": 27}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "async_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create asynchronous code with proper concurrency handling with asyncio coroutines\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from typing import Annotated\n\nfrom fastapi import Body, FastAPI, status\nfrom fastapi.responses import JSONResponse\n\napp = FastAPI()\n\nitems = {\"foo\": {\"name\": \"Fighters\", \"size\": 6}, \"bar\": {\"name\": \"Tenders\", \"size\": 3}}\n\n\n@app.put(\"/items/{item_id}\")\nasync def upsert_item(\n    item_id: str,\n    name: Annotated[str | None, Body()] = None,\n    size: Annotated[int | None, Body()] = None,\n):\n    if item_id in items:\n        item = items[item_id]\n        item[\"name\"] = name\n        item[\"size\"] = size\n        return item\n    else:\n        item = {\"name\": name, \"size\": size}\n        items[item_id] = item\n        return JSONResponse(status_code=status.HTTP_201_CREATED, content=item)\n", "metadata": {"source": "github.com/fastapi/fastapi", "file_path": "docs_src/additional_status_codes/tutorial001_an_py310.py", "language": "python", "complexity": "simple", "star_count": 87656, "quality_score": 0.89, "features": {"has_classes": false, "has_functions": true, "has_async": true, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 1, "class_count": 0, "line_count": 26}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "async_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create asynchronous code with proper concurrency handling with asyncio coroutines\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from typing import Annotated, Union\n\nfrom fastapi import Body, FastAPI, status\nfrom fastapi.responses import JSONResponse\n\napp = FastAPI()\n\nitems = {\"foo\": {\"name\": \"Fighters\", \"size\": 6}, \"bar\": {\"name\": \"Tenders\", \"size\": 3}}\n\n\n@app.put(\"/items/{item_id}\")\nasync def upsert_item(\n    item_id: str,\n    name: Annotated[Union[str, None], Body()] = None,\n    size: Annotated[Union[int, None], Body()] = None,\n):\n    if item_id in items:\n        item = items[item_id]\n        item[\"name\"] = name\n        item[\"size\"] = size\n        return item\n    else:\n        item = {\"name\": name, \"size\": size}\n        items[item_id] = item\n        return JSONResponse(status_code=status.HTTP_201_CREATED, content=item)\n", "metadata": {"source": "github.com/fastapi/fastapi", "file_path": "docs_src/additional_status_codes/tutorial001_an_py39.py", "language": "python", "complexity": "simple", "star_count": 87656, "quality_score": 0.89, "features": {"has_classes": false, "has_functions": true, "has_async": true, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 1, "class_count": 0, "line_count": 26}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "async_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create asynchronous code with proper concurrency handling with asyncio coroutines\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from fastapi import Body, FastAPI, status\nfrom fastapi.responses import JSONResponse\n\napp = FastAPI()\n\nitems = {\"foo\": {\"name\": \"Fighters\", \"size\": 6}, \"bar\": {\"name\": \"Tenders\", \"size\": 3}}\n\n\n@app.put(\"/items/{item_id}\")\nasync def upsert_item(\n    item_id: str,\n    name: str | None = Body(default=None),\n    size: int | None = Body(default=None),\n):\n    if item_id in items:\n        item = items[item_id]\n        item[\"name\"] = name\n        item[\"size\"] = size\n        return item\n    else:\n        item = {\"name\": name, \"size\": size}\n        items[item_id] = item\n        return JSONResponse(status_code=status.HTTP_201_CREATED, content=item)\n", "metadata": {"source": "github.com/fastapi/fastapi", "file_path": "docs_src/additional_status_codes/tutorial001_py310.py", "language": "python", "complexity": "simple", "star_count": 87656, "quality_score": 0.89, "features": {"has_classes": false, "has_functions": true, "has_async": true, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 1, "class_count": 0, "line_count": 24}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "async_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create asynchronous code with proper concurrency handling using Flask/FastAPI decorators\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from fastapi import FastAPI\nfrom fastapi.middleware.httpsredirect import HTTPSRedirectMiddleware\n\napp = FastAPI()\n\napp.add_middleware(HTTPSRedirectMiddleware)\n\n\n@app.get(\"/\")\nasync def main():\n    return {\"message\": \"Hello World\"}\n", "metadata": {"source": "github.com/fastapi/fastapi", "file_path": "docs_src/advanced_middleware/tutorial001.py", "language": "python", "complexity": "simple", "star_count": 87656, "quality_score": 0.89, "features": {"has_classes": false, "has_functions": true, "has_async": true, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 1, "class_count": 0, "line_count": 12}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "async_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create asynchronous code with proper concurrency handling using Flask/FastAPI decorators\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from fastapi import FastAPI\nfrom fastapi.middleware.trustedhost import TrustedHostMiddleware\n\napp = FastAPI()\n\napp.add_middleware(\n    TrustedHostMiddleware, allowed_hosts=[\"example.com\", \"*.example.com\"]\n)\n\n\n@app.get(\"/\")\nasync def main():\n    return {\"message\": \"Hello World\"}\n", "metadata": {"source": "github.com/fastapi/fastapi", "file_path": "docs_src/advanced_middleware/tutorial002.py", "language": "python", "complexity": "simple", "star_count": 87656, "quality_score": 0.89, "features": {"has_classes": false, "has_functions": true, "has_async": true, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 1, "class_count": 0, "line_count": 14}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "async_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create asynchronous code with proper concurrency handling using Flask/FastAPI decorators\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from fastapi import FastAPI\nfrom fastapi.middleware.gzip import GZipMiddleware\n\napp = FastAPI()\n\napp.add_middleware(GZipMiddleware, minimum_size=1000, compresslevel=5)\n\n\n@app.get(\"/\")\nasync def main():\n    return \"somebigcontent\"\n", "metadata": {"source": "github.com/fastapi/fastapi", "file_path": "docs_src/advanced_middleware/tutorial003.py", "language": "python", "complexity": "simple", "star_count": 87656, "quality_score": 0.89, "features": {"has_classes": false, "has_functions": true, "has_async": true, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": false, "has_testing": false, "has_logging": false, "function_count": 1, "class_count": 0, "line_count": 12}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "async_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create asynchronous code with proper concurrency handling using Flask/FastAPI decorators\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from typing import Union\n\nfrom fastapi import FastAPI, Header, HTTPException\nfrom pydantic import BaseModel\n\nfake_secret_token = \"coneofsilence\"\n\nfake_db = {\n    \"foo\": {\"id\": \"foo\", \"title\": \"Foo\", \"description\": \"There goes my hero\"},\n    \"bar\": {\"id\": \"bar\", \"title\": \"Bar\", \"description\": \"The bartenders\"},\n}\n\napp = FastAPI()\n\n\nclass Item(BaseModel):\n    id: str\n    title: str\n    description: Union[str, None] = None\n\n\n@app.get(\"/items/{item_id}\", response_model=Item)\nasync def read_main(item_id: str, x_token: str = Header()):\n    if x_token != fake_secret_token:\n        raise HTTPException(status_code=400, detail=\"Invalid X-Token header\")\n    if item_id not in fake_db:\n        raise HTTPException(status_code=404, detail=\"Item not found\")\n    return fake_db[item_id]\n\n\n@app.post(\"/items/\", response_model=Item)\nasync def create_item(item: Item, x_token: str = Header()):\n    if x_token != fake_secret_token:\n        raise HTTPException(status_code=400, detail=\"Invalid X-Token header\")\n    if item.id in fake_db:\n        raise HTTPException(status_code=409, detail=\"Item already exists\")\n    fake_db[item.id] = item\n    return item\n", "metadata": {"source": "github.com/fastapi/fastapi", "file_path": "docs_src/app_testing/app_b/main.py", "language": "python", "complexity": "simple", "star_count": 87656, "quality_score": 0.92, "features": {"has_classes": true, "has_functions": true, "has_async": true, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 2, "class_count": 1, "line_count": 39}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "async_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create asynchronous code with proper concurrency handling using Flask/FastAPI decorators\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from typing import Union\n\nfrom fastapi import FastAPI, Header, HTTPException\nfrom pydantic import BaseModel\nfrom typing_extensions import Annotated\n\nfake_secret_token = \"coneofsilence\"\n\nfake_db = {\n    \"foo\": {\"id\": \"foo\", \"title\": \"Foo\", \"description\": \"There goes my hero\"},\n    \"bar\": {\"id\": \"bar\", \"title\": \"Bar\", \"description\": \"The bartenders\"},\n}\n\napp = FastAPI()\n\n\nclass Item(BaseModel):\n    id: str\n    title: str\n    description: Union[str, None] = None\n\n\n@app.get(\"/items/{item_id}\", response_model=Item)\nasync def read_main(item_id: str, x_token: Annotated[str, Header()]):\n    if x_token != fake_secret_token:\n        raise HTTPException(status_code=400, detail=\"Invalid X-Token header\")\n    if item_id not in fake_db:\n        raise HTTPException(status_code=404, detail=\"Item not found\")\n    return fake_db[item_id]\n\n\n@app.post(\"/items/\", response_model=Item)\nasync def create_item(item: Item, x_token: Annotated[str, Header()]):\n    if x_token != fake_secret_token:\n        raise HTTPException(status_code=400, detail=\"Invalid X-Token header\")\n    if item.id in fake_db:\n        raise HTTPException(status_code=409, detail=\"Item already exists\")\n    fake_db[item.id] = item\n    return item\n", "metadata": {"source": "github.com/fastapi/fastapi", "file_path": "docs_src/app_testing/app_b_an/main.py", "language": "python", "complexity": "simple", "star_count": 87656, "quality_score": 0.92, "features": {"has_classes": true, "has_functions": true, "has_async": true, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 2, "class_count": 1, "line_count": 40}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "async_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create asynchronous code with proper concurrency handling using Flask/FastAPI decorators\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from typing import Annotated\n\nfrom fastapi import FastAPI, Header, HTTPException\nfrom pydantic import BaseModel\n\nfake_secret_token = \"coneofsilence\"\n\nfake_db = {\n    \"foo\": {\"id\": \"foo\", \"title\": \"Foo\", \"description\": \"There goes my hero\"},\n    \"bar\": {\"id\": \"bar\", \"title\": \"Bar\", \"description\": \"The bartenders\"},\n}\n\napp = FastAPI()\n\n\nclass Item(BaseModel):\n    id: str\n    title: str\n    description: str | None = None\n\n\n@app.get(\"/items/{item_id}\", response_model=Item)\nasync def read_main(item_id: str, x_token: Annotated[str, Header()]):\n    if x_token != fake_secret_token:\n        raise HTTPException(status_code=400, detail=\"Invalid X-Token header\")\n    if item_id not in fake_db:\n        raise HTTPException(status_code=404, detail=\"Item not found\")\n    return fake_db[item_id]\n\n\n@app.post(\"/items/\", response_model=Item)\nasync def create_item(item: Item, x_token: Annotated[str, Header()]):\n    if x_token != fake_secret_token:\n        raise HTTPException(status_code=400, detail=\"Invalid X-Token header\")\n    if item.id in fake_db:\n        raise HTTPException(status_code=409, detail=\"Item already exists\")\n    fake_db[item.id] = item\n    return item\n", "metadata": {"source": "github.com/fastapi/fastapi", "file_path": "docs_src/app_testing/app_b_an_py310/main.py", "language": "python", "complexity": "simple", "star_count": 87656, "quality_score": 0.92, "features": {"has_classes": true, "has_functions": true, "has_async": true, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 2, "class_count": 1, "line_count": 39}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "async_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create asynchronous code with proper concurrency handling with asyncio coroutines\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"Provide an authentication layer for Home Assistant.\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\nfrom collections import OrderedDict\nfrom collections.abc import Mapping\nfrom datetime import datetime, timedelta\nfrom functools import partial\nimport time\nfrom typing import Any, cast\n\nimport jwt\n\nfrom homeassistant.core import (\n    CALLBACK_TYPE,\n    HassJob,\n    HassJobType,\n    HomeAssistant,\n    callback,\n)\nfrom homeassistant.data_entry_flow import FlowHandler, FlowManager, FlowResultType\nfrom homeassistant.helpers.event import async_track_point_in_utc_time\nfrom homeassistant.util import dt as dt_util\n\nfrom . import auth_store, jwt_wrapper, models\nfrom .const import ACCESS_TOKEN_EXPIRATION, GROUP_ID_ADMIN, REFRESH_TOKEN_EXPIRATION\nfrom .mfa_modules import MultiFactorAuthModule, auth_mfa_module_from_config\nfrom .models import AuthFlowContext, AuthFlowResult\nfrom .providers import AuthProvider, LoginFlow, auth_provider_from_config\nfrom .providers.homeassistant import HassAuthProvider\n\nEVENT_USER_ADDED = \"user_added\"\nEVENT_USER_UPDATED = \"user_updated\"\nEVENT_USER_REMOVED = \"user_removed\"\n\ntype _MfaModuleDict = dict[str, MultiFactorAuthModule]\ntype _ProviderKey = tuple[str, str | None]\ntype _ProviderDict = dict[_ProviderKey, AuthProvider]\n\n\nclass InvalidAuthError(Exception):\n    \"\"\"Raised when a authentication error occurs.\"\"\"\n\n\nclass InvalidProvider(Exception):\n    \"\"\"Authentication provider not found.\"\"\"\n\n\nasync def auth_manager_from_config(\n    hass: HomeAssistant,\n    provider_configs: list[dict[str, Any]],\n    module_configs: list[dict[str, Any]],\n) -> AuthManager:\n    \"\"\"Initialize an auth manager from config.\n\n    CORE_CONFIG_SCHEMA will make sure no duplicated auth providers or\n    mfa modules exist in configs.\n    \"\"\"\n    store = auth_store.AuthStore(hass)\n    await store.async_load()\n    if provider_configs:\n        providers = await asyncio.gather(\n            *(\n                auth_provider_from_config(hass, store, config)\n                for config in provider_configs\n            )\n        )\n    else:\n        providers = []\n    # So returned auth providers are in same order as config\n    provider_hash: _ProviderDict = OrderedDict()\n    for provider in providers:\n        key = (provider.type, provider.id)\n        provider_hash[key] = provider\n\n        if isinstance(provider, HassAuthProvider):\n            # Can be removed in 2026.7 with the legacy mode of homeassistant auth provider\n            # We need to initialize the provider to create the repair if needed as otherwise\n            # the provider will be initialized on first use, which could be rare as users\n            # don't frequently change auth settings\n            await provider.async_initialize()\n\n    if module_configs:\n        modules = await asyncio.gather(\n            *(auth_mfa_module_from_config(hass, config) for config in module_configs)\n        )\n    else:\n        modules = []\n    # So returned auth modules are in same order as config\n    module_hash: _MfaModuleDict = OrderedDict()\n    for module in modules:\n        module_hash[module.id] = module\n\n    manager = AuthManager(hass, store, provider_hash, module_hash)\n    await manager.async_setup()\n    return manager\n\n\nclass AuthManagerFlowManager(\n    FlowManager[AuthFlowContext, AuthFlowResult, tuple[str, str]]\n):\n    \"\"\"Manage authentication flows.\"\"\"\n\n    _flow_result = AuthFlowResult\n\n    def __init__(self, hass: HomeAssistant, auth_manager: AuthManager) -> None:\n        \"\"\"Init auth manager flows.\"\"\"\n        super().__init__(hass)\n        self.auth_manager = auth_manager\n\n    async def async_create_flow(\n        self,\n        handler_key: tuple[str, str],\n        *,\n        context: AuthFlowContext | None = None,\n        data: dict[str, Any] | None = None,\n    ) -> LoginFlow[Any]:\n        \"\"\"Create a login flow.\"\"\"\n        auth_provider = self.auth_manager.get_auth_provider(*handler_key)\n        if not auth_provider:\n            raise KeyError(f\"Unknown auth provider {handler_key}\")\n        return await auth_provider.async_login_flow(context)\n\n    async def async_finish_flow(\n        self,\n        flow: FlowHandler[AuthFlowContext, AuthFlowResult, tuple[str, str]],\n        result: AuthFlowResult,\n    ) -> AuthFlowResult:\n        \"\"\"Return a user as result of login flow.\n\n        This method is called when a flow step returns FlowResultType.ABORT or\n        FlowResultType.CREATE_ENTRY.\n        \"\"\"\n        flow = cast(LoginFlow, flow)\n\n        if result[\"type\"] != FlowResultType.CREATE_ENTRY:\n            return result\n\n        # we got final result\n        if isinstance(result[\"data\"], models.Credentials):\n            result[\"result\"] = result[\"data\"]\n            return result\n\n        auth_provider = self.auth_manager.get_auth_provider(*result[\"handler\"])\n        if not auth_provider:\n            raise KeyError(f\"Unknown auth provider {result['handler']}\")\n\n        credentials = await auth_provider.async_get_or_create_credentials(\n            cast(Mapping[str, str], result[\"data\"]),\n        )\n\n        if flow.context.get(\"credential_only\"):\n            result[\"result\"] = credentials\n            return result\n\n        # multi-factor module cannot enabled for new credential\n        # which has not linked to a user yet\n        if auth_provider.support_mfa and not credentials.is_new:\n            user = await self.auth_manager.async_get_user_by_credentials(credentials)\n            if user is not None:\n                modules = await self.auth_manager.async_get_enabled_mfa(user)\n\n                if modules:\n                    flow.credential = credentials\n                    flow.user = user\n                    flow.available_mfa_modules = modules\n                    return await flow.async_step_select_mfa_module()\n\n        result[\"result\"] = credentials\n        return result\n\n\nclass AuthManager:\n    \"\"\"Manage the authentication for Home Assistant.\"\"\"\n\n    def __init__(\n        self,\n        hass: HomeAssistant,\n        store: auth_store.AuthStore,\n        providers: _ProviderDict,\n        mfa_modules: _MfaModuleDict,\n    ) -> None:\n        \"\"\"Initialize the auth manager.\"\"\"\n        self.hass = hass\n        self._store = store\n        self._providers = providers\n        self._mfa_modules = mfa_modules\n        self.login_flow = AuthManagerFlowManager(hass, self)\n        self._revoke_callbacks: dict[str, set[CALLBACK_TYPE]] = {}\n        self._expire_callback: CALLBACK_TYPE | None = None\n        self._remove_expired_job = HassJob(\n            self._async_remove_expired_refresh_tokens, job_type=HassJobType.Callback\n        )\n\n    async def async_setup(self) -> None:\n        \"\"\"Set up the auth manager.\"\"\"\n        hass = self.hass\n        hass.async_add_shutdown_job(\n            HassJob(\n                self._async_cancel_expiration_schedule, job_type=HassJobType.Callback\n            )\n        )\n        self._async_track_next_refresh_token_expiration()\n\n    @property\n    def auth_providers(self) -> list[AuthProvider]:\n        \"\"\"Return a list of available auth providers.\"\"\"\n        return list(self._providers.values())\n\n    @property\n    def auth_mfa_modules(self) -> list[MultiFactorAuthModule]:\n        \"\"\"Return a list of available auth modules.\"\"\"\n        return list(self._mfa_modules.values())\n\n    def get_auth_provider(\n        self, provider_type: str, provider_id: str | None\n    ) -> AuthProvider | None:\n        \"\"\"Return an auth provider, None if not found.\"\"\"\n        return self._providers.get((provider_type, provider_id))\n\n    def get_auth_providers(self, provider_type: str) -> list[AuthProvider]:\n        \"\"\"Return a List of auth provider of one type, Empty if not found.\"\"\"\n        return [\n            provider\n            for (p_type, _), provider in self._providers.items()\n            if p_type == provider_type\n        ]\n\n    def get_auth_mfa_module(self, module_id: str) -> MultiFactorAuthModule | None:\n        \"\"\"Return a multi-factor auth module, None if not found.\"\"\"\n        return self._mfa_modules.get(module_id)\n\n    async def async_get_users(self) -> list[models.User]:\n        \"\"\"Retrieve all users.\"\"\"\n        return await self._store.async_get_users()\n\n    async def async_get_user(self, user_id: str) -> models.User | None:\n        \"\"\"Retrieve a user.\"\"\"\n        return await self._store.async_get_user(user_id)\n\n    async def async_get_owner(self) -> models.User | None:\n        \"\"\"Retrieve the owner.\"\"\"\n        users = await self.async_get_users()\n        return next((user for user in users if user.is_owner), None)\n\n    async def async_get_group(self, group_id: str) -> models.Group | None:\n        \"\"\"Retrieve all groups.\"\"\"\n        return await self._store.async_get_group(group_id)\n\n    async def async_get_user_by_credentials(\n        self, credentials: models.Credentials\n    ) -> models.User | None:\n        \"\"\"Get a user by credential, return None if not found.\"\"\"\n        for user in await self.async_get_users():\n            for creds in user.credentials:\n                if creds.id == credentials.id:\n                    return user\n\n        return None\n\n    async def async_create_system_user(\n        self,\n        name: str,\n        *,\n        group_ids: list[str] | None = None,\n        local_only: bool | None = None,\n    ) -> models.User:\n        \"\"\"Create a system user.\"\"\"\n        user = await self._store.async_create_user(\n            name=name,\n            system_generated=True,\n            is_active=True,\n            group_ids=group_ids or [],\n            local_only=local_only,\n        )\n\n        self.hass.bus.async_fire(EVENT_USER_ADDED, {\"user_id\": user.id})\n\n        return user\n\n    async def async_create_user(\n        self,\n        name: str,\n        *,\n        group_ids: list[str] | None = None,\n        local_only: bool | None = None,\n    ) -> models.User:\n        \"\"\"Create a user.\"\"\"\n        kwargs: dict[str, Any] = {\n            \"name\": name,\n            \"is_active\": True,\n            \"group_ids\": group_ids or [],\n            \"local_only\": local_only,\n        }\n\n        if await self._user_should_be_owner():\n            kwargs[\"is_owner\"] = True\n\n        user = await self._store.async_create_user(**kwargs)\n\n        self.hass.bus.async_fire(EVENT_USER_ADDED, {\"user_id\": user.id})\n\n        return user\n\n    async def async_get_or_create_user(\n        self, credentials: models.Credentials\n    ) -> models.User:\n        \"\"\"Get or create a user.\"\"\"\n        if not credentials.is_new:\n            user = await self.async_get_user_by_credentials(credentials)\n            if user is None:\n                raise ValueError(\"Unable to find the user.\")\n            return user\n\n        auth_provider = self._async_get_auth_provider(credentials)\n\n        if auth_provider is None:\n            raise RuntimeError(\"Credential with unknown provider encountered\")\n\n        info = await auth_provider.async_user_meta_for_credentials(credentials)\n\n        user = await self._store.async_create_user(\n            credentials=credentials,\n            name=info.name,\n            is_active=info.is_active,\n            group_ids=[GROUP_ID_ADMIN if info.group is None else info.group],\n            local_only=info.local_only,\n        )\n\n        self.hass.bus.async_fire(EVENT_USER_ADDED, {\"user_id\": user.id})\n\n        return user\n\n    async def async_link_user(\n        self, user: models.User, credentials: models.Credentials\n    ) -> None:\n        \"\"\"Link credentials to an existing user.\"\"\"\n        linked_user = await self.async_get_user_by_credentials(credentials)\n        if linked_user == user:\n            return\n        if linked_user is not None:\n            raise ValueError(\"Credential is already linked to a user\")\n\n        await self._store.async_link_user(user, credentials)\n\n    async def async_remove_user(self, user: models.User) -> None:\n        \"\"\"Remove a user.\"\"\"\n        tasks = [\n            self.async_remove_credentials(credentials)\n            for credentials in user.credentials\n        ]\n\n        if tasks:\n            await asyncio.gather(*tasks)\n\n        await self._store.async_remove_user(user)\n\n        self.hass.bus.async_fire(EVENT_USER_REMOVED, {\"user_id\": user.id})\n\n    async def async_update_user(\n        self,\n        user: models.User,\n        name: str | None = None,\n        is_active: bool | None = None,\n        group_ids: list[str] | None = None,\n        local_only: bool | None = None,\n    ) -> None:\n        \"\"\"Update a user.\"\"\"\n        kwargs: dict[str, Any] = {\n            attr_name: value\n            for attr_name, value in (\n                (\"name\", name),\n                (\"group_ids\", group_ids),\n                (\"local_only\", local_only),\n            )\n            if value is not None\n        }\n        await self._store.async_update_user(user, **kwargs)\n\n        if is_active is not None:\n            if is_active is True:\n                await self.async_activate_user(user)\n            else:\n                await self.async_deactivate_user(user)\n\n        self.hass.bus.async_fire(EVENT_USER_UPDATED, {\"user_id\": user.id})\n\n    @callback\n    def async_update_user_credentials_data(\n        self, credentials: models.Credentials, data: dict[str, Any]\n    ) -> None:\n        \"\"\"Update credentials data.\"\"\"\n        self._store.async_update_user_credentials_data(credentials, data=data)\n\n    async def async_activate_user(self, user: models.User) -> None:\n        \"\"\"Activate a user.\"\"\"\n        await self._store.async_activate_user(user)\n\n    async def async_deactivate_user(self, user: models.User) -> None:\n        \"\"\"Deactivate a user.\"\"\"\n        if user.is_owner:\n            raise ValueError(\"Unable to deactivate the owner\")\n        await self._store.async_deactivate_user(user)\n\n    async def async_remove_credentials(self, credentials: models.Credentials) -> None:\n        \"\"\"Remove credentials.\"\"\"\n        provider = self._async_get_auth_provider(credentials)\n\n        if provider is not None and hasattr(provider, \"async_will_remove_credentials\"):\n            await provider.async_will_remove_credentials(credentials)\n\n        await self._store.async_remove_credentials(credentials)\n\n    async def async_enable_user_mfa(\n        self, user: models.User, mfa_module_id: str, data: Any\n    ) -> None:\n        \"\"\"Enable a multi-factor auth module for user.\"\"\"\n        if user.system_generated:\n            raise ValueError(\n                \"System generated users cannot enable multi-factor auth module.\"\n            )\n\n        if (module := self.get_auth_mfa_module(mfa_module_id)) is None:\n            raise ValueError(f\"Unable find multi-factor auth module: {mfa_module_id}\")\n\n        await module.async_setup_user(user.id, data)\n\n    async def async_disable_user_mfa(\n        self, user: models.User, mfa_module_id: str\n    ) -> None:\n        \"\"\"Disable a multi-factor auth module for user.\"\"\"\n        if user.system_generated:\n            raise ValueError(\n                \"System generated users cannot disable multi-factor auth module.\"\n            )\n\n        if (module := self.get_auth_mfa_module(mfa_module_id)) is None:\n            raise ValueError(f\"Unable find multi-factor auth module: {mfa_module_id}\")\n\n        await module.async_depose_user(user.id)\n\n    async def async_get_enabled_mfa(self, user: models.User) -> dict[str, str]:\n        \"\"\"List enabled mfa modules for user.\"\"\"\n        modules: dict[str, str] = OrderedDict()\n        for module_id, module in self._mfa_modules.items():\n            if await module.async_is_user_setup(user.id):\n                modules[module_id] = module.name\n        return modules\n\n    async def async_create_refresh_token(\n        self,\n        user: models.User,\n        client_id: str | None = None,\n        client_name: str | None = None,\n        client_icon: str | None = None,\n        token_type: str | None = None,\n        access_token_expiration: timedelta = ACCESS_TOKEN_EXPIRATION,\n        credential: models.Credentials | None = None,\n    ) -> models.RefreshToken:\n        \"\"\"Create a new refresh token for a user.\"\"\"\n        if not user.is_active:\n            raise ValueError(\"User is not active\")\n\n        if user.system_generated and client_id is not None:\n            raise ValueError(\n                \"System generated users cannot have refresh tokens connected \"\n                \"to a client.\"\n            )\n\n        if token_type is None:\n            if user.system_generated:\n                token_type = models.TOKEN_TYPE_SYSTEM\n            else:\n                token_type = models.TOKEN_TYPE_NORMAL\n\n        if token_type is models.TOKEN_TYPE_NORMAL:\n            expire_at = time.time() + REFRESH_TOKEN_EXPIRATION\n        else:\n            expire_at = None\n\n        if user.system_generated != (token_type == models.TOKEN_TYPE_SYSTEM):\n            raise ValueError(\n                \"System generated users can only have system type refresh tokens\"\n            )\n\n        if token_type == models.TOKEN_TYPE_NORMAL and client_id is None:\n            raise ValueError(\"Client is required to generate a refresh token.\")\n\n        if (\n            token_type == models.TOKEN_TYPE_LONG_LIVED_ACCESS_TOKEN\n            and client_name is None\n        ):\n            raise ValueError(\"Client_name is required for long-lived access token\")\n\n        if token_type == models.TOKEN_TYPE_LONG_LIVED_ACCESS_TOKEN:\n            for token in user.refresh_tokens.values():\n                if (\n                    token.client_name == client_name\n                    and token.token_type == models.TOKEN_TYPE_LONG_LIVED_ACCESS_TOKEN\n                ):\n                    # Each client_name can only have one\n                    # long_lived_access_token type of refresh token\n                    raise ValueError(f\"{client_name} already exists\")\n\n        return await self._store.async_create_refresh_token(\n            user,\n            client_id,\n            client_name,\n            client_icon,\n            token_type,\n            access_token_expiration,\n            expire_at,\n            credential,\n        )\n\n    @callback\n    def async_get_refresh_token(self, token_id: str) -> models.RefreshToken | None:\n        \"\"\"Get refresh token by id.\"\"\"\n        return self._store.async_get_refresh_token(token_id)\n\n    @callback\n    def async_get_refresh_token_by_token(\n        self, token: str\n    ) -> models.RefreshToken | None:\n        \"\"\"Get refresh token by token.\"\"\"\n        return self._store.async_get_refresh_token_by_token(token)\n\n    @callback\n    def async_remove_refresh_token(self, refresh_token: models.RefreshToken) -> None:\n        \"\"\"Delete a refresh token.\"\"\"\n        self._store.async_remove_refresh_token(refresh_token)\n\n        callbacks = self._revoke_callbacks.pop(refresh_token.id, ())\n        for revoke_callback in callbacks:\n            revoke_callback()\n\n    @callback\n    def async_set_expiry(\n        self, refresh_token: models.RefreshToken, *, enable_expiry: bool\n    ) -> None:\n        \"\"\"Enable or disable expiry of a refresh token.\"\"\"\n        self._store.async_set_expiry(refresh_token, enable_expiry=enable_expiry)\n\n    @callback\n    def _async_remove_expired_refresh_tokens(self, _: datetime | None = None) -> None:\n        \"\"\"Remove expired refresh tokens.\"\"\"\n        now = time.time()\n        for token in self._store.async_get_refresh_tokens():\n            if (expire_at := token.expire_at) is not None and expire_at <= now:\n                self.async_remove_refresh_token(token)\n        self._async_track_next_refresh_token_expiration()\n\n    @callback\n    def _async_track_next_refresh_token_expiration(self) -> None:\n        \"\"\"Initialise all token expiration scheduled tasks.\"\"\"\n        next_expiration = time.time() + REFRESH_TOKEN_EXPIRATION\n        for token in self._store.async_get_refresh_tokens():\n            if (\n                expire_at := token.expire_at\n            ) is not None and expire_at < next_expiration:\n                next_expiration = expire_at\n\n        self._expire_callback = async_track_point_in_utc_time(\n            self.hass,\n            self._remove_expired_job,\n            dt_util.utc_from_timestamp(next_expiration),\n        )\n\n    @callback\n    def _async_cancel_expiration_schedule(self) -> None:\n        \"\"\"Cancel tracking of expired refresh tokens.\"\"\"\n        if self._expire_callback:\n            self._expire_callback()\n            self._expire_callback = None\n\n    @callback\n    def _async_unregister(\n        self, callbacks: set[CALLBACK_TYPE], callback_: CALLBACK_TYPE\n    ) -> None:\n        \"\"\"Unregister a callback.\"\"\"\n        callbacks.remove(callback_)\n\n    @callback\n    def async_register_revoke_token_callback(\n        self, refresh_token_id: str, revoke_callback: CALLBACK_TYPE\n    ) -> CALLBACK_TYPE:\n        \"\"\"Register a callback to be called when the refresh token id is revoked.\"\"\"\n        if refresh_token_id not in self._revoke_callbacks:\n            self._revoke_callbacks[refresh_token_id] = set()\n\n        callbacks = self._revoke_callbacks[refresh_token_id]\n        callbacks.add(revoke_callback)\n        return partial(self._async_unregister, callbacks, revoke_callback)\n\n    @callback\n    def async_create_access_token(\n        self, refresh_token: models.RefreshToken, remote_ip: str | None = None\n    ) -> str:\n        \"\"\"Create a new access token.\"\"\"\n        self.async_validate_refresh_token(refresh_token, remote_ip)\n\n        self._store.async_log_refresh_token_usage(refresh_token, remote_ip)\n\n        now = int(time.time())\n        expire_seconds = int(refresh_token.access_token_expiration.total_seconds())\n        return jwt.encode(\n            {\n                \"iss\": refresh_token.id,\n                \"iat\": now,\n                \"exp\": now + expire_seconds,\n            },\n            refresh_token.jwt_key,\n            algorithm=\"HS256\",\n        )\n\n    @callback\n    def _async_resolve_provider(\n        self, refresh_token: models.RefreshToken\n    ) -> AuthProvider | None:\n        \"\"\"Get the auth provider for the given refresh token.\n\n        Raises an exception if the expected provider is no longer available or return\n        None if no provider was expected for this refresh token.\n        \"\"\"\n        if refresh_token.credential is None:\n            return None\n\n        provider = self.get_auth_provider(\n            refresh_token.credential.auth_provider_type,\n            refresh_token.credential.auth_provider_id,\n        )\n        if provider is None:\n            raise InvalidProvider(\n                f\"Auth provider {refresh_token.credential.auth_provider_type},\"\n                f\" {refresh_token.credential.auth_provider_id} not available\"\n            )\n        return provider\n\n    @callback\n    def async_validate_refresh_token(\n        self, refresh_token: models.RefreshToken, remote_ip: str | None = None\n    ) -> None:\n        \"\"\"Validate that a refresh token is usable.\n\n        Will raise InvalidAuthError on errors.\n        \"\"\"\n        if provider := self._async_resolve_provider(refresh_token):\n            provider.async_validate_refresh_token(refresh_token, remote_ip)\n\n    @callback\n    def async_validate_access_token(self, token: str) -> models.RefreshToken | None:\n        \"\"\"Return refresh token if an access token is valid.\"\"\"\n        try:\n            unverif_claims = jwt_wrapper.unverified_hs256_token_decode(token)\n        except jwt.InvalidTokenError:\n            return None\n\n        refresh_token = self.async_get_refresh_token(\n            cast(str, unverif_claims.get(\"iss\"))\n        )\n\n        if refresh_token is None:\n            jwt_key = \"\"\n            issuer = \"\"\n        else:\n            jwt_key = refresh_token.jwt_key\n            issuer = refresh_token.id\n\n        try:\n            jwt_wrapper.verify_and_decode(\n                token, jwt_key, leeway=10, issuer=issuer, algorithms=[\"HS256\"]\n            )\n        except jwt.InvalidTokenError:\n            return None\n\n        if refresh_token is None or not refresh_token.user.is_active:\n            return None\n\n        return refresh_token\n\n    @callback\n    def _async_get_auth_provider(\n        self, credentials: models.Credentials\n    ) -> AuthProvider | None:\n        \"\"\"Get auth provider from a set of credentials.\"\"\"\n        auth_provider_key = (\n            credentials.auth_provider_type,\n            credentials.auth_provider_id,\n        )\n        return self._providers.get(auth_provider_key)\n\n    async def _user_should_be_owner(self) -> bool:\n        \"\"\"Determine if user should be owner.\n\n        A user should be an owner if it is the first non-system user that is\n        being created.\n        \"\"\"\n        for user in await self._store.async_get_users():\n            if not user.system_generated:\n                return False\n\n        return True\n", "metadata": {"source": "github.com/home-assistant/core", "file_path": "homeassistant/auth/__init__.py", "language": "python", "complexity": "complex", "star_count": 80370, "quality_score": 1.0, "features": {"has_classes": true, "has_functions": true, "has_async": true, "has_decorators": true, "has_imports": true, "has_error_handling": true, "has_documentation": true, "has_type_hints": true, "has_testing": false, "has_logging": true, "function_count": 45, "class_count": 4, "line_count": 704}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "async_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create asynchronous code with proper concurrency handling with asyncio coroutines\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"Storage for auth models.\"\"\"\n\nfrom __future__ import annotations\n\nfrom datetime import timedelta\nimport hmac\nimport itertools\nfrom logging import getLogger\nfrom typing import Any\n\nfrom homeassistant.core import HomeAssistant, callback\nfrom homeassistant.helpers import device_registry as dr, entity_registry as er\nfrom homeassistant.helpers.storage import Store\nfrom homeassistant.util import dt as dt_util\n\nfrom . import models\nfrom .const import (\n    ACCESS_TOKEN_EXPIRATION,\n    GROUP_ID_ADMIN,\n    GROUP_ID_READ_ONLY,\n    GROUP_ID_USER,\n    REFRESH_TOKEN_EXPIRATION,\n)\nfrom .permissions import system_policies\nfrom .permissions.models import PermissionLookup\nfrom .permissions.types import PolicyType\n\nSTORAGE_VERSION = 1\nSTORAGE_KEY = \"auth\"\nGROUP_NAME_ADMIN = \"Administrators\"\nGROUP_NAME_USER = \"Users\"\nGROUP_NAME_READ_ONLY = \"Read Only\"\n\n# We always save the auth store after we load it since\n# we may migrate data and do not want to have to do it again\n# but we don't want to do it during startup so we schedule\n# the first save 5 minutes out knowing something else may\n# want to save the auth store before then, and since Storage\n# will honor the lower of the two delays, it will save it\n# faster if something else saves it.\nINITIAL_LOAD_SAVE_DELAY = 300\n\nDEFAULT_SAVE_DELAY = 1\n\n\nclass AuthStore:\n    \"\"\"Stores authentication info.\n\n    Any mutation to an object should happen inside the auth store.\n\n    The auth store is lazy. It won't load the data from disk until a method is\n    called that needs it.\n    \"\"\"\n\n    def __init__(self, hass: HomeAssistant) -> None:\n        \"\"\"Initialize the auth store.\"\"\"\n        self.hass = hass\n        self._loaded = False\n        self._users: dict[str, models.User] = None  # type: ignore[assignment]\n        self._groups: dict[str, models.Group] = None  # type: ignore[assignment]\n        self._perm_lookup: PermissionLookup = None  # type: ignore[assignment]\n        self._store = Store[dict[str, list[dict[str, Any]]]](\n            hass, STORAGE_VERSION, STORAGE_KEY, private=True, atomic_writes=True\n        )\n        self._token_id_to_user_id: dict[str, str] = {}\n\n    async def async_get_groups(self) -> list[models.Group]:\n        \"\"\"Retrieve all users.\"\"\"\n        return list(self._groups.values())\n\n    async def async_get_group(self, group_id: str) -> models.Group | None:\n        \"\"\"Retrieve all users.\"\"\"\n        return self._groups.get(group_id)\n\n    async def async_get_users(self) -> list[models.User]:\n        \"\"\"Retrieve all users.\"\"\"\n        return list(self._users.values())\n\n    async def async_get_user(self, user_id: str) -> models.User | None:\n        \"\"\"Retrieve a user by id.\"\"\"\n        return self._users.get(user_id)\n\n    async def async_create_user(\n        self,\n        name: str | None,\n        is_owner: bool | None = None,\n        is_active: bool | None = None,\n        system_generated: bool | None = None,\n        credentials: models.Credentials | None = None,\n        group_ids: list[str] | None = None,\n        local_only: bool | None = None,\n    ) -> models.User:\n        \"\"\"Create a new user.\"\"\"\n        groups = []\n        for group_id in group_ids or []:\n            if (group := self._groups.get(group_id)) is None:\n                raise ValueError(f\"Invalid group specified {group_id}\")\n            groups.append(group)\n\n        kwargs: dict[str, Any] = {\n            \"name\": name,\n            # Until we get group management, we just put everyone in the\n            # same group.\n            \"groups\": groups,\n            \"perm_lookup\": self._perm_lookup,\n        }\n\n        kwargs.update(\n            {\n                attr_name: value\n                for attr_name, value in (\n                    (\"is_owner\", is_owner),\n                    (\"is_active\", is_active),\n                    (\"local_only\", local_only),\n                    (\"system_generated\", system_generated),\n                )\n                if value is not None\n            }\n        )\n\n        new_user = models.User(**kwargs)\n\n        self._users[new_user.id] = new_user\n\n        if credentials is None:\n            self._async_schedule_save()\n            return new_user\n\n        # Saving is done inside the link.\n        await self.async_link_user(new_user, credentials)\n        return new_user\n\n    async def async_link_user(\n        self, user: models.User, credentials: models.Credentials\n    ) -> None:\n        \"\"\"Add credentials to an existing user.\"\"\"\n        user.credentials.append(credentials)\n        self._async_schedule_save()\n        credentials.is_new = False\n\n    async def async_remove_user(self, user: models.User) -> None:\n        \"\"\"Remove a user.\"\"\"\n        user = self._users.pop(user.id)\n        for refresh_token_id in user.refresh_tokens:\n            del self._token_id_to_user_id[refresh_token_id]\n        user.refresh_tokens.clear()\n        self._async_schedule_save()\n\n    async def async_update_user(\n        self,\n        user: models.User,\n        name: str | None = None,\n        is_active: bool | None = None,\n        group_ids: list[str] | None = None,\n        local_only: bool | None = None,\n    ) -> None:\n        \"\"\"Update a user.\"\"\"\n        if group_ids is not None:\n            groups = []\n            for grid in group_ids:\n                if (group := self._groups.get(grid)) is None:\n                    raise ValueError(\"Invalid group specified.\")\n                groups.append(group)\n\n            user.groups = groups\n\n        for attr_name, value in (\n            (\"name\", name),\n            (\"is_active\", is_active),\n            (\"local_only\", local_only),\n        ):\n            if value is not None:\n                setattr(user, attr_name, value)\n\n        self._async_schedule_save()\n\n    async def async_activate_user(self, user: models.User) -> None:\n        \"\"\"Activate a user.\"\"\"\n        user.is_active = True\n        self._async_schedule_save()\n\n    async def async_deactivate_user(self, user: models.User) -> None:\n        \"\"\"Activate a user.\"\"\"\n        user.is_active = False\n        self._async_schedule_save()\n\n    async def async_remove_credentials(self, credentials: models.Credentials) -> None:\n        \"\"\"Remove credentials.\"\"\"\n        for user in self._users.values():\n            found = None\n\n            for index, cred in enumerate(user.credentials):\n                if cred is credentials:\n                    found = index\n                    break\n\n            if found is not None:\n                user.credentials.pop(found)\n                break\n\n        self._async_schedule_save()\n\n    async def async_create_refresh_token(\n        self,\n        user: models.User,\n        client_id: str | None = None,\n        client_name: str | None = None,\n        client_icon: str | None = None,\n        token_type: str = models.TOKEN_TYPE_NORMAL,\n        access_token_expiration: timedelta = ACCESS_TOKEN_EXPIRATION,\n        expire_at: float | None = None,\n        credential: models.Credentials | None = None,\n    ) -> models.RefreshToken:\n        \"\"\"Create a new token for a user.\"\"\"\n        kwargs: dict[str, Any] = {\n            \"user\": user,\n            \"client_id\": client_id,\n            \"token_type\": token_type,\n            \"access_token_expiration\": access_token_expiration,\n            \"expire_at\": expire_at,\n            \"credential\": credential,\n        }\n        if client_name:\n            kwargs[\"client_name\"] = client_name\n        if client_icon:\n            kwargs[\"client_icon\"] = client_icon\n\n        refresh_token = models.RefreshToken(**kwargs)\n        token_id = refresh_token.id\n        user.refresh_tokens[token_id] = refresh_token\n        self._token_id_to_user_id[token_id] = user.id\n\n        self._async_schedule_save()\n        return refresh_token\n\n    @callback\n    def async_remove_refresh_token(self, refresh_token: models.RefreshToken) -> None:\n        \"\"\"Remove a refresh token.\"\"\"\n        refresh_token_id = refresh_token.id\n        if user_id := self._token_id_to_user_id.get(refresh_token_id):\n            del self._users[user_id].refresh_tokens[refresh_token_id]\n            del self._token_id_to_user_id[refresh_token_id]\n            self._async_schedule_save()\n\n    @callback\n    def async_get_refresh_token(self, token_id: str) -> models.RefreshToken | None:\n        \"\"\"Get refresh token by id.\"\"\"\n        if user_id := self._token_id_to_user_id.get(token_id):\n            return self._users[user_id].refresh_tokens.get(token_id)\n        return None\n\n    @callback\n    def async_get_refresh_token_by_token(\n        self, token: str\n    ) -> models.RefreshToken | None:\n        \"\"\"Get refresh token by token.\"\"\"\n        found = None\n\n        for user in self._users.values():\n            for refresh_token in user.refresh_tokens.values():\n                if hmac.compare_digest(refresh_token.token, token):\n                    found = refresh_token\n\n        return found\n\n    @callback\n    def async_get_refresh_tokens(self) -> list[models.RefreshToken]:\n        \"\"\"Get all refresh tokens.\"\"\"\n        return list(\n            itertools.chain.from_iterable(\n                user.refresh_tokens.values() for user in self._users.values()\n            )\n        )\n\n    @callback\n    def async_log_refresh_token_usage(\n        self, refresh_token: models.RefreshToken, remote_ip: str | None = None\n    ) -> None:\n        \"\"\"Update refresh token last used information.\"\"\"\n        refresh_token.last_used_at = dt_util.utcnow()\n        refresh_token.last_used_ip = remote_ip\n        if refresh_token.expire_at:\n            refresh_token.expire_at = (\n                refresh_token.last_used_at.timestamp() + REFRESH_TOKEN_EXPIRATION\n            )\n        self._async_schedule_save()\n\n    @callback\n    def async_set_expiry(\n        self, refresh_token: models.RefreshToken, *, enable_expiry: bool\n    ) -> None:\n        \"\"\"Enable or disable expiry of a refresh token.\"\"\"\n        if enable_expiry:\n            if refresh_token.expire_at is None:\n                refresh_token.expire_at = (\n                    refresh_token.last_used_at or dt_util.utcnow()\n                ).timestamp() + REFRESH_TOKEN_EXPIRATION\n                self._async_schedule_save()\n        else:\n            refresh_token.expire_at = None\n            self._async_schedule_save()\n\n    @callback\n    def async_update_user_credentials_data(\n        self, credentials: models.Credentials, data: dict[str, Any]\n    ) -> None:\n        \"\"\"Update credentials data.\"\"\"\n        credentials.data = data\n        self._async_schedule_save()\n\n    async def async_load(self) -> None:\n        \"\"\"Load the users.\"\"\"\n        if self._loaded:\n            raise RuntimeError(\"Auth storage is already loaded\")\n        self._loaded = True\n\n        dev_reg = dr.async_get(self.hass)\n        ent_reg = er.async_get(self.hass)\n        data = await self._store.async_load()\n\n        perm_lookup = PermissionLookup(ent_reg, dev_reg)\n        self._perm_lookup = perm_lookup\n\n        if data is None or not isinstance(data, dict):\n            self._set_defaults()\n            return\n\n        users: dict[str, models.User] = {}\n        groups: dict[str, models.Group] = {}\n        credentials: dict[str, models.Credentials] = {}\n\n        # Soft-migrating data as we load. We are going to make sure we have a\n        # read only group and an admin group. There are two states that we can\n        # migrate from:\n        # 1. Data from a recent version which has a single group without policy\n        # 2. Data from old version which has no groups\n        has_admin_group = False\n        has_user_group = False\n        has_read_only_group = False\n        group_without_policy = None\n\n        # When creating objects we mention each attribute explicitly. This\n        # prevents crashing if user rolls back HA version after a new property\n        # was added.\n\n        for group_dict in data.get(\"groups\", []):\n            policy: PolicyType | None = None\n\n            if group_dict[\"id\"] == GROUP_ID_ADMIN:\n                has_admin_group = True\n\n                name = GROUP_NAME_ADMIN\n                policy = system_policies.ADMIN_POLICY\n                system_generated = True\n\n            elif group_dict[\"id\"] == GROUP_ID_USER:\n                has_user_group = True\n\n                name = GROUP_NAME_USER\n                policy = system_policies.USER_POLICY\n                system_generated = True\n\n            elif group_dict[\"id\"] == GROUP_ID_READ_ONLY:\n                has_read_only_group = True\n\n                name = GROUP_NAME_READ_ONLY\n                policy = system_policies.READ_ONLY_POLICY\n                system_generated = True\n\n            else:\n                name = group_dict[\"name\"]\n                policy = group_dict.get(\"policy\")\n                system_generated = False\n\n            # We don't want groups without a policy that are not system groups\n            # This is part of migrating from state 1\n            if policy is None:\n                group_without_policy = group_dict[\"id\"]\n                continue\n\n            groups[group_dict[\"id\"]] = models.Group(\n                id=group_dict[\"id\"],\n                name=name,\n                policy=policy,\n                system_generated=system_generated,\n            )\n\n        # If there are no groups, add all existing users to the admin group.\n        # This is part of migrating from state 2\n        migrate_users_to_admin_group = not groups and group_without_policy is None\n\n        # If we find a no_policy_group, we need to migrate all users to the\n        # admin group. We only do this if there are no other groups, as is\n        # the expected state. If not expected state, not marking people admin.\n        # This is part of migrating from state 1\n        if groups and group_without_policy is not None:\n            group_without_policy = None\n\n        # This is part of migrating from state 1 and 2\n        if not has_admin_group:\n            admin_group = _system_admin_group()\n            groups[admin_group.id] = admin_group\n\n        # This is part of migrating from state 1 and 2\n        if not has_read_only_group:\n            read_only_group = _system_read_only_group()\n            groups[read_only_group.id] = read_only_group\n\n        if not has_user_group:\n            user_group = _system_user_group()\n            groups[user_group.id] = user_group\n\n        for user_dict in data[\"users\"]:\n            # Collect the users group.\n            user_groups = []\n            for group_id in user_dict.get(\"group_ids\", []):\n                # This is part of migrating from state 1\n                if group_id == group_without_policy:\n                    group_id = GROUP_ID_ADMIN\n                user_groups.append(groups[group_id])\n\n            # This is part of migrating from state 2\n            if not user_dict[\"system_generated\"] and migrate_users_to_admin_group:\n                user_groups.append(groups[GROUP_ID_ADMIN])\n\n            users[user_dict[\"id\"]] = models.User(\n                name=user_dict[\"name\"],\n                groups=user_groups,\n                id=user_dict[\"id\"],\n                is_owner=user_dict[\"is_owner\"],\n                is_active=user_dict[\"is_active\"],\n                system_generated=user_dict[\"system_generated\"],\n                perm_lookup=perm_lookup,\n                # New in 2021.11\n                local_only=user_dict.get(\"local_only\", False),\n            )\n\n        for cred_dict in data[\"credentials\"]:\n            credential = models.Credentials(\n                id=cred_dict[\"id\"],\n                is_new=False,\n                auth_provider_type=cred_dict[\"auth_provider_type\"],\n                auth_provider_id=cred_dict[\"auth_provider_id\"],\n                data=cred_dict[\"data\"],\n            )\n            credentials[cred_dict[\"id\"]] = credential\n            users[cred_dict[\"user_id\"]].credentials.append(credential)\n\n        for rt_dict in data[\"refresh_tokens\"]:\n            # Filter out the old keys that don't have jwt_key (pre-0.76)\n            if \"jwt_key\" not in rt_dict:\n                continue\n\n            created_at = dt_util.parse_datetime(rt_dict[\"created_at\"])\n            if created_at is None:\n                getLogger(__name__).error(\n                    (\n                        \"Ignoring refresh token %(id)s with invalid created_at \"\n                        \"%(created_at)s for user_id %(user_id)s\"\n                    ),\n                    rt_dict,\n                )\n                continue\n\n            if (token_type := rt_dict.get(\"token_type\")) is None:\n                if rt_dict[\"client_id\"] is None:\n                    token_type = models.TOKEN_TYPE_SYSTEM\n                else:\n                    token_type = models.TOKEN_TYPE_NORMAL\n\n            # old refresh_token don't have last_used_at (pre-0.78)\n            if last_used_at_str := rt_dict.get(\"last_used_at\"):\n                last_used_at = dt_util.parse_datetime(last_used_at_str)\n            else:\n                last_used_at = None\n\n            token = models.RefreshToken(\n                id=rt_dict[\"id\"],\n                user=users[rt_dict[\"user_id\"]],\n                client_id=rt_dict[\"client_id\"],\n                # use dict.get to keep backward compatibility\n                client_name=rt_dict.get(\"client_name\"),\n                client_icon=rt_dict.get(\"client_icon\"),\n                token_type=token_type,\n                created_at=created_at,\n                access_token_expiration=timedelta(\n                    seconds=rt_dict[\"access_token_expiration\"]\n                ),\n                token=rt_dict[\"token\"],\n                jwt_key=rt_dict[\"jwt_key\"],\n                last_used_at=last_used_at,\n                last_used_ip=rt_dict.get(\"last_used_ip\"),\n                expire_at=rt_dict.get(\"expire_at\"),\n                version=rt_dict.get(\"version\"),\n            )\n            if \"credential_id\" in rt_dict:\n                token.credential = credentials.get(rt_dict[\"credential_id\"])\n            users[rt_dict[\"user_id\"]].refresh_tokens[token.id] = token\n\n        self._groups = groups\n        self._users = users\n        self._build_token_id_to_user_id()\n        self._async_schedule_save(INITIAL_LOAD_SAVE_DELAY)\n\n    @callback\n    def _build_token_id_to_user_id(self) -> None:\n        \"\"\"Build a map of token id to user id.\"\"\"\n        self._token_id_to_user_id = {\n            token_id: user_id\n            for user_id, user in self._users.items()\n            for token_id in user.refresh_tokens\n        }\n\n    @callback\n    def _async_schedule_save(self, delay: float = DEFAULT_SAVE_DELAY) -> None:\n        \"\"\"Save users.\"\"\"\n        self._store.async_delay_save(self._data_to_save, delay)\n\n    @callback\n    def _data_to_save(self) -> dict[str, list[dict[str, Any]]]:\n        \"\"\"Return the data to store.\"\"\"\n        users = [\n            {\n                \"id\": user.id,\n                \"group_ids\": [group.id for group in user.groups],\n                \"is_owner\": user.is_owner,\n                \"is_active\": user.is_active,\n                \"name\": user.name,\n                \"system_generated\": user.system_generated,\n                \"local_only\": user.local_only,\n            }\n            for user in self._users.values()\n        ]\n\n        groups = []\n        for group in self._groups.values():\n            g_dict: dict[str, Any] = {\n                \"id\": group.id,\n                # Name not read for sys groups. Kept here for backwards compat\n                \"name\": group.name,\n            }\n\n            if not group.system_generated:\n                g_dict[\"policy\"] = group.policy\n\n            groups.append(g_dict)\n\n        credentials = [\n            {\n                \"id\": credential.id,\n                \"user_id\": user.id,\n                \"auth_provider_type\": credential.auth_provider_type,\n                \"auth_provider_id\": credential.auth_provider_id,\n                \"data\": credential.data,\n            }\n            for user in self._users.values()\n            for credential in user.credentials\n        ]\n\n        refresh_tokens = [\n            {\n                \"id\": refresh_token.id,\n                \"user_id\": user.id,\n                \"client_id\": refresh_token.client_id,\n                \"client_name\": refresh_token.client_name,\n                \"client_icon\": refresh_token.client_icon,\n                \"token_type\": refresh_token.token_type,\n                \"created_at\": refresh_token.created_at.isoformat(),\n                \"access_token_expiration\": (\n                    refresh_token.access_token_expiration.total_seconds()\n                ),\n                \"token\": refresh_token.token,\n                \"jwt_key\": refresh_token.jwt_key,\n                \"last_used_at\": refresh_token.last_used_at.isoformat()\n                if refresh_token.last_used_at\n                else None,\n                \"last_used_ip\": refresh_token.last_used_ip,\n                \"expire_at\": refresh_token.expire_at,\n                \"credential_id\": refresh_token.credential.id\n                if refresh_token.credential\n                else None,\n                \"version\": refresh_token.version,\n            }\n            for user in self._users.values()\n            for refresh_token in user.refresh_tokens.values()\n        ]\n\n        return {\n            \"users\": users,\n            \"groups\": groups,\n            \"credentials\": credentials,\n            \"refresh_tokens\": refresh_tokens,\n        }\n\n    def _set_defaults(self) -> None:\n        \"\"\"Set default values for auth store.\"\"\"\n        self._users = {}\n\n        groups: dict[str, models.Group] = {}\n        admin_group = _system_admin_group()\n        groups[admin_group.id] = admin_group\n        user_group = _system_user_group()\n        groups[user_group.id] = user_group\n        read_only_group = _system_read_only_group()\n        groups[read_only_group.id] = read_only_group\n        self._groups = groups\n        self._build_token_id_to_user_id()\n\n\ndef _system_admin_group() -> models.Group:\n    \"\"\"Create system admin group.\"\"\"\n    return models.Group(\n        name=GROUP_NAME_ADMIN,\n        id=GROUP_ID_ADMIN,\n        policy=system_policies.ADMIN_POLICY,\n        system_generated=True,\n    )\n\n\ndef _system_user_group() -> models.Group:\n    \"\"\"Create system user group.\"\"\"\n    return models.Group(\n        name=GROUP_NAME_USER,\n        id=GROUP_ID_USER,\n        policy=system_policies.USER_POLICY,\n        system_generated=True,\n    )\n\n\ndef _system_read_only_group() -> models.Group:\n    \"\"\"Create read only group.\"\"\"\n    return models.Group(\n        name=GROUP_NAME_READ_ONLY,\n        id=GROUP_ID_READ_ONLY,\n        policy=system_policies.READ_ONLY_POLICY,\n        system_generated=True,\n    )\n", "metadata": {"source": "github.com/home-assistant/core", "file_path": "homeassistant/auth/auth_store.py", "language": "python", "complexity": "complex", "star_count": 80370, "quality_score": 0.98, "features": {"has_classes": true, "has_functions": true, "has_async": true, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": true, "has_type_hints": true, "has_testing": false, "has_logging": true, "function_count": 28, "class_count": 1, "line_count": 638}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "async_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create asynchronous code with proper concurrency handling with asyncio coroutines\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"Pluggable auth modules for Home Assistant.\"\"\"\n\nfrom __future__ import annotations\n\nimport logging\nimport types\nfrom typing import Any\n\nimport voluptuous as vol\nfrom voluptuous.humanize import humanize_error\n\nfrom homeassistant import data_entry_flow, requirements\nfrom homeassistant.const import CONF_ID, CONF_NAME, CONF_TYPE\nfrom homeassistant.core import HomeAssistant\nfrom homeassistant.data_entry_flow import FlowResult\nfrom homeassistant.exceptions import HomeAssistantError\nfrom homeassistant.helpers.importlib import async_import_module\nfrom homeassistant.util.decorator import Registry\nfrom homeassistant.util.hass_dict import HassKey\n\nMULTI_FACTOR_AUTH_MODULES: Registry[str, type[MultiFactorAuthModule]] = Registry()\n\nMULTI_FACTOR_AUTH_MODULE_SCHEMA = vol.Schema(\n    {\n        vol.Required(CONF_TYPE): str,\n        vol.Optional(CONF_NAME): str,\n        # Specify ID if you have two mfa auth module for same type.\n        vol.Optional(CONF_ID): str,\n    },\n    extra=vol.ALLOW_EXTRA,\n)\n\nDATA_REQS: HassKey[set[str]] = HassKey(\"mfa_auth_module_reqs_processed\")\n\n_LOGGER = logging.getLogger(__name__)\n\n\nclass MultiFactorAuthModule:\n    \"\"\"Multi-factor Auth Module of validation function.\"\"\"\n\n    DEFAULT_TITLE = \"Unnamed auth module\"\n    MAX_RETRY_TIME = 3\n\n    def __init__(self, hass: HomeAssistant, config: dict[str, Any]) -> None:\n        \"\"\"Initialize an auth module.\"\"\"\n        self.hass = hass\n        self.config = config\n\n    @property\n    def id(self) -> str:\n        \"\"\"Return id of the auth module.\n\n        Default is same as type\n        \"\"\"\n        return self.config.get(CONF_ID, self.type)  # type: ignore[no-any-return]\n\n    @property\n    def type(self) -> str:\n        \"\"\"Return type of the module.\"\"\"\n        return self.config[CONF_TYPE]  # type: ignore[no-any-return]\n\n    @property\n    def name(self) -> str:\n        \"\"\"Return the name of the auth module.\"\"\"\n        return self.config.get(CONF_NAME, self.DEFAULT_TITLE)  # type: ignore[no-any-return]\n\n    # Implement by extending class\n\n    @property\n    def input_schema(self) -> vol.Schema:\n        \"\"\"Return a voluptuous schema to define mfa auth module's input.\"\"\"\n        raise NotImplementedError\n\n    async def async_setup_flow(self, user_id: str) -> SetupFlow[Any]:\n        \"\"\"Return a data entry flow handler for setup module.\n\n        Mfa module should extend SetupFlow\n        \"\"\"\n        raise NotImplementedError\n\n    async def async_setup_user(self, user_id: str, setup_data: Any) -> Any:\n        \"\"\"Set up user for mfa auth module.\"\"\"\n        raise NotImplementedError\n\n    async def async_depose_user(self, user_id: str) -> None:\n        \"\"\"Remove user from mfa module.\"\"\"\n        raise NotImplementedError\n\n    async def async_is_user_setup(self, user_id: str) -> bool:\n        \"\"\"Return whether user is setup.\"\"\"\n        raise NotImplementedError\n\n    async def async_validate(self, user_id: str, user_input: dict[str, Any]) -> bool:\n        \"\"\"Return True if validation passed.\"\"\"\n        raise NotImplementedError\n\n\nclass SetupFlow[_MultiFactorAuthModuleT: MultiFactorAuthModule = MultiFactorAuthModule](\n    data_entry_flow.FlowHandler\n):\n    \"\"\"Handler for the setup flow.\"\"\"\n\n    def __init__(\n        self,\n        auth_module: _MultiFactorAuthModuleT,\n        setup_schema: vol.Schema,\n        user_id: str,\n    ) -> None:\n        \"\"\"Initialize the setup flow.\"\"\"\n        self._auth_module = auth_module\n        self._setup_schema = setup_schema\n        self._user_id = user_id\n\n    async def async_step_init(\n        self, user_input: dict[str, str] | None = None\n    ) -> FlowResult:\n        \"\"\"Handle the first step of setup flow.\n\n        Return self.async_show_form(step_id='init') if user_input is None.\n        Return self.async_create_entry(data={'result': result}) if finish.\n        \"\"\"\n        errors: dict[str, str] = {}\n\n        if user_input:\n            result = await self._auth_module.async_setup_user(self._user_id, user_input)\n            return self.async_create_entry(data={\"result\": result})\n\n        return self.async_show_form(\n            step_id=\"init\", data_schema=self._setup_schema, errors=errors\n        )\n\n\nasync def auth_mfa_module_from_config(\n    hass: HomeAssistant, config: dict[str, Any]\n) -> MultiFactorAuthModule:\n    \"\"\"Initialize an auth module from a config.\"\"\"\n    module_name: str = config[CONF_TYPE]\n    module = await _load_mfa_module(hass, module_name)\n\n    try:\n        config = module.CONFIG_SCHEMA(config)\n    except vol.Invalid as err:\n        _LOGGER.error(\n            \"Invalid configuration for multi-factor module %s: %s\",\n            module_name,\n            humanize_error(config, err),\n        )\n        raise\n\n    return MULTI_FACTOR_AUTH_MODULES[module_name](hass, config)\n\n\nasync def _load_mfa_module(hass: HomeAssistant, module_name: str) -> types.ModuleType:\n    \"\"\"Load an mfa auth module.\"\"\"\n    module_path = f\"homeassistant.auth.mfa_modules.{module_name}\"\n\n    try:\n        module = await async_import_module(hass, module_path)\n    except ImportError as err:\n        _LOGGER.error(\"Unable to load mfa module %s: %s\", module_name, err)\n        raise HomeAssistantError(\n            f\"Unable to load mfa module {module_name}: {err}\"\n        ) from err\n\n    if hass.config.skip_pip or not hasattr(module, \"REQUIREMENTS\"):\n        return module\n\n    processed = hass.data.get(DATA_REQS)\n    if processed and module_name in processed:\n        return module\n\n    processed = hass.data[DATA_REQS] = set()\n\n    await requirements.async_process_requirements(\n        hass, module_path, module.REQUIREMENTS\n    )\n\n    processed.add(module_name)\n    return module\n", "metadata": {"source": "github.com/home-assistant/core", "file_path": "homeassistant/auth/mfa_modules/__init__.py", "language": "python", "complexity": "complex", "star_count": 80370, "quality_score": 1.0, "features": {"has_classes": true, "has_functions": true, "has_async": true, "has_decorators": true, "has_imports": true, "has_error_handling": true, "has_documentation": true, "has_type_hints": true, "has_testing": false, "has_logging": true, "function_count": 14, "class_count": 2, "line_count": 180}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "async_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create asynchronous code with proper concurrency handling with asyncio coroutines\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"Example auth module.\"\"\"\n\nfrom __future__ import annotations\n\nfrom typing import Any\n\nimport voluptuous as vol\n\nfrom homeassistant.core import HomeAssistant\n\nfrom . import (\n    MULTI_FACTOR_AUTH_MODULE_SCHEMA,\n    MULTI_FACTOR_AUTH_MODULES,\n    MultiFactorAuthModule,\n    SetupFlow,\n)\n\nCONFIG_SCHEMA = MULTI_FACTOR_AUTH_MODULE_SCHEMA.extend(\n    {\n        vol.Required(\"data\"): [\n            vol.Schema({vol.Required(\"user_id\"): str, vol.Required(\"pin\"): str})\n        ]\n    },\n    extra=vol.PREVENT_EXTRA,\n)\n\n\n@MULTI_FACTOR_AUTH_MODULES.register(\"insecure_example\")\nclass InsecureExampleModule(MultiFactorAuthModule):\n    \"\"\"Example auth module validate pin.\"\"\"\n\n    DEFAULT_TITLE = \"Insecure Personal Identify Number\"\n\n    def __init__(self, hass: HomeAssistant, config: dict[str, Any]) -> None:\n        \"\"\"Initialize the user data store.\"\"\"\n        super().__init__(hass, config)\n        self._data = config[\"data\"]\n\n    @property\n    def input_schema(self) -> vol.Schema:\n        \"\"\"Validate login flow input data.\"\"\"\n        return vol.Schema({vol.Required(\"pin\"): str})\n\n    @property\n    def setup_schema(self) -> vol.Schema:\n        \"\"\"Validate async_setup_user input data.\"\"\"\n        return vol.Schema({vol.Required(\"pin\"): str})\n\n    async def async_setup_flow(self, user_id: str) -> SetupFlow:\n        \"\"\"Return a data entry flow handler for setup module.\n\n        Mfa module should extend SetupFlow\n        \"\"\"\n        return SetupFlow(self, self.setup_schema, user_id)\n\n    async def async_setup_user(self, user_id: str, setup_data: Any) -> Any:\n        \"\"\"Set up user to use mfa module.\"\"\"\n        # data shall has been validate in caller\n        pin = setup_data[\"pin\"]\n\n        for data in self._data:\n            if data[\"user_id\"] == user_id:\n                # already setup, override\n                data[\"pin\"] = pin\n                return\n\n        self._data.append({\"user_id\": user_id, \"pin\": pin})\n\n    async def async_depose_user(self, user_id: str) -> None:\n        \"\"\"Remove user from mfa module.\"\"\"\n        found = None\n        for data in self._data:\n            if data[\"user_id\"] == user_id:\n                found = data\n                break\n        if found:\n            self._data.remove(found)\n\n    async def async_is_user_setup(self, user_id: str) -> bool:\n        \"\"\"Return whether user is setup.\"\"\"\n        return any(data[\"user_id\"] == user_id for data in self._data)\n\n    async def async_validate(self, user_id: str, user_input: dict[str, Any]) -> bool:\n        \"\"\"Return True if validation passed.\"\"\"\n        return any(\n            data[\"user_id\"] == user_id and data[\"pin\"] == user_input[\"pin\"]\n            for data in self._data\n        )\n", "metadata": {"source": "github.com/home-assistant/core", "file_path": "homeassistant/auth/mfa_modules/insecure_example.py", "language": "python", "complexity": "medium", "star_count": 80370, "quality_score": 0.98, "features": {"has_classes": true, "has_functions": true, "has_async": true, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": true, "has_type_hints": true, "has_testing": false, "has_logging": true, "function_count": 8, "class_count": 1, "line_count": 89}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "async_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create asynchronous code with proper concurrency handling with asyncio coroutines\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"HMAC-based One-time Password auth module.\n\nSending HOTP through notify service\n\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\nfrom collections import OrderedDict\nimport logging\nfrom typing import Any, cast\n\nimport attr\nimport voluptuous as vol\n\nfrom homeassistant.const import CONF_EXCLUDE, CONF_INCLUDE\nfrom homeassistant.core import HomeAssistant, callback\nfrom homeassistant.data_entry_flow import FlowResult\nfrom homeassistant.exceptions import ServiceNotFound\nfrom homeassistant.helpers import config_validation as cv\nfrom homeassistant.helpers.storage import Store\n\nfrom . import (\n    MULTI_FACTOR_AUTH_MODULE_SCHEMA,\n    MULTI_FACTOR_AUTH_MODULES,\n    MultiFactorAuthModule,\n    SetupFlow,\n)\n\nREQUIREMENTS = [\"pyotp==2.8.0\"]\n\nCONF_MESSAGE = \"message\"\n\nCONFIG_SCHEMA = MULTI_FACTOR_AUTH_MODULE_SCHEMA.extend(\n    {\n        vol.Optional(CONF_INCLUDE): vol.All(cv.ensure_list, [cv.string]),\n        vol.Optional(CONF_EXCLUDE): vol.All(cv.ensure_list, [cv.string]),\n        vol.Optional(CONF_MESSAGE, default=\"{} is your Home Assistant login code\"): str,\n    },\n    extra=vol.PREVENT_EXTRA,\n)\n\nSTORAGE_VERSION = 1\nSTORAGE_KEY = \"auth_module.notify\"\nSTORAGE_USERS = \"users\"\nSTORAGE_USER_ID = \"user_id\"\n\nINPUT_FIELD_CODE = \"code\"\n\n_LOGGER = logging.getLogger(__name__)\n\n\ndef _generate_secret() -> str:\n    \"\"\"Generate a secret.\"\"\"\n    import pyotp  # noqa: PLC0415\n\n    return str(pyotp.random_base32())\n\n\ndef _generate_random() -> int:\n    \"\"\"Generate a 32 digit number.\"\"\"\n    import pyotp  # noqa: PLC0415\n\n    return int(pyotp.random_base32(length=32, chars=list(\"1234567890\")))\n\n\ndef _generate_otp(secret: str, count: int) -> str:\n    \"\"\"Generate one time password.\"\"\"\n    import pyotp  # noqa: PLC0415\n\n    return str(pyotp.HOTP(secret).at(count))\n\n\ndef _verify_otp(secret: str, otp: str, count: int) -> bool:\n    \"\"\"Verify one time password.\"\"\"\n    import pyotp  # noqa: PLC0415\n\n    return bool(pyotp.HOTP(secret).verify(otp, count))\n\n\n@attr.s(slots=True)\nclass NotifySetting:\n    \"\"\"Store notify setting for one user.\"\"\"\n\n    secret: str = attr.ib(factory=_generate_secret)  # not persistent\n    counter: int = attr.ib(factory=_generate_random)  # not persistent\n    notify_service: str | None = attr.ib(default=None)\n    target: str | None = attr.ib(default=None)\n\n\ntype _UsersDict = dict[str, NotifySetting]\n\n\n@MULTI_FACTOR_AUTH_MODULES.register(\"notify\")\nclass NotifyAuthModule(MultiFactorAuthModule):\n    \"\"\"Auth module send hmac-based one time password by notify service.\"\"\"\n\n    DEFAULT_TITLE = \"Notify One-Time Password\"\n\n    def __init__(self, hass: HomeAssistant, config: dict[str, Any]) -> None:\n        \"\"\"Initialize the user data store.\"\"\"\n        super().__init__(hass, config)\n        self._user_settings: _UsersDict | None = None\n        self._user_store = Store[dict[str, dict[str, Any]]](\n            hass, STORAGE_VERSION, STORAGE_KEY, private=True, atomic_writes=True\n        )\n        self._include = config.get(CONF_INCLUDE, [])\n        self._exclude = config.get(CONF_EXCLUDE, [])\n        self._message_template = config[CONF_MESSAGE]\n        self._init_lock = asyncio.Lock()\n\n    @property\n    def input_schema(self) -> vol.Schema:\n        \"\"\"Validate login flow input data.\"\"\"\n        return vol.Schema({vol.Required(INPUT_FIELD_CODE): str})\n\n    async def _async_load(self) -> None:\n        \"\"\"Load stored data.\"\"\"\n        async with self._init_lock:\n            if self._user_settings is not None:\n                return\n\n            if (data := await self._user_store.async_load()) is None:\n                data = cast(dict[str, dict[str, Any]], {STORAGE_USERS: {}})\n\n            self._user_settings = {\n                user_id: NotifySetting(**setting)\n                for user_id, setting in data.get(STORAGE_USERS, {}).items()\n            }\n\n    async def _async_save(self) -> None:\n        \"\"\"Save data.\"\"\"\n        if self._user_settings is None:\n            return\n\n        await self._user_store.async_save(\n            {\n                STORAGE_USERS: {\n                    user_id: attr.asdict(\n                        notify_setting,\n                        filter=attr.filters.exclude(\n                            attr.fields(NotifySetting).secret,\n                            attr.fields(NotifySetting).counter,\n                        ),\n                    )\n                    for user_id, notify_setting in self._user_settings.items()\n                }\n            }\n        )\n\n    @callback\n    def aync_get_available_notify_services(self) -> list[str]:\n        \"\"\"Return list of notify services.\"\"\"\n        unordered_services = set()\n\n        for service in self.hass.services.async_services_for_domain(\"notify\"):\n            if service not in self._exclude:\n                unordered_services.add(service)\n\n        if self._include:\n            unordered_services &= set(self._include)\n\n        return sorted(unordered_services)\n\n    async def async_setup_flow(self, user_id: str) -> NotifySetupFlow:\n        \"\"\"Return a data entry flow handler for setup module.\n\n        Mfa module should extend SetupFlow\n        \"\"\"\n        return NotifySetupFlow(\n            self, self.input_schema, user_id, self.aync_get_available_notify_services()\n        )\n\n    async def async_setup_user(self, user_id: str, setup_data: Any) -> Any:\n        \"\"\"Set up auth module for user.\"\"\"\n        if self._user_settings is None:\n            await self._async_load()\n            assert self._user_settings is not None\n\n        self._user_settings[user_id] = NotifySetting(\n            notify_service=setup_data.get(\"notify_service\"),\n            target=setup_data.get(\"target\"),\n        )\n\n        await self._async_save()\n\n    async def async_depose_user(self, user_id: str) -> None:\n        \"\"\"Depose auth module for user.\"\"\"\n        if self._user_settings is None:\n            await self._async_load()\n            assert self._user_settings is not None\n\n        if self._user_settings.pop(user_id, None):\n            await self._async_save()\n\n    async def async_is_user_setup(self, user_id: str) -> bool:\n        \"\"\"Return whether user is setup.\"\"\"\n        if self._user_settings is None:\n            await self._async_load()\n            assert self._user_settings is not None\n\n        return user_id in self._user_settings\n\n    async def async_validate(self, user_id: str, user_input: dict[str, Any]) -> bool:\n        \"\"\"Return True if validation passed.\"\"\"\n        if self._user_settings is None:\n            await self._async_load()\n            assert self._user_settings is not None\n\n        if (notify_setting := self._user_settings.get(user_id)) is None:\n            return False\n\n        # user_input has been validate in caller\n        return await self.hass.async_add_executor_job(\n            _verify_otp,\n            notify_setting.secret,\n            user_input.get(INPUT_FIELD_CODE, \"\"),\n            notify_setting.counter,\n        )\n\n    async def async_initialize_login_mfa_step(self, user_id: str) -> None:\n        \"\"\"Generate code and notify user.\"\"\"\n        if self._user_settings is None:\n            await self._async_load()\n            assert self._user_settings is not None\n\n        if (notify_setting := self._user_settings.get(user_id)) is None:\n            raise ValueError(\"Cannot find user_id\")\n\n        def generate_secret_and_one_time_password() -> str:\n            \"\"\"Generate and send one time password.\"\"\"\n            assert notify_setting\n            # secret and counter are not persistent\n            notify_setting.secret = _generate_secret()\n            notify_setting.counter = _generate_random()\n            return _generate_otp(notify_setting.secret, notify_setting.counter)\n\n        code = await self.hass.async_add_executor_job(\n            generate_secret_and_one_time_password\n        )\n\n        await self.async_notify_user(user_id, code)\n\n    async def async_notify_user(self, user_id: str, code: str) -> None:\n        \"\"\"Send code by user's notify service.\"\"\"\n        if self._user_settings is None:\n            await self._async_load()\n            assert self._user_settings is not None\n\n        if (notify_setting := self._user_settings.get(user_id)) is None:\n            _LOGGER.error(\"Cannot find user %s\", user_id)\n            return\n\n        await self.async_notify(\n            code,\n            notify_setting.notify_service,  # type: ignore[arg-type]\n            notify_setting.target,\n        )\n\n    async def async_notify(\n        self, code: str, notify_service: str, target: str | None = None\n    ) -> None:\n        \"\"\"Send code by notify service.\"\"\"\n        data = {\"message\": self._message_template.format(code)}\n        if target:\n            data[\"target\"] = [target]\n\n        await self.hass.services.async_call(\"notify\", notify_service, data)\n\n\nclass NotifySetupFlow(SetupFlow[NotifyAuthModule]):\n    \"\"\"Handler for the setup flow.\"\"\"\n\n    def __init__(\n        self,\n        auth_module: NotifyAuthModule,\n        setup_schema: vol.Schema,\n        user_id: str,\n        available_notify_services: list[str],\n    ) -> None:\n        \"\"\"Initialize the setup flow.\"\"\"\n        super().__init__(auth_module, setup_schema, user_id)\n        self._available_notify_services = available_notify_services\n        self._secret: str | None = None\n        self._count: int | None = None\n        self._notify_service: str | None = None\n        self._target: str | None = None\n\n    async def async_step_init(\n        self, user_input: dict[str, str] | None = None\n    ) -> FlowResult:\n        \"\"\"Let user select available notify services.\"\"\"\n        errors: dict[str, str] = {}\n\n        hass = self._auth_module.hass\n        if user_input:\n            self._notify_service = user_input[\"notify_service\"]\n            self._target = user_input.get(\"target\")\n            self._secret = await hass.async_add_executor_job(_generate_secret)\n            self._count = await hass.async_add_executor_job(_generate_random)\n\n            return await self.async_step_setup()\n\n        if not self._available_notify_services:\n            return self.async_abort(reason=\"no_available_service\")\n\n        schema: dict[str, Any] = OrderedDict()\n        schema[\"notify_service\"] = vol.In(self._available_notify_services)\n        schema[\"target\"] = vol.Optional(str)\n\n        return self.async_show_form(\n            step_id=\"init\", data_schema=vol.Schema(schema), errors=errors\n        )\n\n    async def async_step_setup(\n        self, user_input: dict[str, str] | None = None\n    ) -> FlowResult:\n        \"\"\"Verify user can receive one-time password.\"\"\"\n        errors: dict[str, str] = {}\n\n        hass = self._auth_module.hass\n        assert self._secret and self._count\n        if user_input:\n            verified = await hass.async_add_executor_job(\n                _verify_otp, self._secret, user_input[\"code\"], self._count\n            )\n            if verified:\n                await self._auth_module.async_setup_user(\n                    self._user_id,\n                    {\"notify_service\": self._notify_service, \"target\": self._target},\n                )\n                return self.async_create_entry(data={})\n\n            errors[\"base\"] = \"invalid_code\"\n\n        # generate code every time, no retry logic\n        code = await hass.async_add_executor_job(\n            _generate_otp, self._secret, self._count\n        )\n\n        assert self._notify_service\n        try:\n            await self._auth_module.async_notify(\n                code, self._notify_service, self._target\n            )\n        except ServiceNotFound:\n            return self.async_abort(reason=\"notify_service_not_exist\")\n\n        return self.async_show_form(\n            step_id=\"setup\",\n            data_schema=self._setup_schema,\n            description_placeholders={\"notify_service\": self._notify_service},\n            errors=errors,\n        )\n", "metadata": {"source": "github.com/home-assistant/core", "file_path": "homeassistant/auth/mfa_modules/notify.py", "language": "python", "complexity": "complex", "star_count": 80370, "quality_score": 1.0, "features": {"has_classes": true, "has_functions": true, "has_async": true, "has_decorators": true, "has_imports": true, "has_error_handling": true, "has_documentation": true, "has_type_hints": true, "has_testing": true, "has_logging": true, "function_count": 21, "class_count": 3, "line_count": 355}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "async_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create asynchronous code with proper concurrency handling with asyncio coroutines\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"Time-based One Time Password auth module.\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\nfrom io import BytesIO\nfrom typing import Any, cast\n\nimport voluptuous as vol\n\nfrom homeassistant.auth.models import User\nfrom homeassistant.core import HomeAssistant\nfrom homeassistant.data_entry_flow import FlowResult\nfrom homeassistant.helpers.storage import Store\n\nfrom . import (\n    MULTI_FACTOR_AUTH_MODULE_SCHEMA,\n    MULTI_FACTOR_AUTH_MODULES,\n    MultiFactorAuthModule,\n    SetupFlow,\n)\n\nREQUIREMENTS = [\"pyotp==2.8.0\", \"PyQRCode==1.2.1\"]\n\nCONFIG_SCHEMA = MULTI_FACTOR_AUTH_MODULE_SCHEMA.extend({}, extra=vol.PREVENT_EXTRA)\n\nSTORAGE_VERSION = 1\nSTORAGE_KEY = \"auth_module.totp\"\nSTORAGE_USERS = \"users\"\nSTORAGE_USER_ID = \"user_id\"\nSTORAGE_OTA_SECRET = \"ota_secret\"\n\nINPUT_FIELD_CODE = \"code\"\n\nDUMMY_SECRET = \"FPPTH34D4E3MI2HG\"\n\n\ndef _generate_qr_code(data: str) -> str:\n    \"\"\"Generate a base64 PNG string represent QR Code image of data.\"\"\"\n    import pyqrcode  # noqa: PLC0415\n\n    qr_code = pyqrcode.create(data)\n\n    with BytesIO() as buffer:\n        qr_code.svg(file=buffer, scale=4)\n        return str(\n            buffer.getvalue()\n            .decode(\"ascii\")\n            .replace(\"\\n\", \"\")\n            .replace(\n                (\n                    '<?xml version=\"1.0\" encoding=\"UTF-8\"?>'\n                    '<svg xmlns=\"http://www.w3.org/2000/svg\"'\n                ),\n                \"<svg\",\n            )\n        )\n\n\ndef _generate_secret_and_qr_code(username: str) -> tuple[str, str, str]:\n    \"\"\"Generate a secret, url, and QR code.\"\"\"\n    import pyotp  # noqa: PLC0415\n\n    ota_secret = pyotp.random_base32()\n    url = pyotp.totp.TOTP(ota_secret).provisioning_uri(\n        username, issuer_name=\"Home Assistant\"\n    )\n    image = _generate_qr_code(url)\n    return ota_secret, url, image\n\n\n@MULTI_FACTOR_AUTH_MODULES.register(\"totp\")\nclass TotpAuthModule(MultiFactorAuthModule):\n    \"\"\"Auth module validate time-based one time password.\"\"\"\n\n    DEFAULT_TITLE = \"Time-based One Time Password\"\n    MAX_RETRY_TIME = 5\n\n    def __init__(self, hass: HomeAssistant, config: dict[str, Any]) -> None:\n        \"\"\"Initialize the user data store.\"\"\"\n        super().__init__(hass, config)\n        self._users: dict[str, str] | None = None\n        self._user_store = Store[dict[str, dict[str, str]]](\n            hass, STORAGE_VERSION, STORAGE_KEY, private=True, atomic_writes=True\n        )\n        self._init_lock = asyncio.Lock()\n\n    @property\n    def input_schema(self) -> vol.Schema:\n        \"\"\"Validate login flow input data.\"\"\"\n        return vol.Schema({vol.Required(INPUT_FIELD_CODE): str})\n\n    async def _async_load(self) -> None:\n        \"\"\"Load stored data.\"\"\"\n        async with self._init_lock:\n            if self._users is not None:\n                return\n\n            if (data := await self._user_store.async_load()) is None:\n                data = cast(dict[str, dict[str, str]], {STORAGE_USERS: {}})\n\n            self._users = data.get(STORAGE_USERS, {})\n\n    async def _async_save(self) -> None:\n        \"\"\"Save data.\"\"\"\n        await self._user_store.async_save({STORAGE_USERS: self._users or {}})\n\n    def _add_ota_secret(self, user_id: str, secret: str | None = None) -> str:\n        \"\"\"Create a ota_secret for user.\"\"\"\n        import pyotp  # noqa: PLC0415\n\n        ota_secret: str = secret or pyotp.random_base32()\n\n        self._users[user_id] = ota_secret  # type: ignore[index]\n        return ota_secret\n\n    async def async_setup_flow(self, user_id: str) -> TotpSetupFlow:\n        \"\"\"Return a data entry flow handler for setup module.\n\n        Mfa module should extend SetupFlow\n        \"\"\"\n        user = await self.hass.auth.async_get_user(user_id)\n        assert user is not None\n        return TotpSetupFlow(self, self.input_schema, user)\n\n    async def async_setup_user(self, user_id: str, setup_data: Any) -> str:\n        \"\"\"Set up auth module for user.\"\"\"\n        if self._users is None:\n            await self._async_load()\n\n        result = await self.hass.async_add_executor_job(\n            self._add_ota_secret, user_id, setup_data.get(\"secret\")\n        )\n\n        await self._async_save()\n        return result\n\n    async def async_depose_user(self, user_id: str) -> None:\n        \"\"\"Depose auth module for user.\"\"\"\n        if self._users is None:\n            await self._async_load()\n\n        if self._users.pop(user_id, None):  # type: ignore[union-attr]\n            await self._async_save()\n\n    async def async_is_user_setup(self, user_id: str) -> bool:\n        \"\"\"Return whether user is setup.\"\"\"\n        if self._users is None:\n            await self._async_load()\n\n        return user_id in self._users  # type: ignore[operator]\n\n    async def async_validate(self, user_id: str, user_input: dict[str, Any]) -> bool:\n        \"\"\"Return True if validation passed.\"\"\"\n        if self._users is None:\n            await self._async_load()\n\n        # user_input has been validate in caller\n        # set INPUT_FIELD_CODE as vol.Required is not user friendly\n        return await self.hass.async_add_executor_job(\n            self._validate_2fa, user_id, user_input.get(INPUT_FIELD_CODE, \"\")\n        )\n\n    def _validate_2fa(self, user_id: str, code: str) -> bool:\n        \"\"\"Validate two factor authentication code.\"\"\"\n        import pyotp  # noqa: PLC0415\n\n        if (ota_secret := self._users.get(user_id)) is None:  # type: ignore[union-attr]\n            # even we cannot find user, we still do verify\n            # to make timing the same as if user was found.\n            pyotp.TOTP(DUMMY_SECRET).verify(code, valid_window=1)\n            return False\n\n        return bool(pyotp.TOTP(ota_secret).verify(code, valid_window=1))\n\n\nclass TotpSetupFlow(SetupFlow[TotpAuthModule]):\n    \"\"\"Handler for the setup flow.\"\"\"\n\n    _ota_secret: str\n    _url: str\n    _image: str\n\n    def __init__(\n        self, auth_module: TotpAuthModule, setup_schema: vol.Schema, user: User\n    ) -> None:\n        \"\"\"Initialize the setup flow.\"\"\"\n        super().__init__(auth_module, setup_schema, user.id)\n        self._user = user\n\n    async def async_step_init(\n        self, user_input: dict[str, str] | None = None\n    ) -> FlowResult:\n        \"\"\"Handle the first step of setup flow.\n\n        Return self.async_show_form(step_id='init') if user_input is None.\n        Return self.async_create_entry(data={'result': result}) if finish.\n        \"\"\"\n        import pyotp  # noqa: PLC0415\n\n        errors: dict[str, str] = {}\n\n        if user_input:\n            verified = await self.hass.async_add_executor_job(\n                pyotp.TOTP(self._ota_secret).verify, user_input[\"code\"]\n            )\n            if verified:\n                result = await self._auth_module.async_setup_user(\n                    self._user_id, {\"secret\": self._ota_secret}\n                )\n                return self.async_create_entry(data={\"result\": result})\n\n            errors[\"base\"] = \"invalid_code\"\n\n        else:\n            (\n                self._ota_secret,\n                self._url,\n                self._image,\n            ) = await self._auth_module.hass.async_add_executor_job(\n                _generate_secret_and_qr_code,\n                str(self._user.name),\n            )\n\n        return self.async_show_form(\n            step_id=\"init\",\n            data_schema=self._setup_schema,\n            description_placeholders={\n                \"code\": self._ota_secret,\n                \"url\": self._url,\n                \"qr_code\": self._image,\n            },\n            errors=errors,\n        )\n", "metadata": {"source": "github.com/home-assistant/core", "file_path": "homeassistant/auth/mfa_modules/totp.py", "language": "python", "complexity": "complex", "star_count": 80370, "quality_score": 1.0, "features": {"has_classes": true, "has_functions": true, "has_async": true, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": true, "has_type_hints": true, "has_testing": true, "has_logging": true, "function_count": 15, "class_count": 2, "line_count": 235}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "async_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create asynchronous code with proper concurrency handling with asyncio coroutines\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"Auth providers for Home Assistant.\"\"\"\n\nfrom __future__ import annotations\n\nfrom collections.abc import Mapping\nimport logging\nimport types\nfrom typing import Any\n\nimport voluptuous as vol\nfrom voluptuous.humanize import humanize_error\n\nfrom homeassistant import requirements\nfrom homeassistant.const import CONF_ID, CONF_NAME, CONF_TYPE\nfrom homeassistant.core import HomeAssistant, callback\nfrom homeassistant.data_entry_flow import FlowHandler\nfrom homeassistant.exceptions import HomeAssistantError\nfrom homeassistant.helpers.importlib import async_import_module\nfrom homeassistant.util import dt as dt_util\nfrom homeassistant.util.decorator import Registry\nfrom homeassistant.util.hass_dict import HassKey\n\nfrom ..auth_store import AuthStore\nfrom ..const import MFA_SESSION_EXPIRATION\nfrom ..models import (\n    AuthFlowContext,\n    AuthFlowResult,\n    Credentials,\n    RefreshToken,\n    User,\n    UserMeta,\n)\n\n_LOGGER = logging.getLogger(__name__)\nDATA_REQS: HassKey[set[str]] = HassKey(\"auth_prov_reqs_processed\")\n\nAUTH_PROVIDERS: Registry[str, type[AuthProvider]] = Registry()\n\nAUTH_PROVIDER_SCHEMA = vol.Schema(\n    {\n        vol.Required(CONF_TYPE): str,\n        vol.Optional(CONF_NAME): str,\n        # Specify ID if you have two auth providers for same type.\n        vol.Optional(CONF_ID): str,\n    },\n    extra=vol.ALLOW_EXTRA,\n)\n\n\nclass AuthProvider:\n    \"\"\"Provider of user authentication.\"\"\"\n\n    DEFAULT_TITLE = \"Unnamed auth provider\"\n\n    def __init__(\n        self, hass: HomeAssistant, store: AuthStore, config: dict[str, Any]\n    ) -> None:\n        \"\"\"Initialize an auth provider.\"\"\"\n        self.hass = hass\n        self.store = store\n        self.config = config\n\n    @property\n    def id(self) -> str | None:\n        \"\"\"Return id of the auth provider.\n\n        Optional, can be None.\n        \"\"\"\n        return self.config.get(CONF_ID)\n\n    @property\n    def type(self) -> str:\n        \"\"\"Return type of the provider.\"\"\"\n        return self.config[CONF_TYPE]  # type: ignore[no-any-return]\n\n    @property\n    def name(self) -> str:\n        \"\"\"Return the name of the auth provider.\"\"\"\n        return self.config.get(CONF_NAME, self.DEFAULT_TITLE)  # type: ignore[no-any-return]\n\n    @property\n    def support_mfa(self) -> bool:\n        \"\"\"Return whether multi-factor auth supported by the auth provider.\"\"\"\n        return True\n\n    async def async_credentials(self) -> list[Credentials]:\n        \"\"\"Return all credentials of this provider.\"\"\"\n        users = await self.store.async_get_users()\n        return [\n            credentials\n            for user in users\n            for credentials in user.credentials\n            if (\n                credentials.auth_provider_type == self.type\n                and credentials.auth_provider_id == self.id\n            )\n        ]\n\n    @callback\n    def async_create_credentials(self, data: dict[str, str]) -> Credentials:\n        \"\"\"Create credentials.\"\"\"\n        return Credentials(\n            auth_provider_type=self.type, auth_provider_id=self.id, data=data\n        )\n\n    # Implement by extending class\n\n    async def async_login_flow(self, context: AuthFlowContext | None) -> LoginFlow[Any]:\n        \"\"\"Return the data flow for logging in with auth provider.\n\n        Auth provider should extend LoginFlow and return an instance.\n        \"\"\"\n        raise NotImplementedError\n\n    async def async_get_or_create_credentials(\n        self, flow_result: Mapping[str, str]\n    ) -> Credentials:\n        \"\"\"Get credentials based on the flow result.\"\"\"\n        raise NotImplementedError\n\n    async def async_user_meta_for_credentials(\n        self, credentials: Credentials\n    ) -> UserMeta:\n        \"\"\"Return extra user metadata for credentials.\n\n        Will be used to populate info when creating a new user.\n        \"\"\"\n        raise NotImplementedError\n\n    async def async_initialize(self) -> None:\n        \"\"\"Initialize the auth provider.\"\"\"\n\n    @callback\n    def async_validate_refresh_token(\n        self, refresh_token: RefreshToken, remote_ip: str | None = None\n    ) -> None:\n        \"\"\"Verify a refresh token is still valid.\n\n        Optional hook for an auth provider to verify validity of a refresh token.\n        Should raise InvalidAuthError on errors.\n        \"\"\"\n\n\nasync def auth_provider_from_config(\n    hass: HomeAssistant, store: AuthStore, config: dict[str, Any]\n) -> AuthProvider:\n    \"\"\"Initialize an auth provider from a config.\"\"\"\n    provider_name: str = config[CONF_TYPE]\n    module = await load_auth_provider_module(hass, provider_name)\n\n    try:\n        config = module.CONFIG_SCHEMA(config)\n    except vol.Invalid as err:\n        _LOGGER.error(\n            \"Invalid configuration for auth provider %s: %s\",\n            provider_name,\n            humanize_error(config, err),\n        )\n        raise\n\n    return AUTH_PROVIDERS[provider_name](hass, store, config)\n\n\nasync def load_auth_provider_module(\n    hass: HomeAssistant, provider: str\n) -> types.ModuleType:\n    \"\"\"Load an auth provider.\"\"\"\n    try:\n        module = await async_import_module(\n            hass, f\"homeassistant.auth.providers.{provider}\"\n        )\n    except ImportError as err:\n        _LOGGER.error(\"Unable to load auth provider %s: %s\", provider, err)\n        raise HomeAssistantError(\n            f\"Unable to load auth provider {provider}: {err}\"\n        ) from err\n\n    if hass.config.skip_pip or not hasattr(module, \"REQUIREMENTS\"):\n        return module\n\n    if (processed := hass.data.get(DATA_REQS)) is None:\n        processed = hass.data[DATA_REQS] = set()\n    elif provider in processed:\n        return module\n\n    reqs = module.REQUIREMENTS\n    await requirements.async_process_requirements(\n        hass, f\"auth provider {provider}\", reqs\n    )\n\n    processed.add(provider)\n    return module\n\n\nclass LoginFlow[_AuthProviderT: AuthProvider = AuthProvider](\n    FlowHandler[AuthFlowContext, AuthFlowResult, tuple[str, str]],\n):\n    \"\"\"Handler for the login flow.\"\"\"\n\n    _flow_result = AuthFlowResult\n\n    def __init__(self, auth_provider: _AuthProviderT) -> None:\n        \"\"\"Initialize the login flow.\"\"\"\n        self._auth_provider = auth_provider\n        self._auth_module_id: str | None = None\n        self._auth_manager = auth_provider.hass.auth\n        self.available_mfa_modules: dict[str, str] = {}\n        self.created_at = dt_util.utcnow()\n        self.invalid_mfa_times = 0\n        self.user: User | None = None\n        self.credential: Credentials | None = None\n\n    async def async_step_init(\n        self, user_input: dict[str, str] | None = None\n    ) -> AuthFlowResult:\n        \"\"\"Handle the first step of login flow.\n\n        Return self.async_show_form(step_id='init') if user_input is None.\n        Return await self.async_finish(flow_result) if login init step pass.\n        \"\"\"\n        raise NotImplementedError\n\n    async def async_step_select_mfa_module(\n        self, user_input: dict[str, str] | None = None\n    ) -> AuthFlowResult:\n        \"\"\"Handle the step of select mfa module.\"\"\"\n        errors = {}\n\n        if user_input is not None:\n            auth_module = user_input.get(\"multi_factor_auth_module\")\n            if auth_module in self.available_mfa_modules:\n                self._auth_module_id = auth_module\n                return await self.async_step_mfa()\n            errors[\"base\"] = \"invalid_auth_module\"\n\n        if len(self.available_mfa_modules) == 1:\n            self._auth_module_id = list(self.available_mfa_modules)[0]\n            return await self.async_step_mfa()\n\n        return self.async_show_form(\n            step_id=\"select_mfa_module\",\n            data_schema=vol.Schema(\n                {\"multi_factor_auth_module\": vol.In(self.available_mfa_modules)}\n            ),\n            errors=errors,\n        )\n\n    async def async_step_mfa(\n        self, user_input: dict[str, str] | None = None\n    ) -> AuthFlowResult:\n        \"\"\"Handle the step of mfa validation.\"\"\"\n        assert self.credential\n        assert self.user\n\n        errors = {}\n\n        assert self._auth_module_id is not None\n        auth_module = self._auth_manager.get_auth_mfa_module(self._auth_module_id)\n        if auth_module is None:\n            # Given an invalid input to async_step_select_mfa_module\n            # will show invalid_auth_module error\n            return await self.async_step_select_mfa_module(user_input={})\n\n        if user_input is None and hasattr(\n            auth_module, \"async_initialize_login_mfa_step\"\n        ):\n            try:\n                await auth_module.async_initialize_login_mfa_step(self.user.id)\n            except HomeAssistantError:\n                _LOGGER.exception(\"Error initializing MFA step\")\n                return self.async_abort(reason=\"unknown_error\")\n\n        if user_input is not None:\n            expires = self.created_at + MFA_SESSION_EXPIRATION\n            if dt_util.utcnow() > expires:\n                return self.async_abort(reason=\"login_expired\")\n\n            result = await auth_module.async_validate(self.user.id, user_input)\n            if not result:\n                errors[\"base\"] = \"invalid_code\"\n                self.invalid_mfa_times += 1\n                if self.invalid_mfa_times >= auth_module.MAX_RETRY_TIME > 0:\n                    return self.async_abort(reason=\"too_many_retry\")\n\n            if not errors:\n                return await self.async_finish(self.credential)\n\n        description_placeholders: dict[str, str] = {\n            \"mfa_module_name\": auth_module.name,\n            \"mfa_module_id\": auth_module.id,\n        }\n\n        return self.async_show_form(\n            step_id=\"mfa\",\n            data_schema=auth_module.input_schema,\n            description_placeholders=description_placeholders,\n            errors=errors,\n        )\n\n    async def async_finish(self, flow_result: Any) -> AuthFlowResult:\n        \"\"\"Handle the pass of login flow.\"\"\"\n        return self.async_create_entry(data=flow_result)\n", "metadata": {"source": "github.com/home-assistant/core", "file_path": "homeassistant/auth/providers/__init__.py", "language": "python", "complexity": "complex", "star_count": 80370, "quality_score": 1.0, "features": {"has_classes": true, "has_functions": true, "has_async": true, "has_decorators": true, "has_imports": true, "has_error_handling": true, "has_documentation": true, "has_type_hints": true, "has_testing": true, "has_logging": true, "function_count": 19, "class_count": 2, "line_count": 303}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "async_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create asynchronous code with proper concurrency handling with asyncio coroutines\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"Auth provider that validates credentials via an external command.\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\nfrom collections.abc import Mapping\nimport logging\nimport os\nfrom typing import Any\n\nimport voluptuous as vol\n\nfrom homeassistant.const import CONF_COMMAND\nfrom homeassistant.exceptions import HomeAssistantError\n\nfrom ..models import AuthFlowContext, AuthFlowResult, Credentials, UserMeta\nfrom . import AUTH_PROVIDER_SCHEMA, AUTH_PROVIDERS, AuthProvider, LoginFlow\n\nCONF_ARGS = \"args\"\nCONF_META = \"meta\"\n\nCONFIG_SCHEMA = AUTH_PROVIDER_SCHEMA.extend(\n    {\n        vol.Required(CONF_COMMAND): vol.All(\n            str, os.path.normpath, msg=\"must be an absolute path\"\n        ),\n        vol.Optional(CONF_ARGS, default=None): vol.Any(vol.DefaultTo(list), [str]),\n        vol.Optional(CONF_META, default=False): bool,\n    },\n    extra=vol.PREVENT_EXTRA,\n)\n\n_LOGGER = logging.getLogger(__name__)\n\n\nclass InvalidAuthError(HomeAssistantError):\n    \"\"\"Raised when authentication with given credentials fails.\"\"\"\n\n\n@AUTH_PROVIDERS.register(\"command_line\")\nclass CommandLineAuthProvider(AuthProvider):\n    \"\"\"Auth provider validating credentials by calling a command.\"\"\"\n\n    DEFAULT_TITLE = \"Command Line Authentication\"\n\n    # which keys to accept from a program's stdout\n    ALLOWED_META_KEYS = (\n        \"name\",\n        \"group\",\n        \"local_only\",\n    )\n\n    def __init__(self, *args: Any, **kwargs: Any) -> None:\n        \"\"\"Extend parent's __init__.\n\n        Adds self._user_meta dictionary to hold the user-specific\n        attributes provided by external programs.\n        \"\"\"\n        super().__init__(*args, **kwargs)\n        self._user_meta: dict[str, dict[str, Any]] = {}\n\n    async def async_login_flow(\n        self, context: AuthFlowContext | None\n    ) -> CommandLineLoginFlow:\n        \"\"\"Return a flow to login.\"\"\"\n        return CommandLineLoginFlow(self)\n\n    async def async_validate_login(self, username: str, password: str) -> None:\n        \"\"\"Validate a username and password.\"\"\"\n        env = {\"username\": username, \"password\": password}\n        try:\n            process = await asyncio.create_subprocess_exec(\n                self.config[CONF_COMMAND],\n                *self.config[CONF_ARGS],\n                env=env,\n                stdout=asyncio.subprocess.PIPE if self.config[CONF_META] else None,\n                close_fds=False,  # required for posix_spawn\n            )\n            stdout, _ = await process.communicate()\n        except OSError as err:\n            # happens when command doesn't exist or permission is denied\n            _LOGGER.error(\"Error while authenticating %r: %s\", username, err)\n            raise InvalidAuthError from err\n\n        if process.returncode != 0:\n            _LOGGER.error(\n                \"User %r failed to authenticate, command exited with code %d\",\n                username,\n                process.returncode,\n            )\n            raise InvalidAuthError\n\n        if self.config[CONF_META]:\n            meta: dict[str, str] = {}\n            for _line in stdout.splitlines():\n                try:\n                    line = _line.decode().lstrip()\n                except ValueError:\n                    # malformed line\n                    continue\n                if line.startswith(\"#\") or \"=\" not in line:\n                    continue\n                key, _, value = line.partition(\"=\")\n                key = key.strip()\n                value = value.strip()\n                if key in self.ALLOWED_META_KEYS:\n                    meta[key] = value\n            self._user_meta[username] = meta\n\n    async def async_get_or_create_credentials(\n        self, flow_result: Mapping[str, str]\n    ) -> Credentials:\n        \"\"\"Get credentials based on the flow result.\"\"\"\n        username = flow_result[\"username\"]\n        for credential in await self.async_credentials():\n            if credential.data[\"username\"] == username:\n                return credential\n\n        # Create new credentials.\n        return self.async_create_credentials({\"username\": username})\n\n    async def async_user_meta_for_credentials(\n        self, credentials: Credentials\n    ) -> UserMeta:\n        \"\"\"Return extra user metadata for credentials.\n\n        Currently, supports name, group and local_only.\n        \"\"\"\n        meta = self._user_meta.get(credentials.data[\"username\"], {})\n        return UserMeta(\n            name=meta.get(\"name\"),\n            is_active=True,\n            group=meta.get(\"group\"),\n            local_only=meta.get(\"local_only\") == \"true\",\n        )\n\n\nclass CommandLineLoginFlow(LoginFlow[CommandLineAuthProvider]):\n    \"\"\"Handler for the login flow.\"\"\"\n\n    async def async_step_init(\n        self, user_input: dict[str, str] | None = None\n    ) -> AuthFlowResult:\n        \"\"\"Handle the step of the form.\"\"\"\n        errors = {}\n\n        if user_input is not None:\n            user_input[\"username\"] = user_input[\"username\"].strip()\n            try:\n                await self._auth_provider.async_validate_login(\n                    user_input[\"username\"], user_input[\"password\"]\n                )\n            except InvalidAuthError:\n                errors[\"base\"] = \"invalid_auth\"\n\n            if not errors:\n                user_input.pop(\"password\")\n                return await self.async_finish(user_input)\n\n        return self.async_show_form(\n            step_id=\"init\",\n            data_schema=vol.Schema(\n                {\n                    vol.Required(\"username\"): str,\n                    vol.Required(\"password\"): str,\n                }\n            ),\n            errors=errors,\n        )\n", "metadata": {"source": "github.com/home-assistant/core", "file_path": "homeassistant/auth/providers/command_line.py", "language": "python", "complexity": "complex", "star_count": 80370, "quality_score": 1.0, "features": {"has_classes": true, "has_functions": true, "has_async": true, "has_decorators": true, "has_imports": true, "has_error_handling": true, "has_documentation": true, "has_type_hints": true, "has_testing": false, "has_logging": true, "function_count": 6, "class_count": 3, "line_count": 170}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "async_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create asynchronous code with proper concurrency handling with asyncio coroutines\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"Home Assistant auth provider.\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\nimport base64\nfrom collections.abc import Mapping\nimport logging\nfrom typing import Any, cast\n\nimport bcrypt\nimport voluptuous as vol\n\nfrom homeassistant.const import CONF_ID\nfrom homeassistant.core import HomeAssistant, callback\nfrom homeassistant.exceptions import HomeAssistantError\nfrom homeassistant.helpers import issue_registry as ir\nfrom homeassistant.helpers.storage import Store\n\nfrom ..models import AuthFlowContext, AuthFlowResult, Credentials, UserMeta\nfrom . import AUTH_PROVIDER_SCHEMA, AUTH_PROVIDERS, AuthProvider, LoginFlow\n\nSTORAGE_VERSION = 1\nSTORAGE_KEY = \"auth_provider.homeassistant\"\n\n\ndef _disallow_id(conf: dict[str, Any]) -> dict[str, Any]:\n    \"\"\"Disallow ID in config.\"\"\"\n    if CONF_ID in conf:\n        raise vol.Invalid(\"ID is not allowed for the homeassistant auth provider.\")\n\n    return conf\n\n\nCONFIG_SCHEMA = vol.All(AUTH_PROVIDER_SCHEMA, _disallow_id)\n\n\n@callback\ndef async_get_provider(hass: HomeAssistant) -> HassAuthProvider:\n    \"\"\"Get the provider.\"\"\"\n    for prv in hass.auth.auth_providers:\n        if prv.type == \"homeassistant\":\n            return cast(HassAuthProvider, prv)\n\n    raise RuntimeError(\"Provider not found\")\n\n\nclass InvalidAuth(HomeAssistantError):\n    \"\"\"Raised when we encounter invalid authentication.\"\"\"\n\n\nclass InvalidUser(HomeAssistantError):\n    \"\"\"Raised when invalid user is specified.\n\n    Will not be raised when validating authentication.\n    \"\"\"\n\n    def __init__(\n        self,\n        *args: object,\n        translation_key: str | None = None,\n        translation_placeholders: dict[str, str] | None = None,\n    ) -> None:\n        \"\"\"Initialize exception.\"\"\"\n        super().__init__(\n            *args,\n            translation_domain=\"auth\",\n            translation_key=translation_key,\n            translation_placeholders=translation_placeholders,\n        )\n\n\nclass InvalidUsername(InvalidUser):\n    \"\"\"Raised when invalid username is specified.\n\n    Will not be raised when validating authentication.\n    \"\"\"\n\n\nclass Data:\n    \"\"\"Hold the user data.\"\"\"\n\n    def __init__(self, hass: HomeAssistant) -> None:\n        \"\"\"Initialize the user data store.\"\"\"\n        self.hass = hass\n        self._store = Store[dict[str, list[dict[str, str]]]](\n            hass, STORAGE_VERSION, STORAGE_KEY, private=True, atomic_writes=True\n        )\n        self._data: dict[str, list[dict[str, str]]] | None = None\n        # Legacy mode will allow usernames to start/end with whitespace\n        # and will compare usernames case-insensitive.\n        # Deprecated in June 2019 and will be removed in 2026.7\n        self.is_legacy = False\n\n    @callback\n    def normalize_username(\n        self, username: str, *, force_normalize: bool = False\n    ) -> str:\n        \"\"\"Normalize a username based on the mode.\"\"\"\n        if self.is_legacy and not force_normalize:\n            return username\n\n        return username.strip().casefold()\n\n    async def async_load(self) -> None:\n        \"\"\"Load stored data.\"\"\"\n        if (data := await self._store.async_load()) is None:\n            data = cast(dict[str, list[dict[str, str]]], {\"users\": []})\n\n        self._async_check_for_not_normalized_usernames(data)\n        self._data = data\n\n    @callback\n    def _async_check_for_not_normalized_usernames(\n        self, data: dict[str, list[dict[str, str]]]\n    ) -> None:\n        not_normalized_usernames: set[str] = set()\n\n        for user in data[\"users\"]:\n            username = user[\"username\"]\n\n            if self.normalize_username(username, force_normalize=True) != username:\n                logging.getLogger(__name__).warning(\n                    (\n                        \"Home Assistant auth provider is running in legacy mode \"\n                        \"because we detected usernames that are normalized (lowercase and without spaces).\"\n                        \" Please change the username: '%s'.\"\n                    ),\n                    username,\n                )\n                not_normalized_usernames.add(username)\n\n        if not_normalized_usernames:\n            self.is_legacy = True\n            ir.async_create_issue(\n                self.hass,\n                \"auth\",\n                \"homeassistant_provider_not_normalized_usernames\",\n                breaks_in_ha_version=\"2026.7.0\",\n                is_fixable=False,\n                severity=ir.IssueSeverity.WARNING,\n                translation_key=\"homeassistant_provider_not_normalized_usernames\",\n                translation_placeholders={\n                    \"usernames\": f'- \"{'\"\\n- \"'.join(sorted(not_normalized_usernames))}\"'\n                },\n                learn_more_url=\"homeassistant://config/users\",\n            )\n        else:\n            self.is_legacy = False\n            ir.async_delete_issue(\n                self.hass, \"auth\", \"homeassistant_provider_not_normalized_usernames\"\n            )\n\n    @property\n    def users(self) -> list[dict[str, str]]:\n        \"\"\"Return users.\"\"\"\n        assert self._data is not None\n        return self._data[\"users\"]\n\n    def validate_login(self, username: str, password: str) -> None:\n        \"\"\"Validate a username and password.\n\n        Raises InvalidAuth if auth invalid.\n        \"\"\"\n        username = self.normalize_username(username)\n        dummy = b\"$2b$12$CiuFGszHx9eNHxPuQcwBWez4CwDTOcLTX5CbOpV6gef2nYuXkY7BO\"\n        found = None\n\n        # Compare all users to avoid timing attacks.\n        for user in self.users:\n            if self.normalize_username(user[\"username\"]) == username:\n                found = user\n\n        if found is None:\n            # check a hash to make timing the same as if user was found\n            bcrypt.checkpw(b\"foo\", dummy)\n            raise InvalidAuth\n\n        user_hash = base64.b64decode(found[\"password\"])\n\n        # bcrypt.checkpw is timing-safe\n        if not bcrypt.checkpw(password.encode(), user_hash):\n            raise InvalidAuth\n\n    def hash_password(self, password: str, for_storage: bool = False) -> bytes:\n        \"\"\"Encode a password.\"\"\"\n        hashed: bytes = bcrypt.hashpw(password.encode(), bcrypt.gensalt(rounds=12))\n\n        if for_storage:\n            hashed = base64.b64encode(hashed)\n        return hashed\n\n    def add_auth(self, username: str, password: str) -> None:\n        \"\"\"Add a new authenticated user/pass.\n\n        Raises InvalidUsername if the new username is invalid.\n        \"\"\"\n        self._validate_new_username(username)\n\n        self.users.append(\n            {\n                \"username\": username,\n                \"password\": self.hash_password(password, True).decode(),\n            }\n        )\n\n    @callback\n    def async_remove_auth(self, username: str) -> None:\n        \"\"\"Remove authentication.\"\"\"\n        username = self.normalize_username(username)\n\n        index = None\n        for i, user in enumerate(self.users):\n            if self.normalize_username(user[\"username\"]) == username:\n                index = i\n                break\n\n        if index is None:\n            raise InvalidUser(translation_key=\"user_not_found\")\n\n        self.users.pop(index)\n\n    def change_password(self, username: str, new_password: str) -> None:\n        \"\"\"Update the password.\n\n        Raises InvalidUser if user cannot be found.\n        \"\"\"\n        username = self.normalize_username(username)\n\n        for user in self.users:\n            if self.normalize_username(user[\"username\"]) == username:\n                user[\"password\"] = self.hash_password(new_password, True).decode()\n                break\n        else:\n            raise InvalidUser(translation_key=\"user_not_found\")\n\n    @callback\n    def _validate_new_username(self, new_username: str) -> None:\n        \"\"\"Validate that username is normalized and unique.\n\n        Raises InvalidUsername if the new username is invalid.\n        \"\"\"\n        normalized_username = self.normalize_username(\n            new_username, force_normalize=True\n        )\n        if normalized_username != new_username:\n            raise InvalidUsername(\n                translation_key=\"username_not_normalized\",\n                translation_placeholders={\"new_username\": new_username},\n            )\n\n        if any(\n            self.normalize_username(user[\"username\"]) == normalized_username\n            for user in self.users\n        ):\n            raise InvalidUsername(\n                translation_key=\"username_already_exists\",\n                translation_placeholders={\"username\": new_username},\n            )\n\n    @callback\n    def change_username(self, username: str, new_username: str) -> None:\n        \"\"\"Update the username.\n\n        Raises InvalidUser if user cannot be found.\n        Raises InvalidUsername if the new username is invalid.\n        \"\"\"\n        username = self.normalize_username(username)\n        self._validate_new_username(new_username)\n\n        for user in self.users:\n            if self.normalize_username(user[\"username\"]) == username:\n                user[\"username\"] = new_username\n                assert self._data is not None\n                self._async_check_for_not_normalized_usernames(self._data)\n                break\n        else:\n            raise InvalidUser(translation_key=\"user_not_found\")\n\n    async def async_save(self) -> None:\n        \"\"\"Save data.\"\"\"\n        if self._data is not None:\n            await self._store.async_save(self._data)\n\n\n@AUTH_PROVIDERS.register(\"homeassistant\")\nclass HassAuthProvider(AuthProvider):\n    \"\"\"Auth provider based on a local storage of users in Home Assistant config dir.\"\"\"\n\n    DEFAULT_TITLE = \"Home Assistant Local\"\n\n    def __init__(self, *args: Any, **kwargs: Any) -> None:\n        \"\"\"Initialize an Home Assistant auth provider.\"\"\"\n        super().__init__(*args, **kwargs)\n        self.data: Data | None = None\n        self._init_lock = asyncio.Lock()\n\n    async def async_initialize(self) -> None:\n        \"\"\"Initialize the auth provider.\"\"\"\n        async with self._init_lock:\n            if self.data is not None:\n                return\n\n            data = Data(self.hass)\n            await data.async_load()\n            self.data = data\n\n    async def async_login_flow(self, context: AuthFlowContext | None) -> HassLoginFlow:\n        \"\"\"Return a flow to login.\"\"\"\n        return HassLoginFlow(self)\n\n    async def async_validate_login(self, username: str, password: str) -> None:\n        \"\"\"Validate a username and password.\"\"\"\n        if self.data is None:\n            await self.async_initialize()\n            assert self.data is not None\n\n        await self.hass.async_add_executor_job(\n            self.data.validate_login, username, password\n        )\n\n    async def async_add_auth(self, username: str, password: str) -> None:\n        \"\"\"Call add_auth on data.\"\"\"\n        if self.data is None:\n            await self.async_initialize()\n            assert self.data is not None\n\n        await self.hass.async_add_executor_job(self.data.add_auth, username, password)\n        await self.data.async_save()\n\n    async def async_remove_auth(self, username: str) -> None:\n        \"\"\"Call remove_auth on data.\"\"\"\n        if self.data is None:\n            await self.async_initialize()\n            assert self.data is not None\n\n        self.data.async_remove_auth(username)\n        await self.data.async_save()\n\n    async def async_change_password(self, username: str, new_password: str) -> None:\n        \"\"\"Call change_password on data.\"\"\"\n        if self.data is None:\n            await self.async_initialize()\n            assert self.data is not None\n\n        await self.hass.async_add_executor_job(\n            self.data.change_password, username, new_password\n        )\n        await self.data.async_save()\n\n    async def async_change_username(\n        self, credential: Credentials, new_username: str\n    ) -> None:\n        \"\"\"Validate new username and change it including updating credentials object.\"\"\"\n        if self.data is None:\n            await self.async_initialize()\n            assert self.data is not None\n\n        self.data.change_username(credential.data[\"username\"], new_username)\n        self.hass.auth.async_update_user_credentials_data(\n            credential, {**credential.data, \"username\": new_username}\n        )\n        await self.data.async_save()\n\n    async def async_get_or_create_credentials(\n        self, flow_result: Mapping[str, str]\n    ) -> Credentials:\n        \"\"\"Get credentials based on the flow result.\"\"\"\n        if self.data is None:\n            await self.async_initialize()\n            assert self.data is not None\n\n        norm_username = self.data.normalize_username\n        username = norm_username(flow_result[\"username\"])\n\n        for credential in await self.async_credentials():\n            if norm_username(credential.data[\"username\"]) == username:\n                return credential\n\n        # Create new credentials.\n        return self.async_create_credentials({\"username\": username})\n\n    async def async_user_meta_for_credentials(\n        self, credentials: Credentials\n    ) -> UserMeta:\n        \"\"\"Get extra info for this credential.\"\"\"\n        return UserMeta(name=credentials.data[\"username\"], is_active=True)\n\n    async def async_will_remove_credentials(self, credentials: Credentials) -> None:\n        \"\"\"When credentials get removed, also remove the auth.\"\"\"\n        if self.data is None:\n            await self.async_initialize()\n            assert self.data is not None\n\n        try:\n            self.data.async_remove_auth(credentials.data[\"username\"])\n            await self.data.async_save()\n        except InvalidUser:\n            # Can happen if somehow we didn't clean up a credential\n            pass\n\n\nclass HassLoginFlow(LoginFlow[HassAuthProvider]):\n    \"\"\"Handler for the login flow.\"\"\"\n\n    async def async_step_init(\n        self, user_input: dict[str, str] | None = None\n    ) -> AuthFlowResult:\n        \"\"\"Handle the step of the form.\"\"\"\n        errors = {}\n\n        if user_input is not None:\n            try:\n                await self._auth_provider.async_validate_login(\n                    user_input[\"username\"], user_input[\"password\"]\n                )\n            except InvalidAuth:\n                errors[\"base\"] = \"invalid_auth\"\n\n            if not errors:\n                user_input.pop(\"password\")\n                return await self.async_finish(user_input)\n\n        return self.async_show_form(\n            step_id=\"init\",\n            data_schema=vol.Schema(\n                {\n                    vol.Required(\"username\"): str,\n                    vol.Required(\"password\"): str,\n                }\n            ),\n            errors=errors,\n        )\n", "metadata": {"source": "github.com/home-assistant/core", "file_path": "homeassistant/auth/providers/homeassistant.py", "language": "python", "complexity": "complex", "star_count": 80370, "quality_score": 1.0, "features": {"has_classes": true, "has_functions": true, "has_async": true, "has_decorators": true, "has_imports": true, "has_error_handling": true, "has_documentation": true, "has_type_hints": true, "has_testing": true, "has_logging": true, "function_count": 28, "class_count": 6, "line_count": 434}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "async_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create asynchronous code with proper concurrency handling with asyncio coroutines\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"Example auth provider.\"\"\"\n\nfrom __future__ import annotations\n\nfrom collections.abc import Mapping\nimport hmac\n\nimport voluptuous as vol\n\nfrom homeassistant.core import callback\nfrom homeassistant.exceptions import HomeAssistantError\n\nfrom ..models import AuthFlowContext, AuthFlowResult, Credentials, UserMeta\nfrom . import AUTH_PROVIDER_SCHEMA, AUTH_PROVIDERS, AuthProvider, LoginFlow\n\nUSER_SCHEMA = vol.Schema(\n    {\n        vol.Required(\"username\"): str,\n        vol.Required(\"password\"): str,\n        vol.Optional(\"name\"): str,\n    }\n)\n\n\nCONFIG_SCHEMA = AUTH_PROVIDER_SCHEMA.extend(\n    {vol.Required(\"users\"): [USER_SCHEMA]}, extra=vol.PREVENT_EXTRA\n)\n\n\nclass InvalidAuthError(HomeAssistantError):\n    \"\"\"Raised when submitting invalid authentication.\"\"\"\n\n\n@AUTH_PROVIDERS.register(\"insecure_example\")\nclass ExampleAuthProvider(AuthProvider):\n    \"\"\"Example auth provider based on hardcoded usernames and passwords.\"\"\"\n\n    async def async_login_flow(\n        self, context: AuthFlowContext | None\n    ) -> ExampleLoginFlow:\n        \"\"\"Return a flow to login.\"\"\"\n        return ExampleLoginFlow(self)\n\n    @callback\n    def async_validate_login(self, username: str, password: str) -> None:\n        \"\"\"Validate a username and password.\"\"\"\n        user = None\n\n        # Compare all users to avoid timing attacks.\n        for usr in self.config[\"users\"]:\n            if hmac.compare_digest(\n                username.encode(\"utf-8\"), usr[\"username\"].encode(\"utf-8\")\n            ):\n                user = usr\n\n        if user is None:\n            # Do one more compare to make timing the same as if user was found.\n            hmac.compare_digest(password.encode(\"utf-8\"), password.encode(\"utf-8\"))\n            raise InvalidAuthError\n\n        if not hmac.compare_digest(\n            user[\"password\"].encode(\"utf-8\"), password.encode(\"utf-8\")\n        ):\n            raise InvalidAuthError\n\n    async def async_get_or_create_credentials(\n        self, flow_result: Mapping[str, str]\n    ) -> Credentials:\n        \"\"\"Get credentials based on the flow result.\"\"\"\n        username = flow_result[\"username\"]\n\n        for credential in await self.async_credentials():\n            if credential.data[\"username\"] == username:\n                return credential\n\n        # Create new credentials.\n        return self.async_create_credentials({\"username\": username})\n\n    async def async_user_meta_for_credentials(\n        self, credentials: Credentials\n    ) -> UserMeta:\n        \"\"\"Return extra user metadata for credentials.\n\n        Will be used to populate info when creating a new user.\n        \"\"\"\n        username = credentials.data[\"username\"]\n        name = None\n\n        for user in self.config[\"users\"]:\n            if user[\"username\"] == username:\n                name = user.get(\"name\")\n                break\n\n        return UserMeta(name=name, is_active=True)\n\n\nclass ExampleLoginFlow(LoginFlow[ExampleAuthProvider]):\n    \"\"\"Handler for the login flow.\"\"\"\n\n    async def async_step_init(\n        self, user_input: dict[str, str] | None = None\n    ) -> AuthFlowResult:\n        \"\"\"Handle the step of the form.\"\"\"\n        errors = None\n\n        if user_input is not None:\n            try:\n                self._auth_provider.async_validate_login(\n                    user_input[\"username\"], user_input[\"password\"]\n                )\n            except InvalidAuthError:\n                errors = {\"base\": \"invalid_auth\"}\n\n            if not errors:\n                user_input.pop(\"password\")\n                return await self.async_finish(user_input)\n\n        return self.async_show_form(\n            step_id=\"init\",\n            data_schema=vol.Schema(\n                {\n                    vol.Required(\"username\"): str,\n                    vol.Required(\"password\"): str,\n                }\n            ),\n            errors=errors,\n        )\n", "metadata": {"source": "github.com/home-assistant/core", "file_path": "homeassistant/auth/providers/insecure_example.py", "language": "python", "complexity": "medium", "star_count": 80370, "quality_score": 1.0, "features": {"has_classes": true, "has_functions": true, "has_async": true, "has_decorators": true, "has_imports": true, "has_error_handling": true, "has_documentation": true, "has_type_hints": true, "has_testing": false, "has_logging": true, "function_count": 5, "class_count": 3, "line_count": 128}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "async_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create asynchronous code with proper concurrency handling with asyncio coroutines\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"Trusted Networks auth provider.\n\nIt shows list of users if access from trusted network.\nAbort login flow if not access from trusted network.\n\"\"\"\n\nfrom __future__ import annotations\n\nfrom collections.abc import Mapping\nfrom ipaddress import (\n    IPv4Address,\n    IPv4Network,\n    IPv6Address,\n    IPv6Network,\n    ip_address,\n    ip_network,\n)\nfrom typing import Any, cast\n\nimport voluptuous as vol\n\nfrom homeassistant.core import callback\nfrom homeassistant.exceptions import HomeAssistantError\nfrom homeassistant.helpers import config_validation as cv\nfrom homeassistant.helpers.network import is_cloud_connection\n\nfrom .. import InvalidAuthError\nfrom ..models import (\n    AuthFlowContext,\n    AuthFlowResult,\n    Credentials,\n    RefreshToken,\n    UserMeta,\n)\nfrom . import AUTH_PROVIDER_SCHEMA, AUTH_PROVIDERS, AuthProvider, LoginFlow\n\ntype IPAddress = IPv4Address | IPv6Address\ntype IPNetwork = IPv4Network | IPv6Network\n\nCONF_TRUSTED_NETWORKS = \"trusted_networks\"\nCONF_TRUSTED_USERS = \"trusted_users\"\nCONF_GROUP = \"group\"\nCONF_ALLOW_BYPASS_LOGIN = \"allow_bypass_login\"\n\nCONFIG_SCHEMA = AUTH_PROVIDER_SCHEMA.extend(\n    {\n        vol.Required(CONF_TRUSTED_NETWORKS): vol.All(cv.ensure_list, [ip_network]),\n        vol.Optional(CONF_TRUSTED_USERS, default={}): vol.Schema(\n            # we only validate the format of user_id or group_id\n            {\n                ip_network: vol.All(\n                    cv.ensure_list,\n                    [\n                        vol.Or(\n                            cv.uuid4_hex,\n                            vol.Schema({vol.Required(CONF_GROUP): str}),\n                        )\n                    ],\n                )\n            }\n        ),\n        vol.Optional(CONF_ALLOW_BYPASS_LOGIN, default=False): cv.boolean,\n    },\n    extra=vol.PREVENT_EXTRA,\n)\n\n\nclass InvalidUserError(HomeAssistantError):\n    \"\"\"Raised when try to login as invalid user.\"\"\"\n\n\n@AUTH_PROVIDERS.register(\"trusted_networks\")\nclass TrustedNetworksAuthProvider(AuthProvider):\n    \"\"\"Trusted Networks auth provider.\n\n    Allow passwordless access from trusted network.\n    \"\"\"\n\n    DEFAULT_TITLE = \"Trusted Networks\"\n\n    @property\n    def trusted_networks(self) -> list[IPNetwork]:\n        \"\"\"Return trusted networks.\"\"\"\n        return cast(list[IPNetwork], self.config[CONF_TRUSTED_NETWORKS])\n\n    @property\n    def trusted_users(self) -> dict[IPNetwork, Any]:\n        \"\"\"Return trusted users per network.\"\"\"\n        return cast(dict[IPNetwork, Any], self.config[CONF_TRUSTED_USERS])\n\n    @property\n    def trusted_proxies(self) -> list[IPNetwork]:\n        \"\"\"Return trusted proxies in the system.\"\"\"\n        if not self.hass.http:\n            return []\n\n        return [\n            ip_network(trusted_proxy)\n            for trusted_proxy in self.hass.http.trusted_proxies\n        ]\n\n    @property\n    def support_mfa(self) -> bool:\n        \"\"\"Trusted Networks auth provider does not support MFA.\"\"\"\n        return False\n\n    async def async_login_flow(\n        self, context: AuthFlowContext | None\n    ) -> TrustedNetworksLoginFlow:\n        \"\"\"Return a flow to login.\"\"\"\n        assert context is not None\n        ip_addr = cast(IPAddress, context.get(\"ip_address\"))\n        users = await self.store.async_get_users()\n        available_users = [\n            user for user in users if not user.system_generated and user.is_active\n        ]\n        for ip_net, user_or_group_list in self.trusted_users.items():\n            if ip_addr not in ip_net:\n                continue\n\n            user_list = [\n                user_id for user_id in user_or_group_list if isinstance(user_id, str)\n            ]\n            group_list = [\n                group[CONF_GROUP]\n                for group in user_or_group_list\n                if isinstance(group, dict)\n            ]\n            flattened_group_list = [\n                group for sublist in group_list for group in sublist\n            ]\n            available_users = [\n                user\n                for user in available_users\n                if (\n                    user.id in user_list\n                    or any(group.id in flattened_group_list for group in user.groups)\n                )\n            ]\n            break\n\n        return TrustedNetworksLoginFlow(\n            self,\n            ip_addr,\n            {user.id: user.name for user in available_users},\n            self.config[CONF_ALLOW_BYPASS_LOGIN],\n        )\n\n    async def async_get_or_create_credentials(\n        self, flow_result: Mapping[str, str]\n    ) -> Credentials:\n        \"\"\"Get credentials based on the flow result.\"\"\"\n        user_id = flow_result[\"user\"]\n\n        users = await self.store.async_get_users()\n        for user in users:\n            if user.id != user_id:\n                continue\n\n            if user.system_generated:\n                continue\n\n            if not user.is_active:\n                continue\n\n            for credential in await self.async_credentials():\n                if credential.data[\"user_id\"] == user_id:\n                    return credential\n\n            cred = self.async_create_credentials({\"user_id\": user_id})\n            await self.store.async_link_user(user, cred)\n            return cred\n\n        # We only allow login as exist user\n        raise InvalidUserError\n\n    async def async_user_meta_for_credentials(\n        self, credentials: Credentials\n    ) -> UserMeta:\n        \"\"\"Return extra user metadata for credentials.\n\n        Trusted network auth provider should never create new user.\n        \"\"\"\n        raise NotImplementedError\n\n    @callback\n    def async_validate_access(self, ip_addr: IPAddress) -> None:\n        \"\"\"Make sure the access from trusted networks.\n\n        Raise InvalidAuthError if not.\n        Raise InvalidAuthError if trusted_networks is not configured.\n        \"\"\"\n        if not self.trusted_networks:\n            raise InvalidAuthError(\"trusted_networks is not configured\")\n\n        if not any(\n            ip_addr in trusted_network for trusted_network in self.trusted_networks\n        ):\n            raise InvalidAuthError(\"Not in trusted_networks\")\n\n        if any(ip_addr in trusted_proxy for trusted_proxy in self.trusted_proxies):\n            raise InvalidAuthError(\"Can't allow access from a proxy server\")\n\n        if is_cloud_connection(self.hass):\n            raise InvalidAuthError(\"Can't allow access from Home Assistant Cloud\")\n\n    @callback\n    def async_validate_refresh_token(\n        self, refresh_token: RefreshToken, remote_ip: str | None = None\n    ) -> None:\n        \"\"\"Verify a refresh token is still valid.\"\"\"\n        if remote_ip is None:\n            raise InvalidAuthError(\n                \"Unknown remote ip can't be used for trusted network provider.\"\n            )\n        self.async_validate_access(ip_address(remote_ip))\n\n\nclass TrustedNetworksLoginFlow(LoginFlow[TrustedNetworksAuthProvider]):\n    \"\"\"Handler for the login flow.\"\"\"\n\n    def __init__(\n        self,\n        auth_provider: TrustedNetworksAuthProvider,\n        ip_addr: IPAddress,\n        available_users: dict[str, str | None],\n        allow_bypass_login: bool,\n    ) -> None:\n        \"\"\"Initialize the login flow.\"\"\"\n        super().__init__(auth_provider)\n        self._available_users = available_users\n        self._ip_address = ip_addr\n        self._allow_bypass_login = allow_bypass_login\n\n    async def async_step_init(\n        self, user_input: dict[str, str] | None = None\n    ) -> AuthFlowResult:\n        \"\"\"Handle the step of the form.\"\"\"\n        try:\n            self._auth_provider.async_validate_access(self._ip_address)\n\n        except InvalidAuthError:\n            return self.async_abort(reason=\"not_allowed\")\n\n        if user_input is not None:\n            return await self.async_finish(user_input)\n\n        if self._allow_bypass_login and len(self._available_users) == 1:\n            return await self.async_finish(\n                {\"user\": next(iter(self._available_users.keys()))}\n            )\n\n        return self.async_show_form(\n            step_id=\"init\",\n            data_schema=vol.Schema(\n                {vol.Required(\"user\"): vol.In(self._available_users)}\n            ),\n        )\n", "metadata": {"source": "github.com/home-assistant/core", "file_path": "homeassistant/auth/providers/trusted_networks.py", "language": "python", "complexity": "complex", "star_count": 80370, "quality_score": 1.0, "features": {"has_classes": true, "has_functions": true, "has_async": true, "has_decorators": true, "has_imports": true, "has_error_handling": true, "has_documentation": true, "has_type_hints": true, "has_testing": true, "has_logging": true, "function_count": 11, "class_count": 3, "line_count": 259}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "async_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create asynchronous code with proper concurrency handling\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"Home Assistant module to handle restoring backups.\"\"\"\n\nfrom __future__ import annotations\n\nfrom collections.abc import Iterable\nfrom dataclasses import dataclass\nimport hashlib\nimport json\nimport logging\nfrom pathlib import Path\nimport shutil\nimport sys\nfrom tempfile import TemporaryDirectory\n\nfrom awesomeversion import AwesomeVersion\nimport securetar\n\nfrom .const import __version__ as HA_VERSION\n\nRESTORE_BACKUP_FILE = \".HA_RESTORE\"\nRESTORE_BACKUP_RESULT_FILE = \".HA_RESTORE_RESULT\"\nKEEP_BACKUPS = (\"backups\",)\nKEEP_DATABASE = (\n    \"home-assistant_v2.db\",\n    \"home-assistant_v2.db-wal\",\n)\n\n\n_LOGGER = logging.getLogger(__name__)\n\n\n@dataclass\nclass RestoreBackupFileContent:\n    \"\"\"Definition for restore backup file content.\"\"\"\n\n    backup_file_path: Path\n    password: str | None\n    remove_after_restore: bool\n    restore_database: bool\n    restore_homeassistant: bool\n\n\ndef password_to_key(password: str) -> bytes:\n    \"\"\"Generate a AES Key from password.\n\n    Matches the implementation in supervisor.backups.utils.password_to_key.\n    \"\"\"\n    key: bytes = password.encode()\n    for _ in range(100):\n        key = hashlib.sha256(key).digest()\n    return key[:16]\n\n\ndef restore_backup_file_content(config_dir: Path) -> RestoreBackupFileContent | None:\n    \"\"\"Return the contents of the restore backup file.\"\"\"\n    instruction_path = config_dir.joinpath(RESTORE_BACKUP_FILE)\n    try:\n        instruction_content = json.loads(instruction_path.read_text(encoding=\"utf-8\"))\n        return RestoreBackupFileContent(\n            backup_file_path=Path(instruction_content[\"path\"]),\n            password=instruction_content[\"password\"],\n            remove_after_restore=instruction_content[\"remove_after_restore\"],\n            restore_database=instruction_content[\"restore_database\"],\n            restore_homeassistant=instruction_content[\"restore_homeassistant\"],\n        )\n    except FileNotFoundError:\n        return None\n    except (KeyError, json.JSONDecodeError) as err:\n        _write_restore_result_file(config_dir, False, err)\n        return None\n    finally:\n        # Always remove the backup instruction file to prevent a boot loop\n        instruction_path.unlink(missing_ok=True)\n\n\ndef _clear_configuration_directory(config_dir: Path, keep: Iterable[str]) -> None:\n    \"\"\"Delete all files and directories in the config directory except entries in the keep list.\"\"\"\n    keep_paths = [config_dir.joinpath(path) for path in keep]\n    entries_to_remove = sorted(\n        entry for entry in config_dir.iterdir() if entry not in keep_paths\n    )\n\n    for entry in entries_to_remove:\n        entrypath = config_dir.joinpath(entry)\n\n        if entrypath.is_file():\n            entrypath.unlink()\n        elif entrypath.is_dir():\n            shutil.rmtree(entrypath)\n\n\ndef _extract_backup(\n    config_dir: Path,\n    restore_content: RestoreBackupFileContent,\n) -> None:\n    \"\"\"Extract the backup file to the config directory.\"\"\"\n    with (\n        TemporaryDirectory() as tempdir,\n        securetar.SecureTarFile(\n            restore_content.backup_file_path,\n            gzip=False,\n            mode=\"r\",\n        ) as ostf,\n    ):\n        ostf.extractall(\n            path=Path(tempdir, \"extracted\"),\n            members=securetar.secure_path(ostf),\n            filter=\"fully_trusted\",\n        )\n        backup_meta_file = Path(tempdir, \"extracted\", \"backup.json\")\n        backup_meta = json.loads(backup_meta_file.read_text(encoding=\"utf8\"))\n\n        if (\n            backup_meta_version := AwesomeVersion(\n                backup_meta[\"homeassistant\"][\"version\"]\n            )\n        ) > HA_VERSION:\n            raise ValueError(\n                f\"You need at least Home Assistant version {backup_meta_version} to restore this backup\"\n            )\n\n        with securetar.SecureTarFile(\n            Path(\n                tempdir,\n                \"extracted\",\n                f\"homeassistant.tar{'.gz' if backup_meta['compressed'] else ''}\",\n            ),\n            gzip=backup_meta[\"compressed\"],\n            key=password_to_key(restore_content.password)\n            if restore_content.password is not None\n            else None,\n            mode=\"r\",\n        ) as istf:\n            istf.extractall(\n                path=Path(tempdir, \"homeassistant\"),\n                members=securetar.secure_path(istf),\n                filter=\"fully_trusted\",\n            )\n            if restore_content.restore_homeassistant:\n                keep = list(KEEP_BACKUPS)\n                if not restore_content.restore_database:\n                    keep.extend(KEEP_DATABASE)\n                _clear_configuration_directory(config_dir, keep)\n                shutil.copytree(\n                    Path(tempdir, \"homeassistant\", \"data\"),\n                    config_dir,\n                    dirs_exist_ok=True,\n                    ignore=shutil.ignore_patterns(*(keep)),\n                    ignore_dangling_symlinks=True,\n                )\n            elif restore_content.restore_database:\n                for entry in KEEP_DATABASE:\n                    entrypath = config_dir / entry\n\n                    if entrypath.is_file():\n                        entrypath.unlink()\n                    elif entrypath.is_dir():\n                        shutil.rmtree(entrypath)\n\n                for entry in KEEP_DATABASE:\n                    shutil.copy(\n                        Path(tempdir, \"homeassistant\", \"data\", entry),\n                        config_dir,\n                    )\n\n\ndef _write_restore_result_file(\n    config_dir: Path, success: bool, error: Exception | None\n) -> None:\n    \"\"\"Write the restore result file.\"\"\"\n    result_path = config_dir.joinpath(RESTORE_BACKUP_RESULT_FILE)\n    result_path.write_text(\n        json.dumps(\n            {\n                \"success\": success,\n                \"error\": str(error) if error else None,\n                \"error_type\": str(type(error).__name__) if error else None,\n            }\n        ),\n        encoding=\"utf-8\",\n    )\n\n\ndef restore_backup(config_dir_path: str) -> bool:\n    \"\"\"Restore the backup file if any.\n\n    Returns True if a restore backup file was found and restored, False otherwise.\n    \"\"\"\n    config_dir = Path(config_dir_path)\n    if not (restore_content := restore_backup_file_content(config_dir)):\n        return False\n\n    logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n    backup_file_path = restore_content.backup_file_path\n    _LOGGER.info(\"Restoring %s\", backup_file_path)\n    try:\n        _extract_backup(\n            config_dir=config_dir,\n            restore_content=restore_content,\n        )\n    except FileNotFoundError as err:\n        file_not_found = ValueError(f\"Backup file {backup_file_path} does not exist\")\n        _write_restore_result_file(config_dir, False, file_not_found)\n        raise file_not_found from err\n    except Exception as err:\n        _write_restore_result_file(config_dir, False, err)\n        raise\n    else:\n        _write_restore_result_file(config_dir, True, None)\n    if restore_content.remove_after_restore:\n        backup_file_path.unlink(missing_ok=True)\n    _LOGGER.info(\"Restore complete, restarting\")\n    return True\n", "metadata": {"source": "github.com/home-assistant/core", "file_path": "homeassistant/backup_restore.py", "language": "python", "complexity": "complex", "star_count": 80370, "quality_score": 1.0, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": true, "has_documentation": true, "has_type_hints": true, "has_testing": false, "has_logging": true, "function_count": 6, "class_count": 1, "line_count": 214}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "async_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create asynchronous code with proper concurrency handling\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"Block blocking calls being done in asyncio.\"\"\"\n\nimport builtins\nfrom collections.abc import Callable\nfrom contextlib import suppress\nfrom dataclasses import dataclass\nimport glob\nfrom http.client import HTTPConnection\nimport importlib\nimport os\nfrom pathlib import Path\nfrom ssl import SSLContext\nimport sys\nimport threading\nimport time\nfrom typing import Any\n\nfrom .helpers.frame import get_current_frame\nfrom .util.loop import protect_loop\n\n_IN_TESTS = \"unittest\" in sys.modules\n\nALLOWED_FILE_PREFIXES = (\"/proc\",)\n\n\ndef _check_import_call_allowed(mapped_args: dict[str, Any]) -> bool:\n    # If the module is already imported, we can ignore it.\n    return bool((args := mapped_args.get(\"args\")) and args[0] in sys.modules)\n\n\ndef _check_file_allowed(mapped_args: dict[str, Any]) -> bool:\n    # If the file is in /proc we can ignore it.\n    args = mapped_args[\"args\"]\n    path = args[0] if type(args[0]) is str else str(args[0])\n    return path.startswith(ALLOWED_FILE_PREFIXES)\n\n\ndef _check_sleep_call_allowed(mapped_args: dict[str, Any]) -> bool:\n    #\n    # Avoid extracting the stack unless we need to since it\n    # will have to access the linecache which can do blocking\n    # I/O and we are trying to avoid blocking calls.\n    #\n    # frame[0] is us\n    # frame[1] is raise_for_blocking_call\n    # frame[2] is protected_loop_func\n    # frame[3] is the offender\n    with suppress(ValueError):\n        return get_current_frame(4).f_code.co_filename.endswith(\"pydevd.py\")\n    return False\n\n\ndef _check_load_verify_locations_call_allowed(mapped_args: dict[str, Any]) -> bool:\n    # If only cadata is passed, we can ignore it\n    kwargs = mapped_args.get(\"kwargs\")\n    return bool(kwargs and len(kwargs) == 1 and \"cadata\" in kwargs)\n\n\n@dataclass(slots=True, frozen=True)\nclass BlockingCall:\n    \"\"\"Class to hold information about a blocking call.\"\"\"\n\n    original_func: Callable\n    object: object\n    function: str\n    check_allowed: Callable[[dict[str, Any]], bool] | None\n    strict: bool\n    strict_core: bool\n    skip_for_tests: bool\n\n\n_BLOCKING_CALLS: tuple[BlockingCall, ...] = (\n    BlockingCall(\n        original_func=HTTPConnection.putrequest,\n        object=HTTPConnection,\n        function=\"putrequest\",\n        check_allowed=None,\n        strict=True,\n        strict_core=True,\n        skip_for_tests=False,\n    ),\n    BlockingCall(\n        original_func=time.sleep,\n        object=time,\n        function=\"sleep\",\n        check_allowed=_check_sleep_call_allowed,\n        strict=True,\n        strict_core=True,\n        skip_for_tests=False,\n    ),\n    BlockingCall(\n        original_func=glob.glob,\n        object=glob,\n        function=\"glob\",\n        check_allowed=None,\n        strict=False,\n        strict_core=False,\n        skip_for_tests=False,\n    ),\n    BlockingCall(\n        original_func=glob.iglob,\n        object=glob,\n        function=\"iglob\",\n        check_allowed=None,\n        strict=False,\n        strict_core=False,\n        skip_for_tests=False,\n    ),\n    BlockingCall(\n        original_func=os.walk,\n        object=os,\n        function=\"walk\",\n        check_allowed=None,\n        strict=False,\n        strict_core=False,\n        skip_for_tests=False,\n    ),\n    BlockingCall(\n        original_func=os.listdir,\n        object=os,\n        function=\"listdir\",\n        check_allowed=None,\n        strict=False,\n        strict_core=False,\n        skip_for_tests=True,\n    ),\n    BlockingCall(\n        original_func=os.scandir,\n        object=os,\n        function=\"scandir\",\n        check_allowed=None,\n        strict=False,\n        strict_core=False,\n        skip_for_tests=True,\n    ),\n    BlockingCall(\n        original_func=builtins.open,\n        object=builtins,\n        function=\"open\",\n        check_allowed=_check_file_allowed,\n        strict=False,\n        strict_core=False,\n        skip_for_tests=True,\n    ),\n    BlockingCall(\n        original_func=importlib.import_module,\n        object=importlib,\n        function=\"import_module\",\n        check_allowed=_check_import_call_allowed,\n        strict=False,\n        strict_core=False,\n        skip_for_tests=True,\n    ),\n    BlockingCall(\n        original_func=SSLContext.load_default_certs,\n        object=SSLContext,\n        function=\"load_default_certs\",\n        check_allowed=None,\n        strict=False,\n        strict_core=False,\n        skip_for_tests=True,\n    ),\n    BlockingCall(\n        original_func=SSLContext.load_verify_locations,\n        object=SSLContext,\n        function=\"load_verify_locations\",\n        check_allowed=_check_load_verify_locations_call_allowed,\n        strict=False,\n        strict_core=False,\n        skip_for_tests=True,\n    ),\n    BlockingCall(\n        original_func=SSLContext.load_cert_chain,\n        object=SSLContext,\n        function=\"load_cert_chain\",\n        check_allowed=None,\n        strict=False,\n        strict_core=False,\n        skip_for_tests=True,\n    ),\n    BlockingCall(\n        original_func=SSLContext.set_default_verify_paths,\n        object=SSLContext,\n        function=\"set_default_verify_paths\",\n        check_allowed=None,\n        strict=False,\n        strict_core=False,\n        skip_for_tests=True,\n    ),\n    BlockingCall(\n        original_func=Path.open,\n        object=Path,\n        function=\"open\",\n        check_allowed=_check_file_allowed,\n        strict=False,\n        strict_core=False,\n        skip_for_tests=True,\n    ),\n    BlockingCall(\n        original_func=Path.read_text,\n        object=Path,\n        function=\"read_text\",\n        check_allowed=_check_file_allowed,\n        strict=False,\n        strict_core=False,\n        skip_for_tests=True,\n    ),\n    BlockingCall(\n        original_func=Path.read_bytes,\n        object=Path,\n        function=\"read_bytes\",\n        check_allowed=_check_file_allowed,\n        strict=False,\n        strict_core=False,\n        skip_for_tests=True,\n    ),\n    BlockingCall(\n        original_func=Path.write_text,\n        object=Path,\n        function=\"write_text\",\n        check_allowed=_check_file_allowed,\n        strict=False,\n        strict_core=False,\n        skip_for_tests=True,\n    ),\n    BlockingCall(\n        original_func=Path.write_bytes,\n        object=Path,\n        function=\"write_bytes\",\n        check_allowed=_check_file_allowed,\n        strict=False,\n        strict_core=False,\n        skip_for_tests=True,\n    ),\n)\n\n\n@dataclass(slots=True)\nclass BlockedCalls:\n    \"\"\"Class to track which calls are blocked.\"\"\"\n\n    calls: set[BlockingCall]\n\n\n_BLOCKED_CALLS = BlockedCalls(set())\n\n\ndef enable() -> None:\n    \"\"\"Enable the detection of blocking calls in the event loop.\"\"\"\n    calls = _BLOCKED_CALLS.calls\n    if calls:\n        raise RuntimeError(\"Blocking call detection is already enabled\")\n\n    loop_thread_id = threading.get_ident()\n    for blocking_call in _BLOCKING_CALLS:\n        if _IN_TESTS and blocking_call.skip_for_tests:\n            continue\n\n        protected_function = protect_loop(\n            blocking_call.original_func,\n            strict=blocking_call.strict,\n            strict_core=blocking_call.strict_core,\n            check_allowed=blocking_call.check_allowed,\n            loop_thread_id=loop_thread_id,\n        )\n        setattr(blocking_call.object, blocking_call.function, protected_function)\n        calls.add(blocking_call)\n", "metadata": {"source": "github.com/home-assistant/core", "file_path": "homeassistant/block_async_io.py", "language": "python", "complexity": "complex", "star_count": 80370, "quality_score": 0.9500000000000001, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": true, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 5, "class_count": 2, "line_count": 268}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "async_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create asynchronous code with proper concurrency handling with asyncio coroutines\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"Provide methods to bootstrap a Home Assistant instance.\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\nfrom collections import defaultdict\nimport contextlib\nfrom functools import partial\nfrom itertools import chain\nimport logging\nfrom logging.handlers import RotatingFileHandler, TimedRotatingFileHandler\nimport mimetypes\nfrom operator import contains, itemgetter\nimport os\nimport platform\nimport sys\nimport threading\nfrom time import monotonic\nfrom typing import TYPE_CHECKING, Any\n\n# Import cryptography early since import openssl is not thread-safe\n# _frozen_importlib._DeadlockError: deadlock detected by _ModuleLock('cryptography.hazmat.backends.openssl.backend')\nimport cryptography.hazmat.backends.openssl.backend  # noqa: F401\nimport voluptuous as vol\nimport yarl\n\nfrom . import (\n    block_async_io,\n    config as conf_util,\n    config_entries,\n    core,\n    loader,\n    requirements,\n)\n\n# Pre-import frontend deps which have no requirements here to avoid\n# loading them at run time and blocking the event loop. We do this ahead\n# of time so that we do not have to flag frontend deps with `import_executor`\n# as it would create a thundering heard of executor jobs trying to import\n# frontend deps at the same time.\nfrom .components import (\n    api as api_pre_import,  # noqa: F401\n    auth as auth_pre_import,  # noqa: F401\n    config as config_pre_import,  # noqa: F401\n    default_config as default_config_pre_import,  # noqa: F401\n    device_automation as device_automation_pre_import,  # noqa: F401\n    diagnostics as diagnostics_pre_import,  # noqa: F401\n    file_upload as file_upload_pre_import,  # noqa: F401\n    group as group_pre_import,  # noqa: F401\n    history as history_pre_import,  # noqa: F401\n    http,  # not named pre_import since it has requirements\n    image_upload as image_upload_import,  # noqa: F401 - not named pre_import since it has requirements\n    logbook as logbook_pre_import,  # noqa: F401\n    lovelace as lovelace_pre_import,  # noqa: F401\n    onboarding as onboarding_pre_import,  # noqa: F401\n    person as person_pre_import,  # noqa: F401\n    recorder as recorder_import,  # noqa: F401 - not named pre_import since it has requirements\n    repairs as repairs_pre_import,  # noqa: F401\n    search as search_pre_import,  # noqa: F401\n    sensor as sensor_pre_import,  # noqa: F401\n    system_log as system_log_pre_import,  # noqa: F401\n    webhook as webhook_pre_import,  # noqa: F401\n    websocket_api as websocket_api_pre_import,  # noqa: F401\n)\nfrom .components.sensor import recorder as sensor_recorder  # noqa: F401\nfrom .const import (\n    BASE_PLATFORMS,\n    FORMAT_DATETIME,\n    KEY_DATA_LOGGING as DATA_LOGGING,\n    REQUIRED_NEXT_PYTHON_HA_RELEASE,\n    REQUIRED_NEXT_PYTHON_VER,\n    SIGNAL_BOOTSTRAP_INTEGRATIONS,\n)\nfrom .core_config import async_process_ha_core_config\nfrom .exceptions import HomeAssistantError\nfrom .helpers import (\n    area_registry,\n    category_registry,\n    condition,\n    config_validation as cv,\n    device_registry,\n    entity,\n    entity_registry,\n    floor_registry,\n    frame,\n    issue_registry,\n    label_registry,\n    recorder,\n    restore_state,\n    template,\n    translation,\n    trigger,\n)\nfrom .helpers.dispatcher import async_dispatcher_send_internal\nfrom .helpers.storage import get_internal_store_manager\nfrom .helpers.system_info import async_get_system_info\nfrom .helpers.typing import ConfigType\nfrom .loader import Integration\nfrom .setup import (\n    # _setup_started is marked as protected to make it clear\n    # that it is not part of the public API and should not be used\n    # by integrations. It is only used for internal tracking of\n    # which integrations are being set up.\n    _setup_started,\n    async_get_setup_timings,\n    async_notify_setup_error,\n    async_set_domains_to_be_loaded,\n    async_setup_component,\n)\nfrom .util.async_ import create_eager_task\nfrom .util.hass_dict import HassKey\nfrom .util.logging import async_activate_log_queue_handler\nfrom .util.package import async_get_user_site, is_docker_env, is_virtual_env\nfrom .util.system_info import is_official_image\n\nwith contextlib.suppress(ImportError):\n    # Ensure anyio backend is imported to avoid it being imported in the event loop\n    from anyio._backends import _asyncio  # noqa: F401\n\nwith contextlib.suppress(ImportError):\n    # httpx will import trio if it is installed which does\n    # blocking I/O in the event loop. We want to avoid that.\n    import trio  # noqa: F401\n\n\nif TYPE_CHECKING:\n    from .runner import RuntimeConfig\n\n_LOGGER = logging.getLogger(__name__)\n\nSETUP_ORDER_SORT_KEY = partial(contains, BASE_PLATFORMS)\n\n\nERROR_LOG_FILENAME = \"home-assistant.log\"\n\n# hass.data key for logging information.\nDATA_REGISTRIES_LOADED: HassKey[None] = HassKey(\"bootstrap_registries_loaded\")\n\nLOG_SLOW_STARTUP_INTERVAL = 60\nSLOW_STARTUP_CHECK_INTERVAL = 1\n\nSTAGE_0_SUBSTAGE_TIMEOUT = 60\nSTAGE_1_TIMEOUT = 120\nSTAGE_2_TIMEOUT = 300\nWRAP_UP_TIMEOUT = 300\nCOOLDOWN_TIME = 60\n\n# Core integrations are unconditionally loaded\nCORE_INTEGRATIONS = {\"homeassistant\", \"persistent_notification\"}\n\n# Integrations that are loaded right after the core is set up\nLOGGING_AND_HTTP_DEPS_INTEGRATIONS = {\n    # isal is loaded right away before `http` to ensure if its\n    # enabled, that `isal` is up to date.\n    \"isal\",\n    # Set log levels\n    \"logger\",\n    # Ensure network config is available\n    # before hassio or any other integration is\n    # loaded that might create an aiohttp client session\n    \"network\",\n    # Error logging\n    \"system_log\",\n    \"sentry\",\n}\nFRONTEND_INTEGRATIONS = {\n    # Get the frontend up and running as soon as possible so problem\n    # integrations can be removed and database migration status is\n    # visible in frontend\n    \"frontend\",\n}\n# Stage 0 is divided into substages. Each substage has a name, a set of integrations and a timeout.\n# The substage containing recorder should have no timeout, as it could cancel a database migration.\n# Recorder freezes \"recorder\" timeout during a migration, but it does not freeze other timeouts.\n# If we add timeouts to the frontend substages, we should make sure they don't apply in recovery mode.\nSTAGE_0_INTEGRATIONS = (\n    # Load logging and http deps as soon as possible\n    (\"logging, http deps\", LOGGING_AND_HTTP_DEPS_INTEGRATIONS, None),\n    # Setup frontend\n    (\"frontend\", FRONTEND_INTEGRATIONS, None),\n    # Setup recorder\n    (\"recorder\", {\"recorder\"}, None),\n    # Start up debuggers. Start these first in case they want to wait.\n    (\"debugger\", {\"debugpy\"}, STAGE_0_SUBSTAGE_TIMEOUT),\n    # Zeroconf is used for mdns resolution in aiohttp client helper.\n    (\"zeroconf\", {\"zeroconf\"}, STAGE_0_SUBSTAGE_TIMEOUT),\n)\n\nDISCOVERY_INTEGRATIONS = (\"bluetooth\", \"dhcp\", \"ssdp\", \"usb\")\n# Stage 1 integrations are not to be preimported in bootstrap.\nSTAGE_1_INTEGRATIONS = {\n    # We need to make sure discovery integrations\n    # update their deps before stage 2 integrations\n    # load them inadvertently before their deps have\n    # been updated which leads to using an old version\n    # of the dep, or worse (import errors).\n    *DISCOVERY_INTEGRATIONS,\n    # To make sure we forward data to other instances\n    \"mqtt_eventstream\",\n    # To provide account link implementations\n    \"cloud\",\n    # Ensure supervisor is available\n    \"hassio\",\n}\n\nDEFAULT_INTEGRATIONS = {\n    # These integrations are set up unless recovery mode is activated.\n    #\n    # Integrations providing core functionality:\n    \"analytics\",  # Needed for onboarding\n    \"application_credentials\",\n    \"backup\",\n    \"frontend\",\n    \"hardware\",\n    \"logger\",\n    \"network\",\n    \"system_health\",\n    #\n    # Key-feature:\n    \"automation\",\n    \"person\",\n    \"scene\",\n    \"script\",\n    \"tag\",\n    \"zone\",\n    #\n    # Built-in helpers:\n    \"counter\",\n    \"input_boolean\",\n    \"input_button\",\n    \"input_datetime\",\n    \"input_number\",\n    \"input_select\",\n    \"input_text\",\n    \"schedule\",\n    \"timer\",\n}\nDEFAULT_INTEGRATIONS_RECOVERY_MODE = {\n    # These integrations are set up if recovery mode is activated.\n    \"frontend\",\n}\nDEFAULT_INTEGRATIONS_SUPERVISOR = {\n    # These integrations are set up if using the Supervisor\n    \"hassio\",\n}\n\nCRITICAL_INTEGRATIONS = {\n    # Recovery mode is activated if these integrations fail to set up\n    \"frontend\",\n}\n\n#\n# Storage keys we are likely to load during startup\n# in order of when we expect to load them.\n#\n# If they do not exist they will not be loaded\n#\nPRELOAD_STORAGE = [\n    \"core.logger\",\n    \"core.network\",\n    \"http.auth\",\n    \"image\",\n    \"lovelace_dashboards\",\n    \"lovelace_resources\",\n    \"core.uuid\",\n    \"lovelace.map\",\n    \"bluetooth.passive_update_processor\",\n    \"bluetooth.remote_scanners\",\n    \"assist_pipeline.pipelines\",\n    \"core.analytics\",\n    \"auth_module.totp\",\n    \"backup\",\n]\n\n\nasync def async_setup_hass(\n    runtime_config: RuntimeConfig,\n) -> core.HomeAssistant | None:\n    \"\"\"Set up Home Assistant.\"\"\"\n\n    async def create_hass() -> core.HomeAssistant:\n        \"\"\"Create the hass object and do basic setup.\"\"\"\n        hass = core.HomeAssistant(runtime_config.config_dir)\n        loader.async_setup(hass)\n\n        await async_enable_logging(\n            hass,\n            runtime_config.verbose,\n            runtime_config.log_rotate_days,\n            runtime_config.log_file,\n            runtime_config.log_no_color,\n        )\n\n        if runtime_config.debug or hass.loop.get_debug():\n            hass.config.debug = True\n\n        hass.config.safe_mode = runtime_config.safe_mode\n        hass.config.skip_pip = runtime_config.skip_pip\n        hass.config.skip_pip_packages = runtime_config.skip_pip_packages\n\n        return hass\n\n    hass = await create_hass()\n\n    if runtime_config.skip_pip or runtime_config.skip_pip_packages:\n        _LOGGER.warning(\n            \"Skipping pip installation of required modules. This may cause issues\"\n        )\n\n    if not await conf_util.async_ensure_config_exists(hass):\n        _LOGGER.error(\"Error getting configuration path\")\n        return None\n\n    _LOGGER.info(\"Config directory: %s\", runtime_config.config_dir)\n\n    block_async_io.enable()\n\n    if not (recovery_mode := runtime_config.recovery_mode):\n        config_dict = None\n        basic_setup_success = False\n\n        await hass.async_add_executor_job(conf_util.process_ha_config_upgrade, hass)\n\n        try:\n            config_dict = await conf_util.async_hass_config_yaml(hass)\n        except HomeAssistantError as err:\n            _LOGGER.error(\n                \"Failed to parse configuration.yaml: %s. Activating recovery mode\",\n                err,\n            )\n        else:\n            if not is_virtual_env():\n                await async_mount_local_lib_path(runtime_config.config_dir)\n\n            if hass.config.safe_mode:\n                _LOGGER.info(\"Starting in safe mode\")\n\n            basic_setup_success = (\n                await async_from_config_dict(config_dict, hass) is not None\n            )\n\n        if config_dict is None:\n            recovery_mode = True\n            await hass.async_stop(force=True)\n            hass = await create_hass()\n\n        elif not basic_setup_success:\n            _LOGGER.warning(\n                \"Unable to set up core integrations. Activating recovery mode\"\n            )\n            recovery_mode = True\n            await hass.async_stop(force=True)\n            hass = await create_hass()\n\n        elif any(\n            domain not in hass.config.components for domain in CRITICAL_INTEGRATIONS\n        ):\n            _LOGGER.warning(\n                \"Detected that %s did not load. Activating recovery mode\",\n                \",\".join(CRITICAL_INTEGRATIONS),\n            )\n\n            old_config = hass.config\n            old_logging = hass.data.get(DATA_LOGGING)\n\n            recovery_mode = True\n            await hass.async_stop(force=True)\n            hass = await create_hass()\n\n            if old_logging:\n                hass.data[DATA_LOGGING] = old_logging\n            hass.config.debug = old_config.debug\n            hass.config.skip_pip = old_config.skip_pip\n            hass.config.skip_pip_packages = old_config.skip_pip_packages\n            hass.config.internal_url = old_config.internal_url\n            hass.config.external_url = old_config.external_url\n            # Setup loader cache after the config dir has been set\n            loader.async_setup(hass)\n\n    if recovery_mode:\n        _LOGGER.info(\"Starting in recovery mode\")\n        hass.config.recovery_mode = True\n\n        http_conf = (await http.async_get_last_config(hass)) or {}\n\n        await async_from_config_dict(\n            {\"recovery_mode\": {}, \"http\": http_conf},\n            hass,\n        )\n\n    if runtime_config.open_ui:\n        hass.add_job(open_hass_ui, hass)\n\n    return hass\n\n\ndef open_hass_ui(hass: core.HomeAssistant) -> None:\n    \"\"\"Open the UI.\"\"\"\n    import webbrowser  # noqa: PLC0415\n\n    if hass.config.api is None or \"frontend\" not in hass.config.components:\n        _LOGGER.warning(\"Cannot launch the UI because frontend not loaded\")\n        return\n\n    scheme = \"https\" if hass.config.api.use_ssl else \"http\"\n    url = str(\n        yarl.URL.build(scheme=scheme, host=\"127.0.0.1\", port=hass.config.api.port)\n    )\n\n    if not webbrowser.open(url):\n        _LOGGER.warning(\n            \"Unable to open the Home Assistant UI in a browser. Open it yourself at %s\",\n            url,\n        )\n\n\ndef _init_blocking_io_modules_in_executor() -> None:\n    \"\"\"Initialize modules that do blocking I/O in executor.\"\"\"\n    # Cache the result of platform.uname().processor in the executor.\n    # Multiple modules call this function at startup which\n    # executes a blocking subprocess call. This is a problem for the\n    # asyncio event loop. By priming the cache of uname we can\n    # avoid the blocking call in the event loop.\n    _ = platform.uname().processor\n    # Initialize the mimetypes module to avoid blocking calls\n    # to the filesystem to load the mime.types file.\n    mimetypes.init()\n    # Initialize is_official_image and is_docker_env to avoid blocking calls\n    # to the filesystem.\n    is_official_image()\n    is_docker_env()\n\n\nasync def async_load_base_functionality(hass: core.HomeAssistant) -> None:\n    \"\"\"Load the registries and modules that will do blocking I/O.\"\"\"\n    if DATA_REGISTRIES_LOADED in hass.data:\n        return\n    hass.data[DATA_REGISTRIES_LOADED] = None\n    entity.async_setup(hass)\n    frame.async_setup(hass)\n    template.async_setup(hass)\n    translation.async_setup(hass)\n    await asyncio.gather(\n        create_eager_task(get_internal_store_manager(hass).async_initialize()),\n        create_eager_task(area_registry.async_load(hass)),\n        create_eager_task(category_registry.async_load(hass)),\n        create_eager_task(device_registry.async_load(hass)),\n        create_eager_task(entity_registry.async_load(hass)),\n        create_eager_task(floor_registry.async_load(hass)),\n        create_eager_task(issue_registry.async_load(hass)),\n        create_eager_task(label_registry.async_load(hass)),\n        hass.async_add_executor_job(_init_blocking_io_modules_in_executor),\n        create_eager_task(template.async_load_custom_templates(hass)),\n        create_eager_task(restore_state.async_load(hass)),\n        create_eager_task(hass.config_entries.async_initialize()),\n        create_eager_task(async_get_system_info(hass)),\n        create_eager_task(condition.async_setup(hass)),\n        create_eager_task(trigger.async_setup(hass)),\n    )\n\n\nasync def async_from_config_dict(\n    config: ConfigType, hass: core.HomeAssistant\n) -> core.HomeAssistant | None:\n    \"\"\"Try to configure Home Assistant from a configuration dictionary.\n\n    Dynamically loads required components and its dependencies.\n    This method is a coroutine.\n    \"\"\"\n    start = monotonic()\n\n    hass.config_entries = config_entries.ConfigEntries(hass, config)\n    # Prime custom component cache early so we know if registry entries are tied\n    # to a custom integration\n    await loader.async_get_custom_components(hass)\n    await async_load_base_functionality(hass)\n\n    # Set up core.\n    _LOGGER.debug(\"Setting up %s\", CORE_INTEGRATIONS)\n\n    if not all(\n        await asyncio.gather(\n            *(\n                create_eager_task(\n                    async_setup_component(hass, domain, config),\n                    name=f\"bootstrap setup {domain}\",\n                    loop=hass.loop,\n                )\n                for domain in CORE_INTEGRATIONS\n            )\n        )\n    ):\n        _LOGGER.error(\"Home Assistant core failed to initialize. \")\n        return None\n\n    _LOGGER.debug(\"Home Assistant core initialized\")\n\n    core_config = config.get(core.DOMAIN, {})\n\n    try:\n        await async_process_ha_core_config(hass, core_config)\n    except vol.Invalid as config_err:\n        conf_util.async_log_schema_error(config_err, core.DOMAIN, core_config, hass)\n        async_notify_setup_error(hass, core.DOMAIN)\n        return None\n    except HomeAssistantError:\n        _LOGGER.error(\n            \"Home Assistant core failed to initialize. Further initialization aborted\"\n        )\n        return None\n\n    await _async_set_up_integrations(hass, config)\n\n    stop = monotonic()\n    _LOGGER.info(\"Home Assistant initialized in %.2fs\", stop - start)\n\n    if (\n        REQUIRED_NEXT_PYTHON_HA_RELEASE\n        and sys.version_info[:3] < REQUIRED_NEXT_PYTHON_VER\n    ):\n        current_python_version = \".\".join(str(x) for x in sys.version_info[:3])\n        required_python_version = \".\".join(str(x) for x in REQUIRED_NEXT_PYTHON_VER[:2])\n        _LOGGER.warning(\n            (\n                \"Support for the running Python version %s is deprecated and \"\n                \"will be removed in Home Assistant %s; \"\n                \"Please upgrade Python to %s\"\n            ),\n            current_python_version,\n            REQUIRED_NEXT_PYTHON_HA_RELEASE,\n            required_python_version,\n        )\n        issue_registry.async_create_issue(\n            hass,\n            core.DOMAIN,\n            f\"python_version_{required_python_version}\",\n            is_fixable=False,\n            severity=issue_registry.IssueSeverity.WARNING,\n            breaks_in_ha_version=REQUIRED_NEXT_PYTHON_HA_RELEASE,\n            translation_key=\"python_version\",\n            translation_placeholders={\n                \"current_python_version\": current_python_version,\n                \"required_python_version\": required_python_version,\n                \"breaks_in_ha_version\": REQUIRED_NEXT_PYTHON_HA_RELEASE,\n            },\n        )\n\n    return hass\n\n\nasync def async_enable_logging(\n    hass: core.HomeAssistant,\n    verbose: bool = False,\n    log_rotate_days: int | None = None,\n    log_file: str | None = None,\n    log_no_color: bool = False,\n) -> None:\n    \"\"\"Set up the logging.\n\n    This method must be run in the event loop.\n    \"\"\"\n    fmt = (\n        \"%(asctime)s.%(msecs)03d %(levelname)s (%(threadName)s) [%(name)s] %(message)s\"\n    )\n\n    if not log_no_color:\n        try:\n            from colorlog import ColoredFormatter  # noqa: PLC0415\n\n            # basicConfig must be called after importing colorlog in order to\n            # ensure that the handlers it sets up wraps the correct streams.\n            logging.basicConfig(level=logging.INFO)\n\n            colorfmt = f\"%(log_color)s{fmt}%(reset)s\"\n            logging.getLogger().handlers[0].setFormatter(\n                ColoredFormatter(\n                    colorfmt,\n                    datefmt=FORMAT_DATETIME,\n                    reset=True,\n                    log_colors={\n                        \"DEBUG\": \"cyan\",\n                        \"INFO\": \"green\",\n                        \"WARNING\": \"yellow\",\n                        \"ERROR\": \"red\",\n                        \"CRITICAL\": \"red\",\n                    },\n                )\n            )\n        except ImportError:\n            pass\n\n    # If the above initialization failed for any reason, setup the default\n    # formatting.  If the above succeeds, this will result in a no-op.\n    logging.basicConfig(format=fmt, datefmt=FORMAT_DATETIME, level=logging.INFO)\n\n    # Capture warnings.warn(...) and friends messages in logs.\n    # The standard destination for them is stderr, which may end up unnoticed.\n    # This way they're where other messages are, and can be filtered as usual.\n    logging.captureWarnings(True)\n\n    # Suppress overly verbose logs from libraries that aren't helpful\n    logging.getLogger(\"requests\").setLevel(logging.WARNING)\n    logging.getLogger(\"urllib3\").setLevel(logging.WARNING)\n    logging.getLogger(\"aiohttp.access\").setLevel(logging.WARNING)\n    logging.getLogger(\"httpx\").setLevel(logging.WARNING)\n\n    sys.excepthook = lambda *args: logging.getLogger().exception(\n        \"Uncaught exception\", exc_info=args\n    )\n    threading.excepthook = lambda args: logging.getLogger().exception(\n        \"Uncaught thread exception\",\n        exc_info=(  # type: ignore[arg-type]\n            args.exc_type,\n            args.exc_value,\n            args.exc_traceback,\n        ),\n    )\n\n    # Log errors to a file if we have write access to file or config dir\n    if log_file is None:\n        err_log_path = hass.config.path(ERROR_LOG_FILENAME)\n    else:\n        err_log_path = os.path.abspath(log_file)\n\n    err_path_exists = os.path.isfile(err_log_path)\n    err_dir = os.path.dirname(err_log_path)\n\n    # Check if we can write to the error log if it exists or that\n    # we can create files in the containing directory if not.\n    if (err_path_exists and os.access(err_log_path, os.W_OK)) or (\n        not err_path_exists and os.access(err_dir, os.W_OK)\n    ):\n        err_handler = await hass.async_add_executor_job(\n            _create_log_file, err_log_path, log_rotate_days\n        )\n\n        err_handler.setFormatter(logging.Formatter(fmt, datefmt=FORMAT_DATETIME))\n\n        logger = logging.getLogger()\n        logger.addHandler(err_handler)\n        logger.setLevel(logging.INFO if verbose else logging.WARNING)\n\n        # Save the log file location for access by other components.\n        hass.data[DATA_LOGGING] = err_log_path\n    else:\n        _LOGGER.error(\"Unable to set up error log %s (access denied)\", err_log_path)\n\n    async_activate_log_queue_handler(hass)\n\n\ndef _create_log_file(\n    err_log_path: str, log_rotate_days: int | None\n) -> RotatingFileHandler | TimedRotatingFileHandler:\n    \"\"\"Create log file and do roll over.\"\"\"\n    err_handler: RotatingFileHandler | TimedRotatingFileHandler\n    if log_rotate_days:\n        err_handler = TimedRotatingFileHandler(\n            err_log_path, when=\"midnight\", backupCount=log_rotate_days\n        )\n    else:\n        err_handler = _RotatingFileHandlerWithoutShouldRollOver(\n            err_log_path, backupCount=1\n        )\n        try:\n            err_handler.doRollover()\n        except OSError as err:\n            _LOGGER.error(\"Error rolling over log file: %s\", err)\n\n    return err_handler\n\n\nclass _RotatingFileHandlerWithoutShouldRollOver(RotatingFileHandler):\n    \"\"\"RotatingFileHandler that does not check if it should roll over on every log.\"\"\"\n\n    def shouldRollover(self, record: logging.LogRecord) -> bool:\n        \"\"\"Never roll over.\n\n        The shouldRollover check is expensive because it has to stat\n        the log file for every log record. Since we do not set maxBytes\n        the result of this check is always False.\n        \"\"\"\n        return False\n\n\nasync def async_mount_local_lib_path(config_dir: str) -> str:\n    \"\"\"Add local library to Python Path.\n\n    This function is a coroutine.\n    \"\"\"\n    deps_dir = os.path.join(config_dir, \"deps\")\n    if (lib_dir := await async_get_user_site(deps_dir)) not in sys.path:\n        sys.path.insert(0, lib_dir)\n    return deps_dir\n\n\ndef _get_domains(hass: core.HomeAssistant, config: dict[str, Any]) -> set[str]:\n    \"\"\"Get domains of components to set up.\"\"\"\n    # The common config section [homeassistant] could be filtered here,\n    # but that is not necessary, since it corresponds to the core integration,\n    # that is always unconditionally loaded.\n    domains = {cv.domain_key(key) for key in config}\n\n    # Add config entry and default domains\n    if not hass.config.recovery_mode:\n        domains.update(DEFAULT_INTEGRATIONS)\n        domains.update(hass.config_entries.async_domains())\n    else:\n        domains.update(DEFAULT_INTEGRATIONS_RECOVERY_MODE)\n\n    # Add domains depending on if the Supervisor is used or not\n    if \"SUPERVISOR\" in os.environ:\n        domains.update(DEFAULT_INTEGRATIONS_SUPERVISOR)\n\n    return domains\n\n\nasync def _async_resolve_domains_and_preload(\n    hass: core.HomeAssistant, config: dict[str, Any]\n) -> tuple[dict[str, Integration], dict[str, Integration]]:\n    \"\"\"Resolve all dependencies and return integrations to set up.\n\n    The return value is a tuple of two dictionaries:\n    - The first dictionary contains integrations\n      specified by the configuration (including config entries).\n    - The second dictionary contains the same integrations as the first dictionary\n      together with all their dependencies.\n    \"\"\"\n    domains_to_setup = _get_domains(hass, config)\n\n    # Also process all base platforms since we do not require the manifest\n    # to list them as dependencies.\n    # We want to later avoid lock contention when multiple integrations try to load\n    # their manifests at once.\n    #\n    # Additionally process integrations that are defined under base platforms\n    # to speed things up.\n    # For example if we have\n    # sensor:\n    #   - platform: template\n    #\n    # `template` has to be loaded to validate the config for sensor.\n    # The more platforms under `sensor:`, the longer\n    # it will take to finish setup for `sensor` because each of these\n    # platforms has to be imported before we can validate the config.\n    #\n    # Thankfully we are migrating away from the platform pattern\n    # so this will be less of a problem in the future.\n    platform_integrations = conf_util.extract_platform_integrations(\n        config, BASE_PLATFORMS\n    )\n    additional_domains_to_process = {\n        *BASE_PLATFORMS,\n        *chain.from_iterable(platform_integrations.values()),\n    }\n\n    # Resolve all dependencies so we know all integrations\n    # that will have to be loaded and start right-away\n    integrations_or_excs = await loader.async_get_integrations(\n        hass, {*domains_to_setup, *additional_domains_to_process}\n    )\n    # Eliminate those missing or with invalid manifest\n    integrations_to_process = {\n        domain: itg\n        for domain, itg in integrations_or_excs.items()\n        if isinstance(itg, Integration)\n    }\n    integrations_dependencies = await loader.resolve_integrations_dependencies(\n        hass, integrations_to_process.values()\n    )\n    # Eliminate those without valid dependencies\n    integrations_to_process = {\n        domain: integrations_to_process[domain] for domain in integrations_dependencies\n    }\n\n    integrations_to_setup = {\n        domain: itg\n        for domain, itg in integrations_to_process.items()\n        if domain in domains_to_setup\n    }\n    all_integrations_to_setup = integrations_to_setup.copy()\n    all_integrations_to_setup.update(\n        (dep, loader.async_get_loaded_integration(hass, dep))\n        for domain in integrations_to_setup\n        for dep in integrations_dependencies[domain].difference(\n            all_integrations_to_setup\n        )\n    )\n\n    # Gather requirements for all integrations,\n    # their dependencies and after dependencies.\n    # To gather all the requirements we must ignore exceptions here.\n    # The exceptions will be detected and handled later in the bootstrap process.\n    integrations_after_dependencies = (\n        await loader.resolve_integrations_after_dependencies(\n            hass, integrations_to_process.values(), ignore_exceptions=True\n        )\n    )\n    integrations_requirements = {\n        domain: itg.requirements for domain, itg in integrations_to_process.items()\n    }\n    integrations_requirements.update(\n        (dep, loader.async_get_loaded_integration(hass, dep).requirements)\n        for deps in integrations_after_dependencies.values()\n        for dep in deps.difference(integrations_requirements)\n    )\n    all_requirements = set(chain.from_iterable(integrations_requirements.values()))\n\n    # Optimistically check if requirements are already installed\n    # ahead of setting up the integrations so we can prime the cache\n    # We do not wait for this since it's an optimization only\n    hass.async_create_background_task(\n        requirements.async_load_installed_versions(hass, all_requirements),\n        \"check installed requirements\",\n        eager_start=True,\n    )\n\n    # Start loading translations for all integrations we are going to set up\n    # in the background so they are ready when we need them. This avoids a\n    # lot of waiting for the translation load lock and a thundering herd of\n    # tasks trying to load the same translations at the same time as each\n    # integration is loaded.\n    #\n    # We do not wait for this since as soon as the task runs it will\n    # hold the translation load lock and if anything is fast enough to\n    # wait for the translation load lock, loading will be done by the\n    # time it gets to it.\n    translations_to_load = {*all_integrations_to_setup, *additional_domains_to_process}\n    hass.async_create_background_task(\n        translation.async_load_integrations(hass, translations_to_load),\n        \"load translations\",\n        eager_start=True,\n    )\n\n    # Preload storage for all integrations we are going to set up\n    # so we do not have to wait for it to be loaded when we need it\n    # in the setup process.\n    hass.async_create_background_task(\n        get_internal_store_manager(hass).async_preload(\n            [*PRELOAD_STORAGE, *all_integrations_to_setup]\n        ),\n        \"preload storage\",\n        eager_start=True,\n    )\n\n    return integrations_to_setup, all_integrations_to_setup\n\n\nasync def _async_set_up_integrations(\n    hass: core.HomeAssistant, config: dict[str, Any]\n) -> None:\n    \"\"\"Set up all the integrations.\"\"\"\n    watcher = _WatchPendingSetups(hass, _setup_started(hass))\n    watcher.async_start()\n\n    integrations, all_integrations = await _async_resolve_domains_and_preload(\n        hass, config\n    )\n    # Detect all cycles\n    integrations_after_dependencies = (\n        await loader.resolve_integrations_after_dependencies(\n            hass, all_integrations.values(), set(all_integrations)\n        )\n    )\n    all_domains = set(integrations_after_dependencies)\n    domains = set(integrations) & all_domains\n\n    _LOGGER.info(\n        \"Domains to be set up: %s\\nDependencies: %s\",\n        domains or \"{}\",\n        (all_domains - domains) or \"{}\",\n    )\n\n    async_set_domains_to_be_loaded(hass, all_domains)\n\n    # Initialize recorder\n    if \"recorder\" in all_domains:\n        recorder.async_initialize_recorder(hass)\n\n    stages: list[tuple[str, set[str], int | None]] = [\n        *(\n            (name, domain_group, timeout)\n            for name, domain_group, timeout in STAGE_0_INTEGRATIONS\n        ),\n        (\"1\", STAGE_1_INTEGRATIONS, STAGE_1_TIMEOUT),\n        (\"2\", domains, STAGE_2_TIMEOUT),\n    ]\n\n    _LOGGER.info(\"Setting up stage 0\")\n    for name, domain_group, timeout in stages:\n        stage_domains_unfiltered = domain_group & all_domains\n        if not stage_domains_unfiltered:\n            _LOGGER.info(\"Nothing to set up in stage %s: %s\", name, domain_group)\n            continue\n\n        stage_domains = stage_domains_unfiltered - hass.config.components\n        if not stage_domains:\n            _LOGGER.info(\"Already set up stage %s: %s\", name, stage_domains_unfiltered)\n            continue\n\n        stage_dep_domains_unfiltered = {\n            dep\n            for domain in stage_domains\n            for dep in integrations_after_dependencies[domain]\n            if dep not in stage_domains\n        }\n        stage_dep_domains = stage_dep_domains_unfiltered - hass.config.components\n\n        stage_all_domains = stage_domains | stage_dep_domains\n\n        _LOGGER.info(\n            \"Setting up stage %s: %s; already set up: %s\\n\"\n            \"Dependencies: %s; already set up: %s\",\n            name,\n            stage_domains,\n            (stage_domains_unfiltered - stage_domains) or \"{}\",\n            stage_dep_domains or \"{}\",\n            (stage_dep_domains_unfiltered - stage_dep_domains) or \"{}\",\n        )\n\n        if timeout is None:\n            await _async_setup_multi_components(hass, stage_all_domains, config)\n            continue\n        try:\n            async with hass.timeout.async_timeout(\n                timeout,\n                cool_down=COOLDOWN_TIME,\n                cancel_message=f\"Bootstrap stage {name} timeout\",\n            ):\n                await _async_setup_multi_components(hass, stage_all_domains, config)\n        except TimeoutError:\n            _LOGGER.warning(\n                \"Setup timed out for stage %s waiting on %s - moving forward\",\n                name,\n                hass._active_tasks,  # noqa: SLF001\n            )\n\n    # Wrap up startup\n    _LOGGER.debug(\"Waiting for startup to wrap up\")\n    try:\n        async with hass.timeout.async_timeout(\n            WRAP_UP_TIMEOUT,\n            cool_down=COOLDOWN_TIME,\n            cancel_message=\"Bootstrap startup wrap up timeout\",\n        ):\n            await hass.async_block_till_done()\n    except TimeoutError:\n        _LOGGER.warning(\n            \"Setup timed out for bootstrap waiting on %s - moving forward\",\n            hass._active_tasks,  # noqa: SLF001\n        )\n\n    watcher.async_stop()\n\n    if _LOGGER.isEnabledFor(logging.DEBUG):\n        setup_time = async_get_setup_timings(hass)\n        _LOGGER.debug(\n            \"Integration setup times: %s\",\n            dict(sorted(setup_time.items(), key=itemgetter(1), reverse=True)),\n        )\n\n\nclass _WatchPendingSetups:\n    \"\"\"Periodic log and dispatch of setups that are pending.\"\"\"\n\n    def __init__(\n        self,\n        hass: core.HomeAssistant,\n        setup_started: dict[tuple[str, str | None], float],\n    ) -> None:\n        \"\"\"Initialize the WatchPendingSetups class.\"\"\"\n        self._hass = hass\n        self._setup_started = setup_started\n        self._duration_count = 0\n        self._handle: asyncio.TimerHandle | None = None\n        self._previous_was_empty = True\n        self._loop = hass.loop\n\n    def _async_watch(self) -> None:\n        \"\"\"Periodic log of setups that are pending.\"\"\"\n        now = monotonic()\n        self._duration_count += SLOW_STARTUP_CHECK_INTERVAL\n\n        remaining_with_setup_started: defaultdict[str, float] = defaultdict(float)\n        for integration_group, start_time in self._setup_started.items():\n            domain, _ = integration_group\n            remaining_with_setup_started[domain] += now - start_time\n\n        if remaining_with_setup_started:\n            _LOGGER.debug(\"Integration remaining: %s\", remaining_with_setup_started)\n        elif waiting_tasks := self._hass._active_tasks:  # noqa: SLF001\n            _LOGGER.debug(\"Waiting on tasks: %s\", waiting_tasks)\n        self._async_dispatch(remaining_with_setup_started)\n        if (\n            self._setup_started\n            and self._duration_count % LOG_SLOW_STARTUP_INTERVAL == 0\n        ):\n            # We log every LOG_SLOW_STARTUP_INTERVAL until all integrations are done\n            # once we take over LOG_SLOW_STARTUP_INTERVAL (60s) to start up\n            _LOGGER.warning(\n                \"Waiting on integrations to complete setup: %s\",\n                self._setup_started,\n            )\n\n        _LOGGER.debug(\"Running timeout Zones: %s\", self._hass.timeout.zones)\n        self._async_schedule_next()\n\n    def _async_dispatch(self, remaining_with_setup_started: dict[str, float]) -> None:\n        \"\"\"Dispatch the signal.\"\"\"\n        if remaining_with_setup_started or not self._previous_was_empty:\n            async_dispatcher_send_internal(\n                self._hass, SIGNAL_BOOTSTRAP_INTEGRATIONS, remaining_with_setup_started\n            )\n        self._previous_was_empty = not remaining_with_setup_started\n\n    def _async_schedule_next(self) -> None:\n        \"\"\"Schedule the next call.\"\"\"\n        self._handle = self._loop.call_later(\n            SLOW_STARTUP_CHECK_INTERVAL, self._async_watch\n        )\n\n    def async_start(self) -> None:\n        \"\"\"Start watching.\"\"\"\n        self._async_schedule_next()\n\n    def async_stop(self) -> None:\n        \"\"\"Stop watching.\"\"\"\n        self._async_dispatch({})\n        if self._handle:\n            self._handle.cancel()\n            self._handle = None\n\n\nasync def _async_setup_multi_components(\n    hass: core.HomeAssistant,\n    domains: set[str],\n    config: dict[str, Any],\n) -> None:\n    \"\"\"Set up multiple domains. Log on failure.\"\"\"\n    # Create setup tasks for base platforms first since everything will have\n    # to wait to be imported, and the sooner we can get the base platforms\n    # loaded the sooner we can start loading the rest of the integrations.\n    futures = {\n        domain: hass.async_create_task_internal(\n            async_setup_component(hass, domain, config),\n            f\"setup component {domain}\",\n            eager_start=True,\n        )\n        for domain in sorted(domains, key=SETUP_ORDER_SORT_KEY, reverse=True)\n    }\n    results = await asyncio.gather(*futures.values(), return_exceptions=True)\n    for idx, domain in enumerate(futures):\n        result = results[idx]\n        if isinstance(result, BaseException):\n            _LOGGER.error(\n                \"Error setting up integration %s - received exception\",\n                domain,\n                exc_info=(type(result), result, result.__traceback__),\n            )\n", "metadata": {"source": "github.com/home-assistant/core", "file_path": "homeassistant/bootstrap.py", "language": "python", "complexity": "complex", "star_count": 80370, "quality_score": 0.9700000000000001, "features": {"has_classes": true, "has_functions": true, "has_async": true, "has_decorators": false, "has_imports": true, "has_error_handling": true, "has_documentation": true, "has_type_hints": true, "has_testing": false, "has_logging": true, "function_count": 20, "class_count": 2, "line_count": 1059}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "async_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create asynchronous code with proper concurrency handling with asyncio coroutines\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"Support for the Abode Security System.\"\"\"\n\nfrom __future__ import annotations\n\nfrom dataclasses import dataclass, field\nfrom functools import partial\nfrom pathlib import Path\n\nfrom jaraco.abode.client import Client as Abode\nimport jaraco.abode.config\nfrom jaraco.abode.exceptions import (\n    AuthenticationException as AbodeAuthenticationException,\n    Exception as AbodeException,\n)\nfrom jaraco.abode.helpers.timeline import Groups as GROUPS\nfrom requests.exceptions import ConnectTimeout, HTTPError\n\nfrom homeassistant.config_entries import ConfigEntry\nfrom homeassistant.const import (\n    ATTR_DATE,\n    ATTR_DEVICE_ID,\n    ATTR_TIME,\n    CONF_PASSWORD,\n    CONF_USERNAME,\n    EVENT_HOMEASSISTANT_STOP,\n    Platform,\n)\nfrom homeassistant.core import CALLBACK_TYPE, Event, HomeAssistant\nfrom homeassistant.exceptions import ConfigEntryAuthFailed, ConfigEntryNotReady\nfrom homeassistant.helpers import config_validation as cv\nfrom homeassistant.helpers.typing import ConfigType\n\nfrom .const import CONF_POLLING, DOMAIN, LOGGER\nfrom .services import async_setup_services\n\nATTR_DEVICE_NAME = \"device_name\"\nATTR_DEVICE_TYPE = \"device_type\"\nATTR_EVENT_CODE = \"event_code\"\nATTR_EVENT_NAME = \"event_name\"\nATTR_EVENT_TYPE = \"event_type\"\nATTR_EVENT_UTC = \"event_utc\"\nATTR_USER_NAME = \"user_name\"\nATTR_APP_TYPE = \"app_type\"\nATTR_EVENT_BY = \"event_by\"\n\nCONFIG_SCHEMA = cv.removed(DOMAIN, raise_if_present=False)\n\nPLATFORMS = [\n    Platform.ALARM_CONTROL_PANEL,\n    Platform.BINARY_SENSOR,\n    Platform.CAMERA,\n    Platform.COVER,\n    Platform.LIGHT,\n    Platform.LOCK,\n    Platform.SENSOR,\n    Platform.SWITCH,\n]\n\n\n@dataclass\nclass AbodeSystem:\n    \"\"\"Abode System class.\"\"\"\n\n    abode: Abode\n    polling: bool\n    entity_ids: set[str | None] = field(default_factory=set)\n    logout_listener: CALLBACK_TYPE | None = None\n\n\nasync def async_setup(hass: HomeAssistant, config: ConfigType) -> bool:\n    \"\"\"Set up the Abode component.\"\"\"\n    async_setup_services(hass)\n    return True\n\n\nasync def async_setup_entry(hass: HomeAssistant, entry: ConfigEntry) -> bool:\n    \"\"\"Set up Abode integration from a config entry.\"\"\"\n    username = entry.data[CONF_USERNAME]\n    password = entry.data[CONF_PASSWORD]\n    polling = entry.data[CONF_POLLING]\n\n    # Configure abode library to use config directory for storing data\n    jaraco.abode.config.paths.override(user_data=Path(hass.config.path(\"Abode\")))\n\n    # For previous config entries where unique_id is None\n    if entry.unique_id is None:\n        hass.config_entries.async_update_entry(\n            entry, unique_id=entry.data[CONF_USERNAME]\n        )\n\n    try:\n        abode = await hass.async_add_executor_job(\n            Abode, username, password, True, True, True\n        )\n\n    except AbodeAuthenticationException as ex:\n        raise ConfigEntryAuthFailed(f\"Invalid credentials: {ex}\") from ex\n\n    except (AbodeException, ConnectTimeout, HTTPError) as ex:\n        raise ConfigEntryNotReady(f\"Unable to connect to Abode: {ex}\") from ex\n\n    hass.data[DOMAIN] = AbodeSystem(abode, polling)\n\n    await hass.config_entries.async_forward_entry_setups(entry, PLATFORMS)\n\n    await setup_hass_events(hass)\n    await hass.async_add_executor_job(setup_abode_events, hass)\n\n    return True\n\n\nasync def async_unload_entry(hass: HomeAssistant, entry: ConfigEntry) -> bool:\n    \"\"\"Unload a config entry.\"\"\"\n    unload_ok = await hass.config_entries.async_unload_platforms(entry, PLATFORMS)\n\n    await hass.async_add_executor_job(hass.data[DOMAIN].abode.events.stop)\n    await hass.async_add_executor_job(hass.data[DOMAIN].abode.logout)\n\n    hass.data[DOMAIN].logout_listener()\n    hass.data.pop(DOMAIN)\n\n    return unload_ok\n\n\nasync def setup_hass_events(hass: HomeAssistant) -> None:\n    \"\"\"Home Assistant start and stop callbacks.\"\"\"\n\n    def logout(event: Event) -> None:\n        \"\"\"Logout of Abode.\"\"\"\n        if not hass.data[DOMAIN].polling:\n            hass.data[DOMAIN].abode.events.stop()\n\n        hass.data[DOMAIN].abode.logout()\n        LOGGER.info(\"Logged out of Abode\")\n\n    if not hass.data[DOMAIN].polling:\n        await hass.async_add_executor_job(hass.data[DOMAIN].abode.events.start)\n\n    hass.data[DOMAIN].logout_listener = hass.bus.async_listen_once(\n        EVENT_HOMEASSISTANT_STOP, logout\n    )\n\n\ndef setup_abode_events(hass: HomeAssistant) -> None:\n    \"\"\"Event callbacks.\"\"\"\n\n    def event_callback(event: str, event_json: dict[str, str]) -> None:\n        \"\"\"Handle an event callback from Abode.\"\"\"\n        data = {\n            ATTR_DEVICE_ID: event_json.get(ATTR_DEVICE_ID, \"\"),\n            ATTR_DEVICE_NAME: event_json.get(ATTR_DEVICE_NAME, \"\"),\n            ATTR_DEVICE_TYPE: event_json.get(ATTR_DEVICE_TYPE, \"\"),\n            ATTR_EVENT_CODE: event_json.get(ATTR_EVENT_CODE, \"\"),\n            ATTR_EVENT_NAME: event_json.get(ATTR_EVENT_NAME, \"\"),\n            ATTR_EVENT_TYPE: event_json.get(ATTR_EVENT_TYPE, \"\"),\n            ATTR_EVENT_UTC: event_json.get(ATTR_EVENT_UTC, \"\"),\n            ATTR_USER_NAME: event_json.get(ATTR_USER_NAME, \"\"),\n            ATTR_APP_TYPE: event_json.get(ATTR_APP_TYPE, \"\"),\n            ATTR_EVENT_BY: event_json.get(ATTR_EVENT_BY, \"\"),\n            ATTR_DATE: event_json.get(ATTR_DATE, \"\"),\n            ATTR_TIME: event_json.get(ATTR_TIME, \"\"),\n        }\n\n        hass.bus.fire(event, data)\n\n    events = [\n        GROUPS.ALARM,\n        GROUPS.ALARM_END,\n        GROUPS.PANEL_FAULT,\n        GROUPS.PANEL_RESTORE,\n        GROUPS.AUTOMATION,\n        GROUPS.DISARM,\n        GROUPS.ARM,\n        GROUPS.ARM_FAULT,\n        GROUPS.TEST,\n        GROUPS.CAPTURE,\n        GROUPS.DEVICE,\n    ]\n\n    for event in events:\n        hass.data[DOMAIN].abode.events.add_event_callback(\n            event, partial(event_callback, event)\n        )\n", "metadata": {"source": "github.com/home-assistant/core", "file_path": "homeassistant/components/abode/__init__.py", "language": "python", "complexity": "complex", "star_count": 80370, "quality_score": 1.0, "features": {"has_classes": true, "has_functions": true, "has_async": true, "has_decorators": true, "has_imports": true, "has_error_handling": true, "has_documentation": true, "has_type_hints": true, "has_testing": false, "has_logging": true, "function_count": 7, "class_count": 1, "line_count": 184}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "async_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create asynchronous code with proper concurrency handling with asyncio coroutines\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# -*- coding: utf-8 -*-\n\nimport asyncio\n\nfrom sanic import Sanic\n\n\napp = Sanic(\"Example\")\n\n\nasync def notify_server_started_after_five_seconds():\n    await asyncio.sleep(5)\n    print(\"Server successfully started!\")\n\n\napp.add_task(notify_server_started_after_five_seconds())\n\nif __name__ == \"__main__\":\n    app.run(host=\"0.0.0.0\", port=8000)\n", "metadata": {"source": "github.com/sanic-org/sanic", "file_path": "examples/add_task_sanic.py", "language": "python", "complexity": "simple", "star_count": 18448, "quality_score": 0.89, "features": {"has_classes": false, "has_functions": true, "has_async": true, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 1, "class_count": 0, "line_count": 20}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "async_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create asynchronous code with proper concurrency handling using Flask/FastAPI decorators\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# -*- coding: utf-8 -*-\n\nfrom functools import wraps\n\nfrom sanic import Sanic\nfrom sanic.response import json\n\n\napp = Sanic(\"Example\")\n\n\ndef check_request_for_authorization_status(request):\n    # Note: Define your check, for instance cookie, session.\n    flag = True\n    return flag\n\n\ndef authorized(f):\n    @wraps(f)\n    async def decorated_function(request, *args, **kwargs):\n        # run some method that checks the request\n        # for the client's authorization status\n        is_authorized = check_request_for_authorization_status(request)\n\n        if is_authorized:\n            # the user is authorized.\n            # run the handler method and return the response\n            response = await f(request, *args, **kwargs)\n            return response\n        else:\n            # the user is not authorized.\n            return json({\"status\": \"not_authorized\"}, 403)\n\n    return decorated_function\n\n\n@app.route(\"/\")\n@authorized\nasync def test(request):\n    return json({\"status\": \"authorized\"})\n\n\nif __name__ == \"__main__\":\n    app.run(host=\"0.0.0.0\", port=8000)\n", "metadata": {"source": "github.com/sanic-org/sanic", "file_path": "examples/authorized_sanic.py", "language": "python", "complexity": "simple", "star_count": 18448, "quality_score": 0.89, "features": {"has_classes": false, "has_functions": true, "has_async": true, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 4, "class_count": 0, "line_count": 45}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "async_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create asynchronous code with proper concurrency handling with asyncio coroutines\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from sanic import Blueprint, Sanic\nfrom sanic.response import file, json\n\n\napp = Sanic(\"Example\")\nblueprint = Blueprint(\"bp_example\", url_prefix=\"/my_blueprint\")\nblueprint2 = Blueprint(\"bp_example2\", url_prefix=\"/my_blueprint2\")\nblueprint3 = Blueprint(\"bp_example3\", url_prefix=\"/my_blueprint3\")\n\n\n@blueprint.route(\"/foo\")\nasync def foo(request):\n    return json({\"msg\": \"hi from blueprint\"})\n\n\n@blueprint2.route(\"/foo\")\nasync def foo2(request):\n    return json({\"msg\": \"hi from blueprint2\"})\n\n\n@blueprint3.route(\"/foo\")\nasync def index(request):\n    return await file(\"websocket.html\")\n\n\n@app.websocket(\"/feed\")\nasync def foo3(request, ws):\n    while True:\n        data = \"hello!\"\n        print(\"Sending: \" + data)\n        await ws.send(data)\n        data = await ws.recv()\n        print(\"Received: \" + data)\n\n\napp.blueprint(blueprint)\napp.blueprint(blueprint2)\napp.blueprint(blueprint3)\n\nif __name__ == \"__main__\":\n    app.run(host=\"0.0.0.0\", port=9999, debug=True)\n", "metadata": {"source": "github.com/sanic-org/sanic", "file_path": "examples/blueprints.py", "language": "python", "complexity": "simple", "star_count": 18448, "quality_score": 0.89, "features": {"has_classes": false, "has_functions": true, "has_async": true, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 4, "class_count": 0, "line_count": 42}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "async_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create asynchronous code with proper concurrency handling using Flask/FastAPI decorators\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from asyncio import sleep\n\nfrom sanic import Sanic, response\n\n\napp = Sanic(\"DelayedResponseApp\", strict_slashes=True)\napp.config.AUTO_EXTEND = False\n\n\n@app.get(\"/\")\nasync def handler(request):\n    return response.redirect(\"/sleep/3\")\n\n\n@app.get(\"/sleep/<t:float>\")\nasync def handler2(request, t=0.3):\n    await sleep(t)\n    return response.text(f\"Slept {t:.1f} seconds.\\n\")\n\n\nif __name__ == \"__main__\":\n    app.run(host=\"0.0.0.0\", port=8000)\n", "metadata": {"source": "github.com/sanic-org/sanic", "file_path": "examples/delayed_response.py", "language": "python", "complexity": "simple", "star_count": 18448, "quality_score": 0.92, "features": {"has_classes": false, "has_functions": true, "has_async": true, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": false, "has_testing": false, "has_logging": false, "function_count": 2, "class_count": 0, "line_count": 23}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "async_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create asynchronous code with proper concurrency handling using Flask/FastAPI decorators\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"\nExample intercepting uncaught exceptions using Sanic's error handler framework.\nThis may be useful for developers wishing to use Sentry, Airbrake, etc.\nor a custom system to log and monitor unexpected errors in production.\nFirst we create our own class inheriting from Handler in sanic.exceptions,\nand pass in an instance of it when we create our Sanic instance. Inside this\nclass' default handler, we can do anything including sending exceptions to\nan external service.\n\"\"\"\n\nfrom sanic import Sanic\nfrom sanic.exceptions import SanicException\nfrom sanic.handlers import ErrorHandler\n\n\n\"\"\"\nImports and code relevant for our CustomHandler class\n(Ordinarily this would be in a separate file)\n\"\"\"\n\n\nclass CustomHandler(ErrorHandler):\n    def default(self, request, exception):\n        # Here, we have access to the exception object\n        # and can do anything with it (log, send to external service, etc)\n\n        # Some exceptions are trivial and built into Sanic (404s, etc)\n        if not isinstance(exception, SanicException):\n            print(exception)\n\n        # Then, we must finish handling the exception by returning\n        # our response to the client\n        # For this we can just call the super class' default handler\n        return super().default(request, exception)\n\n\n\"\"\"\nThis is an ordinary Sanic server, with the exception that we set the\nserver's error_handler to an instance of our CustomHandler\n\"\"\"\n\n\nhandler = CustomHandler()\napp = Sanic(\"Example\", error_handler=handler)\n\n\n@app.route(\"/\")\nasync def test(request):\n    # Here, something occurs which causes an unexpected exception\n    # This exception will flow to our custom handler.\n    raise SanicException(\"You Broke It!\")\n\n\nif __name__ == \"__main__\":\n    app.run(host=\"0.0.0.0\", port=8000, debug=True)\n", "metadata": {"source": "github.com/sanic-org/sanic", "file_path": "examples/exception_monitoring.py", "language": "python", "complexity": "simple", "star_count": 18448, "quality_score": 0.9500000000000001, "features": {"has_classes": true, "has_functions": true, "has_async": true, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": true, "has_type_hints": false, "has_testing": false, "has_logging": true, "function_count": 2, "class_count": 2, "line_count": 56}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "async_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create asynchronous code with proper concurrency handling using Flask/FastAPI decorators\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from sanic import Sanic, response\n\n\napp = Sanic(\"Example\")\n\n\n@app.route(\"/\")\nasync def test(request):\n    return response.json({\"test\": True})\n\n\nif __name__ == \"__main__\":\n    app.run(host=\"0.0.0.0\", port=8000)\n", "metadata": {"source": "github.com/sanic-org/sanic", "file_path": "examples/hello_world.py", "language": "python", "complexity": "simple", "star_count": 18448, "quality_score": 0.89, "features": {"has_classes": false, "has_functions": true, "has_async": true, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 1, "class_count": 0, "line_count": 14}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "async_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create asynchronous code with proper concurrency handling with asyncio coroutines\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from sanic import Sanic, response, text\nfrom sanic.handlers import ErrorHandler\nfrom sanic.server.async_server import AsyncioServer\n\n\nHTTP_PORT = 9999\nHTTPS_PORT = 8888\n\nhttp = Sanic(\"http\")\nhttp.config.SERVER_NAME = f\"localhost:{HTTP_PORT}\"\nhttps = Sanic(\"https\")\nhttps.config.SERVER_NAME = f\"localhost:{HTTPS_PORT}\"\n\n\n@https.get(\"/foo\")\ndef foo(request):\n    return text(\"foo\")\n\n\n@https.get(\"/bar\")\ndef bar(request):\n    return text(\"bar\")\n\n\n@http.get(\"/<path:path>\")\ndef proxy(request, path):\n    url = request.app.url_for(\n        \"proxy\",\n        path=path,\n        _server=https.config.SERVER_NAME,\n        _external=True,\n        _scheme=\"https\",\n    )\n    return response.redirect(url)\n\n\n@https.main_process_start\nasync def start(app, _):\n    http_server = await http.create_server(\n        port=HTTP_PORT, return_asyncio_server=True\n    )\n    app.add_task(runner(http, http_server))\n    app.ctx.http_server = http_server\n    app.ctx.http = http\n\n\n@https.main_process_stop\nasync def stop(app, _):\n    await app.ctx.http_server.before_stop()\n    await app.ctx.http_server.close()\n    for connection in app.ctx.http_server.connections:\n        connection.close_if_idle()\n    await app.ctx.http_server.after_stop()\n    app.ctx.http = False\n\n\nasync def runner(app: Sanic, app_server: AsyncioServer):\n    app.is_running = True\n    try:\n        app.signalize()\n        app.finalize()\n        ErrorHandler.finalize(app.error_handler)\n        app_server.init = True\n\n        await app_server.before_start()\n        await app_server.after_start()\n        await app_server.serve_forever()\n    finally:\n        app.is_running = False\n        app.is_stopping = True\n\n\nif __name__ == \"__main__\":\n    https.run(port=HTTPS_PORT, debug=True)\n", "metadata": {"source": "github.com/sanic-org/sanic", "file_path": "examples/http_redirect.py", "language": "python", "complexity": "medium", "star_count": 18448, "quality_score": 0.98, "features": {"has_classes": false, "has_functions": true, "has_async": true, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 6, "class_count": 0, "line_count": 75}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "async_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create asynchronous code with proper concurrency handling using Flask/FastAPI decorators\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import asyncio\n\nimport httpx\n\nfrom sanic import Sanic\nfrom sanic.response import json\n\n\napp = Sanic(\"Example\")\n\nsem = None\n\n\n@app.before_server_start\ndef init(sanic, _):\n    global sem\n    concurrency_per_worker = 4\n    sem = asyncio.Semaphore(concurrency_per_worker)\n\n\nasync def bounded_fetch(session, url):\n    \"\"\"\n    Use session object to perform 'get' request on url\n    \"\"\"\n    async with sem:\n        response = await session.get(url)\n        return response.json()\n\n\n@app.route(\"/\")\nasync def test(request):\n    \"\"\"\n    Download and serve example JSON\n    \"\"\"\n    url = \"https://api.github.com/repos/sanic-org/sanic\"\n\n    async with httpx.AsyncClient() as session:\n        response = await bounded_fetch(session, url)\n        return json(response)\n\n\nif __name__ == \"__main__\":\n    app.run(host=\"0.0.0.0\", port=8000, workers=2)\n", "metadata": {"source": "github.com/sanic-org/sanic", "file_path": "examples/limit_concurrency.py", "language": "python", "complexity": "simple", "star_count": 18448, "quality_score": 0.92, "features": {"has_classes": false, "has_functions": true, "has_async": true, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": true, "has_type_hints": false, "has_testing": false, "has_logging": false, "function_count": 3, "class_count": 0, "line_count": 44}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "async_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create asynchronous code with proper concurrency handling using Flask/FastAPI decorators\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import logging\n\nfrom contextvars import ContextVar\n\nfrom sanic import Sanic, response\n\n\nlog = logging.getLogger(__name__)\n\n\nclass RequestIdFilter(logging.Filter):\n    def filter(self, record):\n        try:\n            record.request_id = app.ctx.request_id.get(None) or \"n/a\"\n        except AttributeError:\n            record.request_id = \"n/a\"\n        return True\n\n\nLOG_SETTINGS = {\n    \"version\": 1,\n    \"disable_existing_loggers\": False,\n    \"handlers\": {\n        \"console\": {\n            \"class\": \"logging.StreamHandler\",\n            \"level\": \"DEBUG\",\n            \"formatter\": \"default\",\n            \"filters\": [\"requestid\"],\n        },\n    },\n    \"filters\": {\n        \"requestid\": {\n            \"()\": RequestIdFilter,\n        },\n    },\n    \"formatters\": {\n        \"default\": {\n            \"format\": (\n                \"%(asctime)s %(levelname)s %(name)s:%(lineno)d\"\n                \" %(request_id)s | %(message)s\"\n            ),\n        },\n    },\n    \"loggers\": {\n        \"\": {\"level\": \"DEBUG\", \"handlers\": [\"console\"], \"propagate\": True},\n    },\n}\n\n\napp = Sanic(\"Example\", log_config=LOG_SETTINGS)\n\n\n@app.on_request\nasync def set_request_id(request):\n    request.app.ctx.request_id.set(request.id)\n    log.info(f\"Setting {request.id=}\")\n\n\n@app.on_response\nasync def set_request_header(request, response):\n    response.headers[\"X-Request-ID\"] = request.id\n\n\n@app.route(\"/\")\nasync def test(request):\n    log.debug(\"X-Request-ID: %s\", request.id)\n    log.info(\"Hello from test!\")\n    return response.json({\"test\": True})\n\n\n@app.before_server_start\ndef setup(app, loop):\n    app.ctx.request_id = ContextVar(\"request_id\")\n\n\nif __name__ == \"__main__\":\n    app.run(port=9999, debug=True)\n", "metadata": {"source": "github.com/sanic-org/sanic", "file_path": "examples/log_request_id.py", "language": "python", "complexity": "medium", "star_count": 18448, "quality_score": 1.0, "features": {"has_classes": true, "has_functions": true, "has_async": true, "has_decorators": true, "has_imports": true, "has_error_handling": true, "has_documentation": false, "has_type_hints": true, "has_testing": false, "has_logging": true, "function_count": 5, "class_count": 1, "line_count": 78}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "async_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create asynchronous code with proper concurrency handling using Flask/FastAPI decorators\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"pytest-xdist example for sanic server\n\nInstall testing tools:\n\n    $ pip install pytest pytest-xdist\n\nRun with xdist params:\n\n    $ pytest examples/pytest_xdist.py -n 8  # 8 workers\n\"\"\"\n\nimport re\n\nimport pytest\n\nfrom sanic_testing import SanicTestClient\nfrom sanic_testing.testing import PORT as PORT_BASE\n\nfrom sanic import Sanic\nfrom sanic.response import text\n\n\n@pytest.fixture(scope=\"session\")\ndef test_port(worker_id):\n    m = re.search(r\"[0-9]+\", worker_id)\n    if m:\n        num_id = m.group(0)\n    else:\n        num_id = 0\n    port = PORT_BASE + int(num_id)\n    return port\n\n\n@pytest.fixture(scope=\"session\")\ndef app():\n    app = Sanic(\"Example\")\n\n    @app.route(\"/\")\n    async def index(request):\n        return text(\"OK\")\n\n    return app\n\n\n@pytest.fixture(scope=\"session\")\ndef client(app, test_port):\n    return SanicTestClient(app, test_port)\n\n\n@pytest.mark.parametrize(\"run_id\", range(100))\ndef test_index(client, run_id):\n    request, response = client._sanic_endpoint_test(\"get\", \"/\")\n    assert response.status == 200\n    assert response.text == \"OK\"\n", "metadata": {"source": "github.com/sanic-org/sanic", "file_path": "examples/pytest_xdist.py", "language": "python", "complexity": "simple", "star_count": 18448, "quality_score": 0.98, "features": {"has_classes": false, "has_functions": true, "has_async": true, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": true, "has_type_hints": false, "has_testing": true, "has_logging": false, "function_count": 5, "class_count": 0, "line_count": 55}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "async_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create asynchronous code with proper concurrency handling using Flask/FastAPI decorators\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from os import getenv\n\nfrom raygun4py.raygunprovider import RaygunSender\n\nfrom sanic import Sanic\nfrom sanic.exceptions import SanicException\nfrom sanic.handlers import ErrorHandler\n\n\nclass RaygunExceptionReporter(ErrorHandler):\n    def __init__(self, raygun_api_key=None):\n        super().__init__()\n        if raygun_api_key is None:\n            raygun_api_key = getenv(\"RAYGUN_API_KEY\")\n\n        self.sender = RaygunSender(raygun_api_key)\n\n    def default(self, request, exception):\n        self.sender.send_exception(exception=exception)\n        return super().default(request, exception)\n\n\nraygun_error_reporter = RaygunExceptionReporter()\napp = Sanic(\"Example\", error_handler=raygun_error_reporter)\n\n\n@app.route(\"/raise\")\nasync def test(request):\n    raise SanicException(\"You Broke It!\")\n\n\nif __name__ == \"__main__\":\n    app.run(host=\"0.0.0.0\", port=getenv(\"PORT\", 8080))\n", "metadata": {"source": "github.com/sanic-org/sanic", "file_path": "examples/raygun_example.py", "language": "python", "complexity": "simple", "star_count": 18448, "quality_score": 0.92, "features": {"has_classes": true, "has_functions": true, "has_async": true, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": false, "has_testing": false, "has_logging": false, "function_count": 3, "class_count": 1, "line_count": 34}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "async_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create asynchronous code with proper concurrency handling using Flask/FastAPI decorators\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from sanic import Sanic, response\n\n\napp = Sanic(\"Example\")\n\n\n@app.route(\"/\")\ndef handle_request(request):\n    return response.redirect(\"/redirect\")\n\n\n@app.route(\"/redirect\")\nasync def test(request):\n    return response.json({\"Redirected\": True})\n\n\nif __name__ == \"__main__\":\n    app.run(host=\"0.0.0.0\", port=8000)\n", "metadata": {"source": "github.com/sanic-org/sanic", "file_path": "examples/redirect_example.py", "language": "python", "complexity": "simple", "star_count": 18448, "quality_score": 0.89, "features": {"has_classes": false, "has_functions": true, "has_async": true, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 2, "class_count": 0, "line_count": 19}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "async_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create asynchronous code with proper concurrency handling with asyncio coroutines\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from sanic import Sanic\nfrom sanic.blueprints import Blueprint\nfrom sanic.response import stream, text\nfrom sanic.views import HTTPMethodView\nfrom sanic.views import stream as stream_decorator\n\n\nbp = Blueprint(\"bp_example\")\napp = Sanic(\"Example\")\n\n\nclass SimpleView(HTTPMethodView):\n    @stream_decorator\n    async def post(self, request):\n        result = \"\"\n        while True:\n            body = await request.stream.get()\n            if body is None:\n                break\n            result += body.decode(\"utf-8\")\n        return text(result)\n\n\n@app.post(\"/stream\", stream=True)\nasync def handler(request):\n    async def streaming(response):\n        while True:\n            body = await request.stream.get()\n            if body is None:\n                break\n            body = body.decode(\"utf-8\").replace(\"1\", \"A\")\n            await response.write(body)\n\n    return stream(streaming)\n\n\n@bp.put(\"/bp_stream\", stream=True)\nasync def bp_handler(request):\n    result = \"\"\n    while True:\n        body = await request.stream.get()\n        if body is None:\n            break\n        result += body.decode(\"utf-8\").replace(\"1\", \"A\")\n    return text(result)\n\n\nasync def post_handler(request):\n    result = \"\"\n    while True:\n        body = await request.stream.get()\n        if body is None:\n            break\n        result += body.decode(\"utf-8\")\n    return text(result)\n\n\napp.blueprint(bp)\napp.add_route(SimpleView.as_view(), \"/method_view\")\n\n\nif __name__ == \"__main__\":\n    app.run(host=\"0.0.0.0\", port=8000)\n", "metadata": {"source": "github.com/sanic-org/sanic", "file_path": "examples/request_stream/server.py", "language": "python", "complexity": "medium", "star_count": 18448, "quality_score": 0.92, "features": {"has_classes": true, "has_functions": true, "has_async": true, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": false, "has_testing": false, "has_logging": false, "function_count": 5, "class_count": 1, "line_count": 64}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "async_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create asynchronous code with proper concurrency handling using Flask/FastAPI decorators\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import asyncio\n\nfrom sanic import Sanic, response\nfrom sanic.config import Config\nfrom sanic.exceptions import RequestTimeout\n\n\nConfig.REQUEST_TIMEOUT = 1\napp = Sanic(\"Example\")\n\n\n@app.route(\"/\")\nasync def test(request):\n    await asyncio.sleep(3)\n    return response.text(\"Hello, world!\")\n\n\n@app.exception(RequestTimeout)\ndef timeout(request, exception):\n    return response.text(\"RequestTimeout from error_handler.\", 408)\n\n\nif __name__ == \"__main__\":\n    app.run(host=\"0.0.0.0\", port=8000)\n", "metadata": {"source": "github.com/sanic-org/sanic", "file_path": "examples/request_timeout.py", "language": "python", "complexity": "simple", "star_count": 18448, "quality_score": 0.92, "features": {"has_classes": false, "has_functions": true, "has_async": true, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": false, "has_testing": false, "has_logging": false, "function_count": 2, "class_count": 0, "line_count": 25}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "async_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create asynchronous code with proper concurrency handling using Flask/FastAPI decorators\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"\n1. Create a simple Sanic app\n0. Run with an ASGI server:\n    $ uvicorn run_asgi:app\n    or\n    $ hypercorn run_asgi:app\n\"\"\"\n\nfrom pathlib import Path\n\nfrom sanic import Sanic, response\n\n\napp = Sanic(\"Example\")\n\n\n@app.route(\"/text\")\ndef handler_text(request):\n    return response.text(\"Hello\")\n\n\n@app.route(\"/json\")\ndef handler_json(request):\n    return response.json({\"foo\": \"bar\"})\n\n\n@app.websocket(\"/ws\")\nasync def handler_ws(request, ws):\n    name = \"<someone>\"\n    while True:\n        data = f\"Hello {name}\"\n        await ws.send(data)\n        name = await ws.recv()\n\n        if not name:\n            break\n\n\n@app.route(\"/file\")\nasync def handler_file(request):\n    return await response.file(Path(\"../\") / \"setup.py\")\n\n\n@app.route(\"/file_stream\")\nasync def handler_file_stream(request):\n    return await response.file_stream(\n        Path(\"../\") / \"setup.py\", chunk_size=1024\n    )\n\n\n@app.post(\"/stream\", stream=True)\nasync def handler_stream(request):\n    while True:\n        body = await request.stream.read()\n        if body is None:\n            break\n        body = body.decode(\"utf-8\").replace(\"1\", \"A\")\n        await response.write(body)\n    return response.stream(body)\n\n\n@app.before_server_start\nasync def listener_before_server_start(*args, **kwargs):\n    print(\"before_server_start\")\n\n\n@app.after_server_start\nasync def listener_after_server_start(*args, **kwargs):\n    print(\"after_server_start\")\n\n\n@app.before_server_stop\nasync def listener_before_server_stop(*args, **kwargs):\n    print(\"before_server_stop\")\n\n\n@app.after_server_stop\nasync def listener_after_server_stop(*args, **kwargs):\n    print(\"after_server_stop\")\n\n\n@app.on_request\nasync def print_on_request(request):\n    print(\"print_on_request\")\n\n\n@app.on_response\nasync def print_on_response(request, response):\n    print(\"print_on_response\")\n", "metadata": {"source": "github.com/sanic-org/sanic", "file_path": "examples/run_asgi.py", "language": "python", "complexity": "complex", "star_count": 18448, "quality_score": 0.92, "features": {"has_classes": false, "has_functions": true, "has_async": true, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": true, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 12, "class_count": 0, "line_count": 90}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "async_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create asynchronous code with proper concurrency handling\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "__version__ = \"4.0.0a2.dev0\"\n\nfrom typing import TYPE_CHECKING, Tuple\n\nfrom . import hdrs\nfrom .client import (\n    BaseConnector,\n    ClientConnectionError,\n    ClientConnectionResetError,\n    ClientConnectorCertificateError,\n    ClientConnectorDNSError,\n    ClientConnectorError,\n    ClientConnectorSSLError,\n    ClientError,\n    ClientHttpProxyError,\n    ClientOSError,\n    ClientPayloadError,\n    ClientProxyConnectionError,\n    ClientRequest,\n    ClientResponse,\n    ClientResponseError,\n    ClientSession,\n    ClientSSLError,\n    ClientTimeout,\n    ClientWebSocketResponse,\n    ClientWSTimeout,\n    ConnectionTimeoutError,\n    ContentTypeError,\n    Fingerprint,\n    InvalidURL,\n    InvalidUrlClientError,\n    InvalidUrlRedirectClientError,\n    NamedPipeConnector,\n    NonHttpUrlClientError,\n    NonHttpUrlRedirectClientError,\n    RedirectClientError,\n    RequestInfo,\n    ServerConnectionError,\n    ServerDisconnectedError,\n    ServerFingerprintMismatch,\n    ServerTimeoutError,\n    SocketTimeoutError,\n    TCPConnector,\n    TooManyRedirects,\n    UnixConnector,\n    WSMessageTypeError,\n    WSServerHandshakeError,\n    request,\n)\nfrom .client_middleware_digest_auth import DigestAuthMiddleware\nfrom .client_middlewares import ClientHandlerType, ClientMiddlewareType\nfrom .compression_utils import set_zlib_backend\nfrom .connector import AddrInfoType, SocketFactoryType\nfrom .cookiejar import CookieJar, DummyCookieJar\nfrom .formdata import FormData\nfrom .helpers import BasicAuth, ChainMapProxy, ETag\nfrom .http import (\n    HttpVersion,\n    HttpVersion10,\n    HttpVersion11,\n    WebSocketError,\n    WSCloseCode,\n    WSMessage,\n    WSMsgType,\n)\nfrom .multipart import (\n    BadContentDispositionHeader,\n    BadContentDispositionParam,\n    BodyPartReader,\n    MultipartReader,\n    MultipartWriter,\n    content_disposition_filename,\n    parse_content_disposition,\n)\nfrom .payload import (\n    PAYLOAD_REGISTRY,\n    AsyncIterablePayload,\n    BufferedReaderPayload,\n    BytesIOPayload,\n    BytesPayload,\n    IOBasePayload,\n    JsonPayload,\n    Payload,\n    StringIOPayload,\n    StringPayload,\n    TextIOPayload,\n    get_payload,\n    payload_type,\n)\nfrom .resolver import AsyncResolver, DefaultResolver, ThreadedResolver\nfrom .streams import EMPTY_PAYLOAD, DataQueue, EofStream, StreamReader\nfrom .tracing import (\n    TraceConfig,\n    TraceConnectionCreateEndParams,\n    TraceConnectionCreateStartParams,\n    TraceConnectionQueuedEndParams,\n    TraceConnectionQueuedStartParams,\n    TraceConnectionReuseconnParams,\n    TraceDnsCacheHitParams,\n    TraceDnsCacheMissParams,\n    TraceDnsResolveHostEndParams,\n    TraceDnsResolveHostStartParams,\n    TraceRequestChunkSentParams,\n    TraceRequestEndParams,\n    TraceRequestExceptionParams,\n    TraceRequestHeadersSentParams,\n    TraceRequestRedirectParams,\n    TraceRequestStartParams,\n    TraceResponseChunkReceivedParams,\n)\n\nif TYPE_CHECKING:\n    # At runtime these are lazy-loaded at the bottom of the file.\n    from .worker import GunicornUVLoopWebWorker, GunicornWebWorker\n\n__all__: Tuple[str, ...] = (\n    \"hdrs\",\n    # client\n    \"AddrInfoType\",\n    \"BaseConnector\",\n    \"ClientConnectionError\",\n    \"ClientConnectionResetError\",\n    \"ClientConnectorCertificateError\",\n    \"ClientConnectorDNSError\",\n    \"ClientConnectorError\",\n    \"ClientConnectorSSLError\",\n    \"ClientError\",\n    \"ClientHttpProxyError\",\n    \"ClientOSError\",\n    \"ClientPayloadError\",\n    \"ClientProxyConnectionError\",\n    \"ClientResponse\",\n    \"ClientRequest\",\n    \"ClientResponseError\",\n    \"ClientSSLError\",\n    \"ClientSession\",\n    \"ClientTimeout\",\n    \"ClientWebSocketResponse\",\n    \"ClientWSTimeout\",\n    \"ConnectionTimeoutError\",\n    \"ContentTypeError\",\n    \"Fingerprint\",\n    \"InvalidURL\",\n    \"InvalidUrlClientError\",\n    \"InvalidUrlRedirectClientError\",\n    \"NonHttpUrlClientError\",\n    \"NonHttpUrlRedirectClientError\",\n    \"RedirectClientError\",\n    \"RequestInfo\",\n    \"ServerConnectionError\",\n    \"ServerDisconnectedError\",\n    \"ServerFingerprintMismatch\",\n    \"ServerTimeoutError\",\n    \"SocketFactoryType\",\n    \"SocketTimeoutError\",\n    \"TCPConnector\",\n    \"TooManyRedirects\",\n    \"UnixConnector\",\n    \"NamedPipeConnector\",\n    \"WSServerHandshakeError\",\n    \"request\",\n    # client_middleware\n    \"ClientMiddlewareType\",\n    \"ClientHandlerType\",\n    # cookiejar\n    \"CookieJar\",\n    \"DummyCookieJar\",\n    # formdata\n    \"FormData\",\n    # helpers\n    \"BasicAuth\",\n    \"ChainMapProxy\",\n    \"DigestAuthMiddleware\",\n    \"ETag\",\n    \"set_zlib_backend\",\n    # http\n    \"HttpVersion\",\n    \"HttpVersion10\",\n    \"HttpVersion11\",\n    \"WSMsgType\",\n    \"WSCloseCode\",\n    \"WSMessage\",\n    \"WebSocketError\",\n    # multipart\n    \"BadContentDispositionHeader\",\n    \"BadContentDispositionParam\",\n    \"BodyPartReader\",\n    \"MultipartReader\",\n    \"MultipartWriter\",\n    \"content_disposition_filename\",\n    \"parse_content_disposition\",\n    # payload\n    \"AsyncIterablePayload\",\n    \"BufferedReaderPayload\",\n    \"BytesIOPayload\",\n    \"BytesPayload\",\n    \"IOBasePayload\",\n    \"JsonPayload\",\n    \"PAYLOAD_REGISTRY\",\n    \"Payload\",\n    \"StringIOPayload\",\n    \"StringPayload\",\n    \"TextIOPayload\",\n    \"get_payload\",\n    \"payload_type\",\n    # resolver\n    \"AsyncResolver\",\n    \"DefaultResolver\",\n    \"ThreadedResolver\",\n    # streams\n    \"DataQueue\",\n    \"EMPTY_PAYLOAD\",\n    \"EofStream\",\n    \"StreamReader\",\n    # tracing\n    \"TraceConfig\",\n    \"TraceConnectionCreateEndParams\",\n    \"TraceConnectionCreateStartParams\",\n    \"TraceConnectionQueuedEndParams\",\n    \"TraceConnectionQueuedStartParams\",\n    \"TraceConnectionReuseconnParams\",\n    \"TraceDnsCacheHitParams\",\n    \"TraceDnsCacheMissParams\",\n    \"TraceDnsResolveHostEndParams\",\n    \"TraceDnsResolveHostStartParams\",\n    \"TraceRequestChunkSentParams\",\n    \"TraceRequestEndParams\",\n    \"TraceRequestExceptionParams\",\n    \"TraceRequestHeadersSentParams\",\n    \"TraceRequestRedirectParams\",\n    \"TraceRequestStartParams\",\n    \"TraceResponseChunkReceivedParams\",\n    # workers (imported lazily with __getattr__)\n    \"GunicornUVLoopWebWorker\",\n    \"GunicornWebWorker\",\n    \"WSMessageTypeError\",\n)\n\n\ndef __dir__() -> Tuple[str, ...]:\n    return __all__ + (\"__doc__\",)\n\n\ndef __getattr__(name: str) -> object:\n    global GunicornUVLoopWebWorker, GunicornWebWorker\n\n    # Importing gunicorn takes a long time (>100ms), so only import if actually needed.\n    if name in (\"GunicornUVLoopWebWorker\", \"GunicornWebWorker\"):\n        try:\n            from .worker import GunicornUVLoopWebWorker as guv, GunicornWebWorker as gw\n        except ImportError:\n            return None\n\n        GunicornUVLoopWebWorker = guv  # type: ignore[misc]\n        GunicornWebWorker = gw  # type: ignore[misc]\n        return guv if name == \"GunicornUVLoopWebWorker\" else gw\n\n    raise AttributeError(f\"module {__name__} has no attribute {name}\")\n", "metadata": {"source": "github.com/aio-libs/aiohttp", "file_path": "aiohttp/__init__.py", "language": "python", "complexity": "complex", "star_count": 15860, "quality_score": 0.92, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": true, "has_documentation": false, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 2, "class_count": 0, "line_count": 259}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "async_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create asynchronous code with proper concurrency handling\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"\nInternal cookie handling helpers.\n\nThis module contains internal utilities for cookie parsing and manipulation.\nThese are not part of the public API and may change without notice.\n\"\"\"\n\nimport re\nimport sys\nfrom http.cookies import Morsel\nfrom typing import List, Optional, Sequence, Tuple, cast\n\nfrom .log import internal_logger\n\n__all__ = (\n    \"parse_set_cookie_headers\",\n    \"parse_cookie_header\",\n    \"preserve_morsel_with_coded_value\",\n)\n\n# Cookie parsing constants\n# Allow more characters in cookie names to handle real-world cookies\n# that don't strictly follow RFC standards (fixes #2683)\n# RFC 6265 defines cookie-name token as per RFC 2616 Section 2.2,\n# but many servers send cookies with characters like {} [] () etc.\n# This makes the cookie parser more tolerant of real-world cookies\n# while still providing some validation to catch obviously malformed names.\n_COOKIE_NAME_RE = re.compile(r\"^[!#$%&\\'()*+\\-./0-9:<=>?@A-Z\\[\\]^_`a-z{|}~]+$\")\n_COOKIE_KNOWN_ATTRS = frozenset(  # AKA Morsel._reserved\n    (\n        \"path\",\n        \"domain\",\n        \"max-age\",\n        \"expires\",\n        \"secure\",\n        \"httponly\",\n        \"samesite\",\n        \"partitioned\",\n        \"version\",\n        \"comment\",\n    )\n)\n_COOKIE_BOOL_ATTRS = frozenset(  # AKA Morsel._flags\n    (\"secure\", \"httponly\", \"partitioned\")\n)\n\n# SimpleCookie's pattern for parsing cookies with relaxed validation\n# Based on http.cookies pattern but extended to allow more characters in cookie names\n# to handle real-world cookies (fixes #2683)\n_COOKIE_PATTERN = re.compile(\n    r\"\"\"\n    \\s*                            # Optional whitespace at start of cookie\n    (?P<key>                       # Start of group 'key'\n    # aiohttp has extended to include [] for compatibility with real-world cookies\n    [\\w\\d!#%&'~_`><@,:/\\$\\*\\+\\-\\.\\^\\|\\)\\(\\?\\}\\{\\=\\[\\]]+?   # Any word of at least one letter\n    )                              # End of group 'key'\n    (                              # Optional group: there may not be a value.\n    \\s*=\\s*                          # Equal Sign\n    (?P<val>                         # Start of group 'val'\n    \"(?:[^\\\\\"]|\\\\.)*\"                  # Any double-quoted string (properly closed)\n    |                                  # or\n    \"[^\";]*                            # Unmatched opening quote (differs from SimpleCookie - issue #7993)\n    |                                  # or\n    # Special case for \"expires\" attr - RFC 822, RFC 850, RFC 1036, RFC 1123\n    (\\w{3,6}day|\\w{3}),\\s              # Day of the week or abbreviated day (with comma)\n    [\\w\\d\\s-]{9,11}\\s[\\d:]{8}\\s        # Date and time in specific format\n    (GMT|[+-]\\d{4})                     # Timezone: GMT or RFC 2822 offset like -0000, +0100\n                                        # NOTE: RFC 2822 timezone support is an aiohttp extension\n                                        # for issue #4493 - SimpleCookie does NOT support this\n    |                                  # or\n    # ANSI C asctime() format: \"Wed Jun  9 10:18:14 2021\"\n    # NOTE: This is an aiohttp extension for issue #4327 - SimpleCookie does NOT support this format\n    \\w{3}\\s+\\w{3}\\s+[\\s\\d]\\d\\s+\\d{2}:\\d{2}:\\d{2}\\s+\\d{4}\n    |                                  # or\n    [\\w\\d!#%&'~_`><@,:/\\$\\*\\+\\-\\.\\^\\|\\)\\(\\?\\}\\{\\=\\[\\]]*      # Any word or empty string\n    )                                # End of group 'val'\n    )?                             # End of optional value group\n    \\s*                            # Any number of spaces.\n    (\\s+|;|$)                      # Ending either at space, semicolon, or EOS.\n    \"\"\",\n    re.VERBOSE | re.ASCII,\n)\n\n\ndef preserve_morsel_with_coded_value(cookie: Morsel[str]) -> Morsel[str]:\n    \"\"\"\n    Preserve a Morsel's coded_value exactly as received from the server.\n\n    This function ensures that cookie encoding is preserved exactly as sent by\n    the server, which is critical for compatibility with old servers that have\n    strict requirements about cookie formats.\n\n    This addresses the issue described in https://github.com/aio-libs/aiohttp/pull/1453\n    where Python's SimpleCookie would re-encode cookies, breaking authentication\n    with certain servers.\n\n    Args:\n        cookie: A Morsel object from SimpleCookie\n\n    Returns:\n        A Morsel object with preserved coded_value\n\n    \"\"\"\n    mrsl_val = cast(\"Morsel[str]\", cookie.get(cookie.key, Morsel()))\n    # We use __setstate__ instead of the public set() API because it allows us to\n    # bypass validation and set already validated state. This is more stable than\n    # setting protected attributes directly and unlikely to change since it would\n    # break pickling.\n    mrsl_val.__setstate__(  # type: ignore[attr-defined]\n        {\"key\": cookie.key, \"value\": cookie.value, \"coded_value\": cookie.coded_value}\n    )\n    return mrsl_val\n\n\n_unquote_sub = re.compile(r\"\\\\(?:([0-3][0-7][0-7])|(.))\").sub\n\n\ndef _unquote_replace(m: re.Match[str]) -> str:\n    \"\"\"\n    Replace function for _unquote_sub regex substitution.\n\n    Handles escaped characters in cookie values:\n    - Octal sequences are converted to their character representation\n    - Other escaped characters are unescaped by removing the backslash\n    \"\"\"\n    if m[1]:\n        return chr(int(m[1], 8))\n    return m[2]\n\n\ndef _unquote(value: str) -> str:\n    \"\"\"\n    Unquote a cookie value.\n\n    Vendored from http.cookies._unquote to ensure compatibility.\n\n    Note: The original implementation checked for None, but we've removed\n    that check since all callers already ensure the value is not None.\n    \"\"\"\n    # If there aren't any doublequotes,\n    # then there can't be any special characters.  See RFC 2109.\n    if len(value) < 2:\n        return value\n    if value[0] != '\"' or value[-1] != '\"':\n        return value\n\n    # We have to assume that we must decode this string.\n    # Down to work.\n\n    # Remove the \"s\n    value = value[1:-1]\n\n    # Check for special sequences.  Examples:\n    #    \\012 --> \\n\n    #    \\\"   --> \"\n    #\n    return _unquote_sub(_unquote_replace, value)\n\n\ndef parse_cookie_header(header: str) -> List[Tuple[str, Morsel[str]]]:\n    \"\"\"\n    Parse a Cookie header according to RFC 6265 Section 5.4.\n\n    Cookie headers contain only name-value pairs separated by semicolons.\n    There are no attributes in Cookie headers - even names that match\n    attribute names (like 'path' or 'secure') should be treated as cookies.\n\n    This parser uses the same regex-based approach as parse_set_cookie_headers\n    to properly handle quoted values that may contain semicolons.\n\n    Args:\n        header: The Cookie header value to parse\n\n    Returns:\n        List of (name, Morsel) tuples for compatibility with SimpleCookie.update()\n    \"\"\"\n    if not header:\n        return []\n\n    cookies: List[Tuple[str, Morsel[str]]] = []\n    i = 0\n    n = len(header)\n\n    while i < n:\n        # Use the same pattern as parse_set_cookie_headers to find cookies\n        match = _COOKIE_PATTERN.match(header, i)\n        if not match:\n            break\n\n        key = match.group(\"key\")\n        value = match.group(\"val\") or \"\"\n        i = match.end(0)\n\n        # Validate the name\n        if not key or not _COOKIE_NAME_RE.match(key):\n            internal_logger.warning(\"Can not load cookie: Illegal cookie name %r\", key)\n            continue\n\n        # Create new morsel\n        morsel: Morsel[str] = Morsel()\n        # Preserve the original value as coded_value (with quotes if present)\n        # We use __setstate__ instead of the public set() API because it allows us to\n        # bypass validation and set already validated state. This is more stable than\n        # setting protected attributes directly and unlikely to change since it would\n        # break pickling.\n        morsel.__setstate__(  # type: ignore[attr-defined]\n            {\"key\": key, \"value\": _unquote(value), \"coded_value\": value}\n        )\n\n        cookies.append((key, morsel))\n\n    return cookies\n\n\ndef parse_set_cookie_headers(headers: Sequence[str]) -> List[Tuple[str, Morsel[str]]]:\n    \"\"\"\n    Parse cookie headers using a vendored version of SimpleCookie parsing.\n\n    This implementation is based on SimpleCookie.__parse_string to ensure\n    compatibility with how SimpleCookie parses cookies, including handling\n    of malformed cookies with missing semicolons.\n\n    This function is used for both Cookie and Set-Cookie headers in order to be\n    forgiving. Ideally we would have followed RFC 6265 Section 5.2 (for Cookie\n    headers) and RFC 6265 Section 4.2.1 (for Set-Cookie headers), but the\n    real world data makes it impossible since we need to be a bit more forgiving.\n\n    NOTE: This implementation differs from SimpleCookie in handling unmatched quotes.\n    SimpleCookie will stop parsing when it encounters a cookie value with an unmatched\n    quote (e.g., 'cookie=\"value'), causing subsequent cookies to be silently dropped.\n    This implementation handles unmatched quotes more gracefully to prevent cookie loss.\n    See https://github.com/aio-libs/aiohttp/issues/7993\n    \"\"\"\n    parsed_cookies: List[Tuple[str, Morsel[str]]] = []\n\n    for header in headers:\n        if not header:\n            continue\n\n        # Parse cookie string using SimpleCookie's algorithm\n        i = 0\n        n = len(header)\n        current_morsel: Optional[Morsel[str]] = None\n        morsel_seen = False\n\n        while 0 <= i < n:\n            # Start looking for a cookie\n            match = _COOKIE_PATTERN.match(header, i)\n            if not match:\n                # No more cookies\n                break\n\n            key, value = match.group(\"key\"), match.group(\"val\")\n            i = match.end(0)\n            lower_key = key.lower()\n\n            if key[0] == \"$\":\n                if not morsel_seen:\n                    # We ignore attributes which pertain to the cookie\n                    # mechanism as a whole, such as \"$Version\".\n                    continue\n                # Process as attribute\n                if current_morsel is not None:\n                    attr_lower_key = lower_key[1:]\n                    if attr_lower_key in _COOKIE_KNOWN_ATTRS:\n                        current_morsel[attr_lower_key] = value or \"\"\n            elif lower_key in _COOKIE_KNOWN_ATTRS:\n                if not morsel_seen:\n                    # Invalid cookie string - attribute before cookie\n                    break\n                if lower_key in _COOKIE_BOOL_ATTRS:\n                    # Boolean attribute with any value should be True\n                    if current_morsel is not None:\n                        if lower_key == \"partitioned\" and sys.version_info < (3, 14):\n                            dict.__setitem__(current_morsel, lower_key, True)\n                        else:\n                            current_morsel[lower_key] = True\n                elif value is None:\n                    # Invalid cookie string - non-boolean attribute without value\n                    break\n                elif current_morsel is not None:\n                    # Regular attribute with value\n                    current_morsel[lower_key] = _unquote(value)\n            elif value is not None:\n                # This is a cookie name=value pair\n                # Validate the name\n                if key in _COOKIE_KNOWN_ATTRS or not _COOKIE_NAME_RE.match(key):\n                    internal_logger.warning(\n                        \"Can not load cookies: Illegal cookie name %r\", key\n                    )\n                    current_morsel = None\n                else:\n                    # Create new morsel\n                    current_morsel = Morsel()\n                    # Preserve the original value as coded_value (with quotes if present)\n                    # We use __setstate__ instead of the public set() API because it allows us to\n                    # bypass validation and set already validated state. This is more stable than\n                    # setting protected attributes directly and unlikely to change since it would\n                    # break pickling.\n                    current_morsel.__setstate__(  # type: ignore[attr-defined]\n                        {\"key\": key, \"value\": _unquote(value), \"coded_value\": value}\n                    )\n                    parsed_cookies.append((key, current_morsel))\n                    morsel_seen = True\n            else:\n                # Invalid cookie string - no value for non-attribute\n                break\n\n    return parsed_cookies\n", "metadata": {"source": "github.com/aio-libs/aiohttp", "file_path": "aiohttp/_cookie_helpers.py", "language": "python", "complexity": "complex", "star_count": 15860, "quality_score": 0.9500000000000001, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": true, "has_type_hints": true, "has_testing": false, "has_logging": true, "function_count": 5, "class_count": 0, "line_count": 310}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "async_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create asynchronous code with proper concurrency handling with SQLAlchemy/Django ORM patterns\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"Models for WebSocket protocol versions 13 and 8.\"\"\"\n\nimport json\nfrom enum import IntEnum\nfrom typing import Any, Callable, Final, Literal, NamedTuple, Optional, Union, cast\n\nWS_DEFLATE_TRAILING: Final[bytes] = bytes([0x00, 0x00, 0xFF, 0xFF])\n\n\nclass WSCloseCode(IntEnum):\n    OK = 1000\n    GOING_AWAY = 1001\n    PROTOCOL_ERROR = 1002\n    UNSUPPORTED_DATA = 1003\n    ABNORMAL_CLOSURE = 1006\n    INVALID_TEXT = 1007\n    POLICY_VIOLATION = 1008\n    MESSAGE_TOO_BIG = 1009\n    MANDATORY_EXTENSION = 1010\n    INTERNAL_ERROR = 1011\n    SERVICE_RESTART = 1012\n    TRY_AGAIN_LATER = 1013\n    BAD_GATEWAY = 1014\n\n\nclass WSMsgType(IntEnum):\n    # websocket spec types\n    CONTINUATION = 0x0\n    TEXT = 0x1\n    BINARY = 0x2\n    PING = 0x9\n    PONG = 0xA\n    CLOSE = 0x8\n\n    # aiohttp specific types\n    CLOSING = 0x100\n    CLOSED = 0x101\n    ERROR = 0x102\n\n\nclass WSMessageContinuation(NamedTuple):\n    data: bytes\n    size: int\n    extra: Optional[str] = None\n    type: Literal[WSMsgType.CONTINUATION] = WSMsgType.CONTINUATION\n\n\nclass WSMessageText(NamedTuple):\n    data: str\n    size: int\n    extra: Optional[str] = None\n    type: Literal[WSMsgType.TEXT] = WSMsgType.TEXT\n\n    def json(\n        self, *, loads: Callable[[Union[str, bytes, bytearray]], Any] = json.loads\n    ) -> Any:\n        \"\"\"Return parsed JSON data.\"\"\"\n        return loads(self.data)\n\n\nclass WSMessageBinary(NamedTuple):\n    data: bytes\n    size: int\n    extra: Optional[str] = None\n    type: Literal[WSMsgType.BINARY] = WSMsgType.BINARY\n\n    def json(\n        self, *, loads: Callable[[Union[str, bytes, bytearray]], Any] = json.loads\n    ) -> Any:\n        \"\"\"Return parsed JSON data.\"\"\"\n        return loads(self.data)\n\n\nclass WSMessagePing(NamedTuple):\n    data: bytes\n    size: int\n    extra: Optional[str] = None\n    type: Literal[WSMsgType.PING] = WSMsgType.PING\n\n\nclass WSMessagePong(NamedTuple):\n    data: bytes\n    size: int\n    extra: Optional[str] = None\n    type: Literal[WSMsgType.PONG] = WSMsgType.PONG\n\n\nclass WSMessageClose(NamedTuple):\n    data: int\n    size: int\n    extra: Optional[str] = None\n    type: Literal[WSMsgType.CLOSE] = WSMsgType.CLOSE\n\n\nclass WSMessageClosing(NamedTuple):\n    data: None = None\n    size: int = 0\n    extra: Optional[str] = None\n    type: Literal[WSMsgType.CLOSING] = WSMsgType.CLOSING\n\n\nclass WSMessageClosed(NamedTuple):\n    data: None = None\n    size: int = 0\n    extra: Optional[str] = None\n    type: Literal[WSMsgType.CLOSED] = WSMsgType.CLOSED\n\n\nclass WSMessageError(NamedTuple):\n    data: BaseException\n    size: int = 0\n    extra: Optional[str] = None\n    type: Literal[WSMsgType.ERROR] = WSMsgType.ERROR\n\n\nWSMessage = Union[\n    WSMessageContinuation,\n    WSMessageText,\n    WSMessageBinary,\n    WSMessagePing,\n    WSMessagePong,\n    WSMessageClose,\n    WSMessageClosing,\n    WSMessageClosed,\n    WSMessageError,\n]\n\nWS_CLOSED_MESSAGE = WSMessageClosed()\nWS_CLOSING_MESSAGE = WSMessageClosing()\n\n\nclass WebSocketError(Exception):\n    \"\"\"WebSocket protocol parser error.\"\"\"\n\n    def __init__(self, code: int, message: str) -> None:\n        self.code = code\n        super().__init__(code, message)\n\n    def __str__(self) -> str:\n        return cast(str, self.args[1])\n\n\nclass WSHandshakeError(Exception):\n    \"\"\"WebSocket protocol handshake error.\"\"\"\n", "metadata": {"source": "github.com/aio-libs/aiohttp", "file_path": "aiohttp/_websocket/models.py", "language": "python", "complexity": "complex", "star_count": 15860, "quality_score": 0.9500000000000001, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": true, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 4, "class_count": 13, "line_count": 145}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "async_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create asynchronous code with proper concurrency handling with asyncio coroutines\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"Reader for WebSocket protocol versions 13 and 8.\"\"\"\n\nimport asyncio\nimport builtins\nfrom collections import deque\nfrom typing import Deque, Final, Optional, Set, Tuple, Type, Union\n\nfrom ..base_protocol import BaseProtocol\nfrom ..compression_utils import ZLibDecompressor\nfrom ..helpers import _EXC_SENTINEL, set_exception\nfrom ..streams import EofStream\nfrom .helpers import UNPACK_CLOSE_CODE, UNPACK_LEN3, websocket_mask\nfrom .models import (\n    WS_DEFLATE_TRAILING,\n    WebSocketError,\n    WSCloseCode,\n    WSMessage,\n    WSMessageBinary,\n    WSMessageClose,\n    WSMessagePing,\n    WSMessagePong,\n    WSMessageText,\n    WSMsgType,\n)\n\nALLOWED_CLOSE_CODES: Final[Set[int]] = {int(i) for i in WSCloseCode}\n\n# States for the reader, used to parse the WebSocket frame\n# integer values are used so they can be cythonized\nREAD_HEADER = 1\nREAD_PAYLOAD_LENGTH = 2\nREAD_PAYLOAD_MASK = 3\nREAD_PAYLOAD = 4\n\nWS_MSG_TYPE_BINARY = WSMsgType.BINARY\nWS_MSG_TYPE_TEXT = WSMsgType.TEXT\n\n# WSMsgType values unpacked so they can by cythonized to ints\nOP_CODE_NOT_SET = -1\nOP_CODE_CONTINUATION = WSMsgType.CONTINUATION.value\nOP_CODE_TEXT = WSMsgType.TEXT.value\nOP_CODE_BINARY = WSMsgType.BINARY.value\nOP_CODE_CLOSE = WSMsgType.CLOSE.value\nOP_CODE_PING = WSMsgType.PING.value\nOP_CODE_PONG = WSMsgType.PONG.value\n\nEMPTY_FRAME_ERROR = (True, b\"\")\nEMPTY_FRAME = (False, b\"\")\n\nCOMPRESSED_NOT_SET = -1\nCOMPRESSED_FALSE = 0\nCOMPRESSED_TRUE = 1\n\nTUPLE_NEW = tuple.__new__\n\ncython_int = int  # Typed to int in Python, but cython with use a signed int in the pxd\n\n\nclass WebSocketDataQueue:\n    \"\"\"WebSocketDataQueue resumes and pauses an underlying stream.\n\n    It is a destination for WebSocket data.\n    \"\"\"\n\n    def __init__(\n        self, protocol: BaseProtocol, limit: int, *, loop: asyncio.AbstractEventLoop\n    ) -> None:\n        self._size = 0\n        self._protocol = protocol\n        self._limit = limit * 2\n        self._loop = loop\n        self._eof = False\n        self._waiter: Optional[asyncio.Future[None]] = None\n        self._exception: Union[Type[BaseException], BaseException, None] = None\n        self._buffer: Deque[WSMessage] = deque()\n        self._get_buffer = self._buffer.popleft\n        self._put_buffer = self._buffer.append\n\n    def is_eof(self) -> bool:\n        return self._eof\n\n    def exception(self) -> Optional[Union[Type[BaseException], BaseException]]:\n        return self._exception\n\n    def set_exception(\n        self,\n        exc: Union[Type[BaseException], BaseException],\n        exc_cause: builtins.BaseException = _EXC_SENTINEL,\n    ) -> None:\n        self._eof = True\n        self._exception = exc\n        if (waiter := self._waiter) is not None:\n            self._waiter = None\n            set_exception(waiter, exc, exc_cause)\n\n    def _release_waiter(self) -> None:\n        if (waiter := self._waiter) is None:\n            return\n        self._waiter = None\n        if not waiter.done():\n            waiter.set_result(None)\n\n    def feed_eof(self) -> None:\n        self._eof = True\n        self._release_waiter()\n        self._exception = None  # Break cyclic references\n\n    def feed_data(self, data: \"WSMessage\") -> None:\n        size = data.size\n        self._size += size\n        self._put_buffer(data)\n        self._release_waiter()\n        if self._size > self._limit and not self._protocol._reading_paused:\n            self._protocol.pause_reading()\n\n    async def read(self) -> WSMessage:\n        if not self._buffer and not self._eof:\n            assert not self._waiter\n            self._waiter = self._loop.create_future()\n            try:\n                await self._waiter\n            except (asyncio.CancelledError, asyncio.TimeoutError):\n                self._waiter = None\n                raise\n        return self._read_from_buffer()\n\n    def _read_from_buffer(self) -> WSMessage:\n        if self._buffer:\n            data = self._get_buffer()\n            size = data.size\n            self._size -= size\n            if self._size < self._limit and self._protocol._reading_paused:\n                self._protocol.resume_reading()\n            return data\n        if self._exception is not None:\n            raise self._exception\n        raise EofStream\n\n\nclass WebSocketReader:\n    def __init__(\n        self, queue: WebSocketDataQueue, max_msg_size: int, compress: bool = True\n    ) -> None:\n        self.queue = queue\n        self._max_msg_size = max_msg_size\n\n        self._exc: Optional[Exception] = None\n        self._partial = bytearray()\n        self._state = READ_HEADER\n\n        self._opcode: int = OP_CODE_NOT_SET\n        self._frame_fin = False\n        self._frame_opcode: int = OP_CODE_NOT_SET\n        self._payload_fragments: list[bytes] = []\n        self._frame_payload_len = 0\n\n        self._tail: bytes = b\"\"\n        self._has_mask = False\n        self._frame_mask: Optional[bytes] = None\n        self._payload_bytes_to_read = 0\n        self._payload_len_flag = 0\n        self._compressed: int = COMPRESSED_NOT_SET\n        self._decompressobj: Optional[ZLibDecompressor] = None\n        self._compress = compress\n\n    def feed_eof(self) -> None:\n        self.queue.feed_eof()\n\n    # data can be bytearray on Windows because proactor event loop uses bytearray\n    # and asyncio types this to Union[bytes, bytearray, memoryview] so we need\n    # coerce data to bytes if it is not\n    def feed_data(\n        self, data: Union[bytes, bytearray, memoryview]\n    ) -> Tuple[bool, bytes]:\n        if type(data) is not bytes:\n            data = bytes(data)\n\n        if self._exc is not None:\n            return True, data\n\n        try:\n            self._feed_data(data)\n        except Exception as exc:\n            self._exc = exc\n            set_exception(self.queue, exc)\n            return EMPTY_FRAME_ERROR\n\n        return EMPTY_FRAME\n\n    def _handle_frame(\n        self,\n        fin: bool,\n        opcode: Union[int, cython_int],  # Union intended: Cython pxd uses C int\n        payload: Union[bytes, bytearray],\n        compressed: Union[int, cython_int],  # Union intended: Cython pxd uses C int\n    ) -> None:\n        msg: WSMessage\n        if opcode in {OP_CODE_TEXT, OP_CODE_BINARY, OP_CODE_CONTINUATION}:\n            # load text/binary\n            if not fin:\n                # got partial frame payload\n                if opcode != OP_CODE_CONTINUATION:\n                    self._opcode = opcode\n                self._partial += payload\n                if self._max_msg_size and len(self._partial) >= self._max_msg_size:\n                    raise WebSocketError(\n                        WSCloseCode.MESSAGE_TOO_BIG,\n                        f\"Message size {len(self._partial)} \"\n                        f\"exceeds limit {self._max_msg_size}\",\n                    )\n                return\n\n            has_partial = bool(self._partial)\n            if opcode == OP_CODE_CONTINUATION:\n                if self._opcode == OP_CODE_NOT_SET:\n                    raise WebSocketError(\n                        WSCloseCode.PROTOCOL_ERROR,\n                        \"Continuation frame for non started message\",\n                    )\n                opcode = self._opcode\n                self._opcode = OP_CODE_NOT_SET\n            # previous frame was non finished\n            # we should get continuation opcode\n            elif has_partial:\n                raise WebSocketError(\n                    WSCloseCode.PROTOCOL_ERROR,\n                    \"The opcode in non-fin frame is expected \"\n                    f\"to be zero, got {opcode!r}\",\n                )\n\n            assembled_payload: Union[bytes, bytearray]\n            if has_partial:\n                assembled_payload = self._partial + payload\n                self._partial.clear()\n            else:\n                assembled_payload = payload\n\n            if self._max_msg_size and len(assembled_payload) >= self._max_msg_size:\n                raise WebSocketError(\n                    WSCloseCode.MESSAGE_TOO_BIG,\n                    f\"Message size {len(assembled_payload)} \"\n                    f\"exceeds limit {self._max_msg_size}\",\n                )\n\n            # Decompress process must to be done after all packets\n            # received.\n            if compressed:\n                if not self._decompressobj:\n                    self._decompressobj = ZLibDecompressor(suppress_deflate_header=True)\n                # XXX: It's possible that the zlib backend (isal is known to\n                # do this, maybe others too?) will return max_length bytes,\n                # but internally buffer more data such that the payload is\n                # >max_length, so we return one extra byte and if we're able\n                # to do that, then the message is too big.\n                payload_merged = self._decompressobj.decompress_sync(\n                    assembled_payload + WS_DEFLATE_TRAILING,\n                    (\n                        self._max_msg_size + 1\n                        if self._max_msg_size\n                        else self._max_msg_size\n                    ),\n                )\n                if self._max_msg_size and len(payload_merged) > self._max_msg_size:\n                    raise WebSocketError(\n                        WSCloseCode.MESSAGE_TOO_BIG,\n                        f\"Decompressed message exceeds size limit {self._max_msg_size}\",\n                    )\n            elif type(assembled_payload) is bytes:\n                payload_merged = assembled_payload\n            else:\n                payload_merged = bytes(assembled_payload)\n\n            size = len(payload_merged)\n            if opcode == OP_CODE_TEXT:\n                try:\n                    text = payload_merged.decode(\"utf-8\")\n                except UnicodeDecodeError as exc:\n                    raise WebSocketError(\n                        WSCloseCode.INVALID_TEXT, \"Invalid UTF-8 text message\"\n                    ) from exc\n\n                # XXX: The Text and Binary messages here can be a performance\n                # bottleneck, so we use tuple.__new__ to improve performance.\n                # This is not type safe, but many tests should fail in\n                # test_client_ws_functional.py if this is wrong.\n                msg = TUPLE_NEW(WSMessageText, (text, size, \"\", WS_MSG_TYPE_TEXT))\n            else:\n                msg = TUPLE_NEW(\n                    WSMessageBinary, (payload_merged, size, \"\", WS_MSG_TYPE_BINARY)\n                )\n\n            self.queue.feed_data(msg)\n        elif opcode == OP_CODE_CLOSE:\n            payload_len = len(payload)\n            if payload_len >= 2:\n                close_code = UNPACK_CLOSE_CODE(payload[:2])[0]\n                if close_code < 3000 and close_code not in ALLOWED_CLOSE_CODES:\n                    raise WebSocketError(\n                        WSCloseCode.PROTOCOL_ERROR,\n                        f\"Invalid close code: {close_code}\",\n                    )\n                try:\n                    close_message = payload[2:].decode(\"utf-8\")\n                except UnicodeDecodeError as exc:\n                    raise WebSocketError(\n                        WSCloseCode.INVALID_TEXT, \"Invalid UTF-8 text message\"\n                    ) from exc\n                msg = WSMessageClose(\n                    data=close_code, size=payload_len, extra=close_message\n                )\n            elif payload:\n                raise WebSocketError(\n                    WSCloseCode.PROTOCOL_ERROR,\n                    f\"Invalid close frame: {fin} {opcode} {payload!r}\",\n                )\n            else:\n                msg = WSMessageClose(data=0, size=payload_len, extra=\"\")\n\n            self.queue.feed_data(msg)\n        elif opcode == OP_CODE_PING:\n            self.queue.feed_data(\n                WSMessagePing(data=bytes(payload), size=len(payload), extra=\"\")\n            )\n        elif opcode == OP_CODE_PONG:\n            self.queue.feed_data(\n                WSMessagePong(data=bytes(payload), size=len(payload), extra=\"\")\n            )\n        else:\n            raise WebSocketError(\n                WSCloseCode.PROTOCOL_ERROR, f\"Unexpected opcode={opcode!r}\"\n            )\n\n    def _feed_data(self, data: bytes) -> None:\n        \"\"\"Return the next frame from the socket.\"\"\"\n        if self._tail:\n            data, self._tail = self._tail + data, b\"\"\n\n        start_pos: int = 0\n        data_len = len(data)\n        data_cstr = data\n\n        while True:\n            # read header\n            if self._state == READ_HEADER:\n                if data_len - start_pos < 2:\n                    break\n                first_byte = data_cstr[start_pos]\n                second_byte = data_cstr[start_pos + 1]\n                start_pos += 2\n\n                fin = (first_byte >> 7) & 1\n                rsv1 = (first_byte >> 6) & 1\n                rsv2 = (first_byte >> 5) & 1\n                rsv3 = (first_byte >> 4) & 1\n                opcode = first_byte & 0xF\n\n                # frame-fin = %x0 ; more frames of this message follow\n                #           / %x1 ; final frame of this message\n                # frame-rsv1 = %x0 ;\n                #    1 bit, MUST be 0 unless negotiated otherwise\n                # frame-rsv2 = %x0 ;\n                #    1 bit, MUST be 0 unless negotiated otherwise\n                # frame-rsv3 = %x0 ;\n                #    1 bit, MUST be 0 unless negotiated otherwise\n                #\n                # Remove rsv1 from this test for deflate development\n                if rsv2 or rsv3 or (rsv1 and not self._compress):\n                    raise WebSocketError(\n                        WSCloseCode.PROTOCOL_ERROR,\n                        \"Received frame with non-zero reserved bits\",\n                    )\n\n                if opcode > 0x7 and fin == 0:\n                    raise WebSocketError(\n                        WSCloseCode.PROTOCOL_ERROR,\n                        \"Received fragmented control frame\",\n                    )\n\n                has_mask = (second_byte >> 7) & 1\n                length = second_byte & 0x7F\n\n                # Control frames MUST have a payload\n                # length of 125 bytes or less\n                if opcode > 0x7 and length > 125:\n                    raise WebSocketError(\n                        WSCloseCode.PROTOCOL_ERROR,\n                        \"Control frame payload cannot be larger than 125 bytes\",\n                    )\n\n                # Set compress status if last package is FIN\n                # OR set compress status if this is first fragment\n                # Raise error if not first fragment with rsv1 = 0x1\n                if self._frame_fin or self._compressed == COMPRESSED_NOT_SET:\n                    self._compressed = COMPRESSED_TRUE if rsv1 else COMPRESSED_FALSE\n                elif rsv1:\n                    raise WebSocketError(\n                        WSCloseCode.PROTOCOL_ERROR,\n                        \"Received frame with non-zero reserved bits\",\n                    )\n\n                self._frame_fin = bool(fin)\n                self._frame_opcode = opcode\n                self._has_mask = bool(has_mask)\n                self._payload_len_flag = length\n                self._state = READ_PAYLOAD_LENGTH\n\n            # read payload length\n            if self._state == READ_PAYLOAD_LENGTH:\n                len_flag = self._payload_len_flag\n                if len_flag == 126:\n                    if data_len - start_pos < 2:\n                        break\n                    first_byte = data_cstr[start_pos]\n                    second_byte = data_cstr[start_pos + 1]\n                    start_pos += 2\n                    self._payload_bytes_to_read = first_byte << 8 | second_byte\n                elif len_flag > 126:\n                    if data_len - start_pos < 8:\n                        break\n                    self._payload_bytes_to_read = UNPACK_LEN3(data, start_pos)[0]\n                    start_pos += 8\n                else:\n                    self._payload_bytes_to_read = len_flag\n\n                self._state = READ_PAYLOAD_MASK if self._has_mask else READ_PAYLOAD\n\n            # read payload mask\n            if self._state == READ_PAYLOAD_MASK:\n                if data_len - start_pos < 4:\n                    break\n                self._frame_mask = data_cstr[start_pos : start_pos + 4]\n                start_pos += 4\n                self._state = READ_PAYLOAD\n\n            if self._state == READ_PAYLOAD:\n                chunk_len = data_len - start_pos\n                if self._payload_bytes_to_read >= chunk_len:\n                    f_end_pos = data_len\n                    self._payload_bytes_to_read -= chunk_len\n                else:\n                    f_end_pos = start_pos + self._payload_bytes_to_read\n                    self._payload_bytes_to_read = 0\n\n                had_fragments = self._frame_payload_len\n                self._frame_payload_len += f_end_pos - start_pos\n                f_start_pos = start_pos\n                start_pos = f_end_pos\n\n                if self._payload_bytes_to_read != 0:\n                    # If we don't have a complete frame, we need to save the\n                    # data for the next call to feed_data.\n                    self._payload_fragments.append(data_cstr[f_start_pos:f_end_pos])\n                    break\n\n                payload: Union[bytes, bytearray]\n                if had_fragments:\n                    # We have to join the payload fragments get the payload\n                    self._payload_fragments.append(data_cstr[f_start_pos:f_end_pos])\n                    if self._has_mask:\n                        assert self._frame_mask is not None\n                        payload_bytearray = bytearray(b\"\".join(self._payload_fragments))\n                        websocket_mask(self._frame_mask, payload_bytearray)\n                        payload = payload_bytearray\n                    else:\n                        payload = b\"\".join(self._payload_fragments)\n                    self._payload_fragments.clear()\n                elif self._has_mask:\n                    assert self._frame_mask is not None\n                    payload_bytearray = data_cstr[f_start_pos:f_end_pos]  # type: ignore[assignment]\n                    if type(payload_bytearray) is not bytearray:  # pragma: no branch\n                        # Cython will do the conversion for us\n                        # but we need to do it for Python and we\n                        # will always get here in Python\n                        payload_bytearray = bytearray(payload_bytearray)\n                    websocket_mask(self._frame_mask, payload_bytearray)\n                    payload = payload_bytearray\n                else:\n                    payload = data_cstr[f_start_pos:f_end_pos]\n\n                self._handle_frame(\n                    self._frame_fin, self._frame_opcode, payload, self._compressed\n                )\n                self._frame_payload_len = 0\n                self._state = READ_HEADER\n\n        # XXX: Cython needs slices to be bounded, so we can't omit the slice end here.\n        self._tail = data_cstr[start_pos:data_len] if start_pos < data_len else b\"\"\n", "metadata": {"source": "github.com/aio-libs/aiohttp", "file_path": "aiohttp/_websocket/reader_c.py", "language": "python", "complexity": "complex", "star_count": 15860, "quality_score": 1.0, "features": {"has_classes": true, "has_functions": true, "has_async": true, "has_decorators": false, "has_imports": true, "has_error_handling": true, "has_documentation": true, "has_type_hints": true, "has_testing": true, "has_logging": false, "function_count": 14, "class_count": 2, "line_count": 488}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "async_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create asynchronous code with proper concurrency handling with asyncio coroutines\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"Reader for WebSocket protocol versions 13 and 8.\"\"\"\n\nimport asyncio\nimport builtins\nfrom collections import deque\nfrom typing import Deque, Final, Optional, Set, Tuple, Type, Union\n\nfrom ..base_protocol import BaseProtocol\nfrom ..compression_utils import ZLibDecompressor\nfrom ..helpers import _EXC_SENTINEL, set_exception\nfrom ..streams import EofStream\nfrom .helpers import UNPACK_CLOSE_CODE, UNPACK_LEN3, websocket_mask\nfrom .models import (\n    WS_DEFLATE_TRAILING,\n    WebSocketError,\n    WSCloseCode,\n    WSMessage,\n    WSMessageBinary,\n    WSMessageClose,\n    WSMessagePing,\n    WSMessagePong,\n    WSMessageText,\n    WSMsgType,\n)\n\nALLOWED_CLOSE_CODES: Final[Set[int]] = {int(i) for i in WSCloseCode}\n\n# States for the reader, used to parse the WebSocket frame\n# integer values are used so they can be cythonized\nREAD_HEADER = 1\nREAD_PAYLOAD_LENGTH = 2\nREAD_PAYLOAD_MASK = 3\nREAD_PAYLOAD = 4\n\nWS_MSG_TYPE_BINARY = WSMsgType.BINARY\nWS_MSG_TYPE_TEXT = WSMsgType.TEXT\n\n# WSMsgType values unpacked so they can by cythonized to ints\nOP_CODE_NOT_SET = -1\nOP_CODE_CONTINUATION = WSMsgType.CONTINUATION.value\nOP_CODE_TEXT = WSMsgType.TEXT.value\nOP_CODE_BINARY = WSMsgType.BINARY.value\nOP_CODE_CLOSE = WSMsgType.CLOSE.value\nOP_CODE_PING = WSMsgType.PING.value\nOP_CODE_PONG = WSMsgType.PONG.value\n\nEMPTY_FRAME_ERROR = (True, b\"\")\nEMPTY_FRAME = (False, b\"\")\n\nCOMPRESSED_NOT_SET = -1\nCOMPRESSED_FALSE = 0\nCOMPRESSED_TRUE = 1\n\nTUPLE_NEW = tuple.__new__\n\ncython_int = int  # Typed to int in Python, but cython with use a signed int in the pxd\n\n\nclass WebSocketDataQueue:\n    \"\"\"WebSocketDataQueue resumes and pauses an underlying stream.\n\n    It is a destination for WebSocket data.\n    \"\"\"\n\n    def __init__(\n        self, protocol: BaseProtocol, limit: int, *, loop: asyncio.AbstractEventLoop\n    ) -> None:\n        self._size = 0\n        self._protocol = protocol\n        self._limit = limit * 2\n        self._loop = loop\n        self._eof = False\n        self._waiter: Optional[asyncio.Future[None]] = None\n        self._exception: Union[Type[BaseException], BaseException, None] = None\n        self._buffer: Deque[WSMessage] = deque()\n        self._get_buffer = self._buffer.popleft\n        self._put_buffer = self._buffer.append\n\n    def is_eof(self) -> bool:\n        return self._eof\n\n    def exception(self) -> Optional[Union[Type[BaseException], BaseException]]:\n        return self._exception\n\n    def set_exception(\n        self,\n        exc: Union[Type[BaseException], BaseException],\n        exc_cause: builtins.BaseException = _EXC_SENTINEL,\n    ) -> None:\n        self._eof = True\n        self._exception = exc\n        if (waiter := self._waiter) is not None:\n            self._waiter = None\n            set_exception(waiter, exc, exc_cause)\n\n    def _release_waiter(self) -> None:\n        if (waiter := self._waiter) is None:\n            return\n        self._waiter = None\n        if not waiter.done():\n            waiter.set_result(None)\n\n    def feed_eof(self) -> None:\n        self._eof = True\n        self._release_waiter()\n        self._exception = None  # Break cyclic references\n\n    def feed_data(self, data: \"WSMessage\") -> None:\n        size = data.size\n        self._size += size\n        self._put_buffer(data)\n        self._release_waiter()\n        if self._size > self._limit and not self._protocol._reading_paused:\n            self._protocol.pause_reading()\n\n    async def read(self) -> WSMessage:\n        if not self._buffer and not self._eof:\n            assert not self._waiter\n            self._waiter = self._loop.create_future()\n            try:\n                await self._waiter\n            except (asyncio.CancelledError, asyncio.TimeoutError):\n                self._waiter = None\n                raise\n        return self._read_from_buffer()\n\n    def _read_from_buffer(self) -> WSMessage:\n        if self._buffer:\n            data = self._get_buffer()\n            size = data.size\n            self._size -= size\n            if self._size < self._limit and self._protocol._reading_paused:\n                self._protocol.resume_reading()\n            return data\n        if self._exception is not None:\n            raise self._exception\n        raise EofStream\n\n\nclass WebSocketReader:\n    def __init__(\n        self, queue: WebSocketDataQueue, max_msg_size: int, compress: bool = True\n    ) -> None:\n        self.queue = queue\n        self._max_msg_size = max_msg_size\n\n        self._exc: Optional[Exception] = None\n        self._partial = bytearray()\n        self._state = READ_HEADER\n\n        self._opcode: int = OP_CODE_NOT_SET\n        self._frame_fin = False\n        self._frame_opcode: int = OP_CODE_NOT_SET\n        self._payload_fragments: list[bytes] = []\n        self._frame_payload_len = 0\n\n        self._tail: bytes = b\"\"\n        self._has_mask = False\n        self._frame_mask: Optional[bytes] = None\n        self._payload_bytes_to_read = 0\n        self._payload_len_flag = 0\n        self._compressed: int = COMPRESSED_NOT_SET\n        self._decompressobj: Optional[ZLibDecompressor] = None\n        self._compress = compress\n\n    def feed_eof(self) -> None:\n        self.queue.feed_eof()\n\n    # data can be bytearray on Windows because proactor event loop uses bytearray\n    # and asyncio types this to Union[bytes, bytearray, memoryview] so we need\n    # coerce data to bytes if it is not\n    def feed_data(\n        self, data: Union[bytes, bytearray, memoryview]\n    ) -> Tuple[bool, bytes]:\n        if type(data) is not bytes:\n            data = bytes(data)\n\n        if self._exc is not None:\n            return True, data\n\n        try:\n            self._feed_data(data)\n        except Exception as exc:\n            self._exc = exc\n            set_exception(self.queue, exc)\n            return EMPTY_FRAME_ERROR\n\n        return EMPTY_FRAME\n\n    def _handle_frame(\n        self,\n        fin: bool,\n        opcode: Union[int, cython_int],  # Union intended: Cython pxd uses C int\n        payload: Union[bytes, bytearray],\n        compressed: Union[int, cython_int],  # Union intended: Cython pxd uses C int\n    ) -> None:\n        msg: WSMessage\n        if opcode in {OP_CODE_TEXT, OP_CODE_BINARY, OP_CODE_CONTINUATION}:\n            # load text/binary\n            if not fin:\n                # got partial frame payload\n                if opcode != OP_CODE_CONTINUATION:\n                    self._opcode = opcode\n                self._partial += payload\n                if self._max_msg_size and len(self._partial) >= self._max_msg_size:\n                    raise WebSocketError(\n                        WSCloseCode.MESSAGE_TOO_BIG,\n                        f\"Message size {len(self._partial)} \"\n                        f\"exceeds limit {self._max_msg_size}\",\n                    )\n                return\n\n            has_partial = bool(self._partial)\n            if opcode == OP_CODE_CONTINUATION:\n                if self._opcode == OP_CODE_NOT_SET:\n                    raise WebSocketError(\n                        WSCloseCode.PROTOCOL_ERROR,\n                        \"Continuation frame for non started message\",\n                    )\n                opcode = self._opcode\n                self._opcode = OP_CODE_NOT_SET\n            # previous frame was non finished\n            # we should get continuation opcode\n            elif has_partial:\n                raise WebSocketError(\n                    WSCloseCode.PROTOCOL_ERROR,\n                    \"The opcode in non-fin frame is expected \"\n                    f\"to be zero, got {opcode!r}\",\n                )\n\n            assembled_payload: Union[bytes, bytearray]\n            if has_partial:\n                assembled_payload = self._partial + payload\n                self._partial.clear()\n            else:\n                assembled_payload = payload\n\n            if self._max_msg_size and len(assembled_payload) >= self._max_msg_size:\n                raise WebSocketError(\n                    WSCloseCode.MESSAGE_TOO_BIG,\n                    f\"Message size {len(assembled_payload)} \"\n                    f\"exceeds limit {self._max_msg_size}\",\n                )\n\n            # Decompress process must to be done after all packets\n            # received.\n            if compressed:\n                if not self._decompressobj:\n                    self._decompressobj = ZLibDecompressor(suppress_deflate_header=True)\n                # XXX: It's possible that the zlib backend (isal is known to\n                # do this, maybe others too?) will return max_length bytes,\n                # but internally buffer more data such that the payload is\n                # >max_length, so we return one extra byte and if we're able\n                # to do that, then the message is too big.\n                payload_merged = self._decompressobj.decompress_sync(\n                    assembled_payload + WS_DEFLATE_TRAILING,\n                    (\n                        self._max_msg_size + 1\n                        if self._max_msg_size\n                        else self._max_msg_size\n                    ),\n                )\n                if self._max_msg_size and len(payload_merged) > self._max_msg_size:\n                    raise WebSocketError(\n                        WSCloseCode.MESSAGE_TOO_BIG,\n                        f\"Decompressed message exceeds size limit {self._max_msg_size}\",\n                    )\n            elif type(assembled_payload) is bytes:\n                payload_merged = assembled_payload\n            else:\n                payload_merged = bytes(assembled_payload)\n\n            size = len(payload_merged)\n            if opcode == OP_CODE_TEXT:\n                try:\n                    text = payload_merged.decode(\"utf-8\")\n                except UnicodeDecodeError as exc:\n                    raise WebSocketError(\n                        WSCloseCode.INVALID_TEXT, \"Invalid UTF-8 text message\"\n                    ) from exc\n\n                # XXX: The Text and Binary messages here can be a performance\n                # bottleneck, so we use tuple.__new__ to improve performance.\n                # This is not type safe, but many tests should fail in\n                # test_client_ws_functional.py if this is wrong.\n                msg = TUPLE_NEW(WSMessageText, (text, size, \"\", WS_MSG_TYPE_TEXT))\n            else:\n                msg = TUPLE_NEW(\n                    WSMessageBinary, (payload_merged, size, \"\", WS_MSG_TYPE_BINARY)\n                )\n\n            self.queue.feed_data(msg)\n        elif opcode == OP_CODE_CLOSE:\n            payload_len = len(payload)\n            if payload_len >= 2:\n                close_code = UNPACK_CLOSE_CODE(payload[:2])[0]\n                if close_code < 3000 and close_code not in ALLOWED_CLOSE_CODES:\n                    raise WebSocketError(\n                        WSCloseCode.PROTOCOL_ERROR,\n                        f\"Invalid close code: {close_code}\",\n                    )\n                try:\n                    close_message = payload[2:].decode(\"utf-8\")\n                except UnicodeDecodeError as exc:\n                    raise WebSocketError(\n                        WSCloseCode.INVALID_TEXT, \"Invalid UTF-8 text message\"\n                    ) from exc\n                msg = WSMessageClose(\n                    data=close_code, size=payload_len, extra=close_message\n                )\n            elif payload:\n                raise WebSocketError(\n                    WSCloseCode.PROTOCOL_ERROR,\n                    f\"Invalid close frame: {fin} {opcode} {payload!r}\",\n                )\n            else:\n                msg = WSMessageClose(data=0, size=payload_len, extra=\"\")\n\n            self.queue.feed_data(msg)\n        elif opcode == OP_CODE_PING:\n            self.queue.feed_data(\n                WSMessagePing(data=bytes(payload), size=len(payload), extra=\"\")\n            )\n        elif opcode == OP_CODE_PONG:\n            self.queue.feed_data(\n                WSMessagePong(data=bytes(payload), size=len(payload), extra=\"\")\n            )\n        else:\n            raise WebSocketError(\n                WSCloseCode.PROTOCOL_ERROR, f\"Unexpected opcode={opcode!r}\"\n            )\n\n    def _feed_data(self, data: bytes) -> None:\n        \"\"\"Return the next frame from the socket.\"\"\"\n        if self._tail:\n            data, self._tail = self._tail + data, b\"\"\n\n        start_pos: int = 0\n        data_len = len(data)\n        data_cstr = data\n\n        while True:\n            # read header\n            if self._state == READ_HEADER:\n                if data_len - start_pos < 2:\n                    break\n                first_byte = data_cstr[start_pos]\n                second_byte = data_cstr[start_pos + 1]\n                start_pos += 2\n\n                fin = (first_byte >> 7) & 1\n                rsv1 = (first_byte >> 6) & 1\n                rsv2 = (first_byte >> 5) & 1\n                rsv3 = (first_byte >> 4) & 1\n                opcode = first_byte & 0xF\n\n                # frame-fin = %x0 ; more frames of this message follow\n                #           / %x1 ; final frame of this message\n                # frame-rsv1 = %x0 ;\n                #    1 bit, MUST be 0 unless negotiated otherwise\n                # frame-rsv2 = %x0 ;\n                #    1 bit, MUST be 0 unless negotiated otherwise\n                # frame-rsv3 = %x0 ;\n                #    1 bit, MUST be 0 unless negotiated otherwise\n                #\n                # Remove rsv1 from this test for deflate development\n                if rsv2 or rsv3 or (rsv1 and not self._compress):\n                    raise WebSocketError(\n                        WSCloseCode.PROTOCOL_ERROR,\n                        \"Received frame with non-zero reserved bits\",\n                    )\n\n                if opcode > 0x7 and fin == 0:\n                    raise WebSocketError(\n                        WSCloseCode.PROTOCOL_ERROR,\n                        \"Received fragmented control frame\",\n                    )\n\n                has_mask = (second_byte >> 7) & 1\n                length = second_byte & 0x7F\n\n                # Control frames MUST have a payload\n                # length of 125 bytes or less\n                if opcode > 0x7 and length > 125:\n                    raise WebSocketError(\n                        WSCloseCode.PROTOCOL_ERROR,\n                        \"Control frame payload cannot be larger than 125 bytes\",\n                    )\n\n                # Set compress status if last package is FIN\n                # OR set compress status if this is first fragment\n                # Raise error if not first fragment with rsv1 = 0x1\n                if self._frame_fin or self._compressed == COMPRESSED_NOT_SET:\n                    self._compressed = COMPRESSED_TRUE if rsv1 else COMPRESSED_FALSE\n                elif rsv1:\n                    raise WebSocketError(\n                        WSCloseCode.PROTOCOL_ERROR,\n                        \"Received frame with non-zero reserved bits\",\n                    )\n\n                self._frame_fin = bool(fin)\n                self._frame_opcode = opcode\n                self._has_mask = bool(has_mask)\n                self._payload_len_flag = length\n                self._state = READ_PAYLOAD_LENGTH\n\n            # read payload length\n            if self._state == READ_PAYLOAD_LENGTH:\n                len_flag = self._payload_len_flag\n                if len_flag == 126:\n                    if data_len - start_pos < 2:\n                        break\n                    first_byte = data_cstr[start_pos]\n                    second_byte = data_cstr[start_pos + 1]\n                    start_pos += 2\n                    self._payload_bytes_to_read = first_byte << 8 | second_byte\n                elif len_flag > 126:\n                    if data_len - start_pos < 8:\n                        break\n                    self._payload_bytes_to_read = UNPACK_LEN3(data, start_pos)[0]\n                    start_pos += 8\n                else:\n                    self._payload_bytes_to_read = len_flag\n\n                self._state = READ_PAYLOAD_MASK if self._has_mask else READ_PAYLOAD\n\n            # read payload mask\n            if self._state == READ_PAYLOAD_MASK:\n                if data_len - start_pos < 4:\n                    break\n                self._frame_mask = data_cstr[start_pos : start_pos + 4]\n                start_pos += 4\n                self._state = READ_PAYLOAD\n\n            if self._state == READ_PAYLOAD:\n                chunk_len = data_len - start_pos\n                if self._payload_bytes_to_read >= chunk_len:\n                    f_end_pos = data_len\n                    self._payload_bytes_to_read -= chunk_len\n                else:\n                    f_end_pos = start_pos + self._payload_bytes_to_read\n                    self._payload_bytes_to_read = 0\n\n                had_fragments = self._frame_payload_len\n                self._frame_payload_len += f_end_pos - start_pos\n                f_start_pos = start_pos\n                start_pos = f_end_pos\n\n                if self._payload_bytes_to_read != 0:\n                    # If we don't have a complete frame, we need to save the\n                    # data for the next call to feed_data.\n                    self._payload_fragments.append(data_cstr[f_start_pos:f_end_pos])\n                    break\n\n                payload: Union[bytes, bytearray]\n                if had_fragments:\n                    # We have to join the payload fragments get the payload\n                    self._payload_fragments.append(data_cstr[f_start_pos:f_end_pos])\n                    if self._has_mask:\n                        assert self._frame_mask is not None\n                        payload_bytearray = bytearray(b\"\".join(self._payload_fragments))\n                        websocket_mask(self._frame_mask, payload_bytearray)\n                        payload = payload_bytearray\n                    else:\n                        payload = b\"\".join(self._payload_fragments)\n                    self._payload_fragments.clear()\n                elif self._has_mask:\n                    assert self._frame_mask is not None\n                    payload_bytearray = data_cstr[f_start_pos:f_end_pos]  # type: ignore[assignment]\n                    if type(payload_bytearray) is not bytearray:  # pragma: no branch\n                        # Cython will do the conversion for us\n                        # but we need to do it for Python and we\n                        # will always get here in Python\n                        payload_bytearray = bytearray(payload_bytearray)\n                    websocket_mask(self._frame_mask, payload_bytearray)\n                    payload = payload_bytearray\n                else:\n                    payload = data_cstr[f_start_pos:f_end_pos]\n\n                self._handle_frame(\n                    self._frame_fin, self._frame_opcode, payload, self._compressed\n                )\n                self._frame_payload_len = 0\n                self._state = READ_HEADER\n\n        # XXX: Cython needs slices to be bounded, so we can't omit the slice end here.\n        self._tail = data_cstr[start_pos:data_len] if start_pos < data_len else b\"\"\n", "metadata": {"source": "github.com/aio-libs/aiohttp", "file_path": "aiohttp/_websocket/reader_py.py", "language": "python", "complexity": "complex", "star_count": 15860, "quality_score": 1.0, "features": {"has_classes": true, "has_functions": true, "has_async": true, "has_decorators": false, "has_imports": true, "has_error_handling": true, "has_documentation": true, "has_type_hints": true, "has_testing": true, "has_logging": false, "function_count": 14, "class_count": 2, "line_count": 488}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "async_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create asynchronous code with proper concurrency handling with asyncio coroutines\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"WebSocket protocol versions 13 and 8.\"\"\"\n\nimport asyncio\nimport random\nfrom functools import partial\nfrom typing import Any, Final, Optional, Union\n\nfrom ..base_protocol import BaseProtocol\nfrom ..client_exceptions import ClientConnectionResetError\nfrom ..compression_utils import ZLibBackend, ZLibCompressor\nfrom .helpers import (\n    MASK_LEN,\n    MSG_SIZE,\n    PACK_CLOSE_CODE,\n    PACK_LEN1,\n    PACK_LEN2,\n    PACK_LEN3,\n    PACK_RANDBITS,\n    websocket_mask,\n)\nfrom .models import WS_DEFLATE_TRAILING, WSMsgType\n\nDEFAULT_LIMIT: Final[int] = 2**16\n\n# For websockets, keeping latency low is extremely important as implementations\n# generally expect to be able to send and receive messages quickly.  We use a\n# larger chunk size than the default to reduce the number of executor calls\n# since the executor is a significant source of latency and overhead when\n# the chunks are small. A size of 5KiB was chosen because it is also the\n# same value python-zlib-ng choose to use as the threshold to release the GIL.\n\nWEBSOCKET_MAX_SYNC_CHUNK_SIZE = 5 * 1024\n\n\nclass WebSocketWriter:\n    \"\"\"WebSocket writer.\n\n    The writer is responsible for sending messages to the client. It is\n    created by the protocol when a connection is established. The writer\n    should avoid implementing any application logic and should only be\n    concerned with the low-level details of the WebSocket protocol.\n    \"\"\"\n\n    def __init__(\n        self,\n        protocol: BaseProtocol,\n        transport: asyncio.Transport,\n        *,\n        use_mask: bool = False,\n        limit: int = DEFAULT_LIMIT,\n        random: random.Random = random.Random(),\n        compress: int = 0,\n        notakeover: bool = False,\n    ) -> None:\n        \"\"\"Initialize a WebSocket writer.\"\"\"\n        self.protocol = protocol\n        self.transport = transport\n        self.use_mask = use_mask\n        self.get_random_bits = partial(random.getrandbits, 32)\n        self.compress = compress\n        self.notakeover = notakeover\n        self._closing = False\n        self._limit = limit\n        self._output_size = 0\n        self._compressobj: Any = None  # actually compressobj\n\n    async def send_frame(\n        self, message: bytes, opcode: int, compress: Optional[int] = None\n    ) -> None:\n        \"\"\"Send a frame over the websocket with message as its payload.\"\"\"\n        if self._closing and not (opcode & WSMsgType.CLOSE):\n            raise ClientConnectionResetError(\"Cannot write to closing transport\")\n\n        # RSV are the reserved bits in the frame header. They are used to\n        # indicate that the frame is using an extension.\n        # https://datatracker.ietf.org/doc/html/rfc6455#section-5.2\n        rsv = 0\n        # Only compress larger packets (disabled)\n        # Does small packet needs to be compressed?\n        # if self.compress and opcode < 8 and len(message) > 124:\n        if (compress or self.compress) and opcode < 8:\n            # RSV1 (rsv = 0x40) is set for compressed frames\n            # https://datatracker.ietf.org/doc/html/rfc7692#section-7.2.3.1\n            rsv = 0x40\n\n            if compress:\n                # Do not set self._compress if compressing is for this frame\n                compressobj = self._make_compress_obj(compress)\n            else:  # self.compress\n                if not self._compressobj:\n                    self._compressobj = self._make_compress_obj(self.compress)\n                compressobj = self._compressobj\n\n            message = (\n                await compressobj.compress(message)\n                + compressobj.flush(\n                    ZLibBackend.Z_FULL_FLUSH\n                    if self.notakeover\n                    else ZLibBackend.Z_SYNC_FLUSH\n                )\n            ).removesuffix(WS_DEFLATE_TRAILING)\n            # Its critical that we do not return control to the event\n            # loop until we have finished sending all the compressed\n            # data. Otherwise we could end up mixing compressed frames\n            # if there are multiple coroutines compressing data.\n\n        msg_length = len(message)\n\n        use_mask = self.use_mask\n        mask_bit = 0x80 if use_mask else 0\n\n        # Depending on the message length, the header is assembled differently.\n        # The first byte is reserved for the opcode and the RSV bits.\n        first_byte = 0x80 | rsv | opcode\n        if msg_length < 126:\n            header = PACK_LEN1(first_byte, msg_length | mask_bit)\n            header_len = 2\n        elif msg_length < 65536:\n            header = PACK_LEN2(first_byte, 126 | mask_bit, msg_length)\n            header_len = 4\n        else:\n            header = PACK_LEN3(first_byte, 127 | mask_bit, msg_length)\n            header_len = 10\n\n        if self.transport.is_closing():\n            raise ClientConnectionResetError(\"Cannot write to closing transport\")\n\n        # https://datatracker.ietf.org/doc/html/rfc6455#section-5.3\n        # If we are using a mask, we need to generate it randomly\n        # and apply it to the message before sending it. A mask is\n        # a 32-bit value that is applied to the message using a\n        # bitwise XOR operation. It is used to prevent certain types\n        # of attacks on the websocket protocol. The mask is only used\n        # when aiohttp is acting as a client. Servers do not use a mask.\n        if use_mask:\n            mask = PACK_RANDBITS(self.get_random_bits())\n            message_arr = bytearray(message)\n            websocket_mask(mask, message_arr)\n            self.transport.write(header + mask + message_arr)\n            self._output_size += MASK_LEN\n        elif msg_length > MSG_SIZE:\n            self.transport.write(header)\n            self.transport.write(message)\n        else:\n            self.transport.write(header + message)\n\n        self._output_size += header_len + msg_length\n\n        # It is safe to return control to the event loop when using compression\n        # after this point as we have already sent or buffered all the data.\n\n        # Once we have written output_size up to the limit, we call the\n        # drain helper which waits for the transport to be ready to accept\n        # more data. This is a flow control mechanism to prevent the buffer\n        # from growing too large. The drain helper will return right away\n        # if the writer is not paused.\n        if self._output_size > self._limit:\n            self._output_size = 0\n            if self.protocol._paused:\n                await self.protocol._drain_helper()\n\n    def _make_compress_obj(self, compress: int) -> ZLibCompressor:\n        return ZLibCompressor(\n            level=ZLibBackend.Z_BEST_SPEED,\n            wbits=-compress,\n            max_sync_chunk_size=WEBSOCKET_MAX_SYNC_CHUNK_SIZE,\n        )\n\n    async def close(self, code: int = 1000, message: Union[bytes, str] = b\"\") -> None:\n        \"\"\"Close the websocket, sending the specified code and message.\"\"\"\n        if isinstance(message, str):\n            message = message.encode(\"utf-8\")\n        try:\n            await self.send_frame(\n                PACK_CLOSE_CODE(code) + message, opcode=WSMsgType.CLOSE\n            )\n        finally:\n            self._closing = True\n", "metadata": {"source": "github.com/aio-libs/aiohttp", "file_path": "aiohttp/_websocket/writer.py", "language": "python", "complexity": "complex", "star_count": 15860, "quality_score": 1.0, "features": {"has_classes": true, "has_functions": true, "has_async": true, "has_decorators": false, "has_imports": true, "has_error_handling": true, "has_documentation": true, "has_type_hints": true, "has_testing": false, "has_logging": true, "function_count": 4, "class_count": 1, "line_count": 179}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "async_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create asynchronous code with proper concurrency handling with asyncio coroutines\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import logging\nimport socket\nfrom abc import ABC, abstractmethod\nfrom collections.abc import Sized\nfrom http.cookies import BaseCookie, Morsel\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Awaitable,\n    Callable,\n    Dict,\n    Generator,\n    Iterable,\n    List,\n    Optional,\n    Sequence,\n    Tuple,\n    TypedDict,\n    Union,\n)\n\nfrom multidict import CIMultiDict\nfrom yarl import URL\n\nfrom ._cookie_helpers import parse_set_cookie_headers\nfrom .typedefs import LooseCookies\n\nif TYPE_CHECKING:\n    from .web_app import Application\n    from .web_exceptions import HTTPException\n    from .web_request import BaseRequest, Request\n    from .web_response import StreamResponse\nelse:\n    BaseRequest = Request = Application = StreamResponse = None\n    HTTPException = None\n\n\nclass AbstractRouter(ABC):\n    def __init__(self) -> None:\n        self._frozen = False\n\n    def post_init(self, app: Application) -> None:\n        \"\"\"Post init stage.\n\n        Not an abstract method for sake of backward compatibility,\n        but if the router wants to be aware of the application\n        it can override this.\n        \"\"\"\n\n    @property\n    def frozen(self) -> bool:\n        return self._frozen\n\n    def freeze(self) -> None:\n        \"\"\"Freeze router.\"\"\"\n        self._frozen = True\n\n    @abstractmethod\n    async def resolve(self, request: Request) -> \"AbstractMatchInfo\":\n        \"\"\"Return MATCH_INFO for given request\"\"\"\n\n\nclass AbstractMatchInfo(ABC):\n\n    __slots__ = ()\n\n    @property  # pragma: no branch\n    @abstractmethod\n    def handler(self) -> Callable[[Request], Awaitable[StreamResponse]]:\n        \"\"\"Execute matched request handler\"\"\"\n\n    @property\n    @abstractmethod\n    def expect_handler(\n        self,\n    ) -> Callable[[Request], Awaitable[Optional[StreamResponse]]]:\n        \"\"\"Expect handler for 100-continue processing\"\"\"\n\n    @property  # pragma: no branch\n    @abstractmethod\n    def http_exception(self) -> Optional[HTTPException]:\n        \"\"\"HTTPException instance raised on router's resolving, or None\"\"\"\n\n    @abstractmethod  # pragma: no branch\n    def get_info(self) -> Dict[str, Any]:\n        \"\"\"Return a dict with additional info useful for introspection\"\"\"\n\n    @property  # pragma: no branch\n    @abstractmethod\n    def apps(self) -> Tuple[Application, ...]:\n        \"\"\"Stack of nested applications.\n\n        Top level application is left-most element.\n\n        \"\"\"\n\n    @abstractmethod\n    def add_app(self, app: Application) -> None:\n        \"\"\"Add application to the nested apps stack.\"\"\"\n\n    @abstractmethod\n    def freeze(self) -> None:\n        \"\"\"Freeze the match info.\n\n        The method is called after route resolution.\n\n        After the call .add_app() is forbidden.\n\n        \"\"\"\n\n\nclass AbstractView(ABC):\n    \"\"\"Abstract class based view.\"\"\"\n\n    def __init__(self, request: Request) -> None:\n        self._request = request\n\n    @property\n    def request(self) -> Request:\n        \"\"\"Request instance.\"\"\"\n        return self._request\n\n    @abstractmethod\n    def __await__(self) -> Generator[None, None, StreamResponse]:\n        \"\"\"Execute the view handler.\"\"\"\n\n\nclass ResolveResult(TypedDict):\n    \"\"\"Resolve result.\n\n    This is the result returned from an AbstractResolver's\n    resolve method.\n\n    :param hostname: The hostname that was provided.\n    :param host: The IP address that was resolved.\n    :param port: The port that was resolved.\n    :param family: The address family that was resolved.\n    :param proto: The protocol that was resolved.\n    :param flags: The flags that were resolved.\n    \"\"\"\n\n    hostname: str\n    host: str\n    port: int\n    family: int\n    proto: int\n    flags: int\n\n\nclass AbstractResolver(ABC):\n    \"\"\"Abstract DNS resolver.\"\"\"\n\n    @abstractmethod\n    async def resolve(\n        self, host: str, port: int = 0, family: socket.AddressFamily = socket.AF_INET\n    ) -> List[ResolveResult]:\n        \"\"\"Return IP address for given hostname\"\"\"\n\n    @abstractmethod\n    async def close(self) -> None:\n        \"\"\"Release resolver\"\"\"\n\n\nif TYPE_CHECKING:\n    IterableBase = Iterable[Morsel[str]]\nelse:\n    IterableBase = Iterable\n\n\nClearCookiePredicate = Callable[[\"Morsel[str]\"], bool]\n\n\nclass AbstractCookieJar(Sized, IterableBase):\n    \"\"\"Abstract Cookie Jar.\"\"\"\n\n    @property\n    @abstractmethod\n    def quote_cookie(self) -> bool:\n        \"\"\"Return True if cookies should be quoted.\"\"\"\n\n    @abstractmethod\n    def clear(self, predicate: Optional[ClearCookiePredicate] = None) -> None:\n        \"\"\"Clear all cookies if no predicate is passed.\"\"\"\n\n    @abstractmethod\n    def clear_domain(self, domain: str) -> None:\n        \"\"\"Clear all cookies for domain and all subdomains.\"\"\"\n\n    @abstractmethod\n    def update_cookies(self, cookies: LooseCookies, response_url: URL = URL()) -> None:\n        \"\"\"Update cookies.\"\"\"\n\n    def update_cookies_from_headers(\n        self, headers: Sequence[str], response_url: URL\n    ) -> None:\n        \"\"\"Update cookies from raw Set-Cookie headers.\"\"\"\n        if headers and (cookies_to_update := parse_set_cookie_headers(headers)):\n            self.update_cookies(cookies_to_update, response_url)\n\n    @abstractmethod\n    def filter_cookies(self, request_url: URL) -> \"BaseCookie[str]\":\n        \"\"\"Return the jar's cookies filtered by their attributes.\"\"\"\n\n\nclass AbstractStreamWriter(ABC):\n    \"\"\"Abstract stream writer.\"\"\"\n\n    buffer_size: int = 0\n    output_size: int = 0\n    length: Optional[int] = 0\n\n    @abstractmethod\n    async def write(\n        self, chunk: Union[bytes, bytearray, \"memoryview[int]\", \"memoryview[bytes]\"]\n    ) -> None:\n        \"\"\"Write chunk into stream.\"\"\"\n\n    @abstractmethod\n    async def write_eof(self, chunk: bytes = b\"\") -> None:\n        \"\"\"Write last chunk.\"\"\"\n\n    @abstractmethod\n    async def drain(self) -> None:\n        \"\"\"Flush the write buffer.\"\"\"\n\n    @abstractmethod\n    def enable_compression(\n        self, encoding: str = \"deflate\", strategy: Optional[int] = None\n    ) -> None:\n        \"\"\"Enable HTTP body compression\"\"\"\n\n    @abstractmethod\n    def enable_chunking(self) -> None:\n        \"\"\"Enable HTTP chunked mode\"\"\"\n\n    @abstractmethod\n    async def write_headers(\n        self, status_line: str, headers: \"CIMultiDict[str]\"\n    ) -> None:\n        \"\"\"Write HTTP headers\"\"\"\n\n    def send_headers(self) -> None:\n        \"\"\"Force sending buffered headers if not already sent.\n\n        Required only if write_headers() buffers headers instead of sending immediately.\n        For backwards compatibility, this method does nothing by default.\n        \"\"\"\n\n\nclass AbstractAccessLogger(ABC):\n    \"\"\"Abstract writer to access log.\"\"\"\n\n    __slots__ = (\"logger\", \"log_format\")\n\n    def __init__(self, logger: logging.Logger, log_format: str) -> None:\n        self.logger = logger\n        self.log_format = log_format\n\n    @abstractmethod\n    def log(self, request: BaseRequest, response: StreamResponse, time: float) -> None:\n        \"\"\"Emit log to logger.\"\"\"\n\n    @property\n    def enabled(self) -> bool:\n        \"\"\"Check if logger is enabled.\"\"\"\n        return True\n\n\nclass AbstractAsyncAccessLogger(ABC):\n    \"\"\"Abstract asynchronous writer to access log.\"\"\"\n\n    __slots__ = ()\n\n    @abstractmethod\n    async def log(\n        self, request: BaseRequest, response: StreamResponse, request_start: float\n    ) -> None:\n        \"\"\"Emit log to logger.\"\"\"\n\n    @property\n    def enabled(self) -> bool:\n        \"\"\"Check if logger is enabled.\"\"\"\n        return True\n", "metadata": {"source": "github.com/aio-libs/aiohttp", "file_path": "aiohttp/abc.py", "language": "python", "complexity": "complex", "star_count": 15860, "quality_score": 1.0, "features": {"has_classes": true, "has_functions": true, "has_async": true, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": true, "has_type_hints": true, "has_testing": false, "has_logging": true, "function_count": 35, "class_count": 10, "line_count": 284}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "async_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create asynchronous code with proper concurrency handling with asyncio coroutines\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import asyncio\nfrom typing import Optional, cast\n\nfrom .client_exceptions import ClientConnectionResetError\nfrom .helpers import set_exception\nfrom .tcp_helpers import tcp_nodelay\n\n\nclass BaseProtocol(asyncio.Protocol):\n    __slots__ = (\n        \"_loop\",\n        \"_paused\",\n        \"_drain_waiter\",\n        \"_connection_lost\",\n        \"_reading_paused\",\n        \"transport\",\n    )\n\n    def __init__(self, loop: asyncio.AbstractEventLoop) -> None:\n        self._loop: asyncio.AbstractEventLoop = loop\n        self._paused = False\n        self._drain_waiter: Optional[asyncio.Future[None]] = None\n        self._reading_paused = False\n\n        self.transport: Optional[asyncio.Transport] = None\n\n    @property\n    def connected(self) -> bool:\n        \"\"\"Return True if the connection is open.\"\"\"\n        return self.transport is not None\n\n    @property\n    def writing_paused(self) -> bool:\n        return self._paused\n\n    def pause_writing(self) -> None:\n        assert not self._paused\n        self._paused = True\n\n    def resume_writing(self) -> None:\n        assert self._paused\n        self._paused = False\n\n        waiter = self._drain_waiter\n        if waiter is not None:\n            self._drain_waiter = None\n            if not waiter.done():\n                waiter.set_result(None)\n\n    def pause_reading(self) -> None:\n        if not self._reading_paused and self.transport is not None:\n            try:\n                self.transport.pause_reading()\n            except (AttributeError, NotImplementedError, RuntimeError):\n                pass\n            self._reading_paused = True\n\n    def resume_reading(self) -> None:\n        if self._reading_paused and self.transport is not None:\n            try:\n                self.transport.resume_reading()\n            except (AttributeError, NotImplementedError, RuntimeError):\n                pass\n            self._reading_paused = False\n\n    def connection_made(self, transport: asyncio.BaseTransport) -> None:\n        tr = cast(asyncio.Transport, transport)\n        tcp_nodelay(tr, True)\n        self.transport = tr\n\n    def connection_lost(self, exc: Optional[BaseException]) -> None:\n        # Wake up the writer if currently paused.\n        self.transport = None\n        if not self._paused:\n            return\n        waiter = self._drain_waiter\n        if waiter is None:\n            return\n        self._drain_waiter = None\n        if waiter.done():\n            return\n        if exc is None:\n            waiter.set_result(None)\n        else:\n            set_exception(\n                waiter,\n                ConnectionError(\"Connection lost\"),\n                exc,\n            )\n\n    async def _drain_helper(self) -> None:\n        if self.transport is None:\n            raise ClientConnectionResetError(\"Connection lost\")\n        if not self._paused:\n            return\n        waiter = self._drain_waiter\n        if waiter is None:\n            waiter = self._loop.create_future()\n            self._drain_waiter = waiter\n        await asyncio.shield(waiter)\n", "metadata": {"source": "github.com/aio-libs/aiohttp", "file_path": "aiohttp/base_protocol.py", "language": "python", "complexity": "complex", "star_count": 15860, "quality_score": 1.0, "features": {"has_classes": true, "has_functions": true, "has_async": true, "has_decorators": true, "has_imports": true, "has_error_handling": true, "has_documentation": true, "has_type_hints": true, "has_testing": true, "has_logging": false, "function_count": 10, "class_count": 1, "line_count": 101}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "async_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create asynchronous code with proper concurrency handling\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"HTTP related errors.\"\"\"\n\nimport asyncio\nfrom typing import TYPE_CHECKING, Optional, Tuple, Union\n\nfrom multidict import MultiMapping\n\nfrom .typedefs import StrOrURL\n\nif TYPE_CHECKING:\n    import ssl\n\n    SSLContext = ssl.SSLContext\nelse:\n    try:\n        import ssl\n\n        SSLContext = ssl.SSLContext\n    except ImportError:  # pragma: no cover\n        ssl = SSLContext = None  # type: ignore[assignment]\n\nif TYPE_CHECKING:\n    from .client_reqrep import ClientResponse, ConnectionKey, Fingerprint, RequestInfo\n    from .http_parser import RawResponseMessage\nelse:\n    RequestInfo = ClientResponse = ConnectionKey = RawResponseMessage = None\n\n__all__ = (\n    \"ClientError\",\n    \"ClientConnectionError\",\n    \"ClientConnectionResetError\",\n    \"ClientOSError\",\n    \"ClientConnectorError\",\n    \"ClientProxyConnectionError\",\n    \"ClientSSLError\",\n    \"ClientConnectorDNSError\",\n    \"ClientConnectorSSLError\",\n    \"ClientConnectorCertificateError\",\n    \"ConnectionTimeoutError\",\n    \"SocketTimeoutError\",\n    \"ServerConnectionError\",\n    \"ServerTimeoutError\",\n    \"ServerDisconnectedError\",\n    \"ServerFingerprintMismatch\",\n    \"ClientResponseError\",\n    \"ClientHttpProxyError\",\n    \"WSServerHandshakeError\",\n    \"ContentTypeError\",\n    \"ClientPayloadError\",\n    \"InvalidURL\",\n    \"InvalidUrlClientError\",\n    \"RedirectClientError\",\n    \"NonHttpUrlClientError\",\n    \"InvalidUrlRedirectClientError\",\n    \"NonHttpUrlRedirectClientError\",\n    \"WSMessageTypeError\",\n)\n\n\nclass ClientError(Exception):\n    \"\"\"Base class for client connection errors.\"\"\"\n\n\nclass ClientResponseError(ClientError):\n    \"\"\"Base class for exceptions that occur after getting a response.\n\n    request_info: An instance of RequestInfo.\n    history: A sequence of responses, if redirects occurred.\n    status: HTTP status code.\n    message: Error message.\n    headers: Response headers.\n    \"\"\"\n\n    def __init__(\n        self,\n        request_info: RequestInfo,\n        history: Tuple[ClientResponse, ...],\n        *,\n        status: Optional[int] = None,\n        message: str = \"\",\n        headers: Optional[MultiMapping[str]] = None,\n    ) -> None:\n        self.request_info = request_info\n        if status is not None:\n            self.status = status\n        else:\n            self.status = 0\n        self.message = message\n        self.headers = headers\n        self.history = history\n        self.args = (request_info, history)\n\n    def __str__(self) -> str:\n        return \"{}, message={!r}, url={!r}\".format(\n            self.status,\n            self.message,\n            str(self.request_info.real_url),\n        )\n\n    def __repr__(self) -> str:\n        args = f\"{self.request_info!r}, {self.history!r}\"\n        if self.status != 0:\n            args += f\", status={self.status!r}\"\n        if self.message != \"\":\n            args += f\", message={self.message!r}\"\n        if self.headers is not None:\n            args += f\", headers={self.headers!r}\"\n        return f\"{type(self).__name__}({args})\"\n\n\nclass ContentTypeError(ClientResponseError):\n    \"\"\"ContentType found is not valid.\"\"\"\n\n\nclass WSServerHandshakeError(ClientResponseError):\n    \"\"\"websocket server handshake error.\"\"\"\n\n\nclass ClientHttpProxyError(ClientResponseError):\n    \"\"\"HTTP proxy error.\n\n    Raised in :class:`aiohttp.connector.TCPConnector` if\n    proxy responds with status other than ``200 OK``\n    on ``CONNECT`` request.\n    \"\"\"\n\n\nclass TooManyRedirects(ClientResponseError):\n    \"\"\"Client was redirected too many times.\"\"\"\n\n\nclass ClientConnectionError(ClientError):\n    \"\"\"Base class for client socket errors.\"\"\"\n\n\nclass ClientConnectionResetError(ClientConnectionError, ConnectionResetError):\n    \"\"\"ConnectionResetError\"\"\"\n\n\nclass ClientOSError(ClientConnectionError, OSError):\n    \"\"\"OSError error.\"\"\"\n\n\nclass ClientConnectorError(ClientOSError):\n    \"\"\"Client connector error.\n\n    Raised in :class:`aiohttp.connector.TCPConnector` if\n        a connection can not be established.\n    \"\"\"\n\n    def __init__(self, connection_key: ConnectionKey, os_error: OSError) -> None:\n        self._conn_key = connection_key\n        self._os_error = os_error\n        super().__init__(os_error.errno, os_error.strerror)\n        self.args = (connection_key, os_error)\n\n    @property\n    def os_error(self) -> OSError:\n        return self._os_error\n\n    @property\n    def host(self) -> str:\n        return self._conn_key.host\n\n    @property\n    def port(self) -> Optional[int]:\n        return self._conn_key.port\n\n    @property\n    def ssl(self) -> Union[SSLContext, bool, \"Fingerprint\"]:\n        return self._conn_key.ssl\n\n    def __str__(self) -> str:\n        return \"Cannot connect to host {0.host}:{0.port} ssl:{1} [{2}]\".format(\n            self, \"default\" if self.ssl is True else self.ssl, self.strerror\n        )\n\n    # OSError.__reduce__ does too much black magick\n    __reduce__ = BaseException.__reduce__\n\n\nclass ClientConnectorDNSError(ClientConnectorError):\n    \"\"\"DNS resolution failed during client connection.\n\n    Raised in :class:`aiohttp.connector.TCPConnector` if\n        DNS resolution fails.\n    \"\"\"\n\n\nclass ClientProxyConnectionError(ClientConnectorError):\n    \"\"\"Proxy connection error.\n\n    Raised in :class:`aiohttp.connector.TCPConnector` if\n        connection to proxy can not be established.\n    \"\"\"\n\n\nclass UnixClientConnectorError(ClientConnectorError):\n    \"\"\"Unix connector error.\n\n    Raised in :py:class:`aiohttp.connector.UnixConnector`\n    if connection to unix socket can not be established.\n    \"\"\"\n\n    def __init__(\n        self, path: str, connection_key: ConnectionKey, os_error: OSError\n    ) -> None:\n        self._path = path\n        super().__init__(connection_key, os_error)\n\n    @property\n    def path(self) -> str:\n        return self._path\n\n    def __str__(self) -> str:\n        return \"Cannot connect to unix socket {0.path} ssl:{1} [{2}]\".format(\n            self, \"default\" if self.ssl is True else self.ssl, self.strerror\n        )\n\n\nclass ServerConnectionError(ClientConnectionError):\n    \"\"\"Server connection errors.\"\"\"\n\n\nclass ServerDisconnectedError(ServerConnectionError):\n    \"\"\"Server disconnected.\"\"\"\n\n    def __init__(self, message: Union[RawResponseMessage, str, None] = None) -> None:\n        if message is None:\n            message = \"Server disconnected\"\n\n        self.args = (message,)\n        self.message = message\n\n\nclass ServerTimeoutError(ServerConnectionError, asyncio.TimeoutError):\n    \"\"\"Server timeout error.\"\"\"\n\n\nclass ConnectionTimeoutError(ServerTimeoutError):\n    \"\"\"Connection timeout error.\"\"\"\n\n\nclass SocketTimeoutError(ServerTimeoutError):\n    \"\"\"Socket timeout error.\"\"\"\n\n\nclass ServerFingerprintMismatch(ServerConnectionError):\n    \"\"\"SSL certificate does not match expected fingerprint.\"\"\"\n\n    def __init__(self, expected: bytes, got: bytes, host: str, port: int) -> None:\n        self.expected = expected\n        self.got = got\n        self.host = host\n        self.port = port\n        self.args = (expected, got, host, port)\n\n    def __repr__(self) -> str:\n        return \"<{} expected={!r} got={!r} host={!r} port={!r}>\".format(\n            self.__class__.__name__, self.expected, self.got, self.host, self.port\n        )\n\n\nclass ClientPayloadError(ClientError):\n    \"\"\"Response payload error.\"\"\"\n\n\nclass InvalidURL(ClientError, ValueError):\n    \"\"\"Invalid URL.\n\n    URL used for fetching is malformed, e.g. it doesn't contains host\n    part.\n    \"\"\"\n\n    # Derive from ValueError for backward compatibility\n\n    def __init__(self, url: StrOrURL, description: Union[str, None] = None) -> None:\n        # The type of url is not yarl.URL because the exception can be raised\n        # on URL(url) call\n        self._url = url\n        self._description = description\n\n        if description:\n            super().__init__(url, description)\n        else:\n            super().__init__(url)\n\n    @property\n    def url(self) -> StrOrURL:\n        return self._url\n\n    @property\n    def description(self) -> \"str | None\":\n        return self._description\n\n    def __repr__(self) -> str:\n        return f\"<{self.__class__.__name__} {self}>\"\n\n    def __str__(self) -> str:\n        if self._description:\n            return f\"{self._url} - {self._description}\"\n        return str(self._url)\n\n\nclass InvalidUrlClientError(InvalidURL):\n    \"\"\"Invalid URL client error.\"\"\"\n\n\nclass RedirectClientError(ClientError):\n    \"\"\"Client redirect error.\"\"\"\n\n\nclass NonHttpUrlClientError(ClientError):\n    \"\"\"Non http URL client error.\"\"\"\n\n\nclass InvalidUrlRedirectClientError(InvalidUrlClientError, RedirectClientError):\n    \"\"\"Invalid URL redirect client error.\"\"\"\n\n\nclass NonHttpUrlRedirectClientError(NonHttpUrlClientError, RedirectClientError):\n    \"\"\"Non http URL redirect client error.\"\"\"\n\n\nclass ClientSSLError(ClientConnectorError):\n    \"\"\"Base error for ssl.*Errors.\"\"\"\n\n\nif ssl is not None:\n    cert_errors = (ssl.CertificateError,)\n    cert_errors_bases = (\n        ClientSSLError,\n        ssl.CertificateError,\n    )\n\n    ssl_errors = (ssl.SSLError,)\n    ssl_error_bases = (ClientSSLError, ssl.SSLError)\nelse:  # pragma: no cover\n    cert_errors = tuple()  # type: ignore[unreachable]\n    cert_errors_bases = (\n        ClientSSLError,\n        ValueError,\n    )\n\n    ssl_errors = tuple()\n    ssl_error_bases = (ClientSSLError,)\n\n\nclass ClientConnectorSSLError(*ssl_error_bases):  # type: ignore[misc]\n    \"\"\"Response ssl error.\"\"\"\n\n\nclass ClientConnectorCertificateError(*cert_errors_bases):  # type: ignore[misc]\n    \"\"\"Response certificate error.\"\"\"\n\n    def __init__(\n        self, connection_key: ConnectionKey, certificate_error: Exception\n    ) -> None:\n        self._conn_key = connection_key\n        self._certificate_error = certificate_error\n        self.args = (connection_key, certificate_error)\n\n    @property\n    def certificate_error(self) -> Exception:\n        return self._certificate_error\n\n    @property\n    def host(self) -> str:\n        return self._conn_key.host\n\n    @property\n    def port(self) -> Optional[int]:\n        return self._conn_key.port\n\n    @property\n    def ssl(self) -> bool:\n        return self._conn_key.is_ssl\n\n    def __str__(self) -> str:\n        return (\n            \"Cannot connect to host {0.host}:{0.port} ssl:{0.ssl} \"\n            \"[{0.certificate_error.__class__.__name__}: \"\n            \"{0.certificate_error.args}]\".format(self)\n        )\n\n\nclass WSMessageTypeError(TypeError):\n    \"\"\"WebSocket message type is not valid.\"\"\"\n", "metadata": {"source": "github.com/aio-libs/aiohttp", "file_path": "aiohttp/client_exceptions.py", "language": "python", "complexity": "complex", "star_count": 15860, "quality_score": 0.98, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": true, "has_documentation": true, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 26, "class_count": 33, "line_count": 389}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "async_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create asynchronous code with proper concurrency handling with asyncio coroutines\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"\nDigest authentication middleware for aiohttp client.\n\nThis middleware implements HTTP Digest Authentication according to RFC 7616,\nproviding a more secure alternative to Basic Authentication. It supports all\nstandard hash algorithms including MD5, SHA, SHA-256, SHA-512 and their session\nvariants, as well as both 'auth' and 'auth-int' quality of protection (qop) options.\n\"\"\"\n\nimport hashlib\nimport os\nimport re\nimport time\nfrom typing import (\n    Callable,\n    Dict,\n    Final,\n    FrozenSet,\n    List,\n    Literal,\n    Tuple,\n    TypedDict,\n    Union,\n)\n\nfrom yarl import URL\n\nfrom . import hdrs\nfrom .client_exceptions import ClientError\nfrom .client_middlewares import ClientHandlerType\nfrom .client_reqrep import ClientRequest, ClientResponse\nfrom .payload import Payload\n\n\nclass DigestAuthChallenge(TypedDict, total=False):\n    realm: str\n    nonce: str\n    qop: str\n    algorithm: str\n    opaque: str\n    domain: str\n    stale: str\n\n\nDigestFunctions: Dict[str, Callable[[bytes], \"hashlib._Hash\"]] = {\n    \"MD5\": hashlib.md5,\n    \"MD5-SESS\": hashlib.md5,\n    \"SHA\": hashlib.sha1,\n    \"SHA-SESS\": hashlib.sha1,\n    \"SHA256\": hashlib.sha256,\n    \"SHA256-SESS\": hashlib.sha256,\n    \"SHA-256\": hashlib.sha256,\n    \"SHA-256-SESS\": hashlib.sha256,\n    \"SHA512\": hashlib.sha512,\n    \"SHA512-SESS\": hashlib.sha512,\n    \"SHA-512\": hashlib.sha512,\n    \"SHA-512-SESS\": hashlib.sha512,\n}\n\n\n# Compile the regex pattern once at module level for performance\n_HEADER_PAIRS_PATTERN = re.compile(\n    r'(\\w+)\\s*=\\s*(?:\"((?:[^\"\\\\]|\\\\.)*)\"|([^\\s,]+))'\n    # |    |  | | |  |    |      |    |  ||     |\n    # +----|--|-|-|--|----|------|----|--||-----|--> alphanumeric key\n    #      +--|-|-|--|----|------|----|--||-----|--> maybe whitespace\n    #         | | |  |    |      |    |  ||     |\n    #         +-|-|--|----|------|----|--||-----|--> = (delimiter)\n    #           +-|--|----|------|----|--||-----|--> maybe whitespace\n    #             |  |    |      |    |  ||     |\n    #             +--|----|------|----|--||-----|--> group quoted or unquoted\n    #                |    |      |    |  ||     |\n    #                +----|------|----|--||-----|--> if quoted...\n    #                     +------|----|--||-----|--> anything but \" or \\\n    #                            +----|--||-----|--> escaped characters allowed\n    #                                 +--||-----|--> or can be empty string\n    #                                    ||     |\n    #                                    +|-----|--> if unquoted...\n    #                                     +-----|--> anything but , or <space>\n    #                                           +--> at least one char req'd\n)\n\n\n# RFC 7616: Challenge parameters to extract\nCHALLENGE_FIELDS: Final[\n    Tuple[\n        Literal[\"realm\", \"nonce\", \"qop\", \"algorithm\", \"opaque\", \"domain\", \"stale\"], ...\n    ]\n] = (\n    \"realm\",\n    \"nonce\",\n    \"qop\",\n    \"algorithm\",\n    \"opaque\",\n    \"domain\",\n    \"stale\",\n)\n\n# Supported digest authentication algorithms\n# Use a tuple of sorted keys for predictable documentation and error messages\nSUPPORTED_ALGORITHMS: Final[Tuple[str, ...]] = tuple(sorted(DigestFunctions.keys()))\n\n# RFC 7616: Fields that require quoting in the Digest auth header\n# These fields must be enclosed in double quotes in the Authorization header.\n# Algorithm, qop, and nc are never quoted per RFC specifications.\n# This frozen set is used by the template-based header construction to\n# automatically determine which fields need quotes.\nQUOTED_AUTH_FIELDS: Final[FrozenSet[str]] = frozenset(\n    {\"username\", \"realm\", \"nonce\", \"uri\", \"response\", \"opaque\", \"cnonce\"}\n)\n\n\ndef escape_quotes(value: str) -> str:\n    \"\"\"Escape double quotes for HTTP header values.\"\"\"\n    return value.replace('\"', '\\\\\"')\n\n\ndef unescape_quotes(value: str) -> str:\n    \"\"\"Unescape double quotes in HTTP header values.\"\"\"\n    return value.replace('\\\\\"', '\"')\n\n\ndef parse_header_pairs(header: str) -> Dict[str, str]:\n    \"\"\"\n    Parse key-value pairs from WWW-Authenticate or similar HTTP headers.\n\n    This function handles the complex format of WWW-Authenticate header values,\n    supporting both quoted and unquoted values, proper handling of commas in\n    quoted values, and whitespace variations per RFC 7616.\n\n    Examples of supported formats:\n      - key1=\"value1\", key2=value2\n      - key1 = \"value1\" , key2=\"value, with, commas\"\n      - key1=value1,key2=\"value2\"\n      - realm=\"example.com\", nonce=\"12345\", qop=\"auth\"\n\n    Args:\n        header: The header value string to parse\n\n    Returns:\n        Dictionary mapping parameter names to their values\n    \"\"\"\n    return {\n        stripped_key: unescape_quotes(quoted_val) if quoted_val else unquoted_val\n        for key, quoted_val, unquoted_val in _HEADER_PAIRS_PATTERN.findall(header)\n        if (stripped_key := key.strip())\n    }\n\n\nclass DigestAuthMiddleware:\n    \"\"\"\n    HTTP digest authentication middleware for aiohttp client.\n\n    This middleware intercepts 401 Unauthorized responses containing a Digest\n    authentication challenge, calculates the appropriate digest credentials,\n    and automatically retries the request with the proper Authorization header.\n\n    Features:\n    - Handles all aspects of Digest authentication handshake automatically\n    - Supports all standard hash algorithms:\n      - MD5, MD5-SESS\n      - SHA, SHA-SESS\n      - SHA256, SHA256-SESS, SHA-256, SHA-256-SESS\n      - SHA512, SHA512-SESS, SHA-512, SHA-512-SESS\n    - Supports 'auth' and 'auth-int' quality of protection modes\n    - Properly handles quoted strings and parameter parsing\n    - Includes replay attack protection with client nonce count tracking\n    - Supports preemptive authentication per RFC 7616 Section 3.6\n\n    Standards compliance:\n    - RFC 7616: HTTP Digest Access Authentication (primary reference)\n    - RFC 2617: HTTP Authentication (deprecated by RFC 7616)\n    - RFC 1945: Section 11.1 (username restrictions)\n\n    Implementation notes:\n    The core digest calculation is inspired by the implementation in\n    https://github.com/requests/requests/blob/v2.18.4/requests/auth.py\n    with added support for modern digest auth features and error handling.\n    \"\"\"\n\n    def __init__(\n        self,\n        login: str,\n        password: str,\n        preemptive: bool = True,\n    ) -> None:\n        if login is None:\n            raise ValueError(\"None is not allowed as login value\")\n\n        if password is None:\n            raise ValueError(\"None is not allowed as password value\")\n\n        if \":\" in login:\n            raise ValueError('A \":\" is not allowed in username (RFC 1945#section-11.1)')\n\n        self._login_str: Final[str] = login\n        self._login_bytes: Final[bytes] = login.encode(\"utf-8\")\n        self._password_bytes: Final[bytes] = password.encode(\"utf-8\")\n\n        self._last_nonce_bytes = b\"\"\n        self._nonce_count = 0\n        self._challenge: DigestAuthChallenge = {}\n        self._preemptive: bool = preemptive\n        # Set of URLs defining the protection space\n        self._protection_space: List[str] = []\n\n    async def _encode(\n        self, method: str, url: URL, body: Union[Payload, Literal[b\"\"]]\n    ) -> str:\n        \"\"\"\n        Build digest authorization header for the current challenge.\n\n        Args:\n            method: The HTTP method (GET, POST, etc.)\n            url: The request URL\n            body: The request body (used for qop=auth-int)\n\n        Returns:\n            A fully formatted Digest authorization header string\n\n        Raises:\n            ClientError: If the challenge is missing required parameters or\n                         contains unsupported values\n\n        \"\"\"\n        challenge = self._challenge\n        if \"realm\" not in challenge:\n            raise ClientError(\n                \"Malformed Digest auth challenge: Missing 'realm' parameter\"\n            )\n\n        if \"nonce\" not in challenge:\n            raise ClientError(\n                \"Malformed Digest auth challenge: Missing 'nonce' parameter\"\n            )\n\n        # Empty realm values are allowed per RFC 7616 (SHOULD, not MUST, contain host name)\n        realm = challenge[\"realm\"]\n        nonce = challenge[\"nonce\"]\n\n        # Empty nonce values are not allowed as they are security-critical for replay protection\n        if not nonce:\n            raise ClientError(\n                \"Security issue: Digest auth challenge contains empty 'nonce' value\"\n            )\n\n        qop_raw = challenge.get(\"qop\", \"\")\n        algorithm = challenge.get(\"algorithm\", \"MD5\").upper()\n        opaque = challenge.get(\"opaque\", \"\")\n\n        # Convert string values to bytes once\n        nonce_bytes = nonce.encode(\"utf-8\")\n        realm_bytes = realm.encode(\"utf-8\")\n        path = URL(url).path_qs\n\n        # Process QoP\n        qop = \"\"\n        qop_bytes = b\"\"\n        if qop_raw:\n            valid_qops = {\"auth\", \"auth-int\"}.intersection(\n                {q.strip() for q in qop_raw.split(\",\") if q.strip()}\n            )\n            if not valid_qops:\n                raise ClientError(\n                    f\"Digest auth error: Unsupported Quality of Protection (qop) value(s): {qop_raw}\"\n                )\n\n            qop = \"auth-int\" if \"auth-int\" in valid_qops else \"auth\"\n            qop_bytes = qop.encode(\"utf-8\")\n\n        if algorithm not in DigestFunctions:\n            raise ClientError(\n                f\"Digest auth error: Unsupported hash algorithm: {algorithm}. \"\n                f\"Supported algorithms: {', '.join(SUPPORTED_ALGORITHMS)}\"\n            )\n        hash_fn: Final = DigestFunctions[algorithm]\n\n        def H(x: bytes) -> bytes:\n            \"\"\"RFC 7616 Section 3: Hash function H(data) = hex(hash(data)).\"\"\"\n            return hash_fn(x).hexdigest().encode()\n\n        def KD(s: bytes, d: bytes) -> bytes:\n            \"\"\"RFC 7616 Section 3: KD(secret, data) = H(concat(secret, \":\", data)).\"\"\"\n            return H(b\":\".join((s, d)))\n\n        # Calculate A1 and A2\n        A1 = b\":\".join((self._login_bytes, realm_bytes, self._password_bytes))\n        A2 = f\"{method.upper()}:{path}\".encode()\n        if qop == \"auth-int\":\n            if isinstance(body, Payload):  # will always be empty bytes unless Payload\n                entity_bytes = await body.as_bytes()  # Get bytes from Payload\n            else:\n                entity_bytes = body\n            entity_hash = H(entity_bytes)\n            A2 = b\":\".join((A2, entity_hash))\n\n        HA1 = H(A1)\n        HA2 = H(A2)\n\n        # Nonce count handling\n        if nonce_bytes == self._last_nonce_bytes:\n            self._nonce_count += 1\n        else:\n            self._nonce_count = 1\n\n        self._last_nonce_bytes = nonce_bytes\n        ncvalue = f\"{self._nonce_count:08x}\"\n        ncvalue_bytes = ncvalue.encode(\"utf-8\")\n\n        # Generate client nonce\n        cnonce = hashlib.sha1(\n            b\"\".join(\n                [\n                    str(self._nonce_count).encode(\"utf-8\"),\n                    nonce_bytes,\n                    time.ctime().encode(\"utf-8\"),\n                    os.urandom(8),\n                ]\n            )\n        ).hexdigest()[:16]\n        cnonce_bytes = cnonce.encode(\"utf-8\")\n\n        # Special handling for session-based algorithms\n        if algorithm.upper().endswith(\"-SESS\"):\n            HA1 = H(b\":\".join((HA1, nonce_bytes, cnonce_bytes)))\n\n        # Calculate the response digest\n        if qop:\n            noncebit = b\":\".join(\n                (nonce_bytes, ncvalue_bytes, cnonce_bytes, qop_bytes, HA2)\n            )\n            response_digest = KD(HA1, noncebit)\n        else:\n            response_digest = KD(HA1, b\":\".join((nonce_bytes, HA2)))\n\n        # Define a dict mapping of header fields to their values\n        # Group fields into always-present, optional, and qop-dependent\n        header_fields = {\n            # Always present fields\n            \"username\": escape_quotes(self._login_str),\n            \"realm\": escape_quotes(realm),\n            \"nonce\": escape_quotes(nonce),\n            \"uri\": path,\n            \"response\": response_digest.decode(),\n            \"algorithm\": algorithm,\n        }\n\n        # Optional fields\n        if opaque:\n            header_fields[\"opaque\"] = escape_quotes(opaque)\n\n        # QoP-dependent fields\n        if qop:\n            header_fields[\"qop\"] = qop\n            header_fields[\"nc\"] = ncvalue\n            header_fields[\"cnonce\"] = cnonce\n\n        # Build header using templates for each field type\n        pairs: List[str] = []\n        for field, value in header_fields.items():\n            if field in QUOTED_AUTH_FIELDS:\n                pairs.append(f'{field}=\"{value}\"')\n            else:\n                pairs.append(f\"{field}={value}\")\n\n        return f\"Digest {', '.join(pairs)}\"\n\n    def _in_protection_space(self, url: URL) -> bool:\n        \"\"\"\n        Check if the given URL is within the current protection space.\n\n        According to RFC 7616, a URI is in the protection space if any URI\n        in the protection space is a prefix of it (after both have been made absolute).\n        \"\"\"\n        request_str = str(url)\n        for space_str in self._protection_space:\n            # Check if request starts with space URL\n            if not request_str.startswith(space_str):\n                continue\n            # Exact match or space ends with / (proper directory prefix)\n            if len(request_str) == len(space_str) or space_str[-1] == \"/\":\n                return True\n            # Check next char is / to ensure proper path boundary\n            if request_str[len(space_str)] == \"/\":\n                return True\n        return False\n\n    def _authenticate(self, response: ClientResponse) -> bool:\n        \"\"\"\n        Takes the given response and tries digest-auth, if needed.\n\n        Returns true if the original request must be resent.\n        \"\"\"\n        if response.status != 401:\n            return False\n\n        auth_header = response.headers.get(\"www-authenticate\", \"\")\n        if not auth_header:\n            return False  # No authentication header present\n\n        method, sep, headers = auth_header.partition(\" \")\n        if not sep:\n            # No space found in www-authenticate header\n            return False  # Malformed auth header, missing scheme separator\n\n        if method.lower() != \"digest\":\n            # Not a digest auth challenge (could be Basic, Bearer, etc.)\n            return False\n\n        if not headers:\n            # We have a digest scheme but no parameters\n            return False  # Malformed digest header, missing parameters\n\n        # We have a digest auth header with content\n        if not (header_pairs := parse_header_pairs(headers)):\n            # Failed to parse any key-value pairs\n            return False  # Malformed digest header, no valid parameters\n\n        # Extract challenge parameters\n        self._challenge = {}\n        for field in CHALLENGE_FIELDS:\n            if value := header_pairs.get(field):\n                self._challenge[field] = value\n\n        # Update protection space based on domain parameter or default to origin\n        origin = response.url.origin()\n\n        if domain := self._challenge.get(\"domain\"):\n            # Parse space-separated list of URIs\n            self._protection_space = []\n            for uri in domain.split():\n                # Remove quotes if present\n                uri = uri.strip('\"')\n                if uri.startswith(\"/\"):\n                    # Path-absolute, relative to origin\n                    self._protection_space.append(str(origin.join(URL(uri))))\n                else:\n                    # Absolute URI\n                    self._protection_space.append(str(URL(uri)))\n        else:\n            # No domain specified, protection space is entire origin\n            self._protection_space = [str(origin)]\n\n        # Return True only if we found at least one challenge parameter\n        return bool(self._challenge)\n\n    async def __call__(\n        self, request: ClientRequest, handler: ClientHandlerType\n    ) -> ClientResponse:\n        \"\"\"Run the digest auth middleware.\"\"\"\n        response = None\n        for retry_count in range(2):\n            # Apply authorization header if:\n            # 1. This is a retry after 401 (retry_count > 0), OR\n            # 2. Preemptive auth is enabled AND we have a challenge AND the URL is in protection space\n            if retry_count > 0 or (\n                self._preemptive\n                and self._challenge\n                and self._in_protection_space(request.url)\n            ):\n                request.headers[hdrs.AUTHORIZATION] = await self._encode(\n                    request.method, request.url, request.body\n                )\n\n            # Send the request\n            response = await handler(request)\n\n            # Check if we need to authenticate\n            if not self._authenticate(response):\n                break\n\n        # At this point, response is guaranteed to be defined\n        assert response is not None\n        return response\n", "metadata": {"source": "github.com/aio-libs/aiohttp", "file_path": "aiohttp/client_middleware_digest_auth.py", "language": "python", "complexity": "complex", "star_count": 15860, "quality_score": 0.98, "features": {"has_classes": true, "has_functions": true, "has_async": true, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": true, "has_type_hints": true, "has_testing": true, "has_logging": true, "function_count": 10, "class_count": 2, "line_count": 475}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "async_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create asynchronous code with proper concurrency handling with asyncio coroutines\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"Client middleware support.\"\"\"\n\nfrom collections.abc import Awaitable, Callable, Sequence\n\nfrom .client_reqrep import ClientRequest, ClientResponse\n\n__all__ = (\"ClientMiddlewareType\", \"ClientHandlerType\", \"build_client_middlewares\")\n\n# Type alias for client request handlers - functions that process requests and return responses\nClientHandlerType = Callable[[ClientRequest], Awaitable[ClientResponse]]\n\n# Type for client middleware - similar to server but uses ClientRequest/ClientResponse\nClientMiddlewareType = Callable[\n    [ClientRequest, ClientHandlerType], Awaitable[ClientResponse]\n]\n\n\ndef build_client_middlewares(\n    handler: ClientHandlerType,\n    middlewares: Sequence[ClientMiddlewareType],\n) -> ClientHandlerType:\n    \"\"\"\n    Apply middlewares to request handler.\n\n    The middlewares are applied in reverse order, so the first middleware\n    in the list wraps all subsequent middlewares and the handler.\n\n    This implementation avoids using partial/update_wrapper to minimize overhead\n    and doesn't cache to avoid holding references to stateful middleware.\n    \"\"\"\n    # Optimize for single middleware case\n    if len(middlewares) == 1:\n        middleware = middlewares[0]\n\n        async def single_middleware_handler(req: ClientRequest) -> ClientResponse:\n            return await middleware(req, handler)\n\n        return single_middleware_handler\n\n    # Build the chain for multiple middlewares\n    current_handler = handler\n\n    for middleware in reversed(middlewares):\n        # Create a new closure that captures the current state\n        def make_wrapper(\n            mw: ClientMiddlewareType, next_h: ClientHandlerType\n        ) -> ClientHandlerType:\n            async def wrapped(req: ClientRequest) -> ClientResponse:\n                return await mw(req, next_h)\n\n            return wrapped\n\n        current_handler = make_wrapper(middleware, current_handler)\n\n    return current_handler\n", "metadata": {"source": "github.com/aio-libs/aiohttp", "file_path": "aiohttp/client_middlewares.py", "language": "python", "complexity": "simple", "star_count": 15860, "quality_score": 0.92, "features": {"has_classes": false, "has_functions": true, "has_async": true, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": true, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 4, "class_count": 0, "line_count": 56}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "async_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create asynchronous code with proper concurrency handling\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import asyncio\nfrom contextlib import suppress\nfrom typing import Any, Optional, Tuple, Type, Union\n\nfrom .base_protocol import BaseProtocol\nfrom .client_exceptions import (\n    ClientConnectionError,\n    ClientOSError,\n    ClientPayloadError,\n    ServerDisconnectedError,\n    SocketTimeoutError,\n)\nfrom .helpers import (\n    _EXC_SENTINEL,\n    EMPTY_BODY_STATUS_CODES,\n    BaseTimerContext,\n    set_exception,\n    set_result,\n)\nfrom .http import HttpResponseParser, RawResponseMessage, WebSocketReader\nfrom .http_exceptions import HttpProcessingError\nfrom .streams import EMPTY_PAYLOAD, DataQueue, StreamReader\n\n\nclass ResponseHandler(BaseProtocol, DataQueue[Tuple[RawResponseMessage, StreamReader]]):\n    \"\"\"Helper class to adapt between Protocol and StreamReader.\"\"\"\n\n    def __init__(self, loop: asyncio.AbstractEventLoop) -> None:\n        BaseProtocol.__init__(self, loop=loop)\n        DataQueue.__init__(self, loop)\n\n        self._should_close = False\n\n        self._payload: Optional[StreamReader] = None\n        self._skip_payload = False\n        self._payload_parser: Optional[WebSocketReader] = None\n\n        self._timer = None\n\n        self._tail = b\"\"\n        self._upgraded = False\n        self._parser: Optional[HttpResponseParser] = None\n\n        self._read_timeout: Optional[float] = None\n        self._read_timeout_handle: Optional[asyncio.TimerHandle] = None\n\n        self._timeout_ceil_threshold: Optional[float] = 5\n\n        self._closed: Union[None, asyncio.Future[None]] = None\n        self._connection_lost_called = False\n\n    @property\n    def closed(self) -> Union[None, asyncio.Future[None]]:\n        \"\"\"Future that is set when the connection is closed.\n\n        This property returns a Future that will be completed when the connection\n        is closed. The Future is created lazily on first access to avoid creating\n        futures that will never be awaited.\n\n        Returns:\n            - A Future[None] if the connection is still open or was closed after\n              this property was accessed\n            - None if connection_lost() was already called before this property\n              was ever accessed (indicating no one is waiting for the closure)\n        \"\"\"\n        if self._closed is None and not self._connection_lost_called:\n            self._closed = self._loop.create_future()\n        return self._closed\n\n    @property\n    def upgraded(self) -> bool:\n        return self._upgraded\n\n    @property\n    def should_close(self) -> bool:\n        return bool(\n            self._should_close\n            or (self._payload is not None and not self._payload.is_eof())\n            or self._upgraded\n            or self._exception is not None\n            or self._payload_parser is not None\n            or self._buffer\n            or self._tail\n        )\n\n    def force_close(self) -> None:\n        self._should_close = True\n\n    def close(self) -> None:\n        self._exception = None  # Break cyclic references\n        transport = self.transport\n        if transport is not None:\n            transport.close()\n            self.transport = None\n            self._payload = None\n            self._drop_timeout()\n\n    def abort(self) -> None:\n        self._exception = None  # Break cyclic references\n        transport = self.transport\n        if transport is not None:\n            transport.abort()\n            self.transport = None\n            self._payload = None\n            self._drop_timeout()\n\n    def is_connected(self) -> bool:\n        return self.transport is not None and not self.transport.is_closing()\n\n    def connection_lost(self, exc: Optional[BaseException]) -> None:\n        self._connection_lost_called = True\n        self._drop_timeout()\n\n        original_connection_error = exc\n        reraised_exc = original_connection_error\n\n        connection_closed_cleanly = original_connection_error is None\n\n        if self._closed is not None:\n            # If someone is waiting for the closed future,\n            # we should set it to None or an exception. If\n            # self._closed is None, it means that\n            # connection_lost() was called already\n            # or nobody is waiting for it.\n            if connection_closed_cleanly:\n                set_result(self._closed, None)\n            else:\n                assert original_connection_error is not None\n                set_exception(\n                    self._closed,\n                    ClientConnectionError(\n                        f\"Connection lost: {original_connection_error !s}\",\n                    ),\n                    original_connection_error,\n                )\n\n        if self._payload_parser is not None:\n            with suppress(Exception):  # FIXME: log this somehow?\n                self._payload_parser.feed_eof()\n\n        uncompleted = None\n        if self._parser is not None:\n            try:\n                uncompleted = self._parser.feed_eof()\n            except Exception as underlying_exc:\n                if self._payload is not None:\n                    client_payload_exc_msg = (\n                        f\"Response payload is not completed: {underlying_exc !r}\"\n                    )\n                    if not connection_closed_cleanly:\n                        client_payload_exc_msg = (\n                            f\"{client_payload_exc_msg !s}. \"\n                            f\"{original_connection_error !r}\"\n                        )\n                    set_exception(\n                        self._payload,\n                        ClientPayloadError(client_payload_exc_msg),\n                        underlying_exc,\n                    )\n\n        if not self.is_eof():\n            if isinstance(original_connection_error, OSError):\n                reraised_exc = ClientOSError(*original_connection_error.args)\n            if connection_closed_cleanly:\n                reraised_exc = ServerDisconnectedError(uncompleted)\n            # assigns self._should_close to True as side effect,\n            # we do it anyway below\n            underlying_non_eof_exc = (\n                _EXC_SENTINEL\n                if connection_closed_cleanly\n                else original_connection_error\n            )\n            assert underlying_non_eof_exc is not None\n            assert reraised_exc is not None\n            self.set_exception(reraised_exc, underlying_non_eof_exc)\n\n        self._should_close = True\n        self._parser = None\n        self._payload = None\n        self._payload_parser = None\n        self._reading_paused = False\n\n        super().connection_lost(reraised_exc)\n\n    def eof_received(self) -> None:\n        # should call parser.feed_eof() most likely\n        self._drop_timeout()\n\n    def pause_reading(self) -> None:\n        super().pause_reading()\n        self._drop_timeout()\n\n    def resume_reading(self) -> None:\n        super().resume_reading()\n        self._reschedule_timeout()\n\n    def set_exception(\n        self,\n        exc: Union[Type[BaseException], BaseException],\n        exc_cause: BaseException = _EXC_SENTINEL,\n    ) -> None:\n        self._should_close = True\n        self._drop_timeout()\n        super().set_exception(exc, exc_cause)\n\n    def set_parser(self, parser: Any, payload: Any) -> None:\n        # TODO: actual types are:\n        #   parser: WebSocketReader\n        #   payload: WebSocketDataQueue\n        # but they are not generi enough\n        # Need an ABC for both types\n        self._payload = payload\n        self._payload_parser = parser\n\n        self._drop_timeout()\n\n        if self._tail:\n            data, self._tail = self._tail, b\"\"\n            self.data_received(data)\n\n    def set_response_params(\n        self,\n        *,\n        timer: Optional[BaseTimerContext] = None,\n        skip_payload: bool = False,\n        read_until_eof: bool = False,\n        auto_decompress: bool = True,\n        read_timeout: Optional[float] = None,\n        read_bufsize: int = 2**16,\n        timeout_ceil_threshold: float = 5,\n        max_line_size: int = 8190,\n        max_field_size: int = 8190,\n    ) -> None:\n        self._skip_payload = skip_payload\n\n        self._read_timeout = read_timeout\n\n        self._timeout_ceil_threshold = timeout_ceil_threshold\n\n        self._parser = HttpResponseParser(\n            self,\n            self._loop,\n            read_bufsize,\n            timer=timer,\n            payload_exception=ClientPayloadError,\n            response_with_body=not skip_payload,\n            read_until_eof=read_until_eof,\n            auto_decompress=auto_decompress,\n            max_line_size=max_line_size,\n            max_field_size=max_field_size,\n        )\n\n        if self._tail:\n            data, self._tail = self._tail, b\"\"\n            self.data_received(data)\n\n    def _drop_timeout(self) -> None:\n        if self._read_timeout_handle is not None:\n            self._read_timeout_handle.cancel()\n            self._read_timeout_handle = None\n\n    def _reschedule_timeout(self) -> None:\n        timeout = self._read_timeout\n        if self._read_timeout_handle is not None:\n            self._read_timeout_handle.cancel()\n\n        if timeout:\n            self._read_timeout_handle = self._loop.call_later(\n                timeout, self._on_read_timeout\n            )\n        else:\n            self._read_timeout_handle = None\n\n    def start_timeout(self) -> None:\n        self._reschedule_timeout()\n\n    @property\n    def read_timeout(self) -> Optional[float]:\n        return self._read_timeout\n\n    @read_timeout.setter\n    def read_timeout(self, read_timeout: Optional[float]) -> None:\n        self._read_timeout = read_timeout\n\n    def _on_read_timeout(self) -> None:\n        exc = SocketTimeoutError(\"Timeout on reading data from socket\")\n        self.set_exception(exc)\n        if self._payload is not None:\n            set_exception(self._payload, exc)\n\n    def data_received(self, data: bytes) -> None:\n        self._reschedule_timeout()\n\n        if not data:\n            return\n\n        # custom payload parser - currently always WebSocketReader\n        if self._payload_parser is not None:\n            eof, tail = self._payload_parser.feed_data(data)\n            if eof:\n                self._payload = None\n                self._payload_parser = None\n\n                if tail:\n                    self.data_received(tail)\n            return\n\n        if self._upgraded or self._parser is None:\n            # i.e. websocket connection, websocket parser is not set yet\n            self._tail += data\n            return\n\n        # parse http messages\n        try:\n            messages, upgraded, tail = self._parser.feed_data(data)\n        except BaseException as underlying_exc:\n            if self.transport is not None:\n                # connection.release() could be called BEFORE\n                # data_received(), the transport is already\n                # closed in this case\n                self.transport.close()\n            # should_close is True after the call\n            if isinstance(underlying_exc, HttpProcessingError):\n                exc = HttpProcessingError(\n                    code=underlying_exc.code,\n                    message=underlying_exc.message,\n                    headers=underlying_exc.headers,\n                )\n            else:\n                exc = HttpProcessingError()\n            self.set_exception(exc, underlying_exc)\n            return\n\n        self._upgraded = upgraded\n\n        payload: Optional[StreamReader] = None\n        for message, payload in messages:\n            if message.should_close:\n                self._should_close = True\n\n            self._payload = payload\n\n            if self._skip_payload or message.code in EMPTY_BODY_STATUS_CODES:\n                self.feed_data((message, EMPTY_PAYLOAD))\n            else:\n                self.feed_data((message, payload))\n\n        if payload is not None:\n            # new message(s) was processed\n            # register timeout handler unsubscribing\n            # either on end-of-stream or immediately for\n            # EMPTY_PAYLOAD\n            if payload is not EMPTY_PAYLOAD:\n                payload.on_eof(self._drop_timeout)\n            else:\n                self._drop_timeout()\n\n        if upgraded and tail:\n            self.data_received(tail)\n", "metadata": {"source": "github.com/aio-libs/aiohttp", "file_path": "aiohttp/client_proto.py", "language": "python", "complexity": "complex", "star_count": 15860, "quality_score": 1.0, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": true, "has_documentation": true, "has_type_hints": true, "has_testing": true, "has_logging": true, "function_count": 22, "class_count": 2, "line_count": 360}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "async_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create asynchronous code with proper concurrency handling with asyncio coroutines\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"WebSocket client for asyncio.\"\"\"\n\nimport asyncio\nimport sys\nfrom types import TracebackType\nfrom typing import Any, Final, Optional, Type\n\nfrom ._websocket.reader import WebSocketDataQueue\nfrom .client_exceptions import ClientError, ServerTimeoutError, WSMessageTypeError\nfrom .client_reqrep import ClientResponse\nfrom .helpers import calculate_timeout_when, frozen_dataclass_decorator, set_result\nfrom .http import (\n    WS_CLOSED_MESSAGE,\n    WS_CLOSING_MESSAGE,\n    WebSocketError,\n    WSCloseCode,\n    WSMessage,\n    WSMsgType,\n)\nfrom .http_websocket import _INTERNAL_RECEIVE_TYPES, WebSocketWriter, WSMessageError\nfrom .streams import EofStream\nfrom .typedefs import (\n    DEFAULT_JSON_DECODER,\n    DEFAULT_JSON_ENCODER,\n    JSONDecoder,\n    JSONEncoder,\n)\n\nif sys.version_info >= (3, 11):\n    import asyncio as async_timeout\nelse:\n    import async_timeout\n\n\n@frozen_dataclass_decorator\nclass ClientWSTimeout:\n    ws_receive: Optional[float] = None\n    ws_close: Optional[float] = None\n\n\nDEFAULT_WS_CLIENT_TIMEOUT: Final[ClientWSTimeout] = ClientWSTimeout(\n    ws_receive=None, ws_close=10.0\n)\n\n\nclass ClientWebSocketResponse:\n    def __init__(\n        self,\n        reader: WebSocketDataQueue,\n        writer: WebSocketWriter,\n        protocol: Optional[str],\n        response: ClientResponse,\n        timeout: ClientWSTimeout,\n        autoclose: bool,\n        autoping: bool,\n        loop: asyncio.AbstractEventLoop,\n        *,\n        heartbeat: Optional[float] = None,\n        compress: int = 0,\n        client_notakeover: bool = False,\n    ) -> None:\n        self._response = response\n        self._conn = response.connection\n\n        self._writer = writer\n        self._reader = reader\n        self._protocol = protocol\n        self._closed = False\n        self._closing = False\n        self._close_code: Optional[int] = None\n        self._timeout = timeout\n        self._autoclose = autoclose\n        self._autoping = autoping\n        self._heartbeat = heartbeat\n        self._heartbeat_cb: Optional[asyncio.TimerHandle] = None\n        self._heartbeat_when: float = 0.0\n        if heartbeat is not None:\n            self._pong_heartbeat = heartbeat / 2.0\n        self._pong_response_cb: Optional[asyncio.TimerHandle] = None\n        self._loop = loop\n        self._waiting: bool = False\n        self._close_wait: Optional[asyncio.Future[None]] = None\n        self._exception: Optional[BaseException] = None\n        self._compress = compress\n        self._client_notakeover = client_notakeover\n        self._ping_task: Optional[asyncio.Task[None]] = None\n\n        self._reset_heartbeat()\n\n    def _cancel_heartbeat(self) -> None:\n        self._cancel_pong_response_cb()\n        if self._heartbeat_cb is not None:\n            self._heartbeat_cb.cancel()\n            self._heartbeat_cb = None\n        if self._ping_task is not None:\n            self._ping_task.cancel()\n            self._ping_task = None\n\n    def _cancel_pong_response_cb(self) -> None:\n        if self._pong_response_cb is not None:\n            self._pong_response_cb.cancel()\n            self._pong_response_cb = None\n\n    def _reset_heartbeat(self) -> None:\n        if self._heartbeat is None:\n            return\n        self._cancel_pong_response_cb()\n        loop = self._loop\n        assert loop is not None\n        conn = self._conn\n        timeout_ceil_threshold = (\n            conn._connector._timeout_ceil_threshold if conn is not None else 5\n        )\n        now = loop.time()\n        when = calculate_timeout_when(now, self._heartbeat, timeout_ceil_threshold)\n        self._heartbeat_when = when\n        if self._heartbeat_cb is None:\n            # We do not cancel the previous heartbeat_cb here because\n            # it generates a significant amount of TimerHandle churn\n            # which causes asyncio to rebuild the heap frequently.\n            # Instead _send_heartbeat() will reschedule the next\n            # heartbeat if it fires too early.\n            self._heartbeat_cb = loop.call_at(when, self._send_heartbeat)\n\n    def _send_heartbeat(self) -> None:\n        self._heartbeat_cb = None\n        loop = self._loop\n        now = loop.time()\n        if now < self._heartbeat_when:\n            # Heartbeat fired too early, reschedule\n            self._heartbeat_cb = loop.call_at(\n                self._heartbeat_when, self._send_heartbeat\n            )\n            return\n\n        conn = self._conn\n        timeout_ceil_threshold = (\n            conn._connector._timeout_ceil_threshold if conn is not None else 5\n        )\n        when = calculate_timeout_when(now, self._pong_heartbeat, timeout_ceil_threshold)\n        self._cancel_pong_response_cb()\n        self._pong_response_cb = loop.call_at(when, self._pong_not_received)\n\n        coro = self._writer.send_frame(b\"\", WSMsgType.PING)\n        if sys.version_info >= (3, 12):\n            # Optimization for Python 3.12, try to send the ping\n            # immediately to avoid having to schedule\n            # the task on the event loop.\n            ping_task = asyncio.Task(coro, loop=loop, eager_start=True)\n        else:\n            ping_task = loop.create_task(coro)\n\n        if not ping_task.done():\n            self._ping_task = ping_task\n            ping_task.add_done_callback(self._ping_task_done)\n        else:\n            self._ping_task_done(ping_task)\n\n    def _ping_task_done(self, task: \"asyncio.Task[None]\") -> None:\n        \"\"\"Callback for when the ping task completes.\"\"\"\n        if not task.cancelled() and (exc := task.exception()):\n            self._handle_ping_pong_exception(exc)\n        self._ping_task = None\n\n    def _pong_not_received(self) -> None:\n        self._handle_ping_pong_exception(\n            ServerTimeoutError(f\"No PONG received after {self._pong_heartbeat} seconds\")\n        )\n\n    def _handle_ping_pong_exception(self, exc: BaseException) -> None:\n        \"\"\"Handle exceptions raised during ping/pong processing.\"\"\"\n        if self._closed:\n            return\n        self._set_closed()\n        self._close_code = WSCloseCode.ABNORMAL_CLOSURE\n        self._exception = exc\n        self._response.close()\n        if self._waiting and not self._closing:\n            self._reader.feed_data(WSMessageError(data=exc, extra=None))\n\n    def _set_closed(self) -> None:\n        \"\"\"Set the connection to closed.\n\n        Cancel any heartbeat timers and set the closed flag.\n        \"\"\"\n        self._closed = True\n        self._cancel_heartbeat()\n\n    def _set_closing(self) -> None:\n        \"\"\"Set the connection to closing.\n\n        Cancel any heartbeat timers and set the closing flag.\n        \"\"\"\n        self._closing = True\n        self._cancel_heartbeat()\n\n    @property\n    def closed(self) -> bool:\n        return self._closed\n\n    @property\n    def close_code(self) -> Optional[int]:\n        return self._close_code\n\n    @property\n    def protocol(self) -> Optional[str]:\n        return self._protocol\n\n    @property\n    def compress(self) -> int:\n        return self._compress\n\n    @property\n    def client_notakeover(self) -> bool:\n        return self._client_notakeover\n\n    def get_extra_info(self, name: str, default: Any = None) -> Any:\n        \"\"\"extra info from connection transport\"\"\"\n        conn = self._response.connection\n        if conn is None:\n            return default\n        transport = conn.transport\n        if transport is None:\n            return default\n        return transport.get_extra_info(name, default)\n\n    def exception(self) -> Optional[BaseException]:\n        return self._exception\n\n    async def ping(self, message: bytes = b\"\") -> None:\n        await self._writer.send_frame(message, WSMsgType.PING)\n\n    async def pong(self, message: bytes = b\"\") -> None:\n        await self._writer.send_frame(message, WSMsgType.PONG)\n\n    async def send_frame(\n        self, message: bytes, opcode: WSMsgType, compress: Optional[int] = None\n    ) -> None:\n        \"\"\"Send a frame over the websocket.\"\"\"\n        await self._writer.send_frame(message, opcode, compress)\n\n    async def send_str(self, data: str, compress: Optional[int] = None) -> None:\n        if not isinstance(data, str):\n            raise TypeError(\"data argument must be str (%r)\" % type(data))\n        await self._writer.send_frame(\n            data.encode(\"utf-8\"), WSMsgType.TEXT, compress=compress\n        )\n\n    async def send_bytes(self, data: bytes, compress: Optional[int] = None) -> None:\n        if not isinstance(data, (bytes, bytearray, memoryview)):\n            raise TypeError(\"data argument must be byte-ish (%r)\" % type(data))\n        await self._writer.send_frame(data, WSMsgType.BINARY, compress=compress)\n\n    async def send_json(\n        self,\n        data: Any,\n        compress: Optional[int] = None,\n        *,\n        dumps: JSONEncoder = DEFAULT_JSON_ENCODER,\n    ) -> None:\n        await self.send_str(dumps(data), compress=compress)\n\n    async def close(self, *, code: int = WSCloseCode.OK, message: bytes = b\"\") -> bool:\n        # we need to break `receive()` cycle first,\n        # `close()` may be called from different task\n        if self._waiting and not self._closing:\n            assert self._loop is not None\n            self._close_wait = self._loop.create_future()\n            self._set_closing()\n            self._reader.feed_data(WS_CLOSING_MESSAGE)\n            await self._close_wait\n\n        if self._closed:\n            return False\n\n        self._set_closed()\n        try:\n            await self._writer.close(code, message)\n        except asyncio.CancelledError:\n            self._close_code = WSCloseCode.ABNORMAL_CLOSURE\n            self._response.close()\n            raise\n        except Exception as exc:\n            self._close_code = WSCloseCode.ABNORMAL_CLOSURE\n            self._exception = exc\n            self._response.close()\n            return True\n\n        if self._close_code:\n            self._response.close()\n            return True\n\n        while True:\n            try:\n                async with async_timeout.timeout(self._timeout.ws_close):\n                    msg = await self._reader.read()\n            except asyncio.CancelledError:\n                self._close_code = WSCloseCode.ABNORMAL_CLOSURE\n                self._response.close()\n                raise\n            except Exception as exc:\n                self._close_code = WSCloseCode.ABNORMAL_CLOSURE\n                self._exception = exc\n                self._response.close()\n                return True\n\n            if msg.type is WSMsgType.CLOSE:\n                self._close_code = msg.data\n                self._response.close()\n                return True\n\n    async def receive(self, timeout: Optional[float] = None) -> WSMessage:\n        receive_timeout = timeout or self._timeout.ws_receive\n\n        while True:\n            if self._waiting:\n                raise RuntimeError(\"Concurrent call to receive() is not allowed\")\n\n            if self._closed:\n                return WS_CLOSED_MESSAGE\n            elif self._closing:\n                await self.close()\n                return WS_CLOSED_MESSAGE\n\n            try:\n                self._waiting = True\n                try:\n                    if receive_timeout:\n                        # Entering the context manager and creating\n                        # Timeout() object can take almost 50% of the\n                        # run time in this loop so we avoid it if\n                        # there is no read timeout.\n                        async with async_timeout.timeout(receive_timeout):\n                            msg = await self._reader.read()\n                    else:\n                        msg = await self._reader.read()\n                    self._reset_heartbeat()\n                finally:\n                    self._waiting = False\n                    if self._close_wait:\n                        set_result(self._close_wait, None)\n            except (asyncio.CancelledError, asyncio.TimeoutError):\n                self._close_code = WSCloseCode.ABNORMAL_CLOSURE\n                raise\n            except EofStream:\n                self._close_code = WSCloseCode.OK\n                await self.close()\n                return WS_CLOSED_MESSAGE\n            except ClientError:\n                # Likely ServerDisconnectedError when connection is lost\n                self._set_closed()\n                self._close_code = WSCloseCode.ABNORMAL_CLOSURE\n                return WS_CLOSED_MESSAGE\n            except WebSocketError as exc:\n                self._close_code = exc.code\n                await self.close(code=exc.code)\n                return WSMessageError(data=exc)\n            except Exception as exc:\n                self._exception = exc\n                self._set_closing()\n                self._close_code = WSCloseCode.ABNORMAL_CLOSURE\n                await self.close()\n                return WSMessageError(data=exc)\n\n            if msg.type not in _INTERNAL_RECEIVE_TYPES:\n                # If its not a close/closing/ping/pong message\n                # we can return it immediately\n                return msg\n\n            if msg.type is WSMsgType.CLOSE:\n                self._set_closing()\n                self._close_code = msg.data\n                # Could be closed elsewhere while awaiting reader\n                if not self._closed and self._autoclose:  # type: ignore[redundant-expr]\n                    await self.close()\n            elif msg.type is WSMsgType.CLOSING:\n                self._set_closing()\n            elif msg.type is WSMsgType.PING and self._autoping:\n                await self.pong(msg.data)\n                continue\n            elif msg.type is WSMsgType.PONG and self._autoping:\n                continue\n\n            return msg\n\n    async def receive_str(self, *, timeout: Optional[float] = None) -> str:\n        msg = await self.receive(timeout)\n        if msg.type is not WSMsgType.TEXT:\n            raise WSMessageTypeError(\n                f\"Received message {msg.type}:{msg.data!r} is not WSMsgType.TEXT\"\n            )\n        return msg.data\n\n    async def receive_bytes(self, *, timeout: Optional[float] = None) -> bytes:\n        msg = await self.receive(timeout)\n        if msg.type is not WSMsgType.BINARY:\n            raise WSMessageTypeError(\n                f\"Received message {msg.type}:{msg.data!r} is not WSMsgType.BINARY\"\n            )\n        return msg.data\n\n    async def receive_json(\n        self,\n        *,\n        loads: JSONDecoder = DEFAULT_JSON_DECODER,\n        timeout: Optional[float] = None,\n    ) -> Any:\n        data = await self.receive_str(timeout=timeout)\n        return loads(data)\n\n    def __aiter__(self) -> \"ClientWebSocketResponse\":\n        return self\n\n    async def __anext__(self) -> WSMessage:\n        msg = await self.receive()\n        if msg.type in (WSMsgType.CLOSE, WSMsgType.CLOSING, WSMsgType.CLOSED):\n            raise StopAsyncIteration\n        return msg\n\n    async def __aenter__(self) -> \"ClientWebSocketResponse\":\n        return self\n\n    async def __aexit__(\n        self,\n        exc_type: Optional[Type[BaseException]],\n        exc_val: Optional[BaseException],\n        exc_tb: Optional[TracebackType],\n    ) -> None:\n        await self.close()\n", "metadata": {"source": "github.com/aio-libs/aiohttp", "file_path": "aiohttp/client_ws.py", "language": "python", "complexity": "complex", "star_count": 15860, "quality_score": 1.0, "features": {"has_classes": true, "has_functions": true, "has_async": true, "has_decorators": true, "has_imports": true, "has_error_handling": true, "has_documentation": true, "has_type_hints": true, "has_testing": true, "has_logging": false, "function_count": 32, "class_count": 2, "line_count": 430}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "async_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create asynchronous code with proper concurrency handling with asyncio coroutines\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import asyncio\nimport sys\nimport zlib\nfrom concurrent.futures import Executor\nfrom typing import Any, Final, Optional, Protocol, TypedDict, cast\n\nif sys.version_info >= (3, 12):\n    from collections.abc import Buffer\nelse:\n    from typing import Union\n\n    Buffer = Union[bytes, bytearray, \"memoryview[int]\", \"memoryview[bytes]\"]\n\ntry:\n    try:\n        import brotlicffi as brotli\n    except ImportError:\n        import brotli\n\n    HAS_BROTLI = True\nexcept ImportError:\n    HAS_BROTLI = False\n\nif sys.version_info >= (3, 14):\n    import compression.zstd  # noqa: I900\n\n    HAS_ZSTD = True\nelse:\n    try:\n        import zstandard\n\n        HAS_ZSTD = True\n    except ImportError:\n        HAS_ZSTD = False\n\nMAX_SYNC_CHUNK_SIZE = 1024\n\n\nclass ZLibCompressObjProtocol(Protocol):\n    def compress(self, data: Buffer) -> bytes: ...\n    def flush(self, mode: int = ..., /) -> bytes: ...\n\n\nclass ZLibDecompressObjProtocol(Protocol):\n    def decompress(self, data: Buffer, max_length: int = ...) -> bytes: ...\n    def flush(self, length: int = ..., /) -> bytes: ...\n\n    @property\n    def eof(self) -> bool: ...\n\n\nclass ZLibBackendProtocol(Protocol):\n    MAX_WBITS: int\n    Z_FULL_FLUSH: int\n    Z_SYNC_FLUSH: int\n    Z_BEST_SPEED: int\n    Z_FINISH: int\n\n    def compressobj(\n        self,\n        level: int = ...,\n        method: int = ...,\n        wbits: int = ...,\n        memLevel: int = ...,\n        strategy: int = ...,\n        zdict: Optional[Buffer] = ...,\n    ) -> ZLibCompressObjProtocol: ...\n    def decompressobj(\n        self, wbits: int = ..., zdict: Buffer = ...\n    ) -> ZLibDecompressObjProtocol: ...\n\n    def compress(\n        self, data: Buffer, /, level: int = ..., wbits: int = ...\n    ) -> bytes: ...\n    def decompress(\n        self, data: Buffer, /, wbits: int = ..., bufsize: int = ...\n    ) -> bytes: ...\n\n\nclass CompressObjArgs(TypedDict, total=False):\n    wbits: int\n    strategy: int\n    level: int\n\n\nclass ZLibBackendWrapper:\n    def __init__(self, _zlib_backend: ZLibBackendProtocol):\n        self._zlib_backend: ZLibBackendProtocol = _zlib_backend\n\n    @property\n    def name(self) -> str:\n        return getattr(self._zlib_backend, \"__name__\", \"undefined\")\n\n    @property\n    def MAX_WBITS(self) -> int:\n        return self._zlib_backend.MAX_WBITS\n\n    @property\n    def Z_FULL_FLUSH(self) -> int:\n        return self._zlib_backend.Z_FULL_FLUSH\n\n    @property\n    def Z_SYNC_FLUSH(self) -> int:\n        return self._zlib_backend.Z_SYNC_FLUSH\n\n    @property\n    def Z_BEST_SPEED(self) -> int:\n        return self._zlib_backend.Z_BEST_SPEED\n\n    @property\n    def Z_FINISH(self) -> int:\n        return self._zlib_backend.Z_FINISH\n\n    def compressobj(self, *args: Any, **kwargs: Any) -> ZLibCompressObjProtocol:\n        return self._zlib_backend.compressobj(*args, **kwargs)\n\n    def decompressobj(self, *args: Any, **kwargs: Any) -> ZLibDecompressObjProtocol:\n        return self._zlib_backend.decompressobj(*args, **kwargs)\n\n    def compress(self, data: Buffer, *args: Any, **kwargs: Any) -> bytes:\n        return self._zlib_backend.compress(data, *args, **kwargs)\n\n    def decompress(self, data: Buffer, *args: Any, **kwargs: Any) -> bytes:\n        return self._zlib_backend.decompress(data, *args, **kwargs)\n\n    # Everything not explicitly listed in the Protocol we just pass through\n    def __getattr__(self, attrname: str) -> Any:\n        return getattr(self._zlib_backend, attrname)\n\n\nZLibBackend: ZLibBackendWrapper = ZLibBackendWrapper(zlib)\n\n\ndef set_zlib_backend(new_zlib_backend: ZLibBackendProtocol) -> None:\n    ZLibBackend._zlib_backend = new_zlib_backend\n\n\ndef encoding_to_mode(\n    encoding: Optional[str] = None,\n    suppress_deflate_header: bool = False,\n) -> int:\n    if encoding == \"gzip\":\n        return 16 + ZLibBackend.MAX_WBITS\n\n    return -ZLibBackend.MAX_WBITS if suppress_deflate_header else ZLibBackend.MAX_WBITS\n\n\nclass ZlibBaseHandler:\n    def __init__(\n        self,\n        mode: int,\n        executor: Optional[Executor] = None,\n        max_sync_chunk_size: Optional[int] = MAX_SYNC_CHUNK_SIZE,\n    ):\n        self._mode = mode\n        self._executor = executor\n        self._max_sync_chunk_size = max_sync_chunk_size\n\n\nclass ZLibCompressor(ZlibBaseHandler):\n    def __init__(\n        self,\n        encoding: Optional[str] = None,\n        suppress_deflate_header: bool = False,\n        level: Optional[int] = None,\n        wbits: Optional[int] = None,\n        strategy: Optional[int] = None,\n        executor: Optional[Executor] = None,\n        max_sync_chunk_size: Optional[int] = MAX_SYNC_CHUNK_SIZE,\n    ):\n        super().__init__(\n            mode=(\n                encoding_to_mode(encoding, suppress_deflate_header)\n                if wbits is None\n                else wbits\n            ),\n            executor=executor,\n            max_sync_chunk_size=max_sync_chunk_size,\n        )\n        self._zlib_backend: Final = ZLibBackendWrapper(ZLibBackend._zlib_backend)\n\n        kwargs: CompressObjArgs = {}\n        kwargs[\"wbits\"] = self._mode\n        if strategy is not None:\n            kwargs[\"strategy\"] = strategy\n        if level is not None:\n            kwargs[\"level\"] = level\n        self._compressor = self._zlib_backend.compressobj(**kwargs)\n        self._compress_lock = asyncio.Lock()\n\n    def compress_sync(self, data: Buffer) -> bytes:\n        return self._compressor.compress(data)\n\n    async def compress(self, data: Buffer) -> bytes:\n        \"\"\"Compress the data and returned the compressed bytes.\n\n        Note that flush() must be called after the last call to compress()\n\n        If the data size is large than the max_sync_chunk_size, the compression\n        will be done in the executor. Otherwise, the compression will be done\n        in the event loop.\n        \"\"\"\n        async with self._compress_lock:\n            # To ensure the stream is consistent in the event\n            # there are multiple writers, we need to lock\n            # the compressor so that only one writer can\n            # compress at a time.\n            if (\n                self._max_sync_chunk_size is not None\n                and len(data) > self._max_sync_chunk_size\n            ):\n                return await asyncio.get_running_loop().run_in_executor(\n                    self._executor, self._compressor.compress, data\n                )\n            return self.compress_sync(data)\n\n    def flush(self, mode: Optional[int] = None) -> bytes:\n        return self._compressor.flush(\n            mode if mode is not None else self._zlib_backend.Z_FINISH\n        )\n\n\nclass ZLibDecompressor(ZlibBaseHandler):\n    def __init__(\n        self,\n        encoding: Optional[str] = None,\n        suppress_deflate_header: bool = False,\n        executor: Optional[Executor] = None,\n        max_sync_chunk_size: Optional[int] = MAX_SYNC_CHUNK_SIZE,\n    ):\n        super().__init__(\n            mode=encoding_to_mode(encoding, suppress_deflate_header),\n            executor=executor,\n            max_sync_chunk_size=max_sync_chunk_size,\n        )\n        self._zlib_backend: Final = ZLibBackendWrapper(ZLibBackend._zlib_backend)\n        self._decompressor = self._zlib_backend.decompressobj(wbits=self._mode)\n\n    def decompress_sync(self, data: Buffer, max_length: int = 0) -> bytes:\n        return self._decompressor.decompress(data, max_length)\n\n    async def decompress(self, data: Buffer, max_length: int = 0) -> bytes:\n        \"\"\"Decompress the data and return the decompressed bytes.\n\n        If the data size is large than the max_sync_chunk_size, the decompression\n        will be done in the executor. Otherwise, the decompression will be done\n        in the event loop.\n        \"\"\"\n        if (\n            self._max_sync_chunk_size is not None\n            and len(data) > self._max_sync_chunk_size\n        ):\n            return await asyncio.get_running_loop().run_in_executor(\n                self._executor, self._decompressor.decompress, data, max_length\n            )\n        return self.decompress_sync(data, max_length)\n\n    def flush(self, length: int = 0) -> bytes:\n        return (\n            self._decompressor.flush(length)\n            if length > 0\n            else self._decompressor.flush()\n        )\n\n    @property\n    def eof(self) -> bool:\n        return self._decompressor.eof\n\n\nclass BrotliDecompressor:\n    # Supports both 'brotlipy' and 'Brotli' packages\n    # since they share an import name. The top branches\n    # are for 'brotlipy' and bottom branches for 'Brotli'\n    def __init__(self) -> None:\n        if not HAS_BROTLI:\n            raise RuntimeError(\n                \"The brotli decompression is not available. \"\n                \"Please install `Brotli` module\"\n            )\n        self._obj = brotli.Decompressor()\n\n    def decompress_sync(self, data: Buffer) -> bytes:\n        if hasattr(self._obj, \"decompress\"):\n            return cast(bytes, self._obj.decompress(data))\n        return cast(bytes, self._obj.process(data))\n\n    def flush(self) -> bytes:\n        if hasattr(self._obj, \"flush\"):\n            return cast(bytes, self._obj.flush())\n        return b\"\"\n\n\nclass ZSTDDecompressor:\n    def __init__(self) -> None:\n        if not HAS_ZSTD:\n            raise RuntimeError(\n                \"The zstd decompression is not available. \"\n                \"Please install `zstandard` module\"\n            )\n        if sys.version_info >= (3, 14):\n            self._obj = compression.zstd.ZstdDecompressor()\n        else:\n            self._obj = zstandard.ZstdDecompressor()\n\n    def decompress_sync(self, data: bytes) -> bytes:\n        return self._obj.decompress(data)\n\n    def flush(self) -> bytes:\n        return b\"\"\n", "metadata": {"source": "github.com/aio-libs/aiohttp", "file_path": "aiohttp/compression_utils.py", "language": "python", "complexity": "complex", "star_count": 15860, "quality_score": 0.98, "features": {"has_classes": true, "has_functions": true, "has_async": true, "has_decorators": true, "has_imports": true, "has_error_handling": true, "has_documentation": true, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 39, "class_count": 10, "line_count": 310}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "async_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create asynchronous code with proper concurrency handling\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"Various helper functions\"\"\"\n\nimport asyncio\nimport base64\nimport binascii\nimport contextlib\nimport dataclasses\nimport datetime\nimport enum\nimport functools\nimport inspect\nimport netrc\nimport os\nimport platform\nimport re\nimport sys\nimport time\nimport warnings\nimport weakref\nfrom collections import namedtuple\nfrom contextlib import suppress\nfrom email.parser import HeaderParser\nfrom email.utils import parsedate\nfrom http.cookies import SimpleCookie\nfrom math import ceil\nfrom pathlib import Path\nfrom types import MappingProxyType, TracebackType\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Callable,\n    ContextManager,\n    Dict,\n    Generic,\n    Iterable,\n    Iterator,\n    List,\n    Mapping,\n    Optional,\n    Protocol,\n    Tuple,\n    Type,\n    TypeVar,\n    Union,\n    final,\n    get_args,\n    overload,\n)\nfrom urllib.parse import quote\nfrom urllib.request import getproxies, proxy_bypass\n\nfrom multidict import CIMultiDict, MultiDict, MultiDictProxy, MultiMapping\nfrom propcache.api import under_cached_property as reify\nfrom yarl import URL\n\nfrom . import hdrs\nfrom .log import client_logger\nfrom .typedefs import PathLike  # noqa\n\nif sys.version_info >= (3, 11):\n    import asyncio as async_timeout\nelse:\n    import async_timeout\n\nif TYPE_CHECKING:\n    from dataclasses import dataclass as frozen_dataclass_decorator\nelif sys.version_info < (3, 10):\n    frozen_dataclass_decorator = functools.partial(dataclasses.dataclass, frozen=True)\nelse:\n    frozen_dataclass_decorator = functools.partial(\n        dataclasses.dataclass, frozen=True, slots=True\n    )\n\n__all__ = (\"BasicAuth\", \"ChainMapProxy\", \"ETag\", \"frozen_dataclass_decorator\", \"reify\")\n\nPY_310 = sys.version_info >= (3, 10)\n\nCOOKIE_MAX_LENGTH = 4096\n\n_T = TypeVar(\"_T\")\n_S = TypeVar(\"_S\")\n\n_SENTINEL = enum.Enum(\"_SENTINEL\", \"sentinel\")\nsentinel = _SENTINEL.sentinel\n\nNO_EXTENSIONS = bool(os.environ.get(\"AIOHTTP_NO_EXTENSIONS\"))\n\n# https://datatracker.ietf.org/doc/html/rfc9112#section-6.3-2.1\nEMPTY_BODY_STATUS_CODES = frozenset((204, 304, *range(100, 200)))\n# https://datatracker.ietf.org/doc/html/rfc9112#section-6.3-2.1\n# https://datatracker.ietf.org/doc/html/rfc9112#section-6.3-2.2\nEMPTY_BODY_METHODS = hdrs.METH_HEAD_ALL\n\nDEBUG = sys.flags.dev_mode or (\n    not sys.flags.ignore_environment and bool(os.environ.get(\"PYTHONASYNCIODEBUG\"))\n)\n\n\nCHAR = {chr(i) for i in range(0, 128)}\nCTL = {chr(i) for i in range(0, 32)} | {\n    chr(127),\n}\nSEPARATORS = {\n    \"(\",\n    \")\",\n    \"<\",\n    \">\",\n    \"@\",\n    \",\",\n    \";\",\n    \":\",\n    \"\\\\\",\n    '\"',\n    \"/\",\n    \"[\",\n    \"]\",\n    \"?\",\n    \"=\",\n    \"{\",\n    \"}\",\n    \" \",\n    chr(9),\n}\nTOKEN = CHAR ^ CTL ^ SEPARATORS\n\n\njson_re = re.compile(r\"(?:application/|[\\w.-]+/[\\w.+-]+?\\+)json$\", re.IGNORECASE)\n\n\nclass BasicAuth(namedtuple(\"BasicAuth\", [\"login\", \"password\", \"encoding\"])):\n    \"\"\"Http basic authentication helper.\"\"\"\n\n    def __new__(\n        cls, login: str, password: str = \"\", encoding: str = \"latin1\"\n    ) -> \"BasicAuth\":\n        if login is None:\n            raise ValueError(\"None is not allowed as login value\")\n\n        if password is None:\n            raise ValueError(\"None is not allowed as password value\")\n\n        if \":\" in login:\n            raise ValueError('A \":\" is not allowed in login (RFC 1945#section-11.1)')\n\n        return super().__new__(cls, login, password, encoding)\n\n    @classmethod\n    def decode(cls, auth_header: str, encoding: str = \"latin1\") -> \"BasicAuth\":\n        \"\"\"Create a BasicAuth object from an Authorization HTTP header.\"\"\"\n        try:\n            auth_type, encoded_credentials = auth_header.split(\" \", 1)\n        except ValueError:\n            raise ValueError(\"Could not parse authorization header.\")\n\n        if auth_type.lower() != \"basic\":\n            raise ValueError(\"Unknown authorization method %s\" % auth_type)\n\n        try:\n            decoded = base64.b64decode(\n                encoded_credentials.encode(\"ascii\"), validate=True\n            ).decode(encoding)\n        except binascii.Error:\n            raise ValueError(\"Invalid base64 encoding.\")\n\n        try:\n            # RFC 2617 HTTP Authentication\n            # https://www.ietf.org/rfc/rfc2617.txt\n            # the colon must be present, but the username and password may be\n            # otherwise blank.\n            username, password = decoded.split(\":\", 1)\n        except ValueError:\n            raise ValueError(\"Invalid credentials.\")\n\n        return cls(username, password, encoding=encoding)\n\n    @classmethod\n    def from_url(cls, url: URL, *, encoding: str = \"latin1\") -> Optional[\"BasicAuth\"]:\n        \"\"\"Create BasicAuth from url.\"\"\"\n        if not isinstance(url, URL):\n            raise TypeError(\"url should be yarl.URL instance\")\n        # Check raw_user and raw_password first as yarl is likely\n        # to already have these values parsed from the netloc in the cache.\n        if url.raw_user is None and url.raw_password is None:\n            return None\n        return cls(url.user or \"\", url.password or \"\", encoding=encoding)\n\n    def encode(self) -> str:\n        \"\"\"Encode credentials.\"\"\"\n        creds = (f\"{self.login}:{self.password}\").encode(self.encoding)\n        return \"Basic %s\" % base64.b64encode(creds).decode(self.encoding)\n\n\ndef strip_auth_from_url(url: URL) -> Tuple[URL, Optional[BasicAuth]]:\n    \"\"\"Remove user and password from URL if present and return BasicAuth object.\"\"\"\n    # Check raw_user and raw_password first as yarl is likely\n    # to already have these values parsed from the netloc in the cache.\n    if url.raw_user is None and url.raw_password is None:\n        return url, None\n    return url.with_user(None), BasicAuth(url.user or \"\", url.password or \"\")\n\n\ndef netrc_from_env() -> Optional[netrc.netrc]:\n    \"\"\"Load netrc from file.\n\n    Attempt to load it from the path specified by the env-var\n    NETRC or in the default location in the user's home directory.\n\n    Returns None if it couldn't be found or fails to parse.\n    \"\"\"\n    netrc_env = os.environ.get(\"NETRC\")\n\n    if netrc_env is not None:\n        netrc_path = Path(netrc_env)\n    else:\n        try:\n            home_dir = Path.home()\n        except RuntimeError as e:\n            # if pathlib can't resolve home, it may raise a RuntimeError\n            client_logger.debug(\n                \"Could not resolve home directory when \"\n                \"trying to look for .netrc file: %s\",\n                e,\n            )\n            return None\n\n        netrc_path = home_dir / (\n            \"_netrc\" if platform.system() == \"Windows\" else \".netrc\"\n        )\n\n    try:\n        return netrc.netrc(str(netrc_path))\n    except netrc.NetrcParseError as e:\n        client_logger.warning(\"Could not parse .netrc file: %s\", e)\n    except OSError as e:\n        netrc_exists = False\n        with contextlib.suppress(OSError):\n            netrc_exists = netrc_path.is_file()\n        # we couldn't read the file (doesn't exist, permissions, etc.)\n        if netrc_env or netrc_exists:\n            # only warn if the environment wanted us to load it,\n            # or it appears like the default file does actually exist\n            client_logger.warning(\"Could not read .netrc file: %s\", e)\n\n    return None\n\n\n@frozen_dataclass_decorator\nclass ProxyInfo:\n    proxy: URL\n    proxy_auth: Optional[BasicAuth]\n\n\ndef basicauth_from_netrc(netrc_obj: Optional[netrc.netrc], host: str) -> BasicAuth:\n    \"\"\"\n    Return :py:class:`~aiohttp.BasicAuth` credentials for ``host`` from ``netrc_obj``.\n\n    :raises LookupError: if ``netrc_obj`` is :py:data:`None` or if no\n            entry is found for the ``host``.\n    \"\"\"\n    if netrc_obj is None:\n        raise LookupError(\"No .netrc file found\")\n    auth_from_netrc = netrc_obj.authenticators(host)\n\n    if auth_from_netrc is None:\n        raise LookupError(f\"No entry for {host!s} found in the `.netrc` file.\")\n    login, account, password = auth_from_netrc\n\n    # TODO(PY311): username = login or account\n    # Up to python 3.10, account could be None if not specified,\n    # and login will be empty string if not specified. From 3.11,\n    # login and account will be empty string if not specified.\n    username = login if (login or account is None) else account\n\n    # TODO(PY311): Remove this, as password will be empty string\n    # if not specified\n    if password is None:\n        password = \"\"  # type: ignore[unreachable]\n\n    return BasicAuth(username, password)\n\n\ndef proxies_from_env() -> Dict[str, ProxyInfo]:\n    proxy_urls = {\n        k: URL(v)\n        for k, v in getproxies().items()\n        if k in (\"http\", \"https\", \"ws\", \"wss\")\n    }\n    netrc_obj = netrc_from_env()\n    stripped = {k: strip_auth_from_url(v) for k, v in proxy_urls.items()}\n    ret = {}\n    for proto, val in stripped.items():\n        proxy, auth = val\n        if proxy.scheme in (\"https\", \"wss\"):\n            client_logger.warning(\n                \"%s proxies %s are not supported, ignoring\", proxy.scheme.upper(), proxy\n            )\n            continue\n        if netrc_obj and auth is None:\n            if proxy.host is not None:\n                try:\n                    auth = basicauth_from_netrc(netrc_obj, proxy.host)\n                except LookupError:\n                    auth = None\n        ret[proto] = ProxyInfo(proxy, auth)\n    return ret\n\n\ndef get_env_proxy_for_url(url: URL) -> Tuple[URL, Optional[BasicAuth]]:\n    \"\"\"Get a permitted proxy for the given URL from the env.\"\"\"\n    if url.host is not None and proxy_bypass(url.host):\n        raise LookupError(f\"Proxying is disallowed for `{url.host!r}`\")\n\n    proxies_in_env = proxies_from_env()\n    try:\n        proxy_info = proxies_in_env[url.scheme]\n    except KeyError:\n        raise LookupError(f\"No proxies found for `{url!s}` in the env\")\n    else:\n        return proxy_info.proxy, proxy_info.proxy_auth\n\n\n@frozen_dataclass_decorator\nclass MimeType:\n    type: str\n    subtype: str\n    suffix: str\n    parameters: \"MultiDictProxy[str]\"\n\n\n@functools.lru_cache(maxsize=56)\ndef parse_mimetype(mimetype: str) -> MimeType:\n    \"\"\"Parses a MIME type into its components.\n\n    mimetype is a MIME type string.\n\n    Returns a MimeType object.\n\n    Example:\n\n    >>> parse_mimetype('text/html; charset=utf-8')\n    MimeType(type='text', subtype='html', suffix='',\n             parameters={'charset': 'utf-8'})\n\n    \"\"\"\n    if not mimetype:\n        return MimeType(\n            type=\"\", subtype=\"\", suffix=\"\", parameters=MultiDictProxy(MultiDict())\n        )\n\n    parts = mimetype.split(\";\")\n    params: MultiDict[str] = MultiDict()\n    for item in parts[1:]:\n        if not item:\n            continue\n        key, _, value = item.partition(\"=\")\n        params.add(key.lower().strip(), value.strip(' \"'))\n\n    fulltype = parts[0].strip().lower()\n    if fulltype == \"*\":\n        fulltype = \"*/*\"\n\n    mtype, _, stype = fulltype.partition(\"/\")\n    stype, _, suffix = stype.partition(\"+\")\n\n    return MimeType(\n        type=mtype, subtype=stype, suffix=suffix, parameters=MultiDictProxy(params)\n    )\n\n\n@functools.lru_cache(maxsize=56)\ndef parse_content_type(raw: str) -> Tuple[str, MappingProxyType[str, str]]:\n    \"\"\"Parse Content-Type header.\n\n    Returns a tuple of the parsed content type and a\n    MappingProxyType of parameters.\n    \"\"\"\n    msg = HeaderParser().parsestr(f\"Content-Type: {raw}\")\n    content_type = msg.get_content_type()\n    params = msg.get_params(())\n    content_dict = dict(params[1:])  # First element is content type again\n    return content_type, MappingProxyType(content_dict)\n\n\ndef guess_filename(obj: Any, default: Optional[str] = None) -> Optional[str]:\n    name = getattr(obj, \"name\", None)\n    if name and isinstance(name, str) and name[0] != \"<\" and name[-1] != \">\":\n        return Path(name).name\n    return default\n\n\nnot_qtext_re = re.compile(r\"[^\\041\\043-\\133\\135-\\176]\")\nQCONTENT = {chr(i) for i in range(0x20, 0x7F)} | {\"\\t\"}\n\n\ndef quoted_string(content: str) -> str:\n    \"\"\"Return 7-bit content as quoted-string.\n\n    Format content into a quoted-string as defined in RFC5322 for\n    Internet Message Format. Notice that this is not the 8-bit HTTP\n    format, but the 7-bit email format. Content must be in usascii or\n    a ValueError is raised.\n    \"\"\"\n    if not (QCONTENT > set(content)):\n        raise ValueError(f\"bad content for quoted-string {content!r}\")\n    return not_qtext_re.sub(lambda x: \"\\\\\" + x.group(0), content)\n\n\ndef content_disposition_header(\n    disptype: str,\n    quote_fields: bool = True,\n    _charset: str = \"utf-8\",\n    params: Optional[Dict[str, str]] = None,\n) -> str:\n    \"\"\"Sets ``Content-Disposition`` header for MIME.\n\n    This is the MIME payload Content-Disposition header from RFC 2183\n    and RFC 7579 section 4.2, not the HTTP Content-Disposition from\n    RFC 6266.\n\n    disptype is a disposition type: inline, attachment, form-data.\n    Should be valid extension token (see RFC 2183)\n\n    quote_fields performs value quoting to 7-bit MIME headers\n    according to RFC 7578. Set to quote_fields to False if recipient\n    can take 8-bit file names and field values.\n\n    _charset specifies the charset to use when quote_fields is True.\n\n    params is a dict with disposition params.\n    \"\"\"\n    if not disptype or not (TOKEN > set(disptype)):\n        raise ValueError(f\"bad content disposition type {disptype!r}\")\n\n    value = disptype\n    if params:\n        lparams = []\n        for key, val in params.items():\n            if not key or not (TOKEN > set(key)):\n                raise ValueError(f\"bad content disposition parameter {key!r}={val!r}\")\n            if quote_fields:\n                if key.lower() == \"filename\":\n                    qval = quote(val, \"\", encoding=_charset)\n                    lparams.append((key, '\"%s\"' % qval))\n                else:\n                    try:\n                        qval = quoted_string(val)\n                    except ValueError:\n                        qval = \"\".join(\n                            (_charset, \"''\", quote(val, \"\", encoding=_charset))\n                        )\n                        lparams.append((key + \"*\", qval))\n                    else:\n                        lparams.append((key, '\"%s\"' % qval))\n            else:\n                qval = val.replace(\"\\\\\", \"\\\\\\\\\").replace('\"', '\\\\\"')\n                lparams.append((key, '\"%s\"' % qval))\n        sparams = \"; \".join(\"=\".join(pair) for pair in lparams)\n        value = \"; \".join((value, sparams))\n    return value\n\n\ndef is_expected_content_type(\n    response_content_type: str, expected_content_type: str\n) -> bool:\n    \"\"\"Checks if received content type is processable as an expected one.\n\n    Both arguments should be given without parameters.\n    \"\"\"\n    if expected_content_type == \"application/json\":\n        return json_re.match(response_content_type) is not None\n    return expected_content_type in response_content_type\n\n\ndef is_ip_address(host: Optional[str]) -> bool:\n    \"\"\"Check if host looks like an IP Address.\n\n    This check is only meant as a heuristic to ensure that\n    a host is not a domain name.\n    \"\"\"\n    if not host:\n        return False\n    # For a host to be an ipv4 address, it must be all numeric.\n    # The host must contain a colon to be an IPv6 address.\n    return \":\" in host or host.replace(\".\", \"\").isdigit()\n\n\n_cached_current_datetime: Optional[int] = None\n_cached_formatted_datetime = \"\"\n\n\ndef rfc822_formatted_time() -> str:\n    global _cached_current_datetime\n    global _cached_formatted_datetime\n\n    now = int(time.time())\n    if now != _cached_current_datetime:\n        # Weekday and month names for HTTP date/time formatting;\n        # always English!\n        # Tuples are constants stored in codeobject!\n        _weekdayname = (\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\")\n        _monthname = (\n            \"\",  # Dummy so we can use 1-based month numbers\n            \"Jan\",\n            \"Feb\",\n            \"Mar\",\n            \"Apr\",\n            \"May\",\n            \"Jun\",\n            \"Jul\",\n            \"Aug\",\n            \"Sep\",\n            \"Oct\",\n            \"Nov\",\n            \"Dec\",\n        )\n\n        year, month, day, hh, mm, ss, wd, *tail = time.gmtime(now)\n        _cached_formatted_datetime = \"%s, %02d %3s %4d %02d:%02d:%02d GMT\" % (\n            _weekdayname[wd],\n            day,\n            _monthname[month],\n            year,\n            hh,\n            mm,\n            ss,\n        )\n        _cached_current_datetime = now\n    return _cached_formatted_datetime\n\n\ndef _weakref_handle(info: \"Tuple[weakref.ref[object], str]\") -> None:\n    ref, name = info\n    ob = ref()\n    if ob is not None:\n        with suppress(Exception):\n            getattr(ob, name)()\n\n\ndef weakref_handle(\n    ob: object,\n    name: str,\n    timeout: Optional[float],\n    loop: asyncio.AbstractEventLoop,\n    timeout_ceil_threshold: float = 5,\n) -> Optional[asyncio.TimerHandle]:\n    if timeout is not None and timeout > 0:\n        when = loop.time() + timeout\n        if timeout >= timeout_ceil_threshold:\n            when = ceil(when)\n\n        return loop.call_at(when, _weakref_handle, (weakref.ref(ob), name))\n    return None\n\n\ndef call_later(\n    cb: Callable[[], Any],\n    timeout: Optional[float],\n    loop: asyncio.AbstractEventLoop,\n    timeout_ceil_threshold: float = 5,\n) -> Optional[asyncio.TimerHandle]:\n    if timeout is None or timeout <= 0:\n        return None\n    now = loop.time()\n    when = calculate_timeout_when(now, timeout, timeout_ceil_threshold)\n    return loop.call_at(when, cb)\n\n\ndef calculate_timeout_when(\n    loop_time: float,\n    timeout: float,\n    timeout_ceiling_threshold: float,\n) -> float:\n    \"\"\"Calculate when to execute a timeout.\"\"\"\n    when = loop_time + timeout\n    if timeout > timeout_ceiling_threshold:\n        return ceil(when)\n    return when\n\n\nclass TimeoutHandle:\n    \"\"\"Timeout handle\"\"\"\n\n    __slots__ = (\"_timeout\", \"_loop\", \"_ceil_threshold\", \"_callbacks\")\n\n    def __init__(\n        self,\n        loop: asyncio.AbstractEventLoop,\n        timeout: Optional[float],\n        ceil_threshold: float = 5,\n    ) -> None:\n        self._timeout = timeout\n        self._loop = loop\n        self._ceil_threshold = ceil_threshold\n        self._callbacks: List[\n            Tuple[Callable[..., None], Tuple[Any, ...], Dict[str, Any]]\n        ] = []\n\n    def register(\n        self, callback: Callable[..., None], *args: Any, **kwargs: Any\n    ) -> None:\n        self._callbacks.append((callback, args, kwargs))\n\n    def close(self) -> None:\n        self._callbacks.clear()\n\n    def start(self) -> Optional[asyncio.TimerHandle]:\n        timeout = self._timeout\n        if timeout is not None and timeout > 0:\n            when = self._loop.time() + timeout\n            if timeout >= self._ceil_threshold:\n                when = ceil(when)\n            return self._loop.call_at(when, self.__call__)\n        else:\n            return None\n\n    def timer(self) -> \"BaseTimerContext\":\n        if self._timeout is not None and self._timeout > 0:\n            timer = TimerContext(self._loop)\n            self.register(timer.timeout)\n            return timer\n        else:\n            return TimerNoop()\n\n    def __call__(self) -> None:\n        for cb, args, kwargs in self._callbacks:\n            with suppress(Exception):\n                cb(*args, **kwargs)\n\n        self._callbacks.clear()\n\n\nclass BaseTimerContext(ContextManager[\"BaseTimerContext\"]):\n\n    __slots__ = ()\n\n    def assert_timeout(self) -> None:\n        \"\"\"Raise TimeoutError if timeout has been exceeded.\"\"\"\n\n\nclass TimerNoop(BaseTimerContext):\n\n    __slots__ = ()\n\n    def __enter__(self) -> BaseTimerContext:\n        return self\n\n    def __exit__(\n        self,\n        exc_type: Optional[Type[BaseException]],\n        exc_val: Optional[BaseException],\n        exc_tb: Optional[TracebackType],\n    ) -> None:\n        return\n\n\nclass TimerContext(BaseTimerContext):\n    \"\"\"Low resolution timeout context manager\"\"\"\n\n    __slots__ = (\"_loop\", \"_tasks\", \"_cancelled\", \"_cancelling\")\n\n    def __init__(self, loop: asyncio.AbstractEventLoop) -> None:\n        self._loop = loop\n        self._tasks: List[asyncio.Task[Any]] = []\n        self._cancelled = False\n        self._cancelling = 0\n\n    def assert_timeout(self) -> None:\n        \"\"\"Raise TimeoutError if timer has already been cancelled.\"\"\"\n        if self._cancelled:\n            raise asyncio.TimeoutError from None\n\n    def __enter__(self) -> BaseTimerContext:\n        task = asyncio.current_task(loop=self._loop)\n        if task is None:\n            raise RuntimeError(\"Timeout context manager should be used inside a task\")\n\n        if sys.version_info >= (3, 11):\n            # Remember if the task was already cancelling\n            # so when we __exit__ we can decide if we should\n            # raise asyncio.TimeoutError or let the cancellation propagate\n            self._cancelling = task.cancelling()\n\n        if self._cancelled:\n            raise asyncio.TimeoutError from None\n\n        self._tasks.append(task)\n        return self\n\n    def __exit__(\n        self,\n        exc_type: Optional[Type[BaseException]],\n        exc_val: Optional[BaseException],\n        exc_tb: Optional[TracebackType],\n    ) -> Optional[bool]:\n        enter_task: Optional[asyncio.Task[Any]] = None\n        if self._tasks:\n            enter_task = self._tasks.pop()\n\n        if exc_type is asyncio.CancelledError and self._cancelled:\n            assert enter_task is not None\n            # The timeout was hit, and the task was cancelled\n            # so we need to uncancel the last task that entered the context manager\n            # since the cancellation should not leak out of the context manager\n            if sys.version_info >= (3, 11):\n                # If the task was already cancelling don't raise\n                # asyncio.TimeoutError and instead return None\n                # to allow the cancellation to propagate\n                if enter_task.uncancel() > self._cancelling:\n                    return None\n            raise asyncio.TimeoutError from exc_val\n        return None\n\n    def timeout(self) -> None:\n        if not self._cancelled:\n            for task in set(self._tasks):\n                task.cancel()\n\n            self._cancelled = True\n\n\ndef ceil_timeout(\n    delay: Optional[float], ceil_threshold: float = 5\n) -> async_timeout.Timeout:\n    if delay is None or delay <= 0:\n        return async_timeout.timeout(None)\n\n    loop = asyncio.get_running_loop()\n    now = loop.time()\n    when = now + delay\n    if delay > ceil_threshold:\n        when = ceil(when)\n    return async_timeout.timeout_at(when)\n\n\nclass HeadersMixin:\n    \"\"\"Mixin for handling headers.\"\"\"\n\n    _headers: MultiMapping[str]\n    _content_type: Optional[str] = None\n    _content_dict: Optional[Dict[str, str]] = None\n    _stored_content_type: Union[str, None, _SENTINEL] = sentinel\n\n    def _parse_content_type(self, raw: Optional[str]) -> None:\n        self._stored_content_type = raw\n        if raw is None:\n            # default value according to RFC 2616\n            self._content_type = \"application/octet-stream\"\n            self._content_dict = {}\n        else:\n            content_type, content_mapping_proxy = parse_content_type(raw)\n            self._content_type = content_type\n            # _content_dict needs to be mutable so we can update it\n            self._content_dict = content_mapping_proxy.copy()\n\n    @property\n    def content_type(self) -> str:\n        \"\"\"The value of content part for Content-Type HTTP header.\"\"\"\n        raw = self._headers.get(hdrs.CONTENT_TYPE)\n        if self._stored_content_type != raw:\n            self._parse_content_type(raw)\n        assert self._content_type is not None\n        return self._content_type\n\n    @property\n    def charset(self) -> Optional[str]:\n        \"\"\"The value of charset part for Content-Type HTTP header.\"\"\"\n        raw = self._headers.get(hdrs.CONTENT_TYPE)\n        if self._stored_content_type != raw:\n            self._parse_content_type(raw)\n        assert self._content_dict is not None\n        return self._content_dict.get(\"charset\")\n\n    @property\n    def content_length(self) -> Optional[int]:\n        \"\"\"The value of Content-Length HTTP header.\"\"\"\n        content_length = self._headers.get(hdrs.CONTENT_LENGTH)\n        return None if content_length is None else int(content_length)\n\n\ndef set_result(fut: \"asyncio.Future[_T]\", result: _T) -> None:\n    if not fut.done():\n        fut.set_result(result)\n\n\n_EXC_SENTINEL = BaseException()\n\n\nclass ErrorableProtocol(Protocol):\n    def set_exception(\n        self,\n        exc: Union[Type[BaseException], BaseException],\n        exc_cause: BaseException = ...,\n    ) -> None: ...\n\n\ndef set_exception(\n    fut: Union[\"asyncio.Future[_T]\", ErrorableProtocol],\n    exc: Union[Type[BaseException], BaseException],\n    exc_cause: BaseException = _EXC_SENTINEL,\n) -> None:\n    \"\"\"Set future exception.\n\n    If the future is marked as complete, this function is a no-op.\n\n    :param exc_cause: An exception that is a direct cause of ``exc``.\n                      Only set if provided.\n    \"\"\"\n    if asyncio.isfuture(fut) and fut.done():\n        return\n\n    exc_is_sentinel = exc_cause is _EXC_SENTINEL\n    exc_causes_itself = exc is exc_cause\n    if not exc_is_sentinel and not exc_causes_itself:\n        exc.__cause__ = exc_cause\n\n    fut.set_exception(exc)\n\n\n@functools.total_ordering\nclass AppKey(Generic[_T]):\n    \"\"\"Keys for static typing support in Application.\"\"\"\n\n    __slots__ = (\"_name\", \"_t\", \"__orig_class__\")\n\n    # This may be set by Python when instantiating with a generic type. We need to\n    # support this, in order to support types that are not concrete classes,\n    # like Iterable, which can't be passed as the second parameter to __init__.\n    __orig_class__: Type[object]\n\n    # TODO(PY314): Change Type to TypeForm (this should resolve unreachable below).\n    def __init__(self, name: str, t: Optional[Type[_T]] = None):\n        # Prefix with module name to help deduplicate key names.\n        frame = inspect.currentframe()\n        while frame:\n            if frame.f_code.co_name == \"<module>\":\n                module: str = frame.f_globals[\"__name__\"]\n                break\n            frame = frame.f_back\n        else:\n            raise RuntimeError(\"Failed to get module name.\")\n\n        # https://github.com/python/mypy/issues/14209\n        self._name = module + \".\" + name  # type: ignore[possibly-undefined]\n        self._t = t\n\n    def __lt__(self, other: object) -> bool:\n        if isinstance(other, AppKey):\n            return self._name < other._name\n        return True  # Order AppKey above other types.\n\n    def __repr__(self) -> str:\n        t = self._t\n        if t is None:\n            with suppress(AttributeError):\n                # Set to type arg.\n                t = get_args(self.__orig_class__)[0]\n\n        if t is None:\n            t_repr = \"<<Unknown>>\"\n        elif isinstance(t, type):\n            if t.__module__ == \"builtins\":\n                t_repr = t.__qualname__\n            else:\n                t_repr = f\"{t.__module__}.{t.__qualname__}\"\n        else:\n            t_repr = repr(t)  # type: ignore[unreachable]\n        return f\"<AppKey({self._name}, type={t_repr})>\"\n\n\n@final\nclass ChainMapProxy(Mapping[Union[str, AppKey[Any]], Any]):\n    __slots__ = (\"_maps\",)\n\n    def __init__(self, maps: Iterable[Mapping[Union[str, AppKey[Any]], Any]]) -> None:\n        self._maps = tuple(maps)\n\n    def __init_subclass__(cls) -> None:\n        raise TypeError(\n            \"Inheritance class {} from ChainMapProxy \"\n            \"is forbidden\".format(cls.__name__)\n        )\n\n    @overload  # type: ignore[override]\n    def __getitem__(self, key: AppKey[_T]) -> _T: ...\n\n    @overload\n    def __getitem__(self, key: str) -> Any: ...\n\n    def __getitem__(self, key: Union[str, AppKey[_T]]) -> Any:\n        for mapping in self._maps:\n            try:\n                return mapping[key]\n            except KeyError:\n                pass\n        raise KeyError(key)\n\n    @overload  # type: ignore[override]\n    def get(self, key: AppKey[_T], default: _S) -> Union[_T, _S]: ...\n\n    @overload\n    def get(self, key: AppKey[_T], default: None = ...) -> Optional[_T]: ...\n\n    @overload\n    def get(self, key: str, default: Any = ...) -> Any: ...\n\n    def get(self, key: Union[str, AppKey[_T]], default: Any = None) -> Any:\n        try:\n            return self[key]\n        except KeyError:\n            return default\n\n    def __len__(self) -> int:\n        # reuses stored hash values if possible\n        return len(set().union(*self._maps))\n\n    def __iter__(self) -> Iterator[Union[str, AppKey[Any]]]:\n        d: Dict[Union[str, AppKey[Any]], Any] = {}\n        for mapping in reversed(self._maps):\n            # reuses stored hash values if possible\n            d.update(mapping)\n        return iter(d)\n\n    def __contains__(self, key: object) -> bool:\n        return any(key in m for m in self._maps)\n\n    def __bool__(self) -> bool:\n        return any(self._maps)\n\n    def __repr__(self) -> str:\n        content = \", \".join(map(repr, self._maps))\n        return f\"ChainMapProxy({content})\"\n\n\nclass CookieMixin:\n    \"\"\"Mixin for handling cookies.\"\"\"\n\n    _cookies: Optional[SimpleCookie] = None\n\n    @property\n    def cookies(self) -> SimpleCookie:\n        if self._cookies is None:\n            self._cookies = SimpleCookie()\n        return self._cookies\n\n    def set_cookie(\n        self,\n        name: str,\n        value: str,\n        *,\n        expires: Optional[str] = None,\n        domain: Optional[str] = None,\n        max_age: Optional[Union[int, str]] = None,\n        path: str = \"/\",\n        secure: Optional[bool] = None,\n        httponly: Optional[bool] = None,\n        samesite: Optional[str] = None,\n        partitioned: Optional[bool] = None,\n    ) -> None:\n        \"\"\"Set or update response cookie.\n\n        Sets new cookie or updates existent with new value.\n        Also updates only those params which are not None.\n        \"\"\"\n        if self._cookies is None:\n            self._cookies = SimpleCookie()\n\n        self._cookies[name] = value\n        c = self._cookies[name]\n\n        if expires is not None:\n            c[\"expires\"] = expires\n        elif c.get(\"expires\") == \"Thu, 01 Jan 1970 00:00:00 GMT\":\n            del c[\"expires\"]\n\n        if domain is not None:\n            c[\"domain\"] = domain\n\n        if max_age is not None:\n            c[\"max-age\"] = str(max_age)\n        elif \"max-age\" in c:\n            del c[\"max-age\"]\n\n        c[\"path\"] = path\n\n        if secure is not None:\n            c[\"secure\"] = secure\n        if httponly is not None:\n            c[\"httponly\"] = httponly\n        if samesite is not None:\n            c[\"samesite\"] = samesite\n\n        if partitioned is not None:\n            c[\"partitioned\"] = partitioned\n\n        if DEBUG:\n            cookie_length = len(c.output(header=\"\")[1:])\n            if cookie_length > COOKIE_MAX_LENGTH:\n                warnings.warn(\n                    \"The size of is too large, it might get ignored by the client.\",\n                    UserWarning,\n                    stacklevel=2,\n                )\n\n    def del_cookie(\n        self,\n        name: str,\n        *,\n        domain: Optional[str] = None,\n        path: str = \"/\",\n        secure: Optional[bool] = None,\n        httponly: Optional[bool] = None,\n        samesite: Optional[str] = None,\n    ) -> None:\n        \"\"\"Delete cookie.\n\n        Creates new empty expired cookie.\n        \"\"\"\n        # TODO: do we need domain/path here?\n        if self._cookies is not None:\n            self._cookies.pop(name, None)\n        self.set_cookie(\n            name,\n            \"\",\n            max_age=0,\n            expires=\"Thu, 01 Jan 1970 00:00:00 GMT\",\n            domain=domain,\n            path=path,\n            secure=secure,\n            httponly=httponly,\n            samesite=samesite,\n        )\n\n\ndef populate_with_cookies(headers: \"CIMultiDict[str]\", cookies: SimpleCookie) -> None:\n    for cookie in cookies.values():\n        value = cookie.output(header=\"\")[1:]\n        headers.add(hdrs.SET_COOKIE, value)\n\n\n# https://tools.ietf.org/html/rfc7232#section-2.3\n_ETAGC = r\"[!\\x23-\\x7E\\x80-\\xff]+\"\n_ETAGC_RE = re.compile(_ETAGC)\n_QUOTED_ETAG = rf'(W/)?\"({_ETAGC})\"'\nQUOTED_ETAG_RE = re.compile(_QUOTED_ETAG)\nLIST_QUOTED_ETAG_RE = re.compile(rf\"({_QUOTED_ETAG})(?:\\s*,\\s*|$)|(.)\")\n\nETAG_ANY = \"*\"\n\n\n@frozen_dataclass_decorator\nclass ETag:\n    value: str\n    is_weak: bool = False\n\n\ndef validate_etag_value(value: str) -> None:\n    if value != ETAG_ANY and not _ETAGC_RE.fullmatch(value):\n        raise ValueError(\n            f\"Value {value!r} is not a valid etag. Maybe it contains '\\\"'?\"\n        )\n\n\ndef parse_http_date(date_str: Optional[str]) -> Optional[datetime.datetime]:\n    \"\"\"Process a date string, return a datetime object\"\"\"\n    if date_str is not None:\n        timetuple = parsedate(date_str)\n        if timetuple is not None:\n            with suppress(ValueError):\n                return datetime.datetime(*timetuple[:6], tzinfo=datetime.timezone.utc)\n    return None\n\n\n@functools.lru_cache\ndef must_be_empty_body(method: str, code: int) -> bool:\n    \"\"\"Check if a request must return an empty body.\"\"\"\n    return (\n        code in EMPTY_BODY_STATUS_CODES\n        or method in EMPTY_BODY_METHODS\n        or (200 <= code < 300 and method in hdrs.METH_CONNECT_ALL)\n    )\n\n\ndef should_remove_content_length(method: str, code: int) -> bool:\n    \"\"\"Check if a Content-Length header should be removed.\n\n    This should always be a subset of must_be_empty_body\n    \"\"\"\n    # https://www.rfc-editor.org/rfc/rfc9110.html#section-8.6-8\n    # https://www.rfc-editor.org/rfc/rfc9110.html#section-15.4.5-4\n    return code in EMPTY_BODY_STATUS_CODES or (\n        200 <= code < 300 and method in hdrs.METH_CONNECT_ALL\n    )\n", "metadata": {"source": "github.com/aio-libs/aiohttp", "file_path": "aiohttp/helpers.py", "language": "python", "complexity": "complex", "star_count": 15860, "quality_score": 0.9400000000000001, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": true, "has_documentation": true, "has_type_hints": true, "has_testing": true, "has_logging": true, "function_count": 68, "class_count": 15, "line_count": 1093}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "async_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create asynchronous code with proper concurrency handling\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"Application module.\"\"\"\n\nfrom aiohttp import web\n\nfrom .containers import Container\nfrom . import handlers\n\n\ndef create_app() -> web.Application:\n    container = Container()\n    container.config.giphy.api_key.from_env(\"GIPHY_API_KEY\")\n\n    app = web.Application()\n    app.container = container\n    app.add_routes([\n        web.get(\"/\", handlers.index),\n    ])\n    return app\n\n\nif __name__ == \"__main__\":\n    app = create_app()\n    web.run_app(app)\n", "metadata": {"source": "github.com/ets-labs/python-dependency-injector", "file_path": "examples/miniapps/aiohttp/giphynavigator/application.py", "language": "python", "complexity": "simple", "star_count": 4481, "quality_score": 0.75, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": true, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 1, "class_count": 0, "line_count": 24}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "async_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create asynchronous code with proper concurrency handling with asyncio coroutines\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"Giphy client module.\"\"\"\n\nfrom aiohttp import ClientSession, ClientTimeout\n\n\nclass GiphyClient:\n\n    API_URL = \"https://api.giphy.com/v1\"\n\n    def __init__(self, api_key, timeout):\n        self._api_key = api_key\n        self._timeout = ClientTimeout(timeout)\n\n    async def search(self, query, limit):\n        \"\"\"Make search API call and return result.\"\"\"\n        url = f\"{self.API_URL}/gifs/search\"\n        params = {\n            \"q\": query,\n            \"api_key\": self._api_key,\n            \"limit\": limit,\n        }\n        async with ClientSession(timeout=self._timeout) as session:\n            async with session.get(url, params=params) as response:\n                if response.status != 200:\n                    response.raise_for_status()\n                return await response.json()\n", "metadata": {"source": "github.com/ets-labs/python-dependency-injector", "file_path": "examples/miniapps/aiohttp/giphynavigator/giphy.py", "language": "python", "complexity": "simple", "star_count": 4481, "quality_score": 0.75, "features": {"has_classes": true, "has_functions": true, "has_async": true, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": true, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 2, "class_count": 1, "line_count": 27}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "async_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create asynchronous code with proper concurrency handling with asyncio coroutines\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"Handlers module.\"\"\"\n\nfrom aiohttp import web\nfrom dependency_injector.wiring import inject, Provide\n\nfrom .services import SearchService\nfrom .containers import Container\n\n\n@inject\nasync def index(\n        request: web.Request,\n        search_service: SearchService = Provide[Container.search_service],\n        default_query: str = Provide[Container.config.default.query],\n        default_limit: int = Provide[Container.config.default.limit.as_int()],\n) -> web.Response:\n    query = request.query.get(\"query\", default_query)\n    limit = int(request.query.get(\"limit\", default_limit))\n\n    gifs = await search_service.search(query, limit)\n\n    return web.json_response(\n        {\n            \"query\": query,\n            \"limit\": limit,\n            \"gifs\": gifs,\n        },\n    )\n", "metadata": {"source": "github.com/ets-labs/python-dependency-injector", "file_path": "examples/miniapps/aiohttp/giphynavigator/handlers.py", "language": "python", "complexity": "simple", "star_count": 4481, "quality_score": 0.75, "features": {"has_classes": false, "has_functions": true, "has_async": true, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": true, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 1, "class_count": 0, "line_count": 29}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "async_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create asynchronous code with proper concurrency handling with asyncio coroutines\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"Services module.\"\"\"\n\nfrom .giphy import GiphyClient\n\n\nclass SearchService:\n\n    def __init__(self, giphy_client: GiphyClient):\n        self._giphy_client = giphy_client\n\n    async def search(self, query, limit):\n        \"\"\"Search for gifs and return formatted data.\"\"\"\n        if not query:\n            return []\n\n        result = await self._giphy_client.search(query, limit)\n\n        return [{\"url\": gif[\"url\"]} for gif in result[\"data\"]]\n", "metadata": {"source": "github.com/ets-labs/python-dependency-injector", "file_path": "examples/miniapps/aiohttp/giphynavigator/services.py", "language": "python", "complexity": "simple", "star_count": 4481, "quality_score": 0.75, "features": {"has_classes": true, "has_functions": true, "has_async": true, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": true, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 2, "class_count": 1, "line_count": 19}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "async_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create asynchronous code with proper concurrency handling with asyncio coroutines\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"Tests module.\"\"\"\n\nfrom unittest import mock\n\nimport pytest\nimport pytest_asyncio\n\nfrom giphynavigator.application import create_app\nfrom giphynavigator.giphy import GiphyClient\n\n\npytestmark = pytest.mark.asyncio\n\n\n@pytest.fixture\ndef app():\n    app = create_app()\n    yield app\n    app.container.unwire()\n\n\n@pytest_asyncio.fixture\nasync def client(app, aiohttp_client):\n    return await aiohttp_client(app)\n\n\nasync def test_index(client, app):\n    giphy_client_mock = mock.AsyncMock(spec=GiphyClient)\n    giphy_client_mock.search.return_value = {\n        \"data\": [\n            {\"url\": \"https://giphy.com/gif1.gif\"},\n            {\"url\": \"https://giphy.com/gif2.gif\"},\n        ],\n    }\n\n    with app.container.giphy_client.override(giphy_client_mock):\n        response = await client.get(\n            \"/\",\n            params={\n                \"query\": \"test\",\n                \"limit\": 10,\n            },\n        )\n\n    assert response.status == 200\n    data = await response.json()\n    assert data == {\n        \"query\": \"test\",\n        \"limit\": 10,\n        \"gifs\": [\n            {\"url\": \"https://giphy.com/gif1.gif\"},\n            {\"url\": \"https://giphy.com/gif2.gif\"},\n        ],\n    }\n\n\nasync def test_index_no_data(client, app):\n    giphy_client_mock = mock.AsyncMock(spec=GiphyClient)\n    giphy_client_mock.search.return_value = {\n        \"data\": [],\n    }\n\n    with app.container.giphy_client.override(giphy_client_mock):\n        response = await client.get(\"/\")\n\n    assert response.status == 200\n    data = await response.json()\n    assert data[\"gifs\"] == []\n\n\nasync def test_index_default_params(client, app):\n    giphy_client_mock = mock.AsyncMock(spec=GiphyClient)\n    giphy_client_mock.search.return_value = {\n        \"data\": [],\n    }\n\n    with app.container.giphy_client.override(giphy_client_mock):\n        response = await client.get(\"/\")\n\n    assert response.status == 200\n    data = await response.json()\n    assert data[\"query\"] == app.container.config.default.query()\n    assert data[\"limit\"] == app.container.config.default.limit()\n", "metadata": {"source": "github.com/ets-labs/python-dependency-injector", "file_path": "examples/miniapps/aiohttp/giphynavigator/tests.py", "language": "python", "complexity": "medium", "star_count": 4481, "quality_score": 0.8099999999999999, "features": {"has_classes": false, "has_functions": true, "has_async": true, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": true, "has_type_hints": true, "has_testing": true, "has_logging": false, "function_count": 5, "class_count": 0, "line_count": 84}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "async_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create asynchronous code with proper concurrency handling with asyncio coroutines\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"Dispatcher module.\"\"\"\n\nimport asyncio\nimport logging\nimport signal\nimport time\nfrom typing import List\n\nfrom .monitors import Monitor\n\n\nclass Dispatcher:\n\n    def __init__(self, monitors: List[Monitor]) -> None:\n        self._monitors = monitors\n        self._monitor_tasks: List[asyncio.Task] = []\n        self._logger = logging.getLogger(self.__class__.__name__)\n        self._stopping = False\n\n    def run(self) -> None:\n        asyncio.run(self.start())\n\n    async def start(self) -> None:\n        self._logger.info(\"Starting up\")\n\n        for monitor in self._monitors:\n            self._monitor_tasks.append(\n                asyncio.create_task(self._run_monitor(monitor)),\n            )\n\n        asyncio.get_event_loop().add_signal_handler(signal.SIGTERM, self.stop)\n        asyncio.get_event_loop().add_signal_handler(signal.SIGINT, self.stop)\n\n        await asyncio.gather(*self._monitor_tasks, return_exceptions=True)\n\n        self.stop()\n\n    def stop(self) -> None:\n        if self._stopping:\n            return\n\n        self._stopping = True\n\n        self._logger.info(\"Shutting down\")\n        for task, monitor in zip(self._monitor_tasks, self._monitors):\n            task.cancel()\n        self._monitor_tasks.clear()\n        self._logger.info(\"Shutdown finished successfully\")\n\n    @staticmethod\n    async def _run_monitor(monitor: Monitor) -> None:\n        def _until_next(last: float) -> float:\n            time_took = time.time() - last\n            return monitor.check_every - time_took\n\n        while True:\n            time_start = time.time()\n\n            try:\n                await monitor.check()\n            except asyncio.CancelledError:\n                break\n            except Exception:\n                monitor.logger.exception(\"Error executing monitor check\")\n\n            await asyncio.sleep(_until_next(last=time_start))\n", "metadata": {"source": "github.com/ets-labs/python-dependency-injector", "file_path": "examples/miniapps/asyncio-daemon/monitoringdaemon/dispatcher.py", "language": "python", "complexity": "complex", "star_count": 4481, "quality_score": 0.84, "features": {"has_classes": true, "has_functions": true, "has_async": true, "has_decorators": true, "has_imports": true, "has_error_handling": true, "has_documentation": true, "has_type_hints": true, "has_testing": false, "has_logging": true, "function_count": 6, "class_count": 1, "line_count": 67}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "async_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create asynchronous code with proper concurrency handling with asyncio coroutines\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"Http client module.\"\"\"\n\nfrom aiohttp import ClientSession, ClientTimeout, ClientResponse\n\n\nclass HttpClient:\n\n    async def request(self, method: str, url: str, timeout: int) -> ClientResponse:\n        async with ClientSession(timeout=ClientTimeout(timeout)) as session:\n            async with session.request(method, url) as response:\n                return response\n", "metadata": {"source": "github.com/ets-labs/python-dependency-injector", "file_path": "examples/miniapps/asyncio-daemon/monitoringdaemon/http.py", "language": "python", "complexity": "simple", "star_count": 4481, "quality_score": 0.75, "features": {"has_classes": true, "has_functions": true, "has_async": true, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": true, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 1, "class_count": 1, "line_count": 12}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "async_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create asynchronous code with proper concurrency handling with asyncio coroutines\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"Monitors module.\"\"\"\n\nimport logging\nimport time\nfrom typing import Dict, Any\n\nfrom .http import HttpClient\n\n\nclass Monitor:\n\n    def __init__(self, check_every: int) -> None:\n        self.check_every = check_every\n        self.logger = logging.getLogger(self.__class__.__name__)\n\n    async def check(self) -> None:\n        raise NotImplementedError()\n\n\nclass HttpMonitor(Monitor):\n\n    def __init__(\n            self,\n            http_client: HttpClient,\n            options: Dict[str, Any],\n    ) -> None:\n        self._client = http_client\n        self._method = options.pop(\"method\")\n        self._url = options.pop(\"url\")\n        self._timeout = options.pop(\"timeout\")\n        super().__init__(check_every=options.pop(\"check_every\"))\n\n    async def check(self) -> None:\n        time_start = time.time()\n\n        response = await self._client.request(\n            method=self._method,\n            url=self._url,\n            timeout=self._timeout,\n        )\n\n        time_end = time.time()\n        time_took = time_end - time_start\n\n        self.logger.info(\n            \"Check\\n\"\n            \"    %s %s\\n\"\n            \"    response code: %s\\n\"\n            \"    content length: %s\\n\"\n            \"    request took: %s seconds\",\n            self._method,\n            self._url,\n            response.status,\n            response.content_length,\n            round(time_took, 3)\n        )\n", "metadata": {"source": "github.com/ets-labs/python-dependency-injector", "file_path": "examples/miniapps/asyncio-daemon/monitoringdaemon/monitors.py", "language": "python", "complexity": "medium", "star_count": 4481, "quality_score": 0.8099999999999999, "features": {"has_classes": true, "has_functions": true, "has_async": true, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": true, "has_type_hints": true, "has_testing": false, "has_logging": true, "function_count": 4, "class_count": 2, "line_count": 57}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "async_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create asynchronous code with proper concurrency handling with asyncio coroutines\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"Tests module.\"\"\"\n\nimport asyncio\nimport dataclasses\nfrom unittest import mock\n\nimport pytest\n\nfrom .containers import Container\n\n\n@dataclasses.dataclass\nclass RequestStub:\n    status: int\n    content_length: int\n\n\n@pytest.fixture\ndef container():\n    return Container(\n        config={\n            \"log\": {\n                \"level\": \"INFO\",\n                \"formant\": \"[%(asctime)s] [%(levelname)s] [%(name)s]: %(message)s\",\n            },\n            \"monitors\": {\n                \"example\": {\n                    \"method\": \"GET\",\n                    \"url\": \"http://fake-example.com\",\n                    \"timeout\": 1,\n                    \"check_every\": 1,\n                },\n                \"httpbin\": {\n                    \"method\": \"GET\",\n                    \"url\": \"https://fake-httpbin.org/get\",\n                    \"timeout\": 1,\n                    \"check_every\": 1,\n                },\n            },\n        }\n    )\n\n\n@pytest.mark.asyncio\nasync def test_example_monitor(container, caplog):\n    caplog.set_level(\"INFO\")\n\n    http_client_mock = mock.AsyncMock()\n    http_client_mock.request.return_value = RequestStub(\n        status=200,\n        content_length=635,\n    )\n\n    with container.http_client.override(http_client_mock):\n        example_monitor = container.example_monitor()\n        await example_monitor.check()\n\n    assert \"http://fake-example.com\" in caplog.text\n    assert \"response code: 200\" in caplog.text\n    assert \"content length: 635\" in caplog.text\n\n\n@pytest.mark.asyncio\nasync def test_dispatcher(container, caplog):\n    caplog.set_level(\"INFO\")\n\n    example_monitor_mock = mock.AsyncMock()\n    httpbin_monitor_mock = mock.AsyncMock()\n\n    with container.override_providers(\n            example_monitor=example_monitor_mock,\n            httpbin_monitor=httpbin_monitor_mock,\n    ):\n        dispatcher = container.dispatcher()\n        event_loop = asyncio.get_running_loop()\n        event_loop.create_task(dispatcher.start())\n        await asyncio.sleep(0.1)\n        dispatcher.stop()\n\n    assert example_monitor_mock.check.called\n    assert httpbin_monitor_mock.check.called\n", "metadata": {"source": "github.com/ets-labs/python-dependency-injector", "file_path": "examples/miniapps/asyncio-daemon/monitoringdaemon/tests.py", "language": "python", "complexity": "medium", "star_count": 4481, "quality_score": 0.84, "features": {"has_classes": true, "has_functions": true, "has_async": true, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": true, "has_type_hints": true, "has_testing": true, "has_logging": true, "function_count": 3, "class_count": 1, "line_count": 82}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "async_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create asynchronous code with proper concurrency handling with asyncio coroutines\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"Application module.\"\"\"\n\nfrom typing import Annotated\n\nfrom fastapi import Depends, FastAPI\n\nfrom dependency_injector.wiring import Provide, inject\n\nfrom .containers import Container\nfrom .services import Service\n\napp = FastAPI()\n\n\n@app.api_route(\"/\")\n@inject\nasync def index(\n    service: Annotated[Service, Depends(Provide[Container.service])]\n) -> dict[str, str]:\n    value = await service.process()\n    return {\"result\": value}\n\n\ncontainer = Container()\ncontainer.config.redis_host.from_env(\"REDIS_HOST\", \"localhost\")\ncontainer.config.redis_password.from_env(\"REDIS_PASSWORD\", \"password\")\ncontainer.wire(modules=[__name__])\n", "metadata": {"source": "github.com/ets-labs/python-dependency-injector", "file_path": "examples/miniapps/fastapi-redis/fastapiredis/application.py", "language": "python", "complexity": "simple", "star_count": 4481, "quality_score": 0.75, "features": {"has_classes": false, "has_functions": true, "has_async": true, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": true, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 1, "class_count": 0, "line_count": 28}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "async_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create asynchronous code with proper concurrency handling with asyncio coroutines\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from typing import AsyncIterator\n\nfrom redis.asyncio import from_url, Redis\n\n\nasync def init_redis_pool(host: str, password: str) -> AsyncIterator[Redis]:\n    session = from_url(f\"redis://{host}\", password=password, encoding=\"utf-8\", decode_responses=True)\n    yield session\n    session.close()\n    await session.wait_closed()\n", "metadata": {"source": "github.com/ets-labs/python-dependency-injector", "file_path": "examples/miniapps/fastapi-redis/fastapiredis/redis.py", "language": "python", "complexity": "simple", "star_count": 4481, "quality_score": 0.69, "features": {"has_classes": false, "has_functions": true, "has_async": true, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 1, "class_count": 0, "line_count": 11}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "async_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create asynchronous code with proper concurrency handling with asyncio coroutines\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"Services module.\"\"\"\n\nfrom redis.asyncio import Redis\n\n\nclass Service:\n    def __init__(self, redis: Redis) -> None:\n        self._redis = redis\n\n    async def process(self) -> str:\n        await self._redis.set(\"my-key\", \"value\")\n        return await self._redis.get(\"my-key\")\n", "metadata": {"source": "github.com/ets-labs/python-dependency-injector", "file_path": "examples/miniapps/fastapi-redis/fastapiredis/services.py", "language": "python", "complexity": "simple", "star_count": 4481, "quality_score": 0.75, "features": {"has_classes": true, "has_functions": true, "has_async": true, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": true, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 2, "class_count": 1, "line_count": 13}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "async_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create asynchronous code with proper concurrency handling with asyncio coroutines\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"Tests module.\"\"\"\n\nfrom unittest import mock\n\nimport pytest\nfrom httpx import ASGITransport, AsyncClient\n\nfrom .application import app, container\nfrom .services import Service\n\n\n@pytest.fixture\ndef client(event_loop):\n    client = AsyncClient(\n        transport=ASGITransport(app=app),\n        base_url=\"http://test\",\n    )\n    yield client\n    event_loop.run_until_complete(client.aclose())\n\n\n@pytest.mark.asyncio\nasync def test_index(client):\n    service_mock = mock.AsyncMock(spec=Service)\n    service_mock.process.return_value = \"Foo\"\n\n    with container.service.override(service_mock):\n        response = await client.get(\"/\")\n\n    assert response.status_code == 200\n    assert response.json() == {\"result\": \"Foo\"}\n", "metadata": {"source": "github.com/ets-labs/python-dependency-injector", "file_path": "examples/miniapps/fastapi-redis/fastapiredis/tests.py", "language": "python", "complexity": "simple", "star_count": 4481, "quality_score": 0.78, "features": {"has_classes": false, "has_functions": true, "has_async": true, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": true, "has_type_hints": true, "has_testing": true, "has_logging": false, "function_count": 2, "class_count": 0, "line_count": 32}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "async_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create asynchronous code with proper concurrency handling with asyncio coroutines\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from typing import Annotated\n\nfrom fastapi import Depends, FastAPI\n\nfrom dependency_injector import containers, providers\nfrom dependency_injector.wiring import Provide, inject\n\n\nclass Service:\n    async def process(self) -> str:\n        return \"OK\"\n\n\nclass Container(containers.DeclarativeContainer):\n\n    service = providers.Factory(Service)\n\n\napp = FastAPI()\n\n\n@app.api_route(\"/\")\n@inject\nasync def index(\n    service: Annotated[Service, Depends(Provide[Container.service])]\n) -> dict[str, str]:\n    result = await service.process()\n    return {\"result\": result}\n\n\ncontainer = Container()\ncontainer.wire(modules=[__name__])\n", "metadata": {"source": "github.com/ets-labs/python-dependency-injector", "file_path": "examples/miniapps/fastapi-simple/fastapi_di_example.py", "language": "python", "complexity": "medium", "star_count": 4481, "quality_score": 0.72, "features": {"has_classes": true, "has_functions": true, "has_async": true, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 2, "class_count": 2, "line_count": 33}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "async_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create asynchronous code with proper concurrency handling with asyncio coroutines\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from unittest import mock\n\nimport pytest\nimport pytest_asyncio\nfrom httpx import ASGITransport, AsyncClient\n\nfrom fastapi_di_example import app, container, Service\n\n\n@pytest_asyncio.fixture\nasync def client():\n    async with AsyncClient(\n        transport=ASGITransport(app=app),\n        base_url=\"http://test\",\n    ) as client:\n        yield client\n\n\n@pytest.mark.asyncio\nasync def test_index(client):\n    service_mock = mock.AsyncMock(spec=Service)\n    service_mock.process.return_value = \"Foo\"\n\n    with container.service.override(service_mock):\n        response = await client.get(\"/\")\n\n    assert response.status_code == 200\n    assert response.json() == {\"result\": \"Foo\"}\n", "metadata": {"source": "github.com/ets-labs/python-dependency-injector", "file_path": "examples/miniapps/fastapi-simple/tests.py", "language": "python", "complexity": "simple", "star_count": 4481, "quality_score": 0.75, "features": {"has_classes": false, "has_functions": true, "has_async": true, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": true, "has_logging": false, "function_count": 2, "class_count": 0, "line_count": 29}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "async_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create asynchronous code with proper concurrency handling with asyncio coroutines\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import os\nimport unittest\n\nfrom integration_tests.env_variable_names import (\n    SLACK_SDK_TEST_GRID_ORG_ADMIN_USER_TOKEN,\n)\nfrom integration_tests.helpers import async_test\nfrom slack_sdk.audit_logs.async_client import AsyncAuditLogsClient\n\n\nclass TestAuditLogsClient(unittest.TestCase):\n    def setUp(self):\n        self.client = AsyncAuditLogsClient(token=os.environ[SLACK_SDK_TEST_GRID_ORG_ADMIN_USER_TOKEN])\n\n    def tearDown(self):\n        pass\n\n    @async_test\n    async def test_api_call(self):\n        api_response = await self.client.api_call(path=\"schemas\")\n        self.assertEqual(200, api_response.status_code)\n        self.assertTrue(api_response.raw_body.startswith(\"\"\"{\"schemas\":[{\"\"\"))\n        self.assertIsNotNone(api_response.body.get(\"schemas\"))\n\n    @async_test\n    async def test_schemas(self):\n        api_response = await self.client.schemas()\n        self.assertEqual(200, api_response.status_code)\n        self.assertTrue(api_response.raw_body.startswith(\"\"\"{\"schemas\":[{\"\"\"))\n        self.assertIsNotNone(api_response.body.get(\"schemas\"))\n\n    @async_test\n    async def test_actions(self):\n        api_response = await self.client.actions()\n        self.assertEqual(200, api_response.status_code)\n        self.assertTrue(api_response.raw_body.startswith(\"\"\"{\"actions\":{\"\"\"))\n        self.assertIsNotNone(api_response.body.get(\"actions\"))\n\n    @async_test\n    async def test_logs(self):\n        api_response = await self.client.logs(action=\"user_login\", limit=1)\n        self.assertEqual(200, api_response.status_code)\n        self.assertTrue(api_response.raw_body.startswith(\"\"\"{\"entries\":[{\"\"\"))\n        self.assertIsNotNone(api_response.body.get(\"entries\"))\n", "metadata": {"source": "github.com/slackapi/python-slack-sdk", "file_path": "integration_tests/audit_logs/test_async_client.py", "language": "python", "complexity": "medium", "star_count": 3933, "quality_score": 0.8099999999999999, "features": {"has_classes": true, "has_functions": true, "has_async": true, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": true, "has_type_hints": false, "has_testing": true, "has_logging": true, "function_count": 6, "class_count": 1, "line_count": 45}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "async_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create asynchronous code with proper concurrency handling\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import asyncio\nimport inspect\nimport sys\nfrom asyncio.events import AbstractEventLoop\n\n\ndef async_test(coro):\n    loop: AbstractEventLoop = asyncio.new_event_loop()\n    asyncio.set_event_loop(loop)\n\n    def wrapper(*args, **kwargs):\n        current_loop: AbstractEventLoop = asyncio.get_event_loop()\n        return current_loop.run_until_complete(coro(*args, **kwargs))\n\n    return wrapper\n\n\ndef is_not_specified() -> bool:\n    # get the caller's filepath\n    frame = inspect.stack()[1]\n    module = inspect.getmodule(frame[0])\n    filepath: str = module.__file__\n\n    # ./scripts/run_integration_tests.sh web/test_issue_560.py\n    test_target: str = sys.argv[1]  # e.g., web/test_issue_560.py\n    return not test_target or not filepath.endswith(test_target)\n", "metadata": {"source": "github.com/slackapi/python-slack-sdk", "file_path": "integration_tests/helpers.py", "language": "python", "complexity": "simple", "star_count": 3933, "quality_score": 0.72, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": true, "has_logging": false, "function_count": 3, "class_count": 0, "line_count": 27}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "async_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create asynchronous code with proper concurrency handling with asyncio coroutines\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import asyncio\nimport collections\nimport logging\nimport unittest\n\nfrom integration_tests.helpers import async_test\nfrom slack_sdk.rtm import RTMClient\n\n\nclass TestRTMClient(unittest.TestCase):\n    \"\"\"Runs integration tests with real Slack API\n\n    https://github.com/slackapi/python-slack-sdk/issues/530\n    \"\"\"\n\n    def setUp(self):\n        self.logger = logging.getLogger(__name__)\n\n    def tearDown(self):\n        # Reset the decorators by @RTMClient.run_on\n        RTMClient._callbacks = collections.defaultdict(list)\n\n    def test_issue_530(self):\n        try:\n            rtm_client = RTMClient(token=\"I am not a token\", run_async=False, loop=asyncio.new_event_loop())\n            rtm_client.start()\n            self.fail(\"Raising an error here was expected\")\n        except Exception as e:\n            self.assertEqual(\n                \"The request to the Slack API failed.\\n\" \"The server responded with: {'ok': False, 'error': 'invalid_auth'}\",\n                str(e),\n            )\n        finally:\n            if not rtm_client._stopped:\n                rtm_client.stop()\n\n    @async_test\n    async def test_issue_530_async(self):\n        try:\n            rtm_client = RTMClient(token=\"I am not a token\", run_async=True)\n            await rtm_client.start()\n            self.fail(\"Raising an error here was expected\")\n        except Exception as e:\n            self.assertEqual(\n                \"The request to the Slack API failed.\\n\" \"The server responded with: {'ok': False, 'error': 'invalid_auth'}\",\n                str(e),\n            )\n        finally:\n            if not rtm_client._stopped:\n                rtm_client.stop()\n\n    # =============================================================================================== short test summary info ===============================================================================================\n    # FAILED integration_tests/rtm/test_issue_530.py::TestRTMClient::test_issue_530 - AssertionError: \"'NoneType' object is not subscriptable\" != \"The server responded with: {'ok': False, 'error': 'invalid_auth'}\"\n    # FAILED integration_tests/rtm/test_issue_530.py::TestRTMClient::test_issue_530_async - AssertionError: \"'NoneType' object is not subscriptable\" != \"The server responded with: {'ok': False, 'error': 'invalid_auth'}\"\n    # ====================================================================================== 2 failed, 1 skipped, 5 warnings in 1.54s =======================================================================================\n", "metadata": {"source": "github.com/slackapi/python-slack-sdk", "file_path": "integration_tests/rtm/test_issue_530.py", "language": "python", "complexity": "medium", "star_count": 3933, "quality_score": 0.8999999999999999, "features": {"has_classes": true, "has_functions": true, "has_async": true, "has_decorators": true, "has_imports": true, "has_error_handling": true, "has_documentation": true, "has_type_hints": true, "has_testing": true, "has_logging": true, "function_count": 4, "class_count": 1, "line_count": 56}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "async_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create asynchronous code with proper concurrency handling with asyncio coroutines\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import asyncio\nimport collections\nimport logging\nimport os\nimport unittest\n\nimport pytest\n\nfrom integration_tests.env_variable_names import (\n    SLACK_SDK_TEST_CLASSIC_APP_BOT_TOKEN,\n    SLACK_SDK_TEST_RTM_TEST_CHANNEL_ID,\n)\nfrom integration_tests.helpers import async_test, is_not_specified\nfrom slack_sdk.rtm import RTMClient\nfrom slack_sdk.web import WebClient\n\n\nclass TestRTMClient(unittest.TestCase):\n    \"\"\"Runs integration tests with real Slack API\n\n    https://github.com/slackapi/python-slack-sdk/issues/558\n    \"\"\"\n\n    def setUp(self):\n        self.logger = logging.getLogger(__name__)\n        self.bot_token = os.environ[SLACK_SDK_TEST_CLASSIC_APP_BOT_TOKEN]\n\n    def tearDown(self):\n        # Reset the decorators by @RTMClient.run_on\n        RTMClient._callbacks = collections.defaultdict(list)\n\n    @pytest.mark.skipif(condition=is_not_specified(), reason=\"Still unfixed\")\n    @async_test\n    async def test_issue_558(self):\n        channel_id = os.environ[SLACK_SDK_TEST_RTM_TEST_CHANNEL_ID]\n        text = \"This message was sent by <https://slack.dev/python-slackclient/|python-slackclient>! (test_issue_558)\"\n\n        self.message_count, self.reaction_count = 0, 0\n\n        async def process_messages(**payload):\n            self.logger.debug(payload)\n            self.message_count += 1\n            await asyncio.sleep(10)  # this used to block all other handlers\n\n        async def process_reactions(**payload):\n            self.logger.debug(payload)\n            self.reaction_count += 1\n\n        rtm = RTMClient(token=self.bot_token, run_async=True)\n        RTMClient.on(event=\"message\", callback=process_messages)\n        RTMClient.on(event=\"reaction_added\", callback=process_reactions)\n\n        web_client = WebClient(token=self.bot_token, run_async=True)\n        message = await web_client.chat_postMessage(channel=channel_id, text=text)\n        self.assertFalse(\"error\" in message)\n        ts = message[\"ts\"]\n        await asyncio.sleep(3)\n\n        # intentionally not waiting here\n        rtm.start()\n        await asyncio.sleep(3)\n\n        try:\n            first_reaction = await web_client.reactions_add(channel=channel_id, timestamp=ts, name=\"eyes\")\n            self.assertFalse(\"error\" in first_reaction)\n            await asyncio.sleep(2)\n\n            message = await web_client.chat_postMessage(channel=channel_id, text=text)\n            self.assertFalse(\"error\" in message)\n            # used to start blocking here\n\n            # This reaction_add event won't be handled due to a bug\n            second_reaction = await web_client.reactions_add(channel=channel_id, timestamp=ts, name=\"tada\")\n            self.assertFalse(\"error\" in second_reaction)\n            await asyncio.sleep(2)\n\n            self.assertEqual(self.message_count, 1)\n            self.assertEqual(self.reaction_count, 2)  # used to fail\n        finally:\n            if not rtm._stopped:\n                rtm.stop()\n", "metadata": {"source": "github.com/slackapi/python-slack-sdk", "file_path": "integration_tests/rtm/test_issue_558.py", "language": "python", "complexity": "medium", "star_count": 3933, "quality_score": 0.8999999999999999, "features": {"has_classes": true, "has_functions": true, "has_async": true, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": true, "has_type_hints": false, "has_testing": true, "has_logging": true, "function_count": 5, "class_count": 1, "line_count": 82}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "async_patterns", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create asynchronous code with proper concurrency handling with asyncio coroutines\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import asyncio\nimport collections\nimport logging\nimport os\nimport threading\nimport time\nimport unittest\n\nimport psutil\nimport pytest\n\nfrom integration_tests.env_variable_names import (\n    SLACK_SDK_TEST_CLASSIC_APP_BOT_TOKEN,\n    SLACK_SDK_TEST_RTM_TEST_CHANNEL_ID,\n)\nfrom integration_tests.helpers import async_test, is_not_specified\nfrom slack_sdk.rtm import RTMClient\nfrom slack_sdk.web import WebClient\nfrom slack_sdk.web.legacy_client import LegacyWebClient\n\n\nclass TestRTMClient(unittest.TestCase):\n    \"\"\"Runs integration tests with real Slack API\n\n    https://github.com/slackapi/python-slack-sdk/issues/569\n    \"\"\"\n\n    def setUp(self):\n        if not hasattr(self, \"logger\"):\n            self.logger = logging.getLogger(__name__)\n            self.channel_id = os.environ[SLACK_SDK_TEST_RTM_TEST_CHANNEL_ID]\n            self.bot_token = os.environ[SLACK_SDK_TEST_CLASSIC_APP_BOT_TOKEN]\n\n        if not hasattr(self, \"cpu_monitor\") or not TestRTMClient.cpu_monitor.is_alive():\n\n            def run_cpu_monitor(self):\n                self.logger.debug(\"Starting CPU monitor in another thread...\")\n                TestRTMClient.cpu_usage = 0\n                while True:\n                    p = psutil.Process(os.getpid())\n                    current_cpu_usage: float = p.cpu_percent(interval=0.5)\n                    self.logger.debug(current_cpu_usage)\n                    if current_cpu_usage > TestRTMClient.cpu_usage:\n                        TestRTMClient.cpu_usage = current_cpu_usage\n\n            TestRTMClient.cpu_monitor = threading.Thread(target=run_cpu_monitor, args=[self])\n            TestRTMClient.cpu_monitor.daemon = True\n            TestRTMClient.cpu_monitor.start()\n\n        self.rtm_client = None\n        self.web_client = None\n\n    def tearDown(self):\n        # Reset the decorators by @RTMClient.run_on\n        RTMClient._callbacks = collections.defaultdict(list)\n        # Stop the Client\n        if hasattr(self, \"rtm_client\") and not self.rtm_client._stopped:\n            self.rtm_client.stop()\n\n    @pytest.mark.skipif(condition=is_not_specified(), reason=\"To avoid rate_limited errors\")\n    def test_cpu_usage(self):\n        self.rtm_client = RTMClient(token=self.bot_token, run_async=False, loop=asyncio.new_event_loop())\n        self.web_client = WebClient(token=self.bot_token)\n\n        self.call_count = 0\n        TestRTMClient.cpu_usage = 0\n\n        @RTMClient.run_on(event=\"message\")\n        def send_reply(**payload):\n            self.logger.debug(payload)\n            event = payload[\"data\"]\n            if \"text\" in event:\n                if not str(event[\"text\"]).startswith(\"Current CPU usage:\"):\n                    web_client = payload[\"web_client\"]\n                    for i in range(0, 3):\n                        new_message = web_client.chat_postMessage(\n                            channel=event[\"channel\"],\n                            text=f\"Current CPU usage: {TestRTMClient.cpu_usage} % (test_cpu_usage)\",\n                        )\n                        self.logger.debug(new_message)\n                        self.call_count += 1\n\n        def connect():\n            self.logger.debug(\"Starting RTM Client...\")\n            self.rtm_client.start()\n\n        rtm = threading.Thread(target=connect)\n        rtm.daemon = True\n\n        rtm.start()\n        time.sleep(5)\n\n        text = \"This message was sent by <https://slack.dev/python-slackclient/|python-slackclient>! (test_cpu_usage)\"\n        new_message = self.web_client.chat_postMessage(channel=self.channel_id, text=text)\n        self.assertFalse(\"error\" in new_message)\n\n        time.sleep(5)\n        self.assertLess(TestRTMClient.cpu_usage, 30, \"Too high CPU usage detected\")\n        self.assertEqual(self.call_count, 3, \"The RTM handler failed\")\n\n    # >       self.assertLess(TestRTMClient.cpu_usage, 30, \"Too high CPU usage detected\")\n    # E       AssertionError: 100.2 not less than 30 : Too high CPU usage detected\n    #\n    # integration_tests/rtm/test_rtm_client.py:160: AssertionError\n\n    @async_test\n    async def test_cpu_usage_async(self):\n        self.rtm_client = RTMClient(token=self.bot_token, run_async=True)\n        self.web_client = LegacyWebClient(token=self.bot_token, run_async=True)\n\n        self.call_count = 0\n        TestRTMClient.cpu_usage = 0\n\n        @RTMClient.run_on(event=\"message\")\n        async def send_reply_async(**payload):\n            self.logger.debug(payload)\n            event = payload[\"data\"]\n            if \"text\" in event:\n                if not str(event[\"text\"]).startswith(\"Current CPU usage:\"):\n                    web_client = payload[\"web_client\"]\n                    for i in range(0, 3):\n                        new_message = await web_client.chat_postMessage(\n                            channel=event[\"channel\"],\n                            text=f\"Current CPU usage: {TestRTMClient.cpu_usage} % (test_cpu_usage_async)\",\n                        )\n                        self.logger.debug(new_message)\n                        self.call_count += 1\n\n        # intentionally not waiting here\n        self.rtm_client.start()\n\n        await asyncio.sleep(5)\n\n        text = \"This message was sent by <https://slack.dev/python-slackclient/|python-slackclient>! (test_cpu_usage_async)\"\n        new_message = await self.web_client.chat_postMessage(channel=self.channel_id, text=text)\n        self.assertFalse(\"error\" in new_message)\n\n        await asyncio.sleep(5)\n        self.assertLess(TestRTMClient.cpu_usage, 30, \"Too high CPU usage detected\")\n        self.assertEqual(self.call_count, 3, \"The RTM handler failed\")\n", "metadata": {"source": "github.com/slackapi/python-slack-sdk", "file_path": "integration_tests/rtm/test_issue_569.py", "language": "python", "complexity": "medium", "star_count": 3933, "quality_score": 0.87, "features": {"has_classes": true, "has_functions": true, "has_async": true, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": true, "has_type_hints": true, "has_testing": true, "has_logging": true, "function_count": 8, "class_count": 1, "line_count": 141}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "error_handling", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create robust error handling with logging and validation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"Beautiful and helpful exceptions\n\nJust set your `BETTER_EXCEPTIONS` environment variable. It handles the rest.\n\n\n   Name: better_exceptions\n Author: Josh Junon\n  Email: josh@junon.me\n    URL: github.com/qix-/better-exceptions\nLicense: Copyright (c) 2017 Josh Junon, licensed under the MIT license\n\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import print_function\n\nimport logging\nimport sys\n\nfrom .formatter import THEME, MAX_LENGTH, PIPE_CHAR, CAP_CHAR, ExceptionFormatter\nfrom .context import PY3\nfrom .color import SUPPORTS_COLOR, SHOULD_ENCODE, STREAM, to_byte\nfrom .log import BetExcLogger, patch as patch_logging\nfrom .repl import interact, get_repl\n\n\n__version__ = '0.3.3'\n\n\nTHEME = THEME.copy()  # Users customizing the theme should not impact core\n\n\ndef write_stream(data, stream=STREAM):\n    if SHOULD_ENCODE:\n        data = to_byte(data)\n\n        if PY3:\n            stream.buffer.write(data)\n        else:\n            stream.write(data)\n    else:\n        stream.write(data)\n\n\ndef format_exception(exc, value, tb):\n    # Rebuild each time to take into account any changes made by the user to the global parameters\n    formatter = ExceptionFormatter(colored=SUPPORTS_COLOR, theme=THEME, max_length=MAX_LENGTH,\n                                   pipe_char=PIPE_CHAR, cap_char=CAP_CHAR)\n    return list(formatter.format_exception(exc, value, tb))\n\n\ndef excepthook(exc, value, tb):\n    formatted = u''.join(format_exception(exc, value, tb))\n    write_stream(formatted, STREAM)\n\n\ndef hook():\n    sys.excepthook = excepthook\n\n    logging.setLoggerClass(BetExcLogger)\n    patch_logging()\n\n    if hasattr(sys, 'ps1'):\n        print('WARNING: better_exceptions will only inspect code from the command line\\n'\n              '         when using: `python -m better_exceptions\\'. Otherwise, only code\\n'\n              '         loaded from files will be inspected!', file=sys.stderr)\n", "metadata": {"source": "github.com/Qix-/better-exceptions", "file_path": "better_exceptions/__init__.py", "language": "python", "complexity": "medium", "star_count": 4655, "quality_score": 0.75, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": true, "has_type_hints": true, "has_testing": false, "has_logging": true, "function_count": 4, "class_count": 0, "line_count": 66}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "error_handling", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create robust error handling with logging and validation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import argparse\nimport os\n\ntry:\n    import importlib.machinery\n    import importlib.util\n\n    def load_module(name, filepath):\n        loader = importlib.machinery.SourceFileLoader(name, filepath)\n        spec = importlib.util.spec_from_loader(loader.name, loader)\n        mod = importlib.util.module_from_spec(spec)\n        loader.exec_module(mod)\n\nexcept ImportError:\n    import imp\n\n    def load_module(name, filepath):\n        with open(filepath, 'r') as fd:\n            imp.load_module('a_b', fd, filepath, ('.py', 'U', imp.PY_SOURCE))\n\nfrom better_exceptions import interact, hook\nhook()\n\nparser = argparse.ArgumentParser(description='A Python REPL with better exceptions enabled', prog='python -m better_exceptions')\nparser.add_argument('-q', '--quiet', help=\"don't show a banner\", action='store_true')\nparser.add_argument('-i', '--no-init', dest='no_init', help=\"don't load ~/.pyinit\", action='store_true')\nargs = parser.parse_args()\n\nstartup_file = os.getenv('PYTHONSTARTUP')\nif not args.no_init and startup_file is not None:\n    load_module('pystartup', startup_file)\n\ninteract(args.quiet)\n", "metadata": {"source": "github.com/Qix-/better-exceptions", "file_path": "better_exceptions/__main__.py", "language": "python", "complexity": "simple", "star_count": 4655, "quality_score": 0.72, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": true, "has_documentation": false, "has_type_hints": false, "has_testing": false, "has_logging": false, "function_count": 2, "class_count": 0, "line_count": 34}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "error_handling", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create robust error handling with logging and validation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"Checks if the current terminal supports colors.\n\nAlso specifies the stream to write to. On Windows, this is a wrapped\nstream.\n\"\"\"\n\nfrom __future__ import absolute_import\n\nimport codecs\nimport errno\nimport os\nimport struct\nimport sys\n\nfrom .context import PY3\n\n\nSTREAM = sys.stderr\nENCODING = getattr(STREAM, \"encoding\", None) or \"utf-8\"\nSHOULD_ENCODE = True\nSUPPORTS_COLOR = False\n\n\ndef to_byte(val):\n    unicode_type = str if PY3 else unicode\n    if isinstance(val, unicode_type):\n        try:\n            return val.encode(ENCODING)\n        except UnicodeEncodeError:\n            if PY3:\n                return codecs.escape_decode(val)[0]\n            else:\n                return val.encode(\"unicode-escape\").decode(\"string-escape\")\n\n    return val\n\n\ndef to_unicode(val):\n    if isinstance(val, bytes):\n        try:\n            return val.decode(ENCODING)\n        except UnicodeDecodeError:\n            return val.decode(\"unicode-escape\")\n\n    return val\n\n\ndef get_terminfo_file():\n    term = os.getenv('TERM', None)\n\n    if term is None:\n        return None\n\n    terminfo_dirs = [\n            os.path.expanduser('~/.terminfo'),\n            '/etc/terminfo',\n            '/lib/terminfo',\n            '/usr/share/terminfo',\n            '/usr/lib/terminfo',\n            '/usr/share/lib/terminfo',\n            '/usr/local/lib/terminfo',\n            '/usr/local/share/terminfo'\n            ]\n\n    subdirs = [\n            ('%0.2X' % ord(term[0])),\n            term[0]\n            ]\n\n    f = None\n    for terminfo_dir in terminfo_dirs:\n        for subdir in subdirs:\n            terminfo_path = os.path.join(terminfo_dir, subdir, term)\n            try:\n                f = open(terminfo_path, 'rb')\n                break\n            except IOError as e:\n                if e.errno != errno.ENOENT:\n                    raise\n\n    return f\n\n\nclass ProxyBufferStreamWrapper(object):\n\n    def __init__(self, wrapped):\n        self.__wrapped = wrapped\n\n    def __getattr__(self, name):\n        return getattr(self.__wrapped, name)\n\n    def write(self, text):\n        data = to_byte(text)\n        self.__wrapped.buffer.write(data)\n\n\nif os.name == 'nt':\n    from colorama import init as init_colorama, AnsiToWin32\n\n    init_colorama(wrap=False)\n\n    stream = sys.stderr\n\n    if PY3:\n        # Colorama cannot work with bytes-string\n        # The stream is wrapped so that encoding of the stream is done after\n        # (once Colorama found ANSI codes and converted them to win32 calls)\n        # See issue #23 for more information\n        stream = ProxyBufferStreamWrapper(stream)\n        SHOULD_ENCODE = False\n\n    STREAM = AnsiToWin32(stream).stream\n    SUPPORTS_COLOR = True\nelse:\n    if os.getenv('FORCE_COLOR', None) == '1':\n        SUPPORTS_COLOR = True\n    else:\n        try:\n            # May raises an error on some exotic environment like GAE, see #28\n            is_tty = os.isatty(2)\n        except OSError:\n            is_tty = False\n\n        if is_tty:\n            f = get_terminfo_file()\n            if f is not None:\n                with f:\n                    # f is a valid terminfo; seek and read!\n                    magic_number = struct.unpack('<h', f.read(2))[0]\n\n                    if magic_number == 0x11A:\n                        # the opened terminfo file is valid.\n                        offset = 2 + 10  # magic number + size section (the next thing we read from)\n                        offset += struct.unpack('<h', f.read(2))[0]  # skip over names section\n                        offset += struct.unpack('<h', f.read(2))[0]  # skip over bool section\n                        offset += offset % 2  # align to short boundary\n                        offset += 13 * 2  # maxColors is the 13th numeric value\n\n                        f.seek(offset)\n                        max_colors = struct.unpack('<h', f.read(2))[0]\n\n                        if max_colors >= 8:\n                            SUPPORTS_COLOR = True\n", "metadata": {"source": "github.com/Qix-/better-exceptions", "file_path": "better_exceptions/color.py", "language": "python", "complexity": "medium", "star_count": 4655, "quality_score": 0.78, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": true, "has_documentation": true, "has_type_hints": false, "has_testing": false, "has_logging": false, "function_count": 6, "class_count": 1, "line_count": 144}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "error_handling", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create robust error handling with logging and validation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from __future__ import absolute_import\n\nimport ast\nimport inspect\nimport keyword\nimport linecache\nimport os\nimport re\nimport sys\nimport traceback\n\nfrom .color import ENCODING, STREAM, SUPPORTS_COLOR, to_byte, to_unicode\nfrom .context import PY3\nfrom .repl import get_repl\n\n\nPIPE_CHAR = u'\\u2502'\nCAP_CHAR = u'\\u2514'\n\ntry:\n    PIPE_CHAR.encode(ENCODING)\nexcept UnicodeEncodeError:\n    PIPE_CHAR = '|'\n    CAP_CHAR = '->'\n\nTHEME = {\n    'comment': lambda s: '\\x1b[2;37m{}\\x1b[m'.format(s),\n    'keyword': lambda s: '\\x1b[33;1m{}\\x1b[m'.format(s),\n    'builtin': lambda s: '\\x1b[35;1m{}\\x1b[m'.format(s),\n    'literal': lambda s: '\\x1b[31m{}\\x1b[m'.format(s),\n    'inspect': lambda s: u'\\x1b[36m{}\\x1b[m'.format(s),\n}\n\nMAX_LENGTH = 128\n\n\ndef isast(v):\n    return inspect.isclass(v) and issubclass(v, ast.AST)\n\n\nclass ExceptionFormatter(object):\n\n    COMMENT_REGXP = re.compile(r'((?:(?:\"(?:[^\\\\\"]|(\\\\\\\\)*\\\\\")*\")|(?:\\'(?:[^\\\\\\']|(\\\\\\\\)*\\\\\\')*\\')|[^#])*)(#.*)$')\n    CMDLINE_REGXP = re.compile(r'(?:[^\\t ]*([\\'\"])(?:\\\\.|.)*(?:\\1))[^\\t ]*|([^\\t ]+)')\n\n    AST_ELEMENTS = {\n        'builtins': __builtins__.keys() if type(__builtins__) is dict else dir(__builtins__),\n        'keywords': [getattr(ast, cls) for cls in dir(ast) if keyword.iskeyword(cls.lower()) and isast(getattr(ast, cls))],\n    }\n\n    def __init__(self, colored=SUPPORTS_COLOR, theme=THEME, max_length=MAX_LENGTH,\n                       pipe_char=PIPE_CHAR, cap_char=CAP_CHAR):\n        self._colored = colored\n        self._theme = theme\n        self._max_length = max_length\n        self._pipe_char = pipe_char\n        self._cap_char = cap_char\n\n    def colorize_comment(self, source):\n        match = self.COMMENT_REGXP.match(source)\n        if match:\n            source = '{}{}'.format(match.group(1), self._theme['comment'](match.group(4)))\n        return source\n\n    def colorize_tree(self, tree, source):\n        if not self._colored:\n            # quick fail\n            return source\n\n        chunks = []\n\n        offset = 0\n        nodes = [n for n in ast.walk(tree)]\n\n        def append(offset, node, s, theme):\n            begin_col = node.col_offset\n            src_chunk = source[offset:begin_col]\n            chunks.append(src_chunk)\n            chunks.append(self._theme[theme](s))\n            return begin_col + len(s)\n\n        displayed_nodes = []\n\n        for node in nodes:\n            nodecls = node.__class__\n            nodename = nodecls.__name__\n\n            if 'col_offset' not in dir(node):\n                continue\n\n            if nodecls in self.AST_ELEMENTS['keywords']:\n                displayed_nodes.append((node, nodename.lower(), 'keyword'))\n\n            if nodecls == ast.Name and node.id in self.AST_ELEMENTS['builtins']:\n                displayed_nodes.append((node, node.id, 'builtin'))\n\n            if nodecls == ast.Str:\n                displayed_nodes.append((node, \"'{}'\".format(node.s), 'literal'))\n\n            if nodecls == ast.Num:\n                displayed_nodes.append((node, str(node.n), 'literal'))\n\n        displayed_nodes.sort(key=lambda elem: elem[0].col_offset)\n\n        for dn in displayed_nodes:\n            offset = append(offset, *dn)\n\n        chunks.append(source[offset:])\n        return self.colorize_comment(''.join(chunks))\n\n    def get_relevant_names(self, source, tree):\n        return [node for node in ast.walk(tree) if isinstance(node, ast.Name)]\n\n    def format_value(self, v):\n        try:\n            v = repr(v)\n        except KeyboardInterrupt:\n            raise\n        except BaseException:\n            v = u'<unprintable %s object>' % type(v).__name__\n\n        max_length = self._max_length\n        if max_length is not None and len(v) > max_length:\n            v = v[:max_length] + '...'\n        return v\n\n    def get_relevant_values(self, source, frame, tree):\n        names = self.get_relevant_names(source, tree)\n        values = []\n\n        for name in names:\n            text = name.id\n            col = name.col_offset\n            if text in frame.f_locals:\n                val = frame.f_locals.get(text, None)\n                values.append((text, col, self.format_value(val)))\n            elif text in frame.f_globals:\n                val = frame.f_globals.get(text, None)\n                values.append((text, col, self.format_value(val)))\n\n        values.sort(key=lambda e: e[1])\n\n        return values\n\n    def split_cmdline(self, cmdline):\n        return [m.group(0) for m in self.CMDLINE_REGXP.finditer(cmdline)]\n\n    def get_string_source(self):\n        import os\n        import platform\n\n        # import pdb; pdb.set_trace()\n\n        cmdline = None\n        if platform.system() == 'Windows':\n            # TODO use winapi to obtain the command line\n            return ''\n        elif platform.system() == 'Linux':\n            # TODO try to use proc\n            pass\n\n        if cmdline is None and os.name == 'posix':\n            from shutil import which\n\n            if which('ps'):\n                from subprocess import CalledProcessError, check_output as spawn\n\n                try:\n                    cmdline = spawn(['ps', '-ww', '-p', str(os.getpid()), '-o', 'command='])\n                except CalledProcessError:\n                    return ''\n            else:\n                return ''\n        else:\n            # current system doesn't have a way to get the command line\n            return ''\n\n        cmdline = cmdline.decode('utf-8').strip()\n        cmdline = self.split_cmdline(cmdline)\n\n        extra_args = sys.argv[1:]\n        if len(extra_args) > 0:\n            if cmdline[-len(extra_args):] != extra_args:\n                # we can't rely on the output to be correct; fail!\n                return ''\n\n            cmdline = cmdline[1:-len(extra_args)]\n\n        skip = 0\n        for i in range(len(cmdline)):\n            a = cmdline[i].strip()\n            if not a.startswith('-c'):\n                skip += 1\n            else:\n                a = a[2:].strip()\n                if len(a) > 0:\n                    cmdline[i] = a\n                else:\n                    skip += 1\n                break\n\n        cmdline = cmdline[skip:]\n        source = ' '.join(cmdline)\n\n        return source\n\n    def get_traceback_information(self, tb):\n        frame_info = inspect.getframeinfo(tb)\n        filename = frame_info.filename\n        lineno = frame_info.lineno\n        function = frame_info.function\n\n        repl = get_repl()\n        if repl is not None and filename in repl.entries:\n            _, filename, source = repl.entries[filename]\n            source = source.replace('\\r\\n', '\\n').split('\\n')[lineno - 1]\n        elif filename == '<string>':\n            source = self.get_string_source()\n        else:\n            source = linecache.getline(filename, lineno)\n\n        source = source.strip()\n\n        try:\n            tree = ast.parse(source, mode='exec')\n        except SyntaxError:\n            return filename, lineno, function, source, source, []\n\n        relevant_values = self.get_relevant_values(source, tb.tb_frame, tree)\n        color_source = self.colorize_tree(tree, source)\n\n        return filename, lineno, function, source, color_source, relevant_values\n\n\n    def format_traceback_frame(self, tb):\n        filename, lineno, function, source, color_source, relevant_values = self.get_traceback_information(tb)\n\n        lines = [color_source]\n        for i in reversed(range(len(relevant_values))):\n            _, col, val = relevant_values[i]\n            pipe_cols = [pcol for _, pcol, _ in relevant_values[:i]]\n            line = ''\n            index = 0\n            for pc in pipe_cols:\n                line += (' ' * (pc - index)) + self._pipe_char\n                index = pc + 1\n\n            if not PY3 and isinstance(val, str):\n                # In Python2 the Non-ASCII value will be the escaped string,\n                # use string-escape to decode the string to show the text in human way.\n                val = to_unicode(val.decode(\"string-escape\"))\n\n            line += u'{}{} {}'.format((' ' * (col - index)), self._cap_char, val)\n            lines.append(self._theme['inspect'](line) if self._colored else line)\n        formatted = u'\\n    '.join([to_unicode(x) for x in lines])\n\n        return (filename, lineno, function, formatted), color_source\n\n\n    def format_traceback(self, tb=None):\n        omit_last = False\n        if not tb:\n            try:\n                raise Exception()\n            except:\n                omit_last = True\n                _, _, tb = sys.exc_info()\n                assert tb is not None\n\n        frames = []\n        final_source = ''\n        while tb:\n            if omit_last and not tb.tb_next:\n                break\n\n            formatted, colored = self.format_traceback_frame(tb)\n\n            # special case to ignore runcode() here.\n            if not (os.path.basename(formatted[0]) == 'code.py' and formatted[2] == 'runcode'):\n                final_source = colored\n                frames.append(formatted)\n\n            tb = tb.tb_next\n\n        lines = traceback.format_list(frames)\n\n        return ''.join(lines), final_source\n\n    def _format_exception(self, value, tb, seen=None):\n        # Implemented from built-in traceback module:\n        # https://github.com/python/cpython/blob/a5b76167dedf4d15211a216c3ca7b98e3cec33b8/Lib/traceback.py#L468\n\n        exc_type, exc_value, exc_traceback = type(value), value, tb\n\n        if seen is None:\n            seen = set()\n\n        seen.add(id(exc_value))\n\n        if exc_value and PY3:\n            if exc_value.__cause__ is not None and id(exc_value.__cause__) not in seen:\n                for text in self._format_exception(exc_value.__cause__,exc_value.__cause__.__traceback__, seen=seen):\n                    yield text\n                yield u\"\\nThe above exception was the direct cause of the following exception:\\n\\n\"\n            elif exc_value.__context__ is not None and id(exc_value.__context__) not in seen and not exc_value.__suppress_context__:\n                for text in self._format_exception(exc_value.__context__, exc_value.__context__.__traceback__, seen=seen):\n                    yield text\n                yield u\"\\nDuring handling of the above exception, another exception occurred:\\n\\n\"\n\n        if exc_traceback is not None:\n            yield u'Traceback (most recent call last):\\n'\n\n        formatted, colored_source = self.format_traceback(exc_traceback)\n\n        yield formatted\n\n        if not str(value) and exc_type is AssertionError:\n            value.args = (colored_source,)\n        title = traceback.format_exception_only(exc_type, value)\n\n        yield u''.join(title).strip() + u'\\n'\n\n    def format_exception(self, exc, value, tb):\n        for line in self._format_exception(value, tb):\n            yield line\n", "metadata": {"source": "github.com/Qix-/better-exceptions", "file_path": "better_exceptions/formatter.py", "language": "python", "complexity": "complex", "star_count": 4655, "quality_score": 0.8099999999999999, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": true, "has_documentation": false, "has_type_hints": true, "has_testing": true, "has_logging": false, "function_count": 15, "class_count": 1, "line_count": 326}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "error_handling", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create robust error handling with logging and validation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from sys import exc_info\nfrom better_exceptions import excepthook\n\ndef skip_errors_filter(record):\n    return not record.exc_info\n\nclass BetterExceptionsMiddleware:\n    def __init__(self, get_response):\n        self.get_response = get_response\n\n    def __call__(self, request):\n        return self.get_response(request)\n\n    def process_exception(self, _, exception):\n        _, _, traceback = exc_info()\n        excepthook(exception.__class__, exception, traceback)\n", "metadata": {"source": "github.com/Qix-/better-exceptions", "file_path": "better_exceptions/integrations/django.py", "language": "python", "complexity": "medium", "star_count": 4655, "quality_score": 0.72, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": false, "has_testing": false, "has_logging": false, "function_count": 4, "class_count": 1, "line_count": 17}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "error_handling", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create robust error handling with logging and validation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from __future__ import absolute_import\n\nimport sys\n\nfrom logging import Logger, StreamHandler\n\n\ndef patch():\n    import logging\n    from . import format_exception\n\n    logging_format_exception = lambda exc_info: u''.join(format_exception(*exc_info))\n\n    if hasattr(logging, '_defaultFormatter'):\n        logging._defaultFormatter.format_exception = logging_format_exception\n\n    patchables = [handler() for handler in logging._handlerList if isinstance(handler(), StreamHandler)]\n    patchables = [handler for handler in patchables if handler.stream == sys.stderr]\n    patchables = [handler for handler in patchables if handler.formatter is not None]\n    for handler in patchables:\n        handler.formatter.formatException = logging_format_exception\n\n\nclass BetExcLogger(Logger):\n    def __init__(self, *args, **kwargs):\n        super(BetExcLogger, self).__init__(*args, **kwargs)\n        patch()\n", "metadata": {"source": "github.com/Qix-/better-exceptions", "file_path": "better_exceptions/log.py", "language": "python", "complexity": "simple", "star_count": 4655, "quality_score": 0.75, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": false, "has_logging": true, "function_count": 2, "class_count": 1, "line_count": 28}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "error_handling", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create robust error handling with logging and validation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from code import InteractiveConsole\nimport sys\n\n\nREPL_ID_PREFIX = '@@@REPL@@@'\n\n\nrepl = None\n\nclass BetterExceptionsConsole(InteractiveConsole, object):\n    def __init__(self):\n        super(BetterExceptionsConsole, self).__init__()\n        self.last_command = None\n        self.entries = dict()\n        self.last_code = None\n        self.last_id = None\n        self.counter = 0\n\n    def runcode(self, code):\n        assert self.last_code is not None\n        self.entries[self.last_id] = (code,) + self.last_code\n        return super(BetterExceptionsConsole, self).runcode(code)\n\n    def runsource(self, source, loc='<input>', symbol='single'):\n        # we abuse loc here to mark different entries of code.\n        self.last_code = (loc, source)\n        self.last_id = loc = '{}{}'.format(REPL_ID_PREFIX, self.counter)\n        self.counter += 1\n        return super(BetterExceptionsConsole, self).runsource(source, loc, symbol)\n\n    def showtraceback(self):\n        try:\n            exctype, val, tb = sys.exc_info()\n            sys.excepthook(exctype, val, tb)\n        finally:\n            # this is required since tb will never be garbage collected\n            # see notes in `sys`\n            del tb\n\n\ndef get_repl():\n    global repl\n    return repl\n\n\ndef interact(quiet=False):\n    global repl\n    repl = BetterExceptionsConsole()\n    banner = '' if quiet else None\n    repl.interact(banner)\n", "metadata": {"source": "github.com/Qix-/better-exceptions", "file_path": "better_exceptions/repl.py", "language": "python", "complexity": "medium", "star_count": 4655, "quality_score": 0.8099999999999999, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": true, "has_documentation": false, "has_type_hints": false, "has_testing": true, "has_logging": false, "function_count": 6, "class_count": 1, "line_count": 51}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "error_handling", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create robust error handling with logging and validation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import re\nfrom itertools import chain\nfrom os.path import basename\nfrom os.path import dirname\nfrom os.path import join\nfrom os.path import splitext\nfrom distutils.core import setup\nfrom distutils.command.build import build\nfrom setuptools.command.develop import develop\nfrom setuptools.command.easy_install import easy_install\nfrom setuptools.command.install_lib import install_lib\n\n\nclass BuildWithPTH(build):\n    def run(self):\n        build.run(self)\n        path = join(dirname(__file__), 'better_exceptions_hook.pth')\n        dest = join(self.build_lib, basename(path))\n        self.copy_file(path, dest)\n\n\nclass EasyInstallWithPTH(easy_install):\n    def run(self):\n        easy_install.run(self)\n        path = join(dirname(__file__), 'better_exceptions_hook.pth')\n        dest = join(self.install_dir, basename(path))\n        self.copy_file(path, dest)\n\n\nclass InstallLibWithPTH(install_lib):\n    def run(self):\n        install_lib.run(self)\n        path = join(dirname(__file__), 'better_exceptions_hook.pth')\n        dest = join(self.install_dir, basename(path))\n        self.copy_file(path, dest)\n        self.outputs = [dest]\n\n    def get_outputs(self):\n        return chain(install_lib.get_outputs(self), self.outputs)\n\n\nclass DevelopWithPTH(develop):\n    def run(self):\n        develop.run(self)\n        path = join(dirname(__file__), 'better_exceptions_hook.pth')\n        dest = join(self.install_dir, basename(path))\n        self.copy_file(path, dest)\n\n\nwith open('better_exceptions/__init__.py', 'r') as file:\n    version = re.search(r'^__version__\\s*=\\s*[\\'\"]([^\\'\"]*)[\\'\"]',\n                        file.read(), re.MULTILINE).group(1)\n\nsetup(\n    name = 'better_exceptions',\n    packages = ['better_exceptions', 'better_exceptions.integrations'],\n    version = version,\n    description = 'Pretty and helpful exceptions, automatically',\n    author = 'Josh Junon',\n    author_email = 'josh@junon.me',\n    url = 'https://github.com/qix-/better-exceptions',\n    download_url = 'https://github.com/qix-/better-exceptions/archive/{}.tar.gz'.format(version),\n    license = 'MIT',\n    keywords = ['pretty', 'better', 'exceptions', 'exception', 'error', 'local', 'debug', 'debugging', 'locals'],\n    classifiers = [],\n    extras_require = {\n        ':sys_platform==\"win32\"': ['colorama']\n    },\n    # This all comes from pytest-cov repository:\n    # https://github.com/pytest-dev/pytest-cov/blob/cde7c378b6a1971957759f42ac91e2860b41cf89/setup.py\n    cmdclass = {\n        'build': BuildWithPTH,\n        'easy_install': EasyInstallWithPTH,\n        'install_lib': InstallLibWithPTH,\n        'develop': DevelopWithPTH,\n    }\n)\n", "metadata": {"source": "github.com/Qix-/better-exceptions", "file_path": "setup.py", "language": "python", "complexity": "complex", "star_count": 4655, "quality_score": 0.72, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 5, "class_count": 5, "line_count": 78}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "error_handling", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create robust error handling with logging and validation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import better_exceptions\nbetter_exceptions.hook()\n\nfoo = 52\n\n\ndef shallow(a, b):\n    deep(a + b)\n\n\ndef deep(val):\n    global foo\n    assert val > 10 and foo == 60\n\n\nbar = foo - 50\nshallow(bar, 15)\nshallow(bar, 2)\n", "metadata": {"source": "github.com/Qix-/better-exceptions", "file_path": "test/test.py", "language": "python", "complexity": "simple", "star_count": 4655, "quality_score": 0.69, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": false, "has_testing": true, "has_logging": false, "function_count": 2, "class_count": 0, "line_count": 19}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "error_handling", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create robust error handling with logging and validation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# -*- coding:utf-8 -*-\n\nimport better_exceptions\nbetter_exceptions.hook()\n\ndef div(x, y):\n    x / y\n\ndef cause(x, y):\n    try:\n        div(x, y)\n    except Exception:\n        raise ValueError(\"Division error\")\n\ndef context(x, y):\n    try:\n        cause(x, y)\n    except Exception as e:\n        raise ValueError(\"Cause error\") from e\n\ncontext(1, 0)\n", "metadata": {"source": "github.com/Qix-/better-exceptions", "file_path": "test/test_chaining.py", "language": "python", "complexity": "simple", "star_count": 4655, "quality_score": 0.72, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": true, "has_documentation": false, "has_type_hints": false, "has_testing": false, "has_logging": false, "function_count": 3, "class_count": 0, "line_count": 22}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "error_handling", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate module integration for Create robust error handling with logging and validation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from __future__ import print_function\nfrom better_exceptions import color\nprint(color.SUPPORTS_COLOR)\n", "metadata": {"source": "github.com/Qix-/better-exceptions", "file_path": "test/test_color.py", "language": "python", "complexity": "simple", "star_count": 4655, "quality_score": 0.4599999999999999, "features": {"has_classes": false, "has_functions": false, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": false, "has_testing": false, "has_logging": false, "function_count": 0, "class_count": 0, "line_count": 4}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "error_handling", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create robust error handling with logging and validation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# -*- coding:utf-8 -*-\n\nimport better_exceptions\nbetter_exceptions.hook()\n\n\ndef _deep(val):\n    return 1 / val\n\ndef div():\n    return _deep(\"天\")\n\n\ndiv()\n", "metadata": {"source": "github.com/Qix-/better-exceptions", "file_path": "test/test_encoding.py", "language": "python", "complexity": "simple", "star_count": 4655, "quality_score": 0.6599999999999999, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": false, "has_testing": false, "has_logging": false, "function_count": 2, "class_count": 0, "line_count": 15}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "error_handling", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate module integration for Create robust error handling with logging and validation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# -*- coding:utf-8 -*-\n\nimport better_exceptions\nbetter_exceptions.hook()\n\ncode = \"\"\"\nif True:\n    a = 5\n        print(\"foobar\")  #intentional faulty indentation here.\n    b = 7\n\"\"\"\n\nexec(code)\n", "metadata": {"source": "github.com/Qix-/better-exceptions", "file_path": "test/test_indentation_error.py", "language": "python", "complexity": "simple", "star_count": 4655, "quality_score": 0.6599999999999999, "features": {"has_classes": false, "has_functions": false, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": true, "has_type_hints": false, "has_testing": false, "has_logging": false, "function_count": 0, "class_count": 0, "line_count": 14}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "error_handling", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create robust error handling with logging and validation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import better_exceptions\nimport logging\n\nbetter_exceptions.hook()\nlogging.basicConfig()\n\nlogger = logging.getLogger(__name__)\n\nlogging.raiseExceptions = True\n\nqux = 15\n\ndef foo(cb):\n    qix = 20\n    try:\n        cb()\n    except:\n        logger.exception('callback failed')\n\n\ndef bar1():\n    baz = 80.5\n    logger.info('Hello')\n\n\ndef bar2():\n    baz = 890.50\n    logger.info('Hello', exc_info=True)\n\n\ndef bar3():\n    baz = 600.524\n    raise Exception('this is a test exception')\n\n\ndef bar4():\n    baz = 52\n    assert baz == 90\n\n\nFNS = [\n    bar1,\n    bar2,\n    bar3,\n    bar4\n]\n\n\nfor fn in FNS:\n    foo(fn)\n", "metadata": {"source": "github.com/Qix-/better-exceptions", "file_path": "test/test_logging.py", "language": "python", "complexity": "medium", "star_count": 4655, "quality_score": 0.8099999999999999, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": true, "has_documentation": false, "has_type_hints": false, "has_testing": true, "has_logging": true, "function_count": 5, "class_count": 0, "line_count": 51}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "error_handling", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate module integration for Create robust error handling with logging and validation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# -*- coding:utf-8 -*-\n\nimport better_exceptions\nbetter_exceptions.hook()\n\ncode = \"\"\"\nif True:\n    a = 5\n    b = 7 *\n\"\"\"\n\nexec(code)\n", "metadata": {"source": "github.com/Qix-/better-exceptions", "file_path": "test/test_syntax_error.py", "language": "python", "complexity": "simple", "star_count": 4655, "quality_score": 0.6599999999999999, "features": {"has_classes": false, "has_functions": false, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": true, "has_type_hints": false, "has_testing": false, "has_logging": false, "function_count": 0, "class_count": 0, "line_count": 13}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "error_handling", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create robust error handling with logging and validation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "##########\n## GLOBALS\n##########\n\n\nimport urwid\nimport re\nimport sys\nimport os\nfrom bs4 import BeautifulSoup\nimport requests\nfrom queue import Queue\nfrom subprocess import PIPE, Popen\nfrom threading import Thread\nimport webbrowser\nimport time\nfrom urwid.widget import (BOX, FLOW, FIXED)\nimport random\n\nSO_URL = \"https://stackoverflow.com\"\n\n# ASCII color codes\nGREEN = '\\033[92m'\nGRAY = '\\033[90m'\nCYAN = '\\033[36m'\nRED = '\\033[31m'\nYELLOW = '\\033[33m'\nEND = '\\033[0m'\nUNDERLINE = '\\033[4m'\nBOLD = '\\033[1m'\n\n# Scroll actions\nSCROLL_LINE_UP = \"line up\"\nSCROLL_LINE_DOWN = \"line down\"\nSCROLL_PAGE_UP = \"page up\"\nSCROLL_PAGE_DOWN = \"page down\"\nSCROLL_TO_TOP = \"to top\"\nSCROLL_TO_END = \"to end\"\n\n# Scrollbar positions\nSCROLLBAR_LEFT = \"left\"\nSCROLLBAR_RIGHT = \"right\"\n\nUSER_AGENTS = [\n    \"Mozilla/5.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; AcooBrowser; .NET CLR 1.1.4322; .NET CLR 2.0.50727)\",\n    \"Mozilla/5.0 (Windows; U; MSIE 9.0; Windows NT 9.0; en-US)\",\n    \"Mozilla/5.0 (Windows; U; Windows NT 5.1; zh-CN) AppleWebKit/523.15 (KHTML, like Gecko, Safari/419.3) Arora/0.3 (Change: 287 c9dfb30)\",\n    \"Mozilla/5.0 (X11; U; Linux; en-US) AppleWebKit/527+ (KHTML, like Gecko, Safari/419.3) Arora/0.6\",\n    \"Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.8.1.2pre) Gecko/20070215 K-Ninja/2.1.1\",\n    \"Mozilla/5.0 (Windows; U; Windows NT 5.1; zh-CN; rv:1.9) Gecko/20080705 Firefox/3.0 Kapiko/3.0\",\n    \"Mozilla/5.0 (X11; Linux i686; U;) Gecko/20070322 Firefox/59\",\n    \"Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.9.0.8) Gecko Fedora/1.9.0.8-1.fc10 Kazehakase/0.5.6\",\n    \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.56 Safari/535.11\",\n    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_3) AppleWebKit/535.20 (KHTML, like Gecko) Chrome/19.0.1036.7 Safari/535.20\",\n    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',\n    'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.90 Safari/537.36',\n    'Mozilla/5.0 (Windows NT 5.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.90 Safari/537.36',\n    'Mozilla/5.0 (Windows NT 6.2; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.90 Safari/537.36',\n    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36',\n    'Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',\n    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2987.133 Safari/537.36',\n    'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2987.133 Safari/537.36',\n    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36',\n    'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36',\n    'Mozilla/4.0 (compatible; MSIE 9.0; Windows NT 6.1)',\n    'Mozilla/5.0 (Windows NT 6.1; WOW64; Trident/7.0; rv:11.0) like Gecko',\n    'Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; WOW64; Trident/5.0)',\n    'Mozilla/5.0 (Windows NT 6.1; Trident/7.0; rv:11.0) like Gecko',\n    'Mozilla/5.0 (Windows NT 6.2; WOW64; Trident/7.0; rv:11.0) like Gecko',\n    'Mozilla/5.0 (Windows NT 10.0; WOW64; Trident/7.0; rv:11.0) like Gecko',\n    'Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.0; Trident/5.0)',\n    'Mozilla/5.0 (Windows NT 6.3; WOW64; Trident/7.0; rv:11.0) like Gecko',\n    'Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Trident/5.0)',\n    'Mozilla/5.0 (Windows NT 6.1; Win64; x64; Trident/7.0; rv:11.0) like Gecko',\n    'Mozilla/5.0 (compatible; MSIE 10.0; Windows NT 6.1; WOW64; Trident/6.0)',\n]\n\n\n##################\n## FILE ATTRIBUTES\n##################\n\n\ndef get_language(file_path):\n    \"\"\"Returns the language a file is written in.\"\"\"\n    if file_path.endswith(\".py\"):\n        return \"python3\"\n    elif file_path.endswith(\".js\"):\n        return \"node\"\n    elif file_path.endswith(\".go\"):\n        return \"go run\"\n    elif file_path.endswith(\".rb\"):\n        return \"ruby\"\n    elif file_path.endswith(\".java\"):\n        return 'javac' # Compile Java Source File\n    elif file_path.endswith(\".class\"):\n        return 'java' # Run Java Class File\n    else:\n        return '' # Unknown language\n\n\ndef get_error_message(error, language):\n    \"\"\"Filters the stack trace from stderr and returns only the error message.\"\"\"\n    if error == '':\n        return None\n    elif language == \"python3\":\n        if any(e in error for e in [\"KeyboardInterrupt\", \"SystemExit\", \"GeneratorExit\"]): # Non-compiler errors\n            return None\n        else:\n            return error.split('\\n')[-2].strip()\n    elif language == \"node\":\n        return error.split('\\n')[4][1:]\n    elif language == \"go run\":\n        return error.split('\\n')[1].split(\": \", 1)[1][1:]\n    elif language == \"ruby\":\n        error_message = error.split('\\n')[0]\n        return error_message[error_message.rfind(\": \") + 2:]\n    elif language == \"javac\":\n        m = re.search(r'.*error:(.*)', error.split('\\n')[0])\n        return m.group(1) if m else None\n    elif language == \"java\":\n        for line in error.split('\\n'):\n            # Multiple error formats\n            m = re.search(r'.*(Exception|Error):(.*)', line)\n            if m and m.group(2):\n                return m.group(2)\n\n            m = re.search(r'Exception in thread \".*\" (.*)', line)\n            if m and m.group(1):\n                return m.group(1)\n\n        return None\n\n\n#################\n## FILE EXECUTION\n#################\n\n\n## Helper Functions ##\n\n\ndef read(pipe, funcs):\n    \"\"\"Reads and pushes piped output to a shared queue and appropriate lists.\"\"\"\n    for line in iter(pipe.readline, b''):\n        for func in funcs:\n            func(line.decode(\"utf-8\"))\n    pipe.close()\n\n\ndef write(get):\n    \"\"\"Pulls output from shared queue and prints to terminal.\"\"\"\n    for line in iter(get, None):\n        print(line)\n\n\n## Main ##\n\n\ndef execute(command):\n    \"\"\"Executes a given command and clones stdout/err to both variables and the\n    terminal (in real-time).\"\"\"\n    process = Popen(\n        command,\n        cwd=None,\n        shell=False,\n        close_fds=True,\n        stdout=PIPE,\n        stderr=PIPE,\n        bufsize=1\n    )\n\n    output, errors = [], []\n    pipe_queue = Queue() # Wowee, thanks CS 225\n\n    # Threads for reading stdout and stderr pipes and pushing to a shared queue\n    stdout_thread = Thread(target=read, args=(process.stdout, [pipe_queue.put, output.append]))\n    stderr_thread = Thread(target=read, args=(process.stderr, [pipe_queue.put, errors.append]))\n\n    writer_thread = Thread(target=write, args=(pipe_queue.get,)) # Thread for printing items in the queue\n\n    # Spawns each thread\n    for thread in (stdout_thread, stderr_thread, writer_thread):\n        thread.daemon = True\n        thread.start()\n\n    process.wait()\n\n    for thread in (stdout_thread, stderr_thread):\n        thread.join()\n\n    pipe_queue.put(None)\n\n    output = ' '.join(output)\n    errors = ' '.join(errors)\n\n    if \"java\" != command[0] and not os.path.isfile(command[1]): # File doesn't exist, for java, command[1] is a class name instead of a file\n        return (None, None)\n    else:\n        return (output, errors)\n\n\n###############\n## WEB SCRAPING\n###############\n\n\n## Helper Functions ##\n\n\ndef stylize_code(soup):\n    \"\"\"Identifies and stylizes code in a question or answer.\"\"\"\n    # TODO: Handle blockquotes and markdown\n    stylized_text = []\n    code_blocks = [block.get_text() for block in soup.find_all(\"code\")]\n    blockquotes = [block.get_text() for block in soup.find_all(\"blockquote\")]\n    newline = False\n\n    for child in soup.recursiveChildGenerator():\n        name = getattr(child, \"name\", None)\n\n        if name is None: # Leaf (terminal) node\n            if child in code_blocks:\n                if newline: # Code block\n                    #if code_blocks.index(child) == len(code_blocks) - 1: # Last code block\n                        #child = child[:-1]\n                    stylized_text.append((\"code\", u\"\\n%s\" % str(child)))\n                    newline = False\n                else: # In-line code\n                    stylized_text.append((\"code\", u\"%s\" % str(child)))\n            else: # Plaintext\n                newline = child.endswith('\\n')\n                stylized_text.append(u\"%s\" % str(child))\n\n    if type(stylized_text[-2]) == tuple:\n        # Remove newline from questions/answers that end with a code block\n        if stylized_text[-2][1].endswith('\\n'):\n            stylized_text[-2] = (\"code\", stylized_text[-2][1][:-1])\n\n    return urwid.Text(stylized_text)\n\n\ndef get_search_results(soup):\n    \"\"\"Returns a list of dictionaries containing each search result.\"\"\"\n    search_results = []\n\n    for result in soup.find_all(\"div\", class_=\"question-summary search-result\"):\n        title_container = result.find_all(\"div\", class_=\"result-link\")[0].find_all(\"a\")[0]\n\n        if result.find_all(\"div\", class_=\"status answered\") != []: # Has answers\n            answer_count = int(result.find_all(\"div\", class_=\"status answered\")[0].find_all(\"strong\")[0].text)\n        elif result.find_all(\"div\", class_=\"status answered-accepted\") != []: # Has an accepted answer (closed)\n            answer_count = int(result.find_all(\"div\", class_=\"status answered-accepted\")[0].find_all(\"strong\")[0].text)\n        else: # No answers\n            answer_count = 0\n\n        search_results.append({\n            \"Title\": title_container[\"title\"],\n            #\"Body\": result.find_all(\"div\", class_=\"excerpt\")[0].text,\n            #\"Votes\": int(result.find_all(\"span\", class_=\"vote-count-post \")[0].find_all(\"strong\")[0].text),\n            \"Answers\": answer_count,\n            \"URL\": SO_URL + title_container[\"href\"]\n        })\n\n    return search_results\n\n\ndef souper(url):\n    \"\"\"Turns a given URL into a BeautifulSoup object.\"\"\"\n\n    try:\n        html = requests.get(url, headers={\"User-Agent\": random.choice(USER_AGENTS)})\n    except requests.exceptions.RequestException:\n        sys.stdout.write(\"\\n%s%s%s\" % (RED, \"Rebound was unable to fetch Stack Overflow results. \"\n                                            \"Please check that you are connected to the internet.\\n\", END))\n        sys.exit(1)\n\n    if re.search(\"\\.com/nocaptcha\", html.url): # URL is a captcha page\n        return None\n    else:\n        return BeautifulSoup(html.text, \"html.parser\")\n\n\n## Main ##\n\n\ndef search_stackoverflow(query):\n    \"\"\"Wrapper function for get_search_results.\"\"\"\n    soup = souper(SO_URL + \"/search?pagesize=50&q=%s\" % query.replace(' ', '+'))\n\n    # TODO: Randomize the user agent\n\n    if soup == None:\n        return (None, True)\n    else:\n        return (get_search_results(soup), False)\n\n\ndef get_question_and_answers(url):\n    \"\"\"Returns details about a given question and list of its answers.\"\"\"\n    soup = souper(url)\n\n    if soup == None: # Captcha page\n        return \"Sorry, Stack Overflow blocked our request. Try again in a couple seconds.\", \"\", \"\", \"\"\n    else:\n        question_title = soup.find_all('a', class_=\"question-hyperlink\")[0].get_text()\n        question_stats = soup.find(\"div\", attrs={\"itemprop\": \"upvoteCount\"}).get_text() # Vote count\n        question_stats += \" Votes | Asked \" + soup.find(\"time\", attrs={\"itemprop\": \"dateCreated\"}).get_text() # Date created\n        question_desc = stylize_code(soup.find_all(\"div\", class_=\"s-prose js-post-body\")[0]) # TODO: Handle duplicates\n\n        answers = [stylize_code(answer) for answer in soup.find_all(\"div\", class_=\"s-prose js-post-body\")][1:]\n        if len(answers) == 0:\n            answers.append(urwid.Text((\"no answers\", u\"\\nNo answers for this question.\")))\n\n        return question_title, question_desc, question_stats, answers\n\n\n############\n## INTERFACE\n############\n\n\n## Helper Classes ##\n\n\nclass Scrollable(urwid.WidgetDecoration):\n    # TODO: Fix scrolling behavior (works with up/down keys, not with cursor)\n\n    def sizing(self):\n        return frozenset([BOX,])\n\n\n    def selectable(self):\n        return True\n\n\n    def __init__(self, widget):\n        \"\"\"Box widget (wrapper) that makes a fixed or flow widget vertically scrollable.\"\"\"\n        self._trim_top = 0\n        self._scroll_action = None\n        self._forward_keypress = None\n        self._old_cursor_coords = None\n        self._rows_max_cached = 0\n        self._rows_max_displayable = 0\n        self.__super.__init__(widget)\n\n\n    def render(self, size, focus=False):\n        maxcol, maxrow = size\n\n        # Render complete original widget\n        ow = self._original_widget\n        ow_size = self._get_original_widget_size(size)\n        canv = urwid.CompositeCanvas(ow.render(ow_size, focus))\n        canv_cols, canv_rows = canv.cols(), canv.rows()\n\n        if canv_cols <= maxcol:\n            pad_width = maxcol - canv_cols\n            if pad_width > 0: # Canvas is narrower than available horizontal space\n                canv.pad_trim_left_right(0, pad_width)\n\n        if canv_rows <= maxrow:\n            fill_height = maxrow - canv_rows\n            if fill_height > 0: # Canvas is lower than available vertical space\n                canv.pad_trim_top_bottom(0, fill_height)\n        self._rows_max_displayable = maxrow\n        if canv_cols <= maxcol and canv_rows <= maxrow: # Canvas is small enough to fit without trimming\n            return canv\n\n        self._adjust_trim_top(canv, size)\n\n        # Trim canvas if necessary\n        trim_top = self._trim_top\n        trim_end = canv_rows - maxrow - trim_top\n        trim_right = canv_cols - maxcol\n        if trim_top > 0:\n            canv.trim(trim_top)\n        if trim_end > 0:\n            canv.trim_end(trim_end)\n        if trim_right > 0:\n            canv.pad_trim_left_right(0, -trim_right)\n\n        # Disable cursor display if cursor is outside of visible canvas parts\n        if canv.cursor is not None:\n            curscol, cursrow = canv.cursor\n            if cursrow >= maxrow or cursrow < 0:\n                canv.cursor = None\n\n        # Let keypress() know if original_widget should get keys\n        self._forward_keypress = bool(canv.cursor)\n\n        return canv\n\n\n    def keypress(self, size, key):\n        if self._forward_keypress:\n            ow = self._original_widget\n            ow_size = self._get_original_widget_size(size)\n\n            # Remember previous cursor position if possible\n            if hasattr(ow, \"get_cursor_coords\"):\n                self._old_cursor_coords = ow.get_cursor_coords(ow_size)\n\n            key = ow.keypress(ow_size, key)\n            if key is None:\n                return None\n\n        # Handle up/down, page up/down, etc\n        command_map = self._command_map\n        if command_map[key] == urwid.CURSOR_UP:\n            self._scroll_action = SCROLL_LINE_UP\n        elif command_map[key] == urwid.CURSOR_DOWN:\n            self._scroll_action = SCROLL_LINE_DOWN\n        elif command_map[key] == urwid.CURSOR_PAGE_UP:\n            self._scroll_action = SCROLL_PAGE_UP\n        elif command_map[key] == urwid.CURSOR_PAGE_DOWN:\n            self._scroll_action = SCROLL_PAGE_DOWN\n        elif command_map[key] == urwid.CURSOR_MAX_LEFT: # \"home\"\n            self._scroll_action = SCROLL_TO_TOP\n        elif command_map[key] == urwid.CURSOR_MAX_RIGHT: # \"end\"\n            self._scroll_action = SCROLL_TO_END\n        else:\n            return key\n\n        self._invalidate()\n\n\n    def mouse_event(self, size, event, button, col, row, focus):\n        ow = self._original_widget\n        if hasattr(ow, \"mouse_event\"):\n            ow_size = self._get_original_widget_size(size)\n            row += self._trim_top\n            return ow.mouse_event(ow_size, event, button, col, row, focus)\n        else:\n            return False\n\n\n    def _adjust_trim_top(self, canv, size):\n        \"\"\"Adjust self._trim_top according to self._scroll_action\"\"\"\n        action = self._scroll_action\n        self._scroll_action = None\n\n        maxcol, maxrow = size\n        trim_top = self._trim_top\n        canv_rows = canv.rows()\n\n        if trim_top < 0:\n            # Negative trim_top values use bottom of canvas as reference\n            trim_top = canv_rows - maxrow + trim_top + 1\n\n        if canv_rows <= maxrow:\n            self._trim_top = 0  # Reset scroll position\n            return\n\n        def ensure_bounds(new_trim_top):\n            return max(0, min(canv_rows - maxrow, new_trim_top))\n\n        if action == SCROLL_LINE_UP:\n            self._trim_top = ensure_bounds(trim_top - 1)\n        elif action == SCROLL_LINE_DOWN:\n            self._trim_top = ensure_bounds(trim_top + 1)\n        elif action == SCROLL_PAGE_UP:\n            self._trim_top = ensure_bounds(trim_top - maxrow+1)\n        elif action == SCROLL_PAGE_DOWN:\n            self._trim_top = ensure_bounds(trim_top + maxrow-1)\n        elif action == SCROLL_TO_TOP:\n            self._trim_top = 0\n        elif action == SCROLL_TO_END:\n            self._trim_top = canv_rows - maxrow\n        else:\n            self._trim_top = ensure_bounds(trim_top)\n\n        if self._old_cursor_coords is not None and self._old_cursor_coords != canv.cursor:\n            self._old_cursor_coords = None\n            curscol, cursrow = canv.cursor\n            if cursrow < self._trim_top:\n                self._trim_top = cursrow\n            elif cursrow >= self._trim_top + maxrow:\n                self._trim_top = max(0, cursrow - maxrow + 1)\n\n\n    def _get_original_widget_size(self, size):\n        ow = self._original_widget\n        sizing = ow.sizing()\n        if FIXED in sizing:\n            return ()\n        elif FLOW in sizing:\n            return (size[0],)\n\n\n    def get_scrollpos(self, size=None, focus=False):\n        return self._trim_top\n\n\n    def set_scrollpos(self, position):\n        self._trim_top = int(position)\n        self._invalidate()\n\n\n    def rows_max(self, size=None, focus=False):\n        if size is not None:\n            ow = self._original_widget\n            ow_size = self._get_original_widget_size(size)\n            sizing = ow.sizing()\n            if FIXED in sizing:\n                self._rows_max_cached = ow.pack(ow_size, focus)[1]\n            elif FLOW in sizing:\n                self._rows_max_cached = ow.rows(ow_size, focus)\n            else:\n                raise RuntimeError(\"Not a flow/box widget: %r\" % self._original_widget)\n        return self._rows_max_cached\n\n    @property\n    def scroll_ratio(self):\n        return self._rows_max_cached / self._rows_max_displayable\n\nclass ScrollBar(urwid.WidgetDecoration):\n    # TODO: Change scrollbar size and color(?)\n\n    def sizing(self):\n        return frozenset((BOX,))\n\n\n    def selectable(self):\n        return True\n\n\n    def __init__(self, widget, thumb_char=u'\\u2588', trough_char=' ',\n                 side=SCROLLBAR_RIGHT, width=1):\n        \"\"\"Box widget that adds a scrollbar to `widget`.\"\"\"\n        self.__super.__init__(widget)\n        self._thumb_char = thumb_char\n        self._trough_char = trough_char\n        self.scrollbar_side = side\n        self.scrollbar_width = max(1, width)\n        self._original_widget_size = (0, 0)\n        self._dragging = False\n\n\n    def render(self, size, focus=False):\n        maxcol, maxrow = size\n\n        ow = self._original_widget\n        ow_base = self.scrolling_base_widget\n        ow_rows_max = ow_base.rows_max(size, focus)\n        if ow_rows_max <= maxrow: # Canvas fits without scrolling - no scrollbar needed\n            self._original_widget_size = size\n            return ow.render(size, focus)\n\n        sb_width = self._scrollbar_width\n        self._original_widget_size = ow_size = (maxcol-sb_width, maxrow)\n        ow_canv = ow.render(ow_size, focus)\n\n        pos = ow_base.get_scrollpos(ow_size, focus)\n        posmax = ow_rows_max - maxrow\n\n        # Thumb shrinks/grows according to the ratio of\n        # <number of visible lines> / <number of total lines>\n        thumb_weight = min(1, maxrow / max(1, ow_rows_max))\n        thumb_height = max(1, round(thumb_weight * maxrow))\n\n        # Thumb may only touch top/bottom if the first/last row is visible\n        top_weight = float(pos) / max(1, posmax)\n        top_height = int((maxrow-thumb_height) * top_weight)\n        if top_height == 0 and top_weight > 0:\n            top_height = 1\n\n        # Bottom part is remaining space\n        bottom_height = maxrow - thumb_height - top_height\n        assert thumb_height + top_height + bottom_height == maxrow\n\n        # Create scrollbar canvas\n        top = urwid.SolidCanvas(self._trough_char, sb_width, top_height)\n        thumb = urwid.SolidCanvas(self._thumb_char, sb_width, thumb_height)\n        bottom = urwid.SolidCanvas(self._trough_char, sb_width, bottom_height)\n        sb_canv = urwid.CanvasCombine([\n            (top, None, False),\n            (thumb, None, False),\n            (bottom, None, False),\n        ])\n\n        combinelist = [(ow_canv, None, True, ow_size[0]), (sb_canv, None, False, sb_width)]\n        if self._scrollbar_side != SCROLLBAR_LEFT:\n            return urwid.CanvasJoin(combinelist)\n        else:\n            return urwid.CanvasJoin(reversed(combinelist))\n\n\n    @property\n    def scrollbar_width(self):\n        return max(1, self._scrollbar_width)\n\n\n    @scrollbar_width.setter\n    def scrollbar_width(self, width):\n        self._scrollbar_width = max(1, int(width))\n        self._invalidate()\n\n\n    @property\n    def scrollbar_side(self):\n        return self._scrollbar_side\n\n\n    @scrollbar_side.setter\n    def scrollbar_side(self, side):\n        if side not in (SCROLLBAR_LEFT, SCROLLBAR_RIGHT):\n            raise ValueError(\"scrollbar_side must be 'left' or 'right', not %r\" % side)\n        self._scrollbar_side = side\n        self._invalidate()\n\n\n    @property\n    def scrolling_base_widget(self):\n        \"\"\"Nearest `base_widget` that is compatible with the scrolling API.\"\"\"\n        def orig_iter(w):\n            while hasattr(w, \"original_widget\"):\n                w = w.original_widget\n                yield w\n            yield w\n\n        def is_scrolling_widget(w):\n            return hasattr(w, \"get_scrollpos\") and hasattr(w, \"rows_max\")\n\n        for w in orig_iter(self):\n            if is_scrolling_widget(w):\n                return w\n\n    @property\n    def scrollbar_column(self):\n        if self.scrollbar_side == SCROLLBAR_LEFT:\n            return 0\n        if self.scrollbar_side == SCROLLBAR_RIGHT:\n            return self._original_widget_size[0]\n\n    def keypress(self, size, key):\n        return self._original_widget.keypress(self._original_widget_size, key)\n\n\n    def mouse_event(self, size, event, button, col, row, focus):\n        ow = self._original_widget\n        ow_size = self._original_widget_size\n        handled = False\n        if hasattr(ow, \"mouse_event\"):\n            handled = ow.mouse_event(ow_size, event, button, col, row, focus)\n\n        if not handled and hasattr(ow, \"set_scrollpos\"):\n            if button == 4: # Scroll wheel up\n                pos = ow.get_scrollpos(ow_size)\n                if pos > 0:\n                    ow.set_scrollpos(pos - 1)\n                    return True\n            elif button == 5: # Scroll wheel down\n                pos = ow.get_scrollpos(ow_size)\n                ow.set_scrollpos(pos + 1)\n                return True\n            elif col == self.scrollbar_column:\n                ow.set_scrollpos(int(row*ow.scroll_ratio))\n                if event == \"mouse press\":\n                    self._dragging = True\n                elif event == \"mouse release\":\n                    self._dragging = False\n            elif self._dragging:\n                ow.set_scrollpos(int(row*ow.scroll_ratio))\n                if event == \"mouse release\":\n                    self._dragging = False\n\n\n\n        return False\n\n\nclass SelectableText(urwid.Text):\n    def selectable(self):\n        return True\n\n\n    def keypress(self, size, key):\n        return key\n\n\n## Helper Functions ##\n\n\ndef interleave(a, b):\n    result = []\n    while a and b:\n        result.append(a.pop(0))\n        result.append(b.pop(0))\n\n    result.extend(a)\n    result.extend(b)\n\n    return result\n\n\n## Main ##\n\n\nclass App(object):\n    def __init__(self, search_results):\n        self.search_results, self.viewing_answers = search_results, False\n        self.palette = [\n            (\"title\", \"light cyan,bold\", \"default\", \"standout\"),\n            (\"stats\", \"light green\", \"default\", \"standout\"),\n            (\"menu\", \"black\", \"light cyan\", \"standout\"),\n            (\"reveal focus\", \"black\", \"light cyan\", \"standout\"),\n            (\"reveal viewed focus\", \"yellow, bold\", \"light cyan\", \"standout\"),\n            (\"no answers\", \"light red\", \"default\", \"standout\"),\n            (\"code\", \"brown\", \"default\", \"standout\"),\n            (\"viewed\", \"yellow\", \"default\", \"standout\")\n        ]\n        self.menu = urwid.Text([\n            u'\\n',\n            (\"menu\", u\" ENTER \"), (\"light gray\", u\" View answers \"),\n            (\"menu\", u\" B \"), (\"light gray\", u\" Open browser \"),\n            (\"menu\", u\" Q \"), (\"light gray\", u\" Quit\"),\n        ])\n\n        results = list(map(lambda result: urwid.AttrMap(SelectableText(self._stylize_title(result)), None, \"reveal focus\"), self.search_results)) # TODO: Add a wrap='clip' attribute\n        self.content = urwid.SimpleListWalker(results)\n        self.content_container = urwid.ListBox(self.content)\n        layout = urwid.Frame(body=self.content_container, footer=self.menu)\n\n        self.main_loop = urwid.MainLoop(layout, self.palette, unhandled_input=self._handle_input)\n        self.original_widget = self.main_loop.widget\n\n        self.main_loop.run()\n\n\n    def _handle_input(self, input):\n        if input == \"enter\" or (input[0]=='meta mouse press' and input[1]==1): # View answers   Either press Enter or \"ALT + Left Click\"\n            url = self._get_selected_link()\n\n            if url != None:\n                self.viewing_answers = True\n                question_title, question_desc, question_stats, answers = get_question_and_answers(url)\n\n                pile = urwid.Pile(self._stylize_question(question_title, question_desc, question_stats) + [urwid.Divider('*')] +\n                interleave(answers, [urwid.Divider('-')] * (len(answers) - 1)))\n                padding = ScrollBar(Scrollable(urwid.Padding(pile, left=2, right=2)))\n                #filler = urwid.Filler(padding, valign=\"top\")\n                linebox = urwid.LineBox(padding)\n\n                menu = urwid.Text([\n                    u'\\n',\n                    (\"menu\", u\" ESC \"), (\"light gray\", u\" Go back \"),\n                    (\"menu\", u\" B \"), (\"light gray\", u\" Open browser \"),\n                    (\"menu\", u\" Q \"), (\"light gray\", u\" Quit\"),\n                ])\n\n                # highlight the selected answer\n                _, idx = self.content_container.get_focus()\n                txt = self.content[idx].original_widget.text\n                self.content[idx] = urwid.AttrMap(SelectableText(txt), 'viewed', 'reveal viewed focus')\n\n                self.main_loop.widget = urwid.Frame(body=urwid.Overlay(linebox, self.content_container, \"center\", (\"relative\", 60), \"middle\", 23), footer=menu)\n        elif input in ('b', 'B') or (input[0]=='ctrl mouse press' and input[1]==1): # Open link     Either press (B or b) or \"CTRL + Left Click\"\n            url = self._get_selected_link()\n\n            if url != None:\n                webbrowser.open(url)\n        elif input == \"esc\": # Close window\n            if self.viewing_answers:\n                self.main_loop.widget = self.original_widget\n                self.viewing_answers = False\n            else:\n                raise urwid.ExitMainLoop()\n        elif input in ('q', 'Q'): # Quit\n            raise urwid.ExitMainLoop()\n\n\n    def _get_selected_link(self):\n        focus_widget, idx = self.content_container.get_focus() # Gets selected item\n        title = focus_widget.base_widget.text\n\n        for result in self.search_results:\n            if title == self._stylize_title(result): # Found selected title's search_result dict\n                return result[\"URL\"]\n\n\n    def _stylize_title(self, search_result):\n        if search_result[\"Answers\"] == 1:\n            return \"%s (1 Answer)\" % search_result[\"Title\"]\n        else:\n            return \"%s (%s Answers)\" % (search_result[\"Title\"], search_result[\"Answers\"])\n\n\n    def _stylize_question(self, title, desc, stats):\n        new_title = urwid.Text((\"title\", u\"%s\" % title))\n        new_stats = urwid.Text((\"stats\", u\"%s\\n\" % stats))\n\n        return [new_title, desc, new_stats]\n\n\n#######\n## MAIN\n#######\n\n\n## Helper Functions ##\n\n\ndef confirm(question):\n    \"\"\"Prompts a given question and handles user input.\"\"\"\n    valid = {\"yes\": True, 'y': True, \"ye\": True,\n             \"no\": False, 'n': False, '': True}\n    prompt = \" [Y/n] \"\n\n    while True:\n        print(BOLD + CYAN + question + prompt + END)\n        choice = input().lower()\n        if choice in valid:\n            return valid[choice]\n\n        print(\"Please respond with 'yes' or 'no' (or 'y' or 'n').\\n\")\n\n\ndef print_help():\n    \"\"\"Prints usage instructions.\"\"\"\n    print(\"%sRebound, V1.1.9a1 - Made by @shobrook%s\\n\" % (BOLD, END))\n    print(\"Command-line tool that automatically searches Stack Overflow and displays results in your terminal when you get a compiler error.\")\n    print(\"\\n\\n%sUsage:%s $ rebound %s[file_name]%s\\n\" % (UNDERLINE, END, YELLOW, END))\n    print(\"\\n$ python3 %stest.py%s   =>   $ rebound %stest.py%s\" % (YELLOW, END, YELLOW, END))\n    print(\"\\n$ node %stest.js%s     =>   $ rebound %stest.js%s\\n\" % (YELLOW, END, YELLOW, END))\n    print(\"\\nIf you just want to query Stack Overflow, use the -q parameter: $ rebound -q %sWhat is an array comprehension?%s\\n\\n\" % (YELLOW, END))\n\n\n## Main ##\n\n\ndef main():\n    if len(sys.argv) == 1 or sys.argv[1].lower() == \"-h\" or sys.argv[1].lower() == \"--help\":\n        print_help()\n    elif sys.argv[1].lower() == \"-q\" or sys.argv[1].lower() == \"--query\":\n        query = ' '.join(sys.argv[2:])\n        search_results, captcha = search_stackoverflow(query)\n\n        if search_results != []:\n            if captcha:\n                print(\"\\n%s%s%s\" % (RED, \"Sorry, Stack Overflow blocked our request. Try again in a minute.\\n\", END))\n                return\n            else:\n                App(search_results) # Opens interface\n        else:\n            print(\"\\n%s%s%s\" % (RED, \"No Stack Overflow results found.\\n\", END))\n    else:\n        language = get_language(sys.argv[1].lower()) # Gets the language name\n        if language == '': # Unknown language\n            print(\"\\n%s%s%s\" % (RED, \"Sorry, Rebound doesn't support this file type.\\n\", END))\n            return\n\n        file_path = sys.argv[1:]\n        if language == 'java':\n            file_path = [f.replace('.class', '') for f in file_path]\n        output, error = execute([language] + file_path) # Compiles the file and pipes stdout\n        if (output, error) == (None, None): # Invalid file\n            return\n\n        error_msg = get_error_message(error, language) # Prepares error message for search\n        if error_msg != None:\n            language = 'java' if language == 'javac' else language # Fix language compiler command\n            query = \"%s %s\" % (language, error_msg)\n            search_results, captcha = search_stackoverflow(query)\n\n            if search_results != []:\n                if captcha:\n                    print(\"\\n%s%s%s\" % (RED, \"Sorry, Stack Overflow blocked our request. Try again in a minute.\\n\", END))\n                    return\n                elif confirm(\"\\nDisplay Stack Overflow results?\"):\n                    App(search_results) # Opens interface\n            else:\n                print(\"\\n%s%s%s\" % (RED, \"No Stack Overflow results found.\\n\", END))\n        else:\n            print(\"\\n%s%s%s\" % (CYAN, \"No error detected :)\\n\", END))\n\n    return\n", "metadata": {"source": "github.com/shobrook/rebound", "file_path": "rebound/rebound.py", "language": "python", "complexity": "complex", "star_count": 4126, "quality_score": 0.8099999999999999, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": true, "has_documentation": true, "has_type_hints": true, "has_testing": true, "has_logging": false, "function_count": 48, "class_count": 5, "line_count": 878}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "error_handling", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate module integration for Create robust error handling with logging and validation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "try:\n    from setuptools import setup\nexcept ImportError:\n    from distutils.core import setup\nfrom codecs import open\nimport sys\n\nif sys.version_info[:3] < (3, 0, 0):\n    print(\"Requires Python 3 to run.\")\n    sys.exit(1)\n\nwith open(\"README.md\", encoding=\"utf-8\") as file:\n    readme = file.read()\n\nsetup(\n    name=\"rebound-cli\",\n    version=\"2.1.0\",\n    description=\"Command-line tool that automatically fetches Stack Overflow results after compiler errors\",\n    #long_description=readme,\n    #long_description_content_type=\"text/markdown\",\n    url=\"https://github.com/shobrook/rebound\",\n    author=\"shobrook\",\n    author_email=\"shobrookj@gmail.com\",\n    classifiers=[\n        \"Environment :: Console\",\n        \"Intended Audience :: Developers\",\n        \"Topic :: Software Development\",\n        \"Topic :: Software Development :: Debuggers\",\n        \"Natural Language :: English\",\n        \"License :: OSI Approved :: MIT License\",\n        \"Programming Language :: Python\"\n    ],\n    keywords=\"stackoverflow stack overflow debug debugging error-handling compile errors error message cli search commandline\",\n    include_package_data=True,\n    packages=[\"rebound\"],\n    entry_points={\"console_scripts\": [\"rebound = rebound.rebound:main\"]},\n    install_requires=[\"BeautifulSoup4\", \"requests\", \"urllib3\", \"urwid\"],\n    requires=[\"BeautifulSoup4\", \"requests\", \"urllib3\", \"urwid\"],\n    python_requires=\">=3\",\n    license=\"MIT\"\n)\n", "metadata": {"source": "github.com/shobrook/rebound", "file_path": "setup.py", "language": "python", "complexity": "simple", "star_count": 4126, "quality_score": 0.69, "features": {"has_classes": false, "has_functions": false, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": true, "has_documentation": false, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 0, "class_count": 0, "line_count": 42}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "error_handling", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create robust error handling with logging and validation with pytest testing framework\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import pytest\nimport traceback\nimport sys\nimport os\nsys.path.insert(0, os.path.join(os.path.dirname( __file__ ), \"..\", \"rebound\"))\nimport rebound\n\n# Constants and helper functions\nEXCEPTION_DETAILS = \"Exception details\"\n\ndef gen_python_exception(exception_type):\n    stack_trace = None\n    try:\n        raise exception_type(EXCEPTION_DETAILS)\n    except Exception:\n        stack_trace = traceback.format_exc()\n    return stack_trace\n\ndef gen_expected_message(exception_type_str):\n    return exception_type_str + \": \" + EXCEPTION_DETAILS\n\n# Tests\n@pytest.mark.parametrize(\"exception_type, exception_type_str\", [\n    (StopIteration, \"StopIteration\"),\n    (StopAsyncIteration, \"StopAsyncIteration\"),\n    (ArithmeticError, \"ArithmeticError\"),\n    (AssertionError, \"AssertionError\"),\n    (AttributeError, \"AttributeError\"),\n    (BufferError, \"BufferError\"),\n    (EOFError, \"EOFError\"),\n    (ImportError, \"ImportError\"),\n    (MemoryError, \"MemoryError\"),\n    (NameError, \"NameError\"),\n    (OSError, \"OSError\"),\n    (ReferenceError, \"ReferenceError\"),\n    (RuntimeError, \"RuntimeError\"),\n    (SyntaxError, \"SyntaxError\"),\n    (SystemError, \"SystemError\"),\n    (TypeError, \"TypeError\"),\n    (ValueError, \"ValueError\"),\n    (Warning, \"Warning\")\n])\ndef test_get_error_message(exception_type, exception_type_str):\n    error_message = rebound.get_error_message(gen_python_exception(exception_type), \"python3\")\n    expected_error_message = gen_expected_message(exception_type_str)\n    assert error_message == expected_error_message", "metadata": {"source": "github.com/shobrook/rebound", "file_path": "tests/python_test.py", "language": "python", "complexity": "simple", "star_count": 4126, "quality_score": 0.75, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": true, "has_documentation": false, "has_type_hints": true, "has_testing": true, "has_logging": false, "function_count": 3, "class_count": 0, "line_count": 46}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "error_handling", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate module integration for Create robust error handling with logging and validation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import pretty_errors\n\n\ntry:\n    myval = [1,2]\n    print(myval[3])\nexcept:\n    try:\n        print(foo)\n    except:\n        print(\"First exception\")\n        raise OSError\n", "metadata": {"source": "github.com/onelivesleft/PrettyErrors", "file_path": "multiexcept.py", "language": "python", "complexity": "simple", "star_count": 2893, "quality_score": 0.69, "features": {"has_classes": false, "has_functions": false, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": true, "has_documentation": false, "has_type_hints": false, "has_testing": false, "has_logging": false, "function_count": 0, "class_count": 0, "line_count": 13}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "error_handling", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create robust error handling with logging and validation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import sys, re, colorama, os, time, linecache\ncolorama.init()\noutput_stderr = sys.stderr\nterminal_is_interactive = sys.stderr.isatty()\n\nname = \"pretty_errors\"\n__version__ = \"1.2.25\"  # remember to update version in setup.py!\n\nactive = 'PYTHON_PRETTY_ERRORS' not in os.environ or os.environ['PYTHON_PRETTY_ERRORS'] != '0'\ninteractive_tty_only = 'PYTHON_PRETTY_ERRORS_ISATTY_ONLY' in os.environ and os.environ['PYTHON_PRETTY_ERRORS_ISATTY_ONLY'] != '0'\n\n\nFILENAME_COMPACT  = 0\nFILENAME_EXTENDED = 1\nFILENAME_FULL     = 2\n\nRESET_COLOR = '\\033[m'\n\nBLACK   = '\\033[0;30m'\nRED     = '\\033[0;31m'\nGREEN   = '\\033[0;32m'\nYELLOW  = '\\033[0;33m'\nBLUE    = '\\033[0;34m'\nMAGENTA = '\\033[0;35m'\nCYAN    = '\\033[0;36m'\nWHITE   = '\\033[0;37m'\n\nBRIGHT_BLACK   = GREY = '\\033[1;30m'\nBRIGHT_RED     = '\\033[1;31m'\nBRIGHT_GREEN   = '\\033[1;32m'\nBRIGHT_YELLOW  = '\\033[1;33m'\nBRIGHT_BLUE    = '\\033[1;34m'\nBRIGHT_MAGENTA = '\\033[1;35m'\nBRIGHT_CYAN    = '\\033[1;36m'\nBRIGHT_WHITE   = '\\033[1;37m'\n\nBLACK_BACKGROUND   = '\\033[40m'\nRED_BACKGROUND     = '\\033[41m'\nGREEN_BACKGROUND   = '\\033[42m'\nYELLOW_BACKGROUND  = '\\033[43m'\nBLUE_BACKGROUND    = '\\033[44m'\nMAGENTA_BACKGROUND = '\\033[45m'\nCYAN_BACKGROUND    = '\\033[46m'\nWHITE_BACKGROUND   = '\\033[47m'\n\n\nansi_escape = re.compile(r'(\\x9B|\\x1B\\[)[0-?]*[ -\\/]*[@-~]')\nwhitelist_paths = []\nblacklist_paths = []\nconfig_paths = {}\n\n\nclass PrettyErrorsConfig():\n    def __init__(self, instance = None):\n        if instance is None:\n            self.name                      = \"default\"\n            self.line_length               = 0\n            self.full_line_newline         = False\n            self.filename_display          = FILENAME_COMPACT\n            self.display_timestamp         = False\n            try:\n                self.timestamp_function    = time.perf_counter\n            except AttributeError:\n                self.timestamp_function    = time.time\n            self.display_link              = False\n            self.separator_character       = '-'\n            self.line_number_first         = False\n            self.top_first                 = False\n            self.always_display_bottom     = True\n            self.stack_depth               = 0\n            self.exception_above           = False\n            self.exception_below           = True\n            self.trace_lines_before        = 0\n            self.trace_lines_after         = 0\n            self.lines_before              = 0\n            self.lines_after               = 0\n            self.display_locals            = False\n            self.display_trace_locals      = False\n            self.truncate_locals           = True\n            self.truncate_code             = False\n            self.display_arrow             = True\n            self.arrow_tail_character      = '-'\n            self.arrow_head_character      = '^'\n            self.header_color              = GREY\n            self.timestamp_color           = GREY\n            self.line_color                = BRIGHT_WHITE\n            self.code_color                = GREY\n            self.filename_color            = BRIGHT_CYAN\n            self.line_number_color         = BRIGHT_GREEN\n            self.function_color            = BRIGHT_BLUE\n            self.link_color                = GREY\n            self.local_name_color          = BRIGHT_MAGENTA\n            self.local_value_color         = RESET_COLOR\n            self.local_len_color           = GREY\n            self.exception_color           = BRIGHT_RED\n            self.exception_arg_color       = BRIGHT_YELLOW\n            self.exception_file_color      = BRIGHT_MAGENTA\n            self.syntax_error_color        = BRIGHT_GREEN\n            self.arrow_tail_color          = BRIGHT_GREEN\n            self.arrow_head_color          = BRIGHT_GREEN\n            self.inner_exception_message   = None\n            self.inner_exception_separator = False\n            self.prefix                    = None\n            self.infix                     = None\n            self.postfix                   = None\n            self.reset_stdout              = False\n            self.show_suppressed           = False\n        else:\n            self.name                      = instance.name\n            self.line_length               = instance.line_length\n            self.full_line_newline         = instance.full_line_newline\n            self.filename_display          = instance.filename_display\n            self.display_timestamp         = instance.display_timestamp\n            self.timestamp_function        = instance.timestamp_function\n            self.display_link              = instance.display_link\n            self.separator_character       = instance.separator_character\n            self.line_number_first         = instance.line_number_first\n            self.top_first                 = instance.top_first\n            self.always_display_bottom     = instance.always_display_bottom\n            self.stack_depth               = instance.stack_depth\n            self.exception_above           = instance.exception_above\n            self.exception_below           = instance.exception_below\n            self.trace_lines_before        = instance.trace_lines_before\n            self.trace_lines_after         = instance.trace_lines_after\n            self.lines_before              = instance.lines_before\n            self.lines_after               = instance.lines_after\n            self.display_locals            = instance.display_locals\n            self.display_trace_locals      = instance.display_trace_locals\n            self.truncate_locals           = instance.truncate_locals\n            self.truncate_code             = instance.truncate_code\n            self.display_arrow             = instance.display_arrow\n            self.arrow_tail_character      = instance.arrow_tail_character\n            self.arrow_head_character      = instance.arrow_head_character\n            self.header_color              = instance.header_color\n            self.timestamp_color           = instance.timestamp_color\n            self.line_color                = instance.line_color\n            self.code_color                = instance.code_color\n            self.filename_color            = instance.filename_color\n            self.line_number_color         = instance.line_number_color\n            self.function_color            = instance.function_color\n            self.link_color                = instance.link_color\n            self.local_name_color          = instance.local_name_color\n            self.local_value_color         = instance.local_value_color\n            self.local_len_color           = instance.local_len_color\n            self.exception_color           = instance.exception_color\n            self.exception_arg_color       = instance.exception_arg_color\n            self.exception_file_color      = instance.exception_file_color\n            self.syntax_error_color        = instance.syntax_error_color\n            self.arrow_tail_color          = instance.arrow_tail_color\n            self.arrow_head_color          = instance.arrow_head_color\n            self.inner_exception_message   = instance.inner_exception_message\n            self.inner_exception_separator = instance.inner_exception_separator\n            self.prefix                    = instance.prefix\n            self.infix                     = instance.infix\n            self.postfix                   = instance.postfix\n            self.reset_stdout              = instance.reset_stdout\n            self.show_suppressed           = instance.show_suppressed\n\n\n    def configure(self, **kwargs):\n        \"\"\"Configure settings governing how exceptions are displayed.\"\"\"\n        for setting in kwargs:\n            if kwargs[setting] is not None: setattr(self, setting, kwargs[setting])\n\n\n    def copy(self):\n        c = PrettyErrorsConfig()\n        c.name                      = self.name\n        c.line_length               = self.line_length\n        c.full_line_newline         = self.full_line_newline\n        c.filename_display          = self.filename_display\n        c.display_timestamp         = self.display_timestamp\n        c.timestamp_function        = self.timestamp_function\n        c.display_link              = self.display_link\n        c.separator_character       = self.separator_character\n        c.line_number_first         = self.line_number_first\n        c.top_first                 = self.top_first\n        c.always_display_bottom     = self.always_display_bottom\n        c.stack_depth               = self.stack_depth\n        c.exception_above           = self.exception_above\n        c.exception_below           = self.exception_below\n        c.trace_lines_before        = self.trace_lines_before\n        c.trace_lines_after         = self.trace_lines_after\n        c.lines_before              = self.lines_before\n        c.lines_after               = self.lines_after\n        c.display_locals            = self.display_locals\n        c.display_trace_locals      = self.display_trace_locals\n        c.truncate_locals           = self.truncate_locals\n        c.truncate_code             = self.truncate_code\n        c.display_arrow             = self.display_arrow\n        c.arrow_tail_character      = self.arrow_tail_character\n        c.arrow_head_character      = self.arrow_head_character\n        c.header_color              = self.header_color\n        c.timestamp_color           = self.timestamp_color\n        c.line_color                = self.line_color\n        c.code_color                = self.code_color\n        c.filename_color            = self.filename_color\n        c.line_number_color         = self.line_number_color\n        c.function_color            = self.function_color\n        c.link_color                = self.link_color\n        c.local_name_color          = self.local_name_color\n        c.local_value_color         = self.local_value_color\n        c.local_len_color           = self.local_len_color\n        c.exception_color           = self.exception_color\n        c.exception_arg_color       = self.exception_arg_color\n        c.exception_file_color       = self.exception_file_color\n        c.syntax_error_color        = self.syntax_error_color\n        c.arrow_tail_color          = self.arrow_tail_color\n        c.arrow_head_color          = self.arrow_head_color\n        c.inner_exception_message   = self.inner_exception_message\n        c.inner_exception_separator = self.inner_exception_separator\n        c.prefix                    = self.prefix\n        c.infix                     = self.infix\n        c.postfix                   = self.postfix\n        c.reset_stdout              = self.reset_stdout\n        c.show_suppressed           = self.show_suppressed\n        return c\n\n    __copy__ = copy\n\n\nconfig = PrettyErrorsConfig()\ndefault_config = PrettyErrorsConfig()\n\n\ndef configure(\n        always_display_bottom     = None,\n        arrow_head_character      = None,\n        arrow_tail_character      = None,\n        arrow_head_color          = None,\n        arrow_tail_color          = None,\n        code_color                = None,\n        display_arrow             = None,\n        display_link              = None,\n        display_locals            = None,\n        display_timestamp         = None,\n        display_trace_locals      = None,\n        exception_above           = None,\n        exception_arg_color       = None,\n        exception_below           = None,\n        exception_color           = None,\n        exception_file_color      = None,\n        filename_color            = None,\n        filename_display          = None,\n        full_line_newline         = None,\n        function_color            = None,\n        header_color              = None,\n        infix                     = None,\n        inner_exception_message   = None,\n        inner_exception_separator = None,\n        line_color                = None,\n        line_length               = None,\n        line_number_color         = None,\n        line_number_first         = None,\n        lines_after               = None,\n        lines_before              = None,\n        link_color                = None,\n        local_len_color           = None,\n        local_name_color          = None,\n        local_value_color         = None,\n        name                      = None,\n        postfix                   = None,\n        prefix                    = None,\n        reset_stdout              = None,\n        separator_character       = None,\n        show_suppressed           = None,\n        stack_depth               = None,\n        syntax_error_color        = None,\n        timestamp_color           = None,\n        timestamp_function        = None,\n        top_first                 = None,\n        trace_lines_after         = None,\n        trace_lines_before        = None,\n        truncate_code             = None,\n        truncate_locals           = None\n        ):\n    \"\"\"Configure settings governing how exceptions are displayed.\"\"\"\n    config.configure(\n        always_display_bottom     = always_display_bottom,\n        arrow_head_character      = arrow_head_character,\n        arrow_tail_character      = arrow_tail_character,\n        arrow_head_color          = arrow_head_color,\n        arrow_tail_color          = arrow_tail_color,\n        code_color                = code_color,\n        display_arrow             = display_arrow,\n        display_link              = display_link,\n        display_locals            = display_locals,\n        display_timestamp         = display_timestamp,\n        display_trace_locals      = display_trace_locals,\n        exception_above           = exception_above,\n        exception_arg_color       = exception_arg_color,\n        exception_below           = exception_below,\n        exception_color           = exception_color,\n        exception_file_color      = exception_file_color,\n        filename_color            = filename_color,\n        filename_display          = filename_display,\n        full_line_newline         = full_line_newline,\n        function_color            = function_color,\n        header_color              = header_color,\n        infix                     = infix,\n        inner_exception_message   = inner_exception_message,\n        inner_exception_separator = inner_exception_separator,\n        line_color                = line_color,\n        line_length               = line_length,\n        line_number_color         = line_number_color,\n        line_number_first         = line_number_first,\n        lines_after               = lines_after,\n        lines_before              = lines_before,\n        link_color                = link_color,\n        local_len_color           = local_len_color,\n        local_name_color          = local_name_color,\n        local_value_color         = local_value_color,\n        name                      = name,\n        postfix                   = postfix,\n        prefix                    = prefix,\n        reset_stdout              = reset_stdout,\n        separator_character       = separator_character,\n        show_suppressed           = show_suppressed,\n        stack_depth               = stack_depth,\n        syntax_error_color        = syntax_error_color,\n        timestamp_color           = timestamp_color,\n        timestamp_function        = timestamp_function,\n        top_first                 = top_first,\n        trace_lines_after         = trace_lines_after,\n        trace_lines_before        = trace_lines_before,\n        truncate_code             = truncate_code,\n        truncate_locals           = truncate_locals\n    )\n\n\ndef mono():\n    global RESET_COLOR\n    RESET_COLOR = ''\n    configure(\n        name                 = \"mono\",\n        infix                = '\\n---\\n',\n        line_number_first    = True,\n        code_color           = '| ',\n        exception_arg_color  = '',\n        exception_color      = '',\n        exception_file_color = '',\n        filename_color       = '',\n        function_color       = '',\n        header_color         = '',\n        line_color           = '> ',\n        line_number_color    = '',\n        link_color           = '',\n        local_len_color      = '',\n        local_name_color     = '= ',\n        local_value_color    = '',\n        timestamp_color      = '',\n        arrow_head_color     = '',\n        arrow_tail_color     = '',\n        syntax_error_color   = ''\n    )\n\n\ndef whitelist(*paths):\n    \"\"\"If the whitelist has any entries, then only files which begin with\n    one of its entries will be included in the stack trace.\n    \"\"\"\n    for path in paths:\n        whitelist_paths.append(os.path.normpath(path).lower())\n\n\ndef blacklist(*paths):\n    \"\"\"Files which begin with a path on the blacklist will not be\n    included in the stack trace.\n    \"\"\"\n    for path in paths:\n        blacklist_paths.append(os.path.normpath(path).lower())\n\n\ndef pathed_config(configuration, *paths):\n    \"\"\"Use alternate configuration for files in the stack trace whose path\n    begins with one of these paths.\"\"\"\n    for path in paths:\n        config_paths[os.path.normpath(path).lower()] = configuration\n\n\n\nclass ExceptionWriter():\n    \"\"\"ExceptionWriter class for outputing exceptions to the screen.\n    Methods beginning 'write_' are the primary candidates for overriding.\n\n    Inherit from this class, then set:\n        pretty_errors.exception_writer = MyExceptionWriter()\n    \"\"\"\n    def __init__(self):\n        self.config = None\n\n\n    def get_terminal_width(self):\n        \"\"\"Width of terminal in characters.\"\"\"\n        try:\n            width = os.get_terminal_size()[0]\n            return 79 if width <= 0 else width\n        except Exception:\n            return 79\n\n\n    def get_line_length(self):\n        \"\"\"Calculated line length.\"\"\"\n        if self.config.line_length <= 0:\n            return self.get_terminal_width()\n        else:\n            return self.config.line_length\n\n\n    def visible_length(self, s):\n        \"\"\"Visible length of string (i.e. without ansi escape sequences)\"\"\"\n        return len(ansi_escape.sub('', s))\n\n\n    def output_text(self, texts):\n        \"\"\"Write list of texts to stderr.\n        Use this function for all output.\n\n            texts: a string or a list of strings\n        \"\"\"\n        if not isinstance(texts, (list, tuple)):\n            texts = [texts]\n        count = 0\n        for text in texts:\n            text = str(text)\n            output_stderr.write(text)\n            count += self.visible_length(text)\n        line_length = self.get_line_length()\n        if count == 0 or count % line_length != 0 or self.config.full_line_newline:\n            output_stderr.write('\\n')\n        output_stderr.write(RESET_COLOR)\n        if self.config.reset_stdout:\n            sys.stdout.write(RESET_COLOR)\n\n\n    def write_header(self):\n        \"\"\"Write stack trace header to screen.\n\n            Should make use of:\n                self.config.separator_character\n                self.config.display_timestamp\n                self.config.timestamp_function()\n                self.config.header_color\"\"\"\n        if not self.config.separator_character: return\n        line_length = self.get_line_length()\n        if self.config.display_timestamp:\n            timestamp = str(self.config.timestamp_function())\n            separator = (line_length - len(timestamp)) * self.config.separator_character + timestamp\n        else:\n            separator = line_length * self.config.separator_character\n        self.output_text('')\n        self.output_text([self.config.header_color, separator])\n\n\n    def write_location(self, path, line, function):\n        \"\"\"Write location of frame to screen.\n\n        Should make use of:\n            self.config.filename_display\n            self.config.filename_color\n            self.config.line_number_color\n            self.config.function_color\n            self.config.line_number_first\n            self.config.function_color\n            self.config.display_link\n            self.config.link_color\n        \"\"\"\n        line_number = str(line) + ' '\n        self.output_text('')\n        if self.config.filename_display == FILENAME_FULL:\n            filename = \"\"\n            self.output_text([self.config.filename_color, path])\n            self.output_text([self.config.line_number_color, line_number, self.config.function_color, function])\n        else:\n            if self.config.filename_display == FILENAME_EXTENDED:\n                line_length = self.get_line_length()\n                filename = path[-(line_length - len(line_number) - len(function) - 4):]\n                if filename != path:\n                    filename = '...' + filename\n            else:\n                filename = os.path.basename(path)\n            if self.config.line_number_first:\n                self.output_text([\n                    self.config.line_number_color, line_number,\n                    self.config.function_color,    function + ' ',\n                    self.config.filename_color,    filename\n                ])\n            else:\n                self.output_text([\n                    self.config.filename_color,    filename + ' ',\n                    self.config.line_number_color, line_number,\n                    self.config.function_color,    function\n                ])\n        if self.config.display_link:\n            self.write_link(path, line)\n\n\n    def write_link(self, filepath, line):\n        \"\"\"Write link of location to screen.  Default version is clickable in VSCode.\n\n        Should make use of:\n            self.config.link_color\n        \"\"\"\n\n        self.output_text([self.config.link_color, '\"%s\", line %s' % (filepath, line)])\n\n\n    def write_code(self, filepath, line, module_globals, is_final, point_at = None):\n        \"\"\"Write frame code to screen.\n        Parameters:\n            filepath:        path to code file\n            line:            line number in file\n            module_globals:  pass to linecache.getline()\n            is_final:        True if this is the last frame\n            point_at:        character position to point at\n\n        Should make use of:\n            self.config.lines_before\n            self.config.lines_after\n            self.config.trace_lines_before\n            self.config.trace_lines_after\n            self.config.truncate_code\n            self.config.display_arrow\n            self.config.arrow_head_character\n            self.config.arrow_tail_character\n            self.config.line_color\n            self.config.code_color\n            self.config.arrow_head_color\n            self.config.arrow_tail_color\n            self.config.syntax_error_color\n        \"\"\"\n\n        lines = []\n        if filepath == '<stdin>':\n            lines.append(str(line).rstrip())\n            line = target_line = start = end = 0\n        else:\n            if is_final:\n                target_line = self.config.lines_before\n                start = line - self.config.lines_before\n                end   = line + self.config.lines_after\n            else:\n                target_line = self.config.trace_lines_before\n                start = line - self.config.trace_lines_before\n                end   = line + self.config.trace_lines_after\n\n            if start < 1:\n                target_line -= (1 - start)\n                start = 1\n\n            for i in range(start, end + 1):\n                lines.append(linecache.getline(filepath, i, module_globals).rstrip())\n\n        min_lead = None\n        for line in lines:\n            if line.strip() == '': continue\n            c = 0\n            while c < len(line) and line[c] in (' ', '\\t'):\n                c += 1\n            if min_lead is None or c < min_lead:\n                min_lead = c\n        if min_lead is None:\n            min_lead = 0\n        if min_lead > 0:\n            lines = [line[min_lead:] for line in lines]\n\n        line_length = self.get_line_length()\n\n        for i, line in enumerate(lines):\n            if i == target_line:\n                color = self.config.line_color\n                if point_at is not None:\n                    point_at -= (min_lead + 1)\n            else:\n                color = self.config.code_color\n            color_length = self.visible_length(color)\n            if self.config.truncate_code and len(line) + color_length > line_length:\n                line = line[:line_length - color_length + 3] + '...'\n            if i == target_line and point_at is not None:\n                if point_at >= line_length:\n                    point_at = line_length - 1\n                start_char = point_at\n                while start_char > 0 and line[start_char - 1] not in (' ', '\\t'):\n                    start_char -= 1\n                end_char = point_at + 1\n                while end_char < len(line) - 1 and line[end_char] not in (' ', '\\t'):\n                    end_char += 1\n                self.output_text([\n                    color, line[:start_char], RESET_COLOR,\n                    self.config.syntax_error_color, line[start_char:end_char], RESET_COLOR,\n                    color, line[end_char:]\n                ])\n                if self.config.display_arrow:\n                    self.output_text([\n                        self.config.arrow_tail_color, self.config.arrow_tail_character * point_at,\n                        self.config.arrow_head_color, self.config.arrow_head_character\n                    ])\n            else:\n                self.output_text([color, line])\n\n        return '\\n'.join(lines)\n\n\n    def exception_name(self, exception):\n        \"\"\"Name of exception.\"\"\"\n        label = str(exception)\n        if label.startswith(\"<class '\"):\n            label = label[8:-2]\n        return label\n\n\n    def write_exception(self, exception_type, exception_value):\n        \"\"\"Write exception to screen.\n\n        Should make use of:\n            self.exception_name()\n            self.config.exception_color\n            self.config.exception_arg_color\n        \"\"\"\n        if exception_value and len(exception_value.args) > 0:\n            output = [\n                self.config.exception_color, self.exception_name(exception_type), ':\\n',\n                self.config.exception_arg_color, '\\n'.join((str(x) for x in exception_value.args))\n            ]\n        else:\n            output = [self.config.exception_color, self.exception_name(exception_type)]\n\n        for attr in (\"filename\", \"filename2\"):\n            if hasattr(exception_value, attr):\n                path = getattr(exception_value, attr)\n                if path is not None:\n                    output.append('\\n')\n                    output.append(self.config.exception_file_color)\n                    output.append(path)\n\n        self.output_text(output)\n\n\n\nexception_writer = ExceptionWriter()\n\n\n\ndef excepthook(exception_type, exception_value, traceback):\n    \"Replaces sys.excepthook to output pretty errors.\"\n\n    writer = exception_writer\n    writer.config = writer.default_config = config\n\n\n    if (not writer.config.top_first and exception_value.__context__\n            and (not exception_value.__suppress_context__ or config.show_suppressed)):\n        excepthook(type(exception_value.__context__), exception_value.__context__, exception_value.__context__.__traceback__)\n        writer.config = writer.default_config\n        if writer.config.inner_exception_message != None:\n            if writer.config.inner_exception_separator:\n                writer.write_header()\n            output_stderr.write(writer.config.inner_exception_message)\n\n    def check_for_pathed_config(path):\n        writer.config = writer.default_config\n        for config_path in config_paths:\n            if path.startswith(config_path):\n                writer.config = config_paths[config_path]\n                break\n\n    if traceback:\n        tb = traceback\n        while tb.tb_next != None:\n            tb = tb.tb_next\n        check_for_pathed_config(os.path.normpath(tb.tb_frame.f_code.co_filename).lower())\n        writer.default_config = writer.config\n\n    writer.write_header()\n\n    if writer.config.prefix != None:\n        output_stderr.write(writer.config.prefix)\n\n    syntax_error_info = None\n    if exception_type == SyntaxError:\n        syntax_error_info = exception_value.args[1]\n        exception_value.args = [exception_value.args[0]]\n\n    if writer.config.exception_above:\n        writer.output_text('')\n        writer.write_exception(exception_type, exception_value)\n\n    if syntax_error_info:\n        check_for_pathed_config(os.path.normpath(syntax_error_info[0]).lower())\n        writer.write_location(syntax_error_info[0], syntax_error_info[1], '')\n        if(syntax_error_info[0] == '<stdin>'):\n            writer.write_code(syntax_error_info[0], syntax_error_info[3], [], True, syntax_error_info[2])\n        else:\n            writer.write_code(syntax_error_info[0], syntax_error_info[1], [], True, syntax_error_info[2])\n    else:\n        tracebacks = []\n        while traceback != None:\n            path = os.path.normpath(traceback.tb_frame.f_code.co_filename).lower()\n            if traceback.tb_next == None or (writer.config.always_display_bottom and tracebacks == []):\n                tracebacks.append(traceback)\n            else:\n                if whitelist_paths:\n                    for white in whitelist_paths:\n                        if path.startswith(white): break\n                    else:\n                        traceback = traceback.tb_next\n                        continue\n                for black in blacklist_paths:\n                    if path.startswith(black): break\n                else:\n                    tracebacks.append(traceback)\n            traceback = traceback.tb_next\n\n        if writer.config.top_first:\n            tracebacks.reverse()\n            if writer.config.stack_depth > 0:\n                if writer.config.always_display_bottom and len(tracebacks) > 1:\n                    tracebacks = tracebacks[:writer.config.stack_depth] + tracebacks[-1:]\n                else:\n                    tracebacks = tracebacks[:writer.config.stack_depth]\n            final = 0\n        else:\n            if writer.config.stack_depth > 0:\n                if writer.config.always_display_bottom and len(tracebacks) > 1:\n                    tracebacks = tracebacks[:1] + tracebacks[-writer.config.stack_depth:]\n                else:\n                    tracebacks = tracebacks[-writer.config.stack_depth:]\n            final = len(tracebacks) - 1\n\n        for count, traceback in enumerate(tracebacks):\n            path = os.path.normpath(traceback.tb_frame.f_code.co_filename).lower()\n            check_for_pathed_config(path)\n\n            if writer.config.infix != None and count != 0:\n                output_stderr.write(writer.config.infix)\n\n            frame = traceback.tb_frame\n            code = frame.f_code\n            writer.write_location(code.co_filename, traceback.tb_lineno, code.co_name)\n            code_string = writer.write_code(code.co_filename, traceback.tb_lineno, frame.f_globals, count == final)\n\n            if (writer.config.display_locals and count == final) or (writer.config.display_trace_locals and count != final):\n                local_variables = [(code_string.find(x), x) for x in frame.f_locals]\n                local_variables.sort()\n                local_variables = [x[1] for x in local_variables if x[0] >= 0]\n                if local_variables:\n                    writer.output_text('')\n                    spacer = ': '\n                    len_spacer = '... '\n                    line_length = writer.get_line_length()\n                    for local in local_variables:\n                        value = str(frame.f_locals[local])\n                        output = [writer.config.local_name_color, local, spacer, writer.config.local_value_color]\n                        if writer.config.truncate_locals and len(local) + len(spacer) + len(value) > line_length:\n                            length = '[' + str(len(value)) + ']'\n                            value = value[:line_length - (len(local) + len(spacer) + len(len_spacer) + len(length))]\n                            output += [value, len_spacer, writer.config.local_len_color, length]\n                        else:\n                            output += [value]\n                        writer.output_text(output)\n\n    writer.config = writer.default_config\n\n    if writer.config.exception_below:\n        writer.output_text('')\n        writer.write_exception(exception_type, exception_value)\n\n    if writer.config.postfix != None:\n        output_stderr.write(writer.config.postfix)\n\n    if (writer.config.top_first and exception_value.__context__ and\n        (not exception_value.__suppress_context__ or config.show_suppressed)):\n        if writer.config.inner_exception_message != None:\n            if writer.config.inner_exception_separator:\n                writer.write_header()\n            output_stderr.write(writer.config.inner_exception_message)\n        excepthook(type(exception_value.__context__), exception_value.__context__, exception_value.__context__.__traceback__)\n\n\n\nlocation_expression = re.compile(r'.*File \"([^\"]*)\", line ([0-9]+), in (.*)')\n\nclass StdErr():\n    \"\"\"Replaces sys.stderr in order to scrape it, when capturing stack trace is unavailable.\"\"\"\n    def __init__(self):\n        self.__in_exception = False\n        self.__awaiting_code = False\n        self.__awaiting_end = 0\n        self.__frames = []\n        self.__point_at = None\n\n\n    def __getattr__(self, name):\n        return getattr(output_stderr, name)\n\n\n    def __enter__(self, *args, **kwargs):\n        return output_stderr.__enter__(*args, **kwargs)\n\n\n    def __is_header(self, text):\n        \"\"\"Is text a traceback header?\"\"\"\n        return text.startswith('Traceback (most recent call last):')\n\n\n    def __is_outer_exception(self, text):\n        \"\"\"Is text a notification for an outer exception?\"\"\"\n        return text.startswith('During handling of the above exception, another exception occurred:')\n\n\n    def __get_location(self, text):\n        \"\"\"Extract location of exception.  If it returns None then text was not a location identifier.\"\"\"\n        location = location_expression.match(text)\n        if location:\n            return (location.group(1), int(location.group(2)), location.group(3))\n        else:\n            return None\n\n\n    def __write_exception(self):\n        \"\"\"Write exception, prettily.\"\"\"\n        if not self.__frames: return\n\n        writer = exception_writer\n        writer.config = writer.default_config = config\n\n        def check_for_pathed_config(path):\n            writer.config = writer.default_config\n            for config_path in config_paths:\n                if path.startswith(config_path):\n                    writer.config = config_paths[config_path]\n                    break\n\n        path, line_number, function = self.__frames[-1]\n        check_for_pathed_config(os.path.normpath(path.lower()))\n        writer.default_config = writer.config\n\n        writer.write_header()\n\n        if writer.config.prefix != None:\n            output_stderr.write(writer.config.prefix)\n\n        exception_type = self.__exception\n        if self.__exception_args:\n            class ExceptionValue():\n                def __init__(self):\n                    self.args = []\n            exception_value = ExceptionValue()\n            if self.__exception_args.startswith('('):\n                for arg in self.__exception_args[1:-1].split(', '):\n                    exception_value.args.append(arg)\n            else:\n                exception_value.args.append(self.__exception_args)\n        else:\n            exception_value = None\n\n        syntax_error_info = None\n        if self.__exception == 'SyntaxError':\n            syntax_error_info = self.__frames[-1]\n            syntax_error_info[2] = self.__point_at\n\n        if writer.config.exception_above:\n            writer.output_text('')\n            writer.write_exception(exception_type, exception_value)\n\n        if syntax_error_info:\n            check_for_pathed_config(os.path.normpath(syntax_error_info[0]).lower())\n            writer.write_location(syntax_error_info[0], syntax_error_info[1], '')\n            writer.write_code(syntax_error_info[0], syntax_error_info[1], [], True, syntax_error_info[2])\n        else:\n            tracebacks = []\n            for i, frame in enumerate(self.__frames):\n                path, line_number, function = frame\n                path = os.path.normpath(path).lower()\n                if (i == len(self.__frames) - 1) or (writer.config.always_display_bottom and tracebacks == []):\n                    tracebacks.append(frame)\n                else:\n                    if whitelist_paths:\n                        for white in whitelist_paths:\n                            if path.startswith(white): break\n                        else:\n                            continue\n                    for black in blacklist_paths:\n                        if path.startswith(black): break\n                    else:\n                        tracebacks.append(frame)\n\n            if writer.config.top_first:\n                tracebacks.reverse()\n                if writer.config.stack_depth > 0:\n                    if writer.config.always_display_bottom and len(tracebacks) > 1:\n                        tracebacks = tracebacks[:writer.config.stack_depth] + tracebacks[-1:]\n                    else:\n                        tracebacks = tracebacks[:writer.config.stack_depth]\n                final = 0\n            else:\n                if writer.config.stack_depth > 0:\n                    if writer.config.always_display_bottom and len(tracebacks) > 1:\n                        tracebacks = tracebacks[:1] + tracebacks[-writer.config.stack_depth:]\n                    else:\n                        tracebacks = tracebacks[-writer.config.stack_depth:]\n                final = len(tracebacks) - 1\n\n            for count, traceback in enumerate(tracebacks):\n                path, line_number, function = traceback\n                normpath = os.path.normpath(path).lower()\n                check_for_pathed_config(normpath)\n\n                if writer.config.infix != None and count != 0:\n                    output_stderr.write(writer.config.infix)\n\n                writer.write_location(path, line_number, function)\n                writer.write_code(path, line_number, [], count == final)\n\n        writer.config = writer.default_config\n\n        if writer.config.exception_below:\n            writer.output_text('')\n            writer.write_exception(exception_type, exception_value)\n\n        if writer.config.postfix != None:\n            output_stderr.write(writer.config.postfix)\n\n\n    def write(self, text):\n        \"\"\"Replaces sys.stderr.write, outputing pretty errors.\"\"\"\n        if not self.__in_exception and not text.split():\n            output_stderr.write(text)\n        else:\n            for line in text.split('\\n'):\n                if self.__awaiting_end > 0:\n                    self.__awaiting_end -= 1\n                    if self.__awaiting_end == 0:\n                        self.__exception_args = line.strip()\n                        self.__in_exception = False\n                        self.__write_exception()\n                elif self.__in_exception:\n                    if not line.strip():\n                        pass\n                    elif self.__awaiting_code:\n                        self.__awaiting_code = False\n                    else:\n                        location = self.__get_location(line)\n                        if location:\n                            self.__frames.append(location)\n                            self.__awaiting_code = True\n                        elif line.strip() == '^':\n                            self.__point_at = len(line) - 2\n                        else: # end of traceback\n                            if ': ' in line:\n                                self.__exception, self.__exception_args = line.strip().split(': ', 1)\n                                self.__in_exception = False\n                                self.__write_exception()\n                            else:\n                                self.__awaiting_end = 2\n                                self.__exception = line.strip()\n                elif self.__is_header(line):\n                    self.__in_exception = True\n                    self.__awaiting_code = False\n                    self.__frames = []\n                elif self.__is_outer_exception(line):\n                    if config.inner_exception_message != None:\n                        if config.inner_exception_separator:\n                            exception_writer.write_header()\n                        output_stderr.write(config.inner_exception_message)\n                else:\n                    output_stderr.write(line)\n\n\n\ndef replace_stderr(force = False):\n    \"\"\"Replace sys.stderr, for cases where standard use with activate() does not work.\"\"\"\n    if (force or not interactive_tty_only or terminal_is_interactive):\n        sys.stderr = StdErr()\n\n\ndef activate():\n    \"\"\"Set sys.excepthook to use pretty errors.\"\"\"\n    sys.excepthook = excepthook\n\n\nif active and (not interactive_tty_only or terminal_is_interactive):\n    activate()\n\n\n\nif __name__ == \"__main__\":\n    configure(\n        filename_display    = FILENAME_EXTENDED,\n        line_number_first   = True,\n        display_link        = True,\n        lines_before        = 5,\n        lines_after         = 2,\n        line_color          = RED + '> ' + default_config.line_color,\n        code_color          =       '  ' + default_config.line_color,\n        truncate_code       = True,\n        inner_exception_separator=True,\n        inner_exception_message = MAGENTA + \"\\n  During handling of the above exception, another exception occurred:\\n\",\n        display_locals      = True\n    )\n    blacklist('c:/python')\n    try:\n        myval = [1,2]\n        print(myval[3])\n    except:\n        a = \"C\" * \"B\"\n", "metadata": {"source": "github.com/onelivesleft/PrettyErrors", "file_path": "pretty_errors/__init__.py", "language": "python", "complexity": "complex", "star_count": 2893, "quality_score": 0.77, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": true, "has_documentation": true, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 33, "class_count": 6, "line_count": 1007}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "error_handling", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create robust error handling with logging and validation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import pretty_errors, sys, os, re, site\nif '-h' in sys.argv or '--help' in sys.argv or '/?' in sys.argv:\n    print(\"\"\"\\\nRun without options to use interactive mode, otherwise:\n\n  python -m pretty_errors [-m] <-u|-s> [-p] OR <-f> OR <-c>\n\n-m = monochrome (do not use color escape codes in output of this menu)\n-u = install in default user location\n-s = install in default system location\n-p = with above, install in pretty_errors.pth instead of *customize.py\n-f = find where pretty_errors is installed\n-c = clean pretty_errors from python startup (do so before uninstalling)\"\"\")\n\nelse:\n    find        = ('-f' in sys.argv)\n    add_to_user = ('-u' in sys.argv)\n    add_to_site = ('-s' in sys.argv)\n    pth         = ('-p' in sys.argv)\n    clean       = ('-c' in sys.argv)\n    mono        = ('-m' in sys.argv)\n\n    if not mono:\n        reset_color   = pretty_errors.RESET_COLOR\n        file_color    = pretty_errors.BRIGHT_CYAN\n        message_color = pretty_errors.BRIGHT_RED\n        key_color     = pretty_errors.YELLOW\n        error_color   = pretty_errors.RED\n    else:\n        reset_color   = ''\n        file_color    = ''\n        message_color = ''\n        key_color     = ''\n        error_color   = ''\n\n    check = re.compile(r'^\\s*import\\s+\\bpretty_errors\\b', re.MULTILINE)\n\n    in_virtualenv = 'VIRTUAL_ENV' in os.environ\n    # in virtualenv, `site` has no attribute `getsitepackages` or `getusersitepackages`\n    if in_virtualenv:\n        def getsitepackages():\n            if os.path.sep == \"\\\\\":\n                sep = \"\\\\\\\\\"\n                env_path = os.environ['VIRTUAL_ENV'].replace(\"\\\\\", \"\\\\\\\\\")\n            else:\n                sep = os.path.sep\n                env_path = os.environ['VIRTUAL_ENV']\n            pattern1 = re.compile(r'^%s$' % sep.join([env_path, 'lib', 'python[0-9.]+', 'site-packages']))\n            pattern2 = re.compile(r'^%s$' % sep.join([env_path, 'lib', 'site-packages']))\n            paths = [path for path in set(sys.path) if pattern1.search(path) or pattern2.search(path)]\n            return paths\n\n        def getusersitepackages():\n            return []\n    else:\n        getsitepackages = site.getsitepackages\n\n        def getusersitepackages():\n            return [site.getusersitepackages()]\n\n    def getallsitepackages():\n        return getsitepackages() + getusersitepackages()\n\n\n    def readfile(path):\n        try:\n            return ''.join((x for x in open(path)))\n        except IOError:\n            return ''\n\n\n    def find_install(quiet = False):\n        found = False\n        for path in getallsitepackages():\n            for filename in 'usercustomize.py', 'sitecustomize.py', 'pretty_errors.pth':\n                filepath = os.path.join(path, filename)\n                if check.search(readfile(filepath)):\n                    if not found:\n                        print(message_color + '\\n\\npretty_errors found in:' + file_color)\n                        found = True\n                    print(filepath)\n                    print(reset_color)\n        if not found and not quiet:\n            print(message_color + '\\npretty_errors not currently installed in any expected locations.\\n' + reset_color)\n        return found\n\n    def clean_install():\n        files_to_edit = []\n        files_to_remove = []\n        for path in getallsitepackages():\n            for filename in 'usercustomize.py', 'sitecustomize.py', 'pretty_errors.pth':\n                filepath = os.path.join(path, filename)\n                data = readfile(filepath).strip()\n                if check.search(data):\n                    if filename.endswith('.pth') or (\n                            data.startswith('### BEGIN PRETTY ERRORS') and\n                            data.endswith('### END PRETTY ERRORS')):\n                        files_to_remove.append(filepath)\n                    else:\n                        files_to_edit.append(filepath)\n        if not files_to_edit and not files_to_remove:\n            print(message_color + '\\npretty_errors not currently installed in any expected locations.' + reset_color)\n            return\n        if files_to_remove:\n            print('\\nAttempting to remove the following files:\\n')\n            for filepath in files_to_remove:\n                print(file_color + filepath + reset_color, end='... ')\n                try:\n                    os.remove(filepath)\n                except Exception as e:\n                    print(error_color + 'ERR\\n' + str(e) + reset_color)\n                else:\n                    print('OK')\n        if files_to_edit:\n            print('\\n\\nFound entry for pretty_errors in the following files.\\n' +\n                message_color +\n                'Please edit and remove the relevant section:\\n')\n            for filepath in files_to_edit:\n                print(file_color + filepath + reset_color)\n\n    if find:\n        find_install()\n        sys.exit(0)\n    elif clean:\n        clean_install()\n        sys.exit(0)\n\n    if add_to_user:\n        if in_virtualenv:\n            print('You are now in virtualenv, which has sitepackage but no user sitepackages, ' +\n                  message_color + '`-u`' + reset_color + ' is incompatible with virtualenv.')\n            print('Please use ' + message_color + '`-s`' + reset_color + ' instead.')\n            sys.exit(1)\n        path = getusersitepackages()[0]\n        filename = 'pretty_errors.pth' if pth else 'usercustomize.py'\n    elif add_to_site:\n        path = getsitepackages()[0]\n        filename = 'pretty_errors.pth' if pth else 'sitecustomize.py'\n    else:\n\n        found = find_install(True)\n\n        def prompt(key, caption, is_path = False):\n            if is_path:\n                print(key_color + key + reset_color + ': ' + file_color + caption + reset_color)\n            else:\n                print(key_color + key + reset_color + ': ' + caption + reset_color)\n\n        def get_choice(query, choices, default = None):\n            options = {}\n            for i in range(len(choices)):\n                options[str(i + 1)] = i\n            while True:\n                print()\n                print(' ' + query)\n                print()\n                if found:\n                    prompt('C', 'Clean startup files (do so before uninstalling pretty_errors)')\n                for option, choice in enumerate(choices):\n                    prompt('%d' % (option + 1), choice, True)\n                prompt('0', 'Exit')\n                if default is None:\n                    print('\\nOption: ', end='')\n                else:\n                    print('\\nOption: [default: ' + message_color + str(default + 1) + reset_color + '] ', end='')\n                choice = input()\n                if choice == '' and default is not None:\n                    choice = str(default + 1)\n                if choice == '0':\n                    sys.exit(0)\n                elif choice.lower() == 'c':\n                    clean_install()\n                    sys.exit(0)\n                elif choice in options:\n                    return options[choice]\n\n        formatting = {'file': file_color, 'msg': message_color, 'reset': reset_color, 'key': key_color}\n        print(\"\"\"\nTo have pretty_errors be used when you run any python file you may add it to your\\\n%(file)s usercustomize.py %(reset)s(user level) or\\\n%(file)s sitecustomize.py %(reset)s(system level), or to\\\n%(file)s pretty_errors.pth%(reset)s.\n\n(just hit %(key)s<enter>%(reset)s to accept the defaults if you are unsure)\n\"\"\" % formatting)\n\n        paths = getallsitepackages()\n        path = paths[get_choice('Choose folder to install into:', paths, -1 if found else len(paths) - 1)]\n\n        if path in getsitepackages():\n            filenames = ['sitecustomize.py', 'pretty_errors.pth']\n        else:\n            filenames = ['usercustomize.py', 'pretty_errors.pth']\n        filename = filenames[get_choice('Choose file to install into:', filenames, 0)]\n\n    if filename.endswith('.pth'):\n        output = (os.path.dirname(os.path.dirname(os.path.normpath(__file__))) +\n            '\\nimport pretty_errors; ' +\n            '#pretty_errors.configure()  ' +\n            '# keep on one line, for options see ' +\n            'https://github.com/onelivesleft/PrettyErrors/blob/master/README.md'\n        )\n    else:\n        output = []\n        output.append('''\n\n### BEGIN PRETTY ERRORS\n\n# pretty-errors package to make exception reports legible.\n# v%s generated this config: newer version may have added methods/options not present!\n\ntry:\n    import pretty_errors\nexcept ImportError:\n    print(\n        'You have uninstalled pretty_errors but it is still present in your python startup.' +\n        '  Please remove its section from file:\\\\n ' + __file__ + '\\\\n'\n    )\nelse:\n    pass\n\n    # Use if you do not have a color terminal:\n    #pretty_errors.mono()\n\n    # Use if you are using a framework which is handling all the exceptions before pretty_errors can:\n    #if pretty_errors.active:\n    #    pretty_errors.replace_stderr()\n\n    # Use to hide frames whose file begins with these paths:\n    #pretty_errors.blacklist('/path/to/blacklist', '/other/path/to/blacklist', ...)\n\n    # Use to only show frames whose file begins with these paths:\n    #pretty_errors.whitelist('/path/to/whitelist', '/other/path/to/whitelist', ...)\n\n    # Use to selectively set a config based on the path to the code of the current frame.\n    #alternate_config = pretty_errors.config.copy()\n    #pretty_errors.pathed_config(alternate_config, '/use/alternate/for/this/path')\n\n    # Use to configure output:  Uncomment each line to change that setting.\n    \"\"\"pretty_errors.configure(\n    ''' % pretty_errors.__version__)\n\n        options = []\n        colors = []\n        parameters = []\n        max_length = 0\n        for option in dir(pretty_errors.config):\n            if len(option) > max_length:\n                max_length = len(option)\n            if (option not in ('configure', 'mono', 'copy') and\n                    not option.startswith('_')):\n                if option.endswith('_color'):\n                    colors.append(option)\n                elif option != 'name':\n                    options.append(option)\n        indent = '        '\n        def prefix(s):\n            return indent + '#' + s.ljust(max_length) + ' = '\n        for option in sorted(options):\n            if option == 'filename_display':\n                parameters.append(prefix(option) + 'pretty_errors.FILENAME_COMPACT,  # FILENAME_EXTENDED | FILENAME_FULL')\n            elif option == 'timestamp_function':\n                parameters.append(prefix(option) + 'time.perf_counter')\n            else:\n                parameters.append(prefix(option) + repr(getattr(pretty_errors.config, option)))\n        for option in sorted(colors):\n            parameters.append(prefix(option) + repr(getattr(pretty_errors.config, option)))\n        parameters.append('\\n' + indent + 'name = \"custom\"  # name it whatever you want')\n        output.append(',\\n'.join(parameters))\n        output.append('\\n    )\"\"\"\\n')\n        output.append('### END PRETTY ERRORS\\n')\n        output = '\\n'.join(output)\n\n    print('\\n--------------')\n\n    filepath = os.path.join(path, filename)\n    if check.search(readfile(filepath)):\n        print('\\npretty_errors already present in:\\n' + file_color + '\\n' + filepath +\n            '\\n' + reset_color + '\\nEdit it to set config options.\\n')\n        sys.exit(0)\n\n    try:\n        os.makedirs(path)\n    except Exception:\n        pass\n    try:\n        out = open(filepath, 'a')\n        out.write(output)\n        out.close()\n    except Exception:\n        print('\\nFailed to write to:\\n' + filepath)\n    else:\n        print('\\npretty_errors added to:\\n' + file_color + '\\n' + filepath + '\\n' + reset_color + '\\nEdit it to set config options.\\n')\n        if filepath.endswith('.pth'):\n            print(error_color + '\\n*** Delete this file when you uninstall pretty_errors! ***\\n' + reset_color)\n", "metadata": {"source": "github.com/onelivesleft/PrettyErrors", "file_path": "pretty_errors/__main__.py", "language": "python", "complexity": "complex", "star_count": 2893, "quality_score": 0.8099999999999999, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": true, "has_documentation": true, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 10, "class_count": 0, "line_count": 296}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "error_handling", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate module integration for Create robust error handling with logging and validation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import setuptools\n\nwith open(\"README.md\", \"r\") as fh:\n    long_description = fh.read()\n\nsetuptools.setup(\n    name=\"pretty_errors\",\n    version=\"1.2.25\", # remember to also update __version__ in __init__.py!\n    author=\"Iain King\",\n    author_email=\"iain.king@gmail.com\",\n    description=\"Prettifies Python exception output to make it legible.\",\n    long_description=long_description,\n    long_description_content_type=\"text/markdown\",\n    url=\"https://github.com/onelivesleft/PrettyErrors/\",\n    packages=setuptools.find_packages(),\n    install_requires=[\n        \"colorama\",\n    ],\n    classifiers=[\n        \"Programming Language :: Python :: 3\",\n        \"Programming Language :: Python :: 2\",\n        \"License :: OSI Approved :: MIT License\",\n        \"Operating System :: OS Independent\",\n    ],\n)\n", "metadata": {"source": "github.com/onelivesleft/PrettyErrors", "file_path": "setup.py", "language": "python", "complexity": "simple", "star_count": 2893, "quality_score": 0.63, "features": {"has_classes": false, "has_functions": false, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 0, "class_count": 0, "line_count": 26}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "error_handling", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate module integration for Create robust error handling with logging and validation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import pretty_errors\n\npretty_errors.configure(\n    separator_character = '*',\n    filename_display = pretty_errors.FILENAME_FULL,\n    lines_before=10,\n    code_color = pretty_errors.default_config.line_color,\n    stack_depth=2,\n    exception_file_color=pretty_errors.BRIGHT_BLUE\n)\nirrelevant = pretty_errors.config.copy()\nirrelevant.line_color = irrelevant.code_color = irrelevant.filename_color = irrelevant.function_color = irrelevant.line_number_color = (\n    pretty_errors.GREY\n)\npretty_errors.pathed_config(irrelevant, 'c:/python', 'c:/users')\n\n\nimport os\nos.rename('nofile', 'stillnofile')\n", "metadata": {"source": "github.com/onelivesleft/PrettyErrors", "file_path": "test.py", "language": "python", "complexity": "simple", "star_count": 2893, "quality_score": 0.6599999999999999, "features": {"has_classes": false, "has_functions": false, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": false, "has_testing": false, "has_logging": false, "function_count": 0, "class_count": 0, "line_count": 20}, "authority_level": "high", "authentic": true}}
{"domain": "code_generation", "category": "error_handling", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create robust error handling with logging and validation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import numpy\nimport stackprinter\n\n\ndef dangerous_function(blub):\n    return sorted(blub, key=lambda xs: sum(xs))\n\ntry:\n    somelist = [[1,2], [3,4]]\n    anotherlist = [['5', 6]]\n    spam = numpy.zeros((3,3))\n    supersecret = \"you haven't seen this\"\n    dangerous_function(somelist + anotherlist)\nexcept:\n    stackprinter.show(style='plaintext', source_lines=4, suppressed_vars=[r\".*secret.*\"])\n\n", "metadata": {"source": "github.com/cknd/stackprinter", "file_path": "demo.py", "language": "python", "complexity": "simple", "star_count": 1290, "quality_score": 0.72, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": true, "has_documentation": false, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 1, "class_count": 0, "line_count": 17}, "authority_level": "medium", "authentic": true}}
{"domain": "code_generation", "category": "error_handling", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create robust error handling with logging and validation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import stackprinter\nstackprinter.set_excepthook()\n\n\ndef bomb(msg):\n    try:\n        raise Exception(msg)\n    except Exception as e:\n        raise Exception(msg+'_b') from e\n\ntry:\n    bomb('1')\nexcept Exception as e:\n    try:\n        raise Exception('2') from e\n    except:\n        bomb('3')\n\n", "metadata": {"source": "github.com/cknd/stackprinter", "file_path": "demo_chained_exceptions.py", "language": "python", "complexity": "simple", "star_count": 1290, "quality_score": 0.75, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": true, "has_documentation": false, "has_type_hints": false, "has_testing": false, "has_logging": false, "function_count": 1, "class_count": 0, "line_count": 19}, "authority_level": "medium", "authentic": true}}
{"domain": "code_generation", "category": "error_handling", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate module integration for Create robust error handling with logging and validation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "if __name__ == '__main__':\n    import stackprinter\n    from tests.source import Hovercraft\n\n    try:\n        Hovercraft().eels()\n    except:\n        # raise\n        stackprinter.show(style='darkbg2',\n                          line_wrap=40,\n                          reverse=False,\n                          suppressed_paths=[r\"lib/python.*/site-packages/numpy\"],\n                          suppressed_vars=[r\".*secret.*\"])\n", "metadata": {"source": "github.com/cknd/stackprinter", "file_path": "demo_complex.py", "language": "python", "complexity": "simple", "star_count": 1290, "quality_score": 0.72, "features": {"has_classes": false, "has_functions": false, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": true, "has_documentation": false, "has_type_hints": false, "has_testing": false, "has_logging": false, "function_count": 0, "class_count": 0, "line_count": 14}, "authority_level": "medium", "authentic": true}}
{"domain": "code_generation", "category": "error_handling", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create robust error handling with logging and validation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# flake8: noqa\n\n# Exception groups (new in py 3.11) aren't supported so far,\n# but at least we fall back on the default message.\n\nimport stackprinter\nstackprinter.set_excepthook()\n\n\ndef raise_group():\n    group = ExceptionGroup('A group!',\n        [Exception(\"Something!\"), Exception(\"Something else!\")])\n    raise group\n\n\nraise_group()\n", "metadata": {"source": "github.com/cknd/stackprinter", "file_path": "demo_exceptiongroups.py", "language": "python", "complexity": "simple", "star_count": 1290, "quality_score": 0.6599999999999999, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 1, "class_count": 0, "line_count": 17}, "authority_level": "medium", "authentic": true}}
{"domain": "code_generation", "category": "error_handling", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create robust error handling with logging and validation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import time\nimport stackprinter\n\n\ndef blocking_function(x):\n    print('ctrl-C me')\n    time.sleep(5)\n\ntry:\n    some_value = 'spam'\n    blocking_function(some_value)\nexcept:\n    # Default: Only print summary info\n    stackprinter.show()\n    # Override:\n    # stackprinter.show(suppressed_exceptions=None)\n    # stackprinter.show(suppressed_exceptions=[])\n\n\n", "metadata": {"source": "github.com/cknd/stackprinter", "file_path": "demo_keyboard_interrupt.py", "language": "python", "complexity": "simple", "star_count": 1290, "quality_score": 0.72, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": true, "has_documentation": false, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 1, "class_count": 0, "line_count": 20}, "authority_level": "medium", "authentic": true}}
{"domain": "code_generation", "category": "error_handling", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create robust error handling with logging and validation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"\nCustom exception formatter example\nbased on https://docs.python.org/3/howto/logging-cookbook.html#customized-exception-formatting\n\nThe goal is to add detailed traces to standard `logging` calls, e.g.\n\n    try:\n        something()\n    except:\n        logger.exception('The front fell off.')\n\n\"\"\"\n\nimport logging\n\n\n# =================== Setup ======================= #\nimport stackprinter\n\nclass VerboseExceptionFormatter(logging.Formatter):\n    def formatException(self, exc_info):\n        msg = stackprinter.format(exc_info)\n        lines = msg.split('\\n')\n        lines_indented = [\"  ┆ \" + line + \"\\n\" for line in lines]\n        msg_indented = \"\".join(lines_indented)\n        return msg_indented\n\ndef configure_logger(logger_name=None):\n    fmt = '%(asctime)s %(levelname)s: %(message)s'\n    formatter = VerboseExceptionFormatter(fmt)\n\n\n    handler = logging.StreamHandler()\n    handler.setFormatter(formatter)\n\n    # # Add more, like\n    # handler = logging.FileHandler('log.txt')\n    # ...\n\n    logger = logging.getLogger(logger_name)\n    logger.addHandler(handler)\n\n\nconfigure_logger(\"some_logger\")\n\n\n# =================== Use ======================= #\nprint(\"\\n\\n\")\nlogger = logging.getLogger(\"some_logger\")\n\ndef dangerous_function(something):\n    return something + 1\n\ntry:\n    nothing = {}\n    dangerous_function(nothing.get(\"something\"))\nexcept:\n    logger.exception('My hovercraft is full of eels.')\n    # Or equivalently:\n    # logger.error('My hovercraft is full of eels.', exc_info=True)\n\n\n", "metadata": {"source": "github.com/cknd/stackprinter", "file_path": "demo_logging.py", "language": "python", "complexity": "medium", "star_count": 1290, "quality_score": 0.87, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": true, "has_documentation": true, "has_type_hints": true, "has_testing": false, "has_logging": true, "function_count": 3, "class_count": 1, "line_count": 63}, "authority_level": "medium", "authentic": true}}
{"domain": "code_generation", "category": "error_handling", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create robust error handling with logging and validation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import logging\nimport stackprinter\n\ndef patch_logging(**kwargs):\n    \"\"\"\n    Replace `formatException` on every log handler / formatter we can find\n\n    **kwargs are those of stackprinter.format\n\n    \"\"\"\n    # this is based on https://github.com/Qix-/better-exceptions/blob/master/better_exceptions/log.py\n\n    def format_exc(exc_info):\n        msg = stackprinter.format(exc_info, **kwargs)\n        msg_indented = '    ' + '\\n    '.join(msg.split('\\n')).strip()\n        return msg_indented\n\n    if hasattr(logging, '_defaultFormatter'):\n        logging._defaultFormatter.formatException = format_exc\n\n    handlers = [handler_ref() for handler_ref in logging._handlerList]\n\n    is_patchable = lambda handler: handler.formatter is not None\n\n    patchable_handlers = filter(is_patchable, handlers)\n\n    for hd in patchable_handlers:\n        hd.formatter.formatException = format_exc\n\n# option A: use the root loger:\nlogger = logging.getLogger()\n\n# # option B: use a custom one:\n# logging.basicConfig()\n# logger = logging.getLogger('some logger')\n\npatch_logging(style='darkbg')\n\n#### test:\n\ndef dangerous_function(blub):\n    return sorted(blub, key=lambda xs: sum(xs))\n\ntry:\n    somelist = [[1,2], [3,4]]\n    anotherlist = [['5', 6]]\n    dangerous_function(somelist + anotherlist)\nexcept:\n    logger.exception('the front fell off.')\n\n\n", "metadata": {"source": "github.com/cknd/stackprinter", "file_path": "demo_logging_hack.py", "language": "python", "complexity": "simple", "star_count": 1290, "quality_score": 0.8099999999999999, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": true, "has_documentation": true, "has_type_hints": true, "has_testing": false, "has_logging": true, "function_count": 3, "class_count": 0, "line_count": 52}, "authority_level": "medium", "authentic": true}}
{"domain": "code_generation", "category": "error_handling", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create robust error handling with logging and validation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "if __name__ == '__main__':\n    import time\n    import sys\n    import numpy as np\n    from stackprinter import format\n\n\n    def bump():\n        print('creating crazy data structures.')\n        boing = {k:np.ones((100,100)) for k in range(int(2e4))}\n        print('.')\n        somelist = [1,2,3,boing, boing, 4,5,6,7,8,9] * int(1e5)\n        print('done.')\n\n        somedict = {'various': 'excellent',\n                             123: 'things',\n                             'in': np.ones((42,23)),\n                             'a list': somelist}\n        sometuple = (1,2, somedict, np.ones((32,64)), boing)\n        somedict['recursion'] = somedict\n        raise Exception('hello')\n\n\n    try:\n        bump()\n    except:\n        stuff = sys.exc_info()\n        scopidoped = 'gotcha'\n        tic = time.perf_counter()\n\n        msg = format(stuff, style='color', show_vals='all', reverse=False, truncate_vals=250, suppressed_paths=[\"site-packages\"])\n\n        took = time.perf_counter() - tic\n        print(msg)\n        print('took %.1fms' % (took * 1000))\n", "metadata": {"source": "github.com/cknd/stackprinter", "file_path": "demo_pprint.py", "language": "python", "complexity": "simple", "star_count": 1290, "quality_score": 0.75, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": true, "has_documentation": false, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 1, "class_count": 0, "line_count": 36}, "authority_level": "medium", "authentic": true}}
{"domain": "code_generation", "category": "error_handling", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create robust error handling with logging and validation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from stackprinter import trace, TracePrinter\nimport numpy as np\n\ndef dosomething(x):\n    y = dosomethingelse(x)\n    return y\n\ndef dosomethingelse(y):\n    a = 2*y\n    b = doYetAnotherThing(y)\n    # raise Exception('ahoi')\n    return b\n\ndef doYetAnotherThing(z):\n    a = z\n    b = {'a': np.ones(1)}\n    zup = np.ones(0)\n    return zup\n\n\n\n# with TracePrinter(style='color', suppressed_paths=[r\"lib/python.*/site-packages/numpy\"]):\nwith TracePrinter(style='plaintext'):\n    a = np.ones(123)\n    dosomething(a)\n", "metadata": {"source": "github.com/cknd/stackprinter", "file_path": "demo_traceprinter.py", "language": "python", "complexity": "simple", "star_count": 1290, "quality_score": 0.69, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 3, "class_count": 0, "line_count": 26}, "authority_level": "medium", "authentic": true}}
{"domain": "code_generation", "category": "error_handling", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create robust error handling with logging and validation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import sys\nimport types\nimport warnings\nfrom threading import Thread\nfrom functools import wraps\n\nimport stackprinter.formatting as fmt\nfrom stackprinter.tracing import TracePrinter, trace\n\n\ndef _guess_thing(f):\n    \"\"\" default to the current exception or current stack frame\"\"\"\n\n    # the only reason this happens up here is to keep sys._getframe at the same\n    # call depth relative to an invocation of `show` or `format`, even when\n    # `format` is called _by_ `show`.\n    @wraps(f)\n    def show_or_format(thing=None, *args, **kwargs):\n        if thing is None:\n            thing = sys.exc_info()\n            if thing == (None, None, None):\n                thing = sys._getframe(1)\n        return f(thing, *args, **kwargs)\n    return show_or_format\n\n\n@_guess_thing\ndef format(thing=None, **kwargs):\n    r\"\"\"\n    Render the traceback of an exception or a frame's call stack\n\n\n    Call this without arguments inside an `except` block to get a traceback for\n    the currently handled exception:\n        ```\n        try:\n            something()\n        except:\n            logger.err(stackprinter.format(**kwargs))\n        ```\n\n    Explicitly pass an exception (or a triple as returned by `sys.exc_info()`)\n    to handle that particular exception anywhere, also outside an except block.\n        ```\n        try:\n            something()\n        except Exception as e:\n            last_exc = e\n\n        if last_exc:\n            logger.err(stackprinter.format(last_exc, **kwargs))\n        ```\n\n    Pass a frame object to see the call stack leading up to that frame:\n        ```\n        stack = stackprinter.format(sys._getframe(2), **kwargs))\n        ```\n\n    Pass a thread object to see its current call stack:\n        ```\n        thread = threading.Thread(target=something)\n        thread.start()\n        # (...)\n        stack = stackprinter.format(thread, **kwargs))\n        ```\n\n    Note:\n    This displays variable values as they are _at the time of formatting_. In\n    multi-threaded programs, variables can change while we're busy walking\n    the stack & printing them. So, if nothing seems to make sense, consider that\n    your exception and the traceback messages are from slightly different times.\n    Sadly, there is no responsible way to freeze all other threads as soon\n    as we want to inspect some thread's call stack (...or is there?)\n\n\n    Params\n    ---\n    thing: (optional) exception, sys.exc_info() tuple, frame or thread\n        What to format. Defaults to the currently handled exception or current\n        stack frame.\n\n    style: string\n        'plaintext' (default): Output just text\n\n        'darkbg', 'darkbg2', 'darkbg3', 'lightbg', 'lightbg2', 'lightbg3':\n            Enable colors, for use in terminals that support 256 ansi\n            colors or in jupyter notebooks (or even with `ansi2html`)\n\n    source_lines: int or 'all'\n        Select how much source code context will be shown.\n        int 0: Don't include a source listing.\n        int n > 0: Show n lines of code. (default: 5)\n        string 'all': Show the whole scope of the frame.\n\n    show_signature: bool (default True)\n        Always include the function header in the source code listing.\n\n    show_vals: str or None\n        Select which variable values will be shown.\n        'line': Show only the variables on the highlighted line.\n        'like_source' (default): Show only those visible in the source listing\n        'all': Show every variable in the scope of the frame.\n        None: Don't show any variable values.\n\n    truncate_vals: int\n        Maximum number of characters to be used for each variable value.\n        Default: 500\n\n    line_wrap: int (default 60)\n        Limit how many columns are available to print each variable\n        (excluding its name). Set to 0 or False to disable wrapping.\n\n    suppressed_paths: list of regex patterns\n        Set less verbose formatting for frames whose code lives in certain paths\n        (e.g. library code). Files whose path matches any of the given regex\n        patterns will be considered boring. The first call to boring code is\n        rendered with fewer code lines (but with argument values still visible),\n        while deeper calls within boring code get a single line and no variable\n        values.\n\n        Example: To hide numpy internals from the traceback, set\n        `suppressed_paths=[r\"lib/python.*/site-packages/numpy\"]`\n        or\n        `suppressed_paths=[re.compile(r\"lib/python.*/site-packages/numpy\")]`\n\n    suppressed_exceptions: list of exception classes\n        Show less verbose formatting for exceptions in this list.\n        By default, this list is `[KeyboardInterrupt]`. Set to `[]`\n        to force verbose formatting even on a keyboard interrupt.\n\n    suppressed_vars: list of regex patterns\n        Don't show the content of variables whose name matches any of the given\n        patterns.\n        Internally, this doesn't just filter the output, but stackprinter won't\n        even try to access these values at all. So this can also be used as a\n        workaround for rare issues around dynamic attribute lookups.\n\n        Example:\n        `suppressed_vars=[r\".*password.*\",  r\"certainobject\\.certainproperty\"]`\n\n    reverse: bool\n        List the innermost frame first.\n\n    add_summary: True, False, 'auto'\n        Append a compact list of involved files and source lines, similar\n        to the built-in traceback message.\n        'auto' (default): do that if the main traceback is longer than 50 lines.\n\n    \"\"\"\n    if isinstance(thing, types.FrameType):\n        return fmt.format_stack_from_frame(thing, **kwargs)\n    elif isinstance(thing, Thread):\n        return format_thread(thing, **kwargs)\n    elif isinstance(thing, Exception):\n        exc_info = (thing.__class__, thing, thing.__traceback__)\n        return format(exc_info, **kwargs)\n    elif _is_exc_info(thing):\n        return fmt.format_exc_info(*thing, **kwargs)\n    else:\n        raise ValueError(\"Can't format %s. \"\\\n                         \"Expected an exception instance, sys.exc_info() tuple,\"\\\n                         \"a frame or a thread object.\" % repr(thing))\n\n\n@_guess_thing\ndef show(thing=None, file='stderr', **kwargs):\n    \"\"\"\n    Print the traceback of an exception or a frame's call stack\n\n    Params\n    ---\n    file: 'stderr', 'stdout' or file-like object\n        defaults to stderr\n\n    **kwargs:\n        See `format`\n    \"\"\"\n\n    # First, to handle a very rare edge case:\n    # Apparently there are environments where sys.stdout and stderr\n    # are None (like the pythonw.exe GUI https://stackoverflow.com/a/30313091).\n    # In those cases, it's not clear where our output should go unless the user\n    # specifies their own file for output. So I'll make a pragmatic assumption:\n    # If `show` is called with the default 'stderr' argument but we are in an\n    # environment where that stream doesn't exist, we're most likely running as\n    # part of a library that's imported in someone's GUI project and there just\n    # isn't any error logging (if there was, the user would've given us a file).\n    # So the least annoying behavior for us is to return silently, not crashing.\n    if file == 'stderr' and sys.stderr is None:\n        return\n\n    if file == 'stderr':\n        file = sys.stderr\n    elif file == 'stdout':\n        file = sys.stdout\n\n    print(format(thing, **kwargs), file=file)\n\n\n\ndef format_current_stack(**kwargs):\n    \"\"\" Render the current thread's call stack.\n\n    Params\n    --\n    **kwargs:\n        See `format`\n    \"\"\"\n    return format(sys._getframe(1), **kwargs)\n\ndef show_current_stack(**kwargs):\n    \"\"\" Print the current thread's call stack.\n\n    Params\n    --\n    **kwargs:\n        See `show`\n    \"\"\"\n    show(sys._getframe(1), **kwargs)\n\n\n\n\ndef format_current_exception(**kwargs):\n    \"\"\"\n    Render a traceback for the currently handled exception.\n\n    Params\n    --\n    **kwargs:\n        See `format`\n    \"\"\"\n    return format(sys.exc_info(), **kwargs)\n\ndef show_current_exception(file=sys.stderr, **kwargs):\n    \"\"\"\n    Print a traceback for the currently handled exception.\n\n    Params\n    --\n    **kwargs:\n        See `show`\n    \"\"\"\n    if file is None:\n        return # see explanation in `show()`\n    print(format_current_exception(**kwargs), file=file)\n\n\n\ndef set_excepthook(**kwargs):\n    \"\"\"\n    Set sys.excepthook to print a detailed traceback for any uncaught exception.\n\n    See `format()` for available kwargs.\n\n    Examples:\n    ----\n\n    Print to stdout instead of stderr:\n    ```\n    set_excepthook(file='stdout')\n    ```\n\n    Enable color output:\n    ```\n    set_excepthook(style='darkbg') # or e.g. 'lightbg' (for more options see `format`)\n    ```\n\n    If running under Ipython, this will, with a heavy heart, attempt to monkey\n    patch Ipython's traceback printer (which handles all exceptions internally,\n    thus bypassing the system excepthook). You can decide whether this sounds\n    like a sane idea.\n\n    To undo, call `remove_excepthook`.\n\n    Params\n    --\n    **kwargs:\n        See `show` and `format`\n    \"\"\"\n    if _is_running_in_ipython():\n        _patch_ipython_excepthook(**kwargs)\n    else:\n        def hook(*args):\n            show(args, **kwargs)\n\n        sys.excepthook = hook\n\n\ndef remove_excepthook():\n    \"\"\" Reinstate the default excepthook \"\"\"\n    if _is_running_in_ipython():\n        _unpatch_ipython_excepthook()\n    sys.excepthook = sys.__excepthook__\n\n\ndef _is_running_in_ipython():\n    try:\n        return __IPYTHON__\n    except NameError:\n        return False\n\n\nipy_tb = None\n\ndef _patch_ipython_excepthook(**kwargs):\n    \"\"\" Replace ipython's built-in traceback printer, excellent though it is\"\"\"\n    global ipy_tb\n\n    blacklist = kwargs.get('suppressed_paths', [])\n    blacklist.append('site-packages/IPython/')\n    kwargs['suppressed_paths'] = blacklist\n\n    if 'file' in kwargs:\n        del kwargs['file']\n\n    def format_tb(*exc_tuple, **__):\n        unstructured_tb = format(exc_tuple, **kwargs)\n        structured_tb = [unstructured_tb]  # \\*coughs*\n        return structured_tb\n\n    import IPython\n    shell = IPython.get_ipython()\n    if ipy_tb is None:\n        ipy_tb = shell.InteractiveTB.structured_traceback\n    shell.InteractiveTB.structured_traceback = format_tb\n\n\ndef _unpatch_ipython_excepthook():\n    \"\"\" restore proper order in Ipython \"\"\"\n    import IPython\n    shell = IPython.get_ipython()\n    if ipy_tb is not None:\n        shell.InteractiveTB.structured_traceback = ipy_tb\n\n\ndef _is_exc_info(thing):\n    if not isinstance(thing, tuple) or len(thing) != 3:\n        return False\n    a, b, c = thing\n    return ((a is None or (isinstance(a, type) and BaseException in a.mro())) and\n            (b is None or (isinstance(b, BaseException))))\n\ndef format_thread(thread, add_summary=False, **kwargs):\n    try:\n        fr = sys._current_frames()[thread.ident]\n    except KeyError:\n        return \"%r: no frames found\" % thread\n    else:\n        if 'suppressed_paths' not in kwargs:\n            kwargs['suppressed_paths'] = []\n        kwargs['suppressed_paths'] += [r\"lib/python.*/threading\\.py\"]\n\n        msg = fmt.format_stack_from_frame(fr, **kwargs)\n        msg_indented = '    ' + '\\n    '.join(msg.split('\\n')).strip()\n        return \"%r\\n\\n%s\" % (thread, msg_indented)\n", "metadata": {"source": "github.com/cknd/stackprinter", "file_path": "stackprinter/__init__.py", "language": "python", "complexity": "complex", "star_count": 1290, "quality_score": 0.84, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": true, "has_documentation": true, "has_type_hints": true, "has_testing": false, "has_logging": true, "function_count": 17, "class_count": 0, "line_count": 357}, "authority_level": "medium", "authentic": true}}
{"domain": "code_generation", "category": "error_handling", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create robust error handling with logging and validation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import random\n\n\n__all__ = ['color', 'darkbg', 'darkbg2', 'darkbg3',\n           'lightbg', 'lightbg2', 'lightbg3']\n\nclass ColorScheme():\n\n    def __getitem__(self, name):\n        raise NotImplemented\n\n    def get_random(self):\n        raise NotImplemented\n\n\nclass darkbg(ColorScheme):\n                              # Hue, Sat, Val, Bold\n    colors = {'exception_type': (0.0, 0.9, 0.6, False),\n              'exception_msg':  (0.0, 0.9, 0.6, True),\n\n              'highlight':      (0.0, 0., 0.8, True),\n              'header':         (0., 0., 0.3, False),\n\n              'lineno':         (0., 0.0, 0.1, False),\n              'arrow_lineno':   (0., 0.0, 0.2, True),\n              'dots':           (0., 0.0, 0.6, False),\n\n              'source_bold':    (0.,0., 0.6, True),\n              'source_default': (0.,0., 0.7, False),\n              'source_comment': (0.,0.,0.2, False),\n              'var_invisible':  (0.6, 0.4, 0.4, False)\n             }\n\n    def __init__(self):\n        self.rng = random.Random()\n\n    def __getitem__(self, name):\n        return self.colors[name]\n\n    def get_random(self, seed, highlight):\n        self.rng.seed(seed)\n\n        hue = self.rng.uniform(0.05,0.7)\n        # if hue < 0:\n        #     hue = hue + 1\n        sat = 1. #1. if highlight else 0.5\n        val = 0.5 #1. if highlight else 0.3\n        bold = highlight\n\n        return hue, sat, val, bold\n\n\n\nclass darkbg2(ColorScheme):\n                              # Hue, Sat, Val, Bold\n    colors = {'exception_type': (0., 1., 0.8, True),\n              'exception_msg':  (0., 1., 0.8, True),\n\n              'highlight':      (0., 0., 1., True),\n              'header':         (0, 0, 0.6, False),\n\n              'lineno':         (0, 0, 0.2, True),\n              'arrow_lineno':   (0, 0, 0.8, True),\n              'dots':           (0, 0, 0.4, False),\n\n              'source_bold':    (0.,0.,0.8, True),\n              'source_default': (0.,0.,0.8, False),\n              'source_comment': (0.,0.,0.2, False),\n              'var_invisible':  (0.6, 0.4, 0.4, False)\n             }\n\n    def __init__(self):\n        self.rng = random.Random()\n\n    def __getitem__(self, name):\n        return self.colors[name]\n\n    def get_random(self, seed, highlight):\n        self.rng.seed(seed)\n\n        hue = self.rng.uniform(0.05,0.7)\n        # if hue < 0:\n        #     hue = hue + 1\n        sat = 1. if highlight else 1.\n        val = 0.8 #if highlight else 0.5\n        bold = highlight\n\n        return hue, sat, val, bold\n\n\nclass darkbg3(ColorScheme):\n                              # Hue, Sat, Val, Bold\n    colors = {'exception_type': (0., 1., 0.8, True),\n              'exception_msg':  (0., 1., 0.8, True),\n              'highlight':      (0., 1., 0.8, True),\n              'header':         (0, 0, 0.8, True),\n              'lineno':         (0, 0, 0.2, True),\n              'arrow_lineno':   (0, 0, 0.8, True),\n              'dots':           (0, 0, 0.4, False),\n              'source_bold':    (0.,0.,0.8, True),\n              'source_default': (0.,0.,0.8, False),\n              'source_comment': (0.,0.,0.2, False),\n              'var_invisible':  (0.6, 0.4, 0.4, False)\n             }\n\n    def __init__(self):\n        self.rng = random.Random()\n\n    def __getitem__(self, name):\n        return self.colors[name]\n\n    def get_random(self, seed, highlight):\n        self.rng.seed(seed)\n\n        hue = self.rng.uniform(0.05,0.7)\n        # if hue < 0:\n        #     hue = hue + 1\n        sat = 1. if highlight else 1.\n        val = 0.8 if highlight else 0.5\n        bold = highlight\n\n        return hue, sat, val, bold\n\n\nclass lightbg(ColorScheme):\n                              # Hue, Sat, Val, Bold\n    colors = {'exception_type': (0.0, 1., 0.6, False),\n              'exception_msg':  (0.0, 1., 0.6, True),\n\n              'highlight':      (0.0, 0, 0., True),\n              'header':         (0, 0, 0.2, False),\n\n              'lineno':         (0, 0, 0.8, True),\n              'arrow_lineno':   (0, 0, 0.3, True),\n              'dots':           (0, 0, 0.4, False),\n              'source_bold':    (0.,0.,0.2, True),\n              'source_default': (0.,0.,0.1, False),\n              'source_comment': (0.,0.,0.6, False),\n              'var_invisible':  (0.6, 0.4, 0.2, False)\n             }\n\n    def __init__(self):\n        self.rng = random.Random()\n\n    def __getitem__(self, name):\n        return self.colors[name]\n\n    def get_random(self, seed, highlight):\n        self.rng.seed(seed)\n\n        hue = self.rng.uniform(0.05, 0.7)\n        # if hue < 0:\n        #     hue = hue + 1\n        sat = 1.\n        val = 0.5 #0.5 #0.6 if highlight else 0.2\n        bold = highlight\n\n        return hue, sat, val, bold\n\n\nclass lightbg2(ColorScheme):\n                              # Hue, Sat, Val, Bold\n    colors = {'exception_type': (0.0, 1., 0.6, False),\n              'exception_msg':  (0.0, 1., 0.6, True),\n\n              'highlight':      (0.0, 0, 0., True),\n              'header':         (0, 0, 0.1, False),\n\n              'lineno':         (0, 0, 0.5, True),\n              'arrow_lineno':   (0, 0, 0.1, True),\n              'dots':           (0, 0, 0.4, False),\n\n              'source_bold':    (0.,0.,0.1, True),\n              'source_default': (0.,0.,0., False),\n              'source_comment': (0.,0.,0.6, False),\n              'var_invisible':  (0.6, 0.4, 0.2, False)\n             }\n\n    def __init__(self):\n        self.rng = random.Random()\n\n    def __getitem__(self, name):\n        return self.colors[name]\n\n    def get_random(self, seed, highlight):\n        self.rng.seed(seed)\n\n        hue = self.rng.uniform(0.05, 0.7)\n        # if hue < 0:\n        #     hue = hue + 1\n        sat = 1.\n        val = 0.5\n        bold = True\n\n        return hue, sat, val, bold\n\nclass lightbg3(ColorScheme):\n                              # Hue, Sat, Val, Bold\n    colors = {'exception_type': (0.0, 1., 0.7, False),\n              'exception_msg':  (0.0, 1., 0.7, True),\n\n              'highlight':      (0.0, 1., 0.6, True),\n              'header':         (0, 0, 0.1, True),\n\n              'lineno':         (0, 0, 0.5, True),\n              'arrow_lineno':   (0, 0, 0.1, True),\n              'dots':           (0, 0, 0.4, False),\n\n              'source_bold':    (0.,0.,0., True),\n              'source_default': (0.,0.,0., False),\n              'source_comment': (0.,0.,0.6, False),\n              'var_invisible':  (0.6, 0.4, 0.2, False)\n             }\n\n    def __init__(self):\n        self.rng = random.Random()\n\n    def __getitem__(self, name):\n        return self.colors[name]\n\n    def get_random(self, seed, highlight):\n        self.rng.seed(seed)\n\n        hue = self.rng.uniform(0.05, 0.7)\n        # if hue < 0:\n        #     hue = hue + 1\n        sat = 1.\n        val = 0.5\n        bold = True\n\n        return hue, sat, val, bold\n\n\n\ncolor = darkbg2\n\n\nif __name__ == '__main__':\n    import numpy as np\n    from utils import get_ansi_tpl\n\n    for hue in np.arange(0,1.05,0.05):\n        print('\\n\\nhue %.2f\\nsat' % hue)\n        for sat in np.arange(0,1.05,0.05):\n            print('%.2f  ' % sat, end='')\n            for val in np.arange(0,1.05,0.05):\n                tpl = get_ansi_tpl(hue, sat, val)\n                # number = \" (%.1f %.1f %.1f)\" % (hue, sat, val)\n                number = ' %.2f' % val\n                print(tpl % number, end='')\n            print('  %.2f' % sat)\n", "metadata": {"source": "github.com/cknd/stackprinter", "file_path": "stackprinter/colorschemes.py", "language": "python", "complexity": "complex", "star_count": 1290, "quality_score": 0.72, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": false, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 20, "class_count": 7, "line_count": 252}, "authority_level": "medium", "authentic": true}}
{"domain": "code_generation", "category": "error_handling", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create robust error handling with logging and validation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import types\nimport inspect\nfrom collections import OrderedDict, namedtuple\nfrom stackprinter.source_inspection import annotate\nfrom stackprinter.utils import match\n\nNON_FUNCTION_SCOPES =  ['<module>', '<lambda>', '<listcomp>']\n\n_FrameInfo = namedtuple('_FrameInfo',\n                        ['filename', 'function', 'lineno', 'source_map',\n                         'head_lns', 'line2names', 'name2lines', 'assignments'])\n\nclass FrameInfo(_FrameInfo):\n    # give this namedtuple type a friendlier string representation\n    def __str__(self):\n        return (\"<FrameInfo %s, line %s, scope %s>\" %\n                (self.filename, self.lineno, self.function))\n\n\ndef get_info(tb_or_frame, lineno=None, suppressed_vars=[]):\n    \"\"\"\n    Get a frame representation that's easy to format\n\n\n    Params\n    ---\n    tb: Traceback object or Frame object\n\n    lineno: int (optional)\n        Override which source line is treated as the important one. For trace-\n        back objects this defaults to the last executed line (tb.tb_lineno).\n        For frame objects, it defaults the currently executed one (fr.f_lineno).\n\n\n    Returns\n    ---\n    FrameInfo, a named tuple with the following fields:\n\n         filename: Path of the executed source file\n\n         function: Name of the scope\n\n         lineno: Highlighted line (last executed line)\n\n         source_map: OrderedDict\n            Maps line numbers to a list of tokens. Each token is a (string, type)\n            tuple. Concatenating the first elements of all tokens of all lines\n            restores the original source, weird whitespaces/indentations and all\n            (in contrast to python's built-in `tokenize`). However, multiline\n            statements (those with a trailing backslash) are secretly collapsed\n            into their first line.\n\n         head_lns: (int, int) or (None, None)\n            Line numbers of the beginning and end of the function header\n\n         line2names: dict\n            Maps each line number to a list of variables names that occur there\n\n         name2lines: dict\n            Maps each variable name to a list of line numbers where it occurs\n\n         assignments: OrderedDict\n            Holds current values of all variables that occur in the source and\n            are found in the given frame's locals or globals. Attribute lookups\n            with dot notation are treated as one variable, so if `self.foo.zup`\n            occurs in the source, this dict will get a key 'self.foo.zup' that\n            holds the fully resolved value.\n            (TODO: it would be easy to return the whole attribute lookup chain,\n            so maybe just do that & let formatting decide which parts to show?)\n            (TODO: Support []-lookups just like . lookups)\n    \"\"\"\n\n    if isinstance(tb_or_frame, FrameInfo):\n        return tb_or_frame\n\n    if isinstance(tb_or_frame, types.TracebackType):\n        tb = tb_or_frame\n        lineno = tb.tb_lineno if lineno is None else lineno\n        frame = tb.tb_frame\n    elif isinstance(tb_or_frame, types.FrameType):\n        frame = tb_or_frame\n        lineno = frame.f_lineno if lineno is None else lineno\n    else:\n        raise ValueError('Cant inspect this: ' + repr(tb_or_frame))\n\n    filename = inspect.getsourcefile(frame) or inspect.getfile(frame)\n    function = frame.f_code.co_name\n\n    try:\n        source, startline = get_source(frame)\n        # this can be slow (tens of ms) the first time it is called, since\n        # inspect.get_source internally calls inspect.getmodule, for no\n        # other purpose than updating the linecache. seems like a bad tradeoff\n        # for our case, but this is not the time & place to fork `inspect`.\n    except:\n        source = []\n        startline = lineno\n\n    source_map, line2names, name2lines, head_lns, lineno = annotate(source, startline, lineno)\n\n    if function in NON_FUNCTION_SCOPES:\n        head_lns = []\n\n    names = name2lines.keys()\n    assignments = get_vars(names, frame.f_locals, frame.f_globals, suppressed_vars)\n\n    finfo =  FrameInfo(filename, function, lineno, source_map, head_lns,\n                       line2names, name2lines, assignments)\n    return finfo\n\n\ndef get_source(frame):\n    \"\"\"\n    get source lines for this frame\n\n    Params\n    ---\n    frame : frame object\n\n    Returns\n    ---\n    lines : list of str\n\n    startline : int\n        location of lines[0] in the original source file\n    \"\"\"\n\n    # TODO find out what's faster: Allowing inspect's getsourcelines\n    # to tokenize the whole file to find the surrounding code block,\n    # or getting the whole file quickly via linecache & feeding all\n    # of it to our own instance of tokenize, then clipping to\n    # desired context afterwards.\n\n    if frame.f_code.co_name in NON_FUNCTION_SCOPES:\n        lines, _ = inspect.findsource(frame)\n        startline = 1\n    else:\n        lines, startline = inspect.getsourcelines(frame)\n\n    return lines, startline\n\n\ndef get_vars(names, loc, glob, suppressed_vars):\n    assignments = []\n    for name in names:\n        if match(name, suppressed_vars):\n            assignments.append((name, CensoredVariable()))\n        else:\n            try:\n                val = lookup(name, loc, glob)\n            except LookupError:\n                pass\n            else:\n                assignments.append((name, val))\n    return OrderedDict(assignments)\n\n\ndef lookup(name, scopeA, scopeB):\n    basename, *attr_path = name.split('.')\n    if basename in scopeA:\n        val = scopeA[basename]\n    elif basename in scopeB:\n        val = scopeB[basename]\n    else:\n        # not all names in the source file will be\n        # defined (yet) when we get to see the frame\n        raise LookupError(basename)\n\n    for k, attr in enumerate(attr_path):\n        try:\n            val = getattr(val, attr)\n        except Exception as e:\n            # return a special value in case of lookup errors\n            # (note: getattr can raise anything, e.g. if a complex\n            # @property fails).\n            return UnresolvedAttribute(basename, attr_path, k, val,\n                                       e.__class__.__name__, str(e))\n    return val\n\n\nclass CensoredVariable():\n    def __repr__(self):\n        return \"*****\"\n\nclass UnresolvedAttribute():\n    \"\"\"\n    Container value for failed dot attribute lookups\n    \"\"\"\n    def __init__(self, basename, attr_path, failure_idx, value,\n                exc_type, exc_str):\n        self.basename = basename\n        self.attr_path = attr_path\n        self.first_failed = attr_path[failure_idx]\n        self.failure_idx = failure_idx\n        self.last_resolvable_value = value\n        self.exc_type = exc_type\n        self.exc_str = exc_str\n\n    @property\n    def last_resolvable_name(self):\n        return self.basename + '.'.join([''] + self.attr_path[:self.failure_idx])\n", "metadata": {"source": "github.com/cknd/stackprinter", "file_path": "stackprinter/extraction.py", "language": "python", "complexity": "complex", "star_count": 1290, "quality_score": 0.8099999999999999, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": true, "has_documentation": true, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 8, "class_count": 3, "line_count": 202}, "authority_level": "medium", "authentic": true}}
{"domain": "code_generation", "category": "error_handling", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create robust error handling with logging and validation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"\nVarious convnenience methods to walk stacks and concatenate formatted frames\n\"\"\"\nimport types\nimport traceback\n\nimport stackprinter.extraction as ex\nimport stackprinter.colorschemes as colorschemes\nfrom stackprinter.utils import match, get_ansi_tpl\nfrom stackprinter.frame_formatting import FrameFormatter, ColorfulFrameFormatter\n\n\ndef get_formatter(style, **kwargs):\n    if style in ['plaintext', 'plain']:\n        return FrameFormatter(**kwargs)\n    else:\n        return ColorfulFrameFormatter(style, **kwargs)\n\n\ndef format_summary(frames, style='plaintext', source_lines=1, reverse=False,\n                   **kwargs):\n    \"\"\"\n    Render a list of frames with 1 line of source context, no variable values.\n\n    keyword args like stackprinter.format()\n    \"\"\"\n    min_src_lines = 0 if source_lines == 0 else 1\n    minimal_formatter = get_formatter(style=style,\n                                      source_lines=min_src_lines,\n                                      show_signature=False,\n                                      show_vals=False)\n\n    frame_msgs = [minimal_formatter(frame) for frame in frames]\n    if reverse:\n        frame_msgs = reversed(frame_msgs)\n\n    return ''.join(frame_msgs)\n\n\ndef format_stack(frames, style='plaintext', source_lines=5,\n                 show_signature=True, show_vals='like_source',\n                 truncate_vals=500, line_wrap=60, reverse=False,\n                 suppressed_paths=None, suppressed_vars=[]):\n    \"\"\"\n    Render a list of frames (or FrameInfo tuples)\n\n    keyword args like stackprinter.format()\n    \"\"\"\n\n\n    min_src_lines = 0 if source_lines == 0 else 1\n\n    minimal_formatter = get_formatter(style=style,\n                                      source_lines=min_src_lines,\n                                      show_signature=False,\n                                      show_vals=False)\n\n    reduced_formatter = get_formatter(style=style,\n                                      source_lines=min_src_lines,\n                                      show_signature=show_signature,\n                                      show_vals=show_vals,\n                                      truncate_vals=truncate_vals,\n                                      line_wrap=line_wrap,\n                                      suppressed_paths=suppressed_paths,\n                                      suppressed_vars=suppressed_vars)\n\n    verbose_formatter = get_formatter(style=style,\n                                      source_lines=source_lines,\n                                      show_signature=show_signature,\n                                      show_vals=show_vals,\n                                      truncate_vals=truncate_vals,\n                                      line_wrap=line_wrap,\n                                      suppressed_paths=suppressed_paths,\n                                      suppressed_vars=suppressed_vars)\n\n    frame_msgs = []\n    parent_is_boring = True\n    for frame in frames:\n        fi = ex.get_info(frame, suppressed_vars=suppressed_vars)\n        is_boring = match(fi.filename, suppressed_paths)\n        if is_boring:\n            if parent_is_boring:\n                formatter = minimal_formatter\n            else:\n                formatter = reduced_formatter\n        else:\n            formatter = verbose_formatter\n\n        parent_is_boring = is_boring\n        frame_msgs.append(formatter(fi))\n\n    if reverse:\n        frame_msgs = reversed(frame_msgs)\n\n    return ''.join(frame_msgs)\n\n\ndef format_stack_from_frame(fr, add_summary=False, **kwargs):\n    \"\"\"\n    Render a frame and its parents\n\n\n    keyword args like stackprinter.format()\n\n    \"\"\"\n    stack = []\n    while fr is not None:\n        stack.append(fr)\n        fr = fr.f_back\n    stack = reversed(stack)\n\n    return format_stack(stack, **kwargs)\n\n\ndef format_exc_info(etype, evalue, tb, style='plaintext', add_summary='auto',\n                    reverse=False, suppressed_exceptions=[KeyboardInterrupt],\n                    suppressed_vars=[], **kwargs):\n    \"\"\"\n    Format an exception traceback, including the exception message\n\n    see stackprinter.format() for docs about the keyword arguments\n    \"\"\"\n    if etype is None:\n        etype = type(None)\n\n    if etype.__name__ == 'ExceptionGroup':\n        # Exception groups (new in py 3.11) aren't supported so far,\n        # but at least we fall back on the default message.\n        return ''.join(traceback.format_exception(etype, evalue, tb))\n\n    msg = ''\n    try:\n        # First, recursively format any chained exceptions (exceptions\n        # during whose handling the given one happened).\n        # TODO: refactor this whole messy function to return a\n        # more... structured datastructure before assembling a string,\n        # so that e.g. a summary of the whole chain can be shown at\n        # the end.\n        context = getattr(evalue, '__context__', None)\n        cause = getattr(evalue, '__cause__', None)\n        suppress_context = getattr(evalue, '__suppress_context__', False)\n        if cause:\n            chained_exc = cause\n            chain_hint = (\"\\n\\nThe above exception was the direct cause \"\n                          \"of the following exception:\\n\\n\")\n        elif context and not suppress_context:\n            chained_exc = context\n            chain_hint = (\"\\n\\nWhile handling the above exception, \"\n                          \"another exception occurred:\\n\\n\")\n        else:\n            chained_exc = None\n\n        if chained_exc:\n            msg += format_exc_info(chained_exc.__class__,\n                                   chained_exc,\n                                   chained_exc.__traceback__,\n                                   style=style,\n                                   add_summary=add_summary,\n                                   reverse=reverse,\n                                   suppressed_vars=suppressed_vars,\n                                   **kwargs)\n\n            if style == 'plaintext':\n                msg +=  chain_hint\n            else:\n                sc = getattr(colorschemes, style)\n                clr = get_ansi_tpl(*sc.colors['exception_type'])\n                msg += clr % chain_hint\n\n        # Now, actually do some formatting:\n        parts = []\n        if tb:\n            frameinfos = [ex.get_info(tb_, suppressed_vars=suppressed_vars)\n                          for tb_ in _walk_traceback(tb)]\n            if (suppressed_exceptions and\n                issubclass(etype, tuple(suppressed_exceptions))):\n                summary = format_summary(frameinfos, style=style,\n                                         reverse=reverse, **kwargs)\n                parts = [summary]\n            else:\n                whole_stack = format_stack(frameinfos, style=style,\n                                           reverse=reverse, **kwargs)\n                parts.append(whole_stack)\n\n                if add_summary == 'auto':\n                    add_summary = whole_stack.count('\\n') > 50\n\n                if add_summary:\n                    summary = format_summary(frameinfos, style=style,\n                                             reverse=reverse, **kwargs)\n                    summary += '\\n'\n                    parts.append('---- (full traceback below) ----\\n\\n' if reverse else\n                                 '---- (full traceback above) ----\\n')\n                    parts.append(summary)\n\n        exc = format_exception_message(etype, evalue, style=style)\n        parts.append('\\n\\n' if reverse else '')\n        parts.append(exc)\n\n        if reverse:\n            parts = reversed(parts)\n\n        msg += ''.join(parts)\n\n    except Exception as exc:\n        import os\n        if 'PY_STACKPRINTER_DEBUG' in os.environ:\n            raise\n\n        our_tb = traceback.format_exception(exc.__class__,\n                                            exc,\n                                            exc.__traceback__,\n                                            chain=False)\n        where = getattr(exc, 'where', None)\n        context = \" while formatting \" + str(where) if where else ''\n        msg = 'Stackprinter failed%s:\\n%s\\n' % (context, ''.join(our_tb[-2:]))\n        msg += 'So here is your original traceback at least:\\n\\n'\n        msg += ''.join(traceback.format_exception(etype, evalue, tb))\n\n\n    return msg\n\n\ndef format_exception_message(etype, evalue, tb=None, style='plaintext'):\n    type_str = etype.__name__\n    val_str = str(evalue)\n\n    if etype == SyntaxError and evalue.text:\n        val_str += '\\n    %s\\n   %s^' % (evalue.text.rstrip(), ' '*evalue.offset)\n\n    if val_str:\n        type_str += \": \"\n\n    if style == 'plaintext':\n        return type_str + val_str\n    else:\n        sc = getattr(colorschemes, style)\n\n        clr_head = get_ansi_tpl(*sc.colors['exception_type'])\n        clr_msg = get_ansi_tpl(*sc.colors['exception_msg'])\n\n        return clr_head % type_str + clr_msg % val_str\n\n\ndef _walk_traceback(tb):\n    \"\"\"\n    Follow a chain of traceback objects outwards\n    \"\"\"\n    while tb:\n        yield tb\n        tb = tb.tb_next\n", "metadata": {"source": "github.com/cknd/stackprinter", "file_path": "stackprinter/formatting.py", "language": "python", "complexity": "complex", "star_count": 1290, "quality_score": 0.75, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": true, "has_documentation": true, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 7, "class_count": 0, "line_count": 252}, "authority_level": "medium", "authentic": true}}
{"domain": "code_generation", "category": "error_handling", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create robust error handling with logging and validation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import types\nimport os\n\nfrom collections import OrderedDict\nimport stackprinter.extraction as ex\nimport stackprinter.source_inspection as sc\nimport stackprinter.colorschemes as colorschemes\n\nfrom stackprinter.prettyprinting import format_value\nfrom stackprinter.utils import inspect_callable, match, trim_source, get_ansi_tpl\n\nclass FrameFormatter():\n    headline_tpl = 'File \"%s\", line %s, in %s\\n'\n    sourceline_tpl = \"    %-3s  %s\"\n    single_sourceline_tpl = \"    %s\"\n    marked_sourceline_tpl = \"--> %-3s  %s\"\n    elipsis_tpl = \" (...)\\n\"\n    var_indent = 5\n    sep_vars = \"%s%s\" % ((' ') * 4, ('.' * 50))\n    sep_source_below = \"\"\n\n    val_tpl = ' ' * var_indent + \"%s = %s\\n\"\n\n    def __init__(self, source_lines=5, source_lines_after=1,\n                 show_signature=True, show_vals='like_source',\n                 truncate_vals=500, line_wrap: int = 60,\n                 suppressed_paths=None, suppressed_vars=None):\n        \"\"\"\n        Formatter for single frames.\n\n        This is essentially a partially applied function -- supply config args\n        to this constructor, then _call_ the resulting object with a frame.\n\n\n        Params\n        ---\n        source_lines: int or 'all'. (default: 5 lines)\n            Select how much source code context will be shown.\n            int 0: Don't include a source listing.\n            int n > 0: Show n lines of code.\n            string 'all': Show the whole scope of the frame.\n\n        source_lines_after: int\n            nr of lines to show after the highlighted one\n\n        show_signature: bool (default True)\n            Always include the function header in the source code listing.\n\n        show_vals: str or None (default 'like_source')\n            Select which variable values will be shown.\n            'line': Show only the variables on the highlighted line.\n            'like_source': Show those visible in the source listing (default).\n            'all': Show every variable in the scope of the frame.\n            None: Don't show any variable values.\n\n        truncate_vals: int (default 500)\n            Maximum number of characters to be used for each variable value\n\n        line_wrap: int (default 60)\n            insert linebreaks after this nr of characters, use 0 to never insert\n            a linebreak\n\n        suppressed_paths: list of regex patterns\n            Set less verbose formatting for frames whose code lives in certain paths\n            (e.g. library code). Files whose path matches any of the given regex\n            patterns will be considered boring. The first call to boring code is\n            rendered with fewer code lines (but with argument values still visible),\n            while deeper calls within boring code get a single line and no variable\n            values.\n\n            Example: To hide numpy internals from the traceback, set\n            `suppressed_paths=[r\"lib/python.*/site-packages/numpy\"]`\n        \"\"\"\n\n\n        if not (isinstance(source_lines, int) or source_lines == 'all'):\n            raise ValueError(\"source_lines must be an integer or 'all', \"\n                             \"was %r\" % source_lines)\n\n        valid_gv = ['all', 'like_source', 'line', None, False]\n        if show_vals not in valid_gv:\n            raise ValueError(\"show_vals must be one of \"\n                             \"%s, was %r\" % (str(valid_gv), show_vals))\n\n        self.lines = source_lines\n        self.lines_after = source_lines_after\n        self.show_signature = show_signature\n        self.show_vals = show_vals\n        self.truncate_vals = truncate_vals\n        self.line_wrap = line_wrap\n        self.suppressed_paths = suppressed_paths\n        self.suppressed_vars = suppressed_vars\n\n    def __call__(self, frame, lineno=None):\n        \"\"\"\n        Render a single stack frame or traceback entry\n\n\n        Params\n        ----\n\n        frame: Frame object, Traceback object (or FrameInfo tuple)\n            The frame or traceback entry to be formatted.\n\n            The only difference between passing a frame or a traceback object is\n            which line gets highlighted in the source listing: For a frame, it's\n            the currently executed line; for a traceback, it's the line where an\n            error occurred. (technically: `frame.f_lineno` vs. `tb.tb_lineno`)\n\n            The third option is interesting only if you're planning to format\n            one frame multiple different ways: It is a little faster to format a\n            pre-chewed verion of the frame, since non-formatting-specific steps\n            like \"finding the source code\", \"finding all the variables\" etc only\n            need to be done once per frame. So, this method also accepts the raw\n            results of `extraction.get_info()` of type FrameInfo. In that case,\n            this method will really just do formatting, no more chewing.\n\n        lineno: int\n            override which line gets highlighted\n        \"\"\"\n        accepted_types = (types.FrameType, types.TracebackType, ex.FrameInfo)\n        if not isinstance(frame, accepted_types):\n            raise ValueError(\"Expected one of these types: \"\n                             \"%s. Got %r\" % (accepted_types, frame))\n\n        try:\n            finfo = ex.get_info(frame, lineno, self.suppressed_vars)\n\n            return self._format_frame(finfo)\n        except Exception as exc:\n            # If we crash, annotate the exception with the thing\n            # we were trying to format, for debug/logging purposes.\n            exc.where = frame\n            raise\n\n    def _format_frame(self, fi):\n        msg = self.headline_tpl % (fi.filename, fi.lineno, fi.function)\n\n        source_map, assignments = self.select_scope(fi)\n\n        if source_map:\n            source_lines = self._format_source(source_map)\n            msg += self._format_listing(source_lines, fi.lineno)\n        if assignments:\n            msg += self._format_assignments(assignments)\n        elif self.lines == 'all' or self.lines > 1 or self.show_signature:\n            msg += '\\n'\n\n        return msg\n\n    def _format_source(self, source_map):\n        lines = OrderedDict()\n        for ln in sorted(source_map):\n            lines[ln] = ''.join(st for st, _, in source_map[ln])\n        return lines\n\n    def _format_listing(self, lines, lineno):\n        ln_prev = None\n        msg = \"\"\n        n_lines = len(lines)\n        for ln in sorted(lines):\n            line = lines[ln]\n            if ln_prev and ln_prev != ln - 1:\n                msg += self.elipsis_tpl\n            ln_prev = ln\n\n            if n_lines > 1:\n                if ln == lineno:\n                    tpl = self.marked_sourceline_tpl\n                else:\n                    tpl = self.sourceline_tpl\n                msg += tpl % (ln, line)\n            else:\n                msg += self.single_sourceline_tpl % line\n\n        msg += self.sep_source_below\n        return msg\n\n    def _format_assignments(self, assignments):\n        msgs = []\n        for name, value in assignments.items():\n            val_str = format_value(value,\n                                   indent=len(name) + self.var_indent + 3,\n                                   truncation=self.truncate_vals,\n                                   wrap=self.line_wrap)\n            assign_str = self.val_tpl % (name, val_str)\n            msgs.append(assign_str)\n        if len(msgs) > 0:\n            return self.sep_vars + '\\n' + ''.join(msgs) + self.sep_vars + '\\n\\n'\n        else:\n            return ''\n\n    def select_scope(self, fi):\n        \"\"\"\n        decide which lines of code and which variables will be visible\n        \"\"\"\n        source_lines = []\n        minl, maxl = 0, 0\n        if len(fi.source_map) > 0:\n            minl, maxl = min(fi.source_map), max(fi.source_map)\n            lineno = fi.lineno\n\n            if self.lines == 0:\n                source_lines = []\n            elif self.lines == 1:\n                source_lines = [lineno]\n            elif self.lines == 'all':\n                source_lines = range(minl, maxl + 1)\n            elif self.lines > 1 or self.lines_after > 0:\n                start = max(lineno - (self.lines - 1), 0)\n                stop = lineno + self.lines_after\n                start = max(start, minl)\n                stop = min(stop, maxl)\n                source_lines = list(range(start, stop + 1))\n\n            if source_lines and self.show_signature:\n                source_lines = sorted(set(source_lines) | set(fi.head_lns))\n\n        if source_lines:\n            # Report a bit more info about a weird class of bug\n            # that I can't reproduce locally.\n            if not set(source_lines).issubset(fi.source_map.keys()):\n                debug_vals = [source_lines, fi.head_lns, fi.source_map.keys()]\n                info = ', '.join(str(p) for p in debug_vals)\n                raise Exception(\"Picked an invalid source context: %s\" % info)\n            trimmed_source_map = trim_source(fi.source_map, source_lines)\n        else:\n            trimmed_source_map = {}\n\n        if self.show_vals:\n            if self.show_vals == 'all':\n                val_lines = range(minl, maxl)\n            elif self.show_vals == 'like_source':\n                val_lines = source_lines\n            elif self.show_vals == 'line':\n                val_lines = [lineno] if source_lines else []\n\n            # TODO refactor the whole blacklistling mechanism below:\n\n            def hide(name):\n                value = fi.assignments[name]\n                if callable(value):\n                    qualified_name, path, *_ = inspect_callable(value)\n                    is_builtin = value.__class__.__name__ == 'builtin_function_or_method'\n                    is_boring = is_builtin or (qualified_name == name) or (path is None)\n                    is_suppressed = match(path, self.suppressed_paths)\n                    return is_boring or is_suppressed\n                return False\n\n            visible_vars = (name for ln in val_lines\n                            for name in fi.line2names[ln]\n                            if name in fi.assignments)\n\n            visible_assignments = OrderedDict([(n, fi.assignments[n])\n                                               for n in visible_vars\n                                               if not hide(n)])\n        else:\n            visible_assignments = {}\n\n        return trimmed_source_map, visible_assignments\n\n\nclass ColorfulFrameFormatter(FrameFormatter):\n\n    def __init__(self, style='darkbg', **kwargs):\n        \"\"\"\n        See FrameFormatter - this just adds some ANSI color codes here and there\n        \"\"\"\n        self.colors = getattr(colorschemes, style)()\n\n        highlight = self.tpl('highlight')\n        header = self.tpl('header')\n        arrow_lineno = self.tpl('arrow_lineno')\n        dots = self.tpl('dots')\n        lineno = self.tpl('lineno')\n\n        self.headline_tpl = header % 'File \"%s%s' + highlight % '%s' + header % '\", line %s, in %s\\n'\n        self.sourceline_tpl = lineno % super().sourceline_tpl\n        self.marked_sourceline_tpl = arrow_lineno % super().marked_sourceline_tpl\n        self.elipsis_tpl = dots % super().elipsis_tpl\n        self.sep_vars = dots % super().sep_vars\n\n        super().__init__(**kwargs)\n\n    def tpl(self, name):\n        return get_ansi_tpl(*self.colors[name])\n\n    def _format_frame(self, fi):\n        basepath, filename = os.path.split(fi.filename)\n        sep = os.sep if basepath else ''\n        msg = self.headline_tpl % (basepath, sep, filename, fi.lineno, fi.function)\n        source_map, assignments = self.select_scope(fi)\n\n        colormap = self._pick_colors(source_map, fi.name2lines, assignments, fi.lineno)\n\n        if source_map:\n            source_lines = self._format_source(source_map, colormap, fi.lineno)\n            msg += self._format_listing(source_lines, fi.lineno)\n\n        if assignments:\n            msg += self._format_assignments(assignments, colormap)\n        elif self.lines == 'all' or self.lines > 1 or self.show_signature:\n            msg += '\\n'\n\n        return msg\n\n    def _format_source(self, source_map, colormap, lineno):\n        bold_tp = self.tpl('source_bold')\n        default_tpl = self.tpl('source_default')\n        comment_tpl = self.tpl('source_comment')\n\n        source_lines = OrderedDict()\n        for ln in source_map:\n            line = ''\n            for snippet, ttype in source_map[ln]:\n                if ttype in [sc.KEYWORD, sc.OP]:\n                    line += bold_tp % snippet\n                elif ttype == sc.VAR:\n                    if snippet not in colormap:\n                        line += default_tpl % snippet\n                    else:\n                        hue, sat, val, bold = colormap[snippet]\n                        var_tpl = get_ansi_tpl(hue, sat, val, bold)\n                        line += var_tpl % snippet\n                elif ttype == sc.CALL:\n                    line += bold_tp % snippet\n                elif ttype == sc.COMMENT:\n                    line += comment_tpl % snippet\n                else:\n                    line += default_tpl % snippet\n            source_lines[ln] = line\n\n        return source_lines\n\n    def _format_assignments(self, assignments, colormap):\n        msgs = []\n        for name, value in assignments.items():\n            val_str = format_value(value,\n                                   indent=len(name) + self.var_indent + 3,\n                                   truncation=self.truncate_vals,\n                                   wrap=self.line_wrap)\n            assign_str = self.val_tpl % (name, val_str)\n            hue, sat, val, bold = colormap.get(name, self.colors['var_invisible'])\n            clr_str = get_ansi_tpl(hue, sat, val, bold) % assign_str\n            msgs.append(clr_str)\n        if len(msgs) > 0:\n            return self.sep_vars + '\\n' + ''.join(msgs) + self.sep_vars + '\\n\\n'\n        else:\n            return ''\n\n    def _pick_colors(self, source_map, name2lines, assignments, lineno):\n        # TODO refactor: pick a hash for each name across frames, _then_ color.\n        # Currently, colors are consistent across frames purely because there's\n        # a fixed map from hashes to colors. It's not bijective though. If colors\n        # were picked after hashing across all frames, that could be fixed.\n        colormap = {}\n        for line in source_map.values():\n            for name, ttype in line:\n                if name not in colormap and ttype == sc.VAR and name in assignments:\n                    value = assignments[name]\n                    highlight = lineno in name2lines[name]\n                    colormap[name] = self._pick_color(name, value, highlight)\n        return colormap\n\n    def _pick_color(self, name, val, highlight=False, method='id'):\n        if method == 'formatted':\n            seed = format_value(val)\n        elif method == 'repr':\n            seed = repr(val)\n        elif method == 'id':\n            seed = id(val)\n        elif method == 'name':\n            seed = name\n        else:\n            raise ValueError('%r' % method)\n\n        return self.colors.get_random(seed, highlight)\n\n\n\n\n", "metadata": {"source": "github.com/cknd/stackprinter", "file_path": "stackprinter/frame_formatting.py", "language": "python", "complexity": "complex", "star_count": 1290, "quality_score": 0.84, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": true, "has_documentation": true, "has_type_hints": true, "has_testing": false, "has_logging": true, "function_count": 15, "class_count": 3, "line_count": 382}, "authority_level": "medium", "authentic": true}}
{"domain": "code_generation", "category": "error_handling", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create robust error handling with logging and validation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import os\n\nfrom stackprinter.extraction import UnresolvedAttribute\nfrom stackprinter.utils import inspect_callable\n\ntry:\n    import numpy as np\nexcept ImportError:\n    np = False\n\nMAXLEN_DICT_KEY_REPR = 25  # truncate dict keys to this nr of characters\n\n# TODO see where the builtin pprint module can be used instead of all this\n# (but how to extend it for e.g. custom np array printing?)\n\ndef format_value(value, indent=0, truncation=None, wrap=60,\n                 max_depth=2, depth=0):\n    \"\"\"\n    Stringify some object\n\n    Params\n    ---\n    value: object to be formatted\n\n    indent: int\n        insert extra spaces on each line\n\n    truncation: int\n        cut after this nr of characters\n\n    wrap: int\n        insert linebreaks after this nr of characters, use 0 to never add a linebreak\n\n    max_depth: int\n        max repeated calls to this function when formatting container types\n\n    depth: int\n        current nesting level\n\n    Returns\n    ---\n    string\n    \"\"\"\n\n    if depth > max_depth:\n        return '...'\n\n    if isinstance(value, UnresolvedAttribute):\n        reason = \"# %s\" % (value.exc_type)\n        val_tpl = reason + \"\\n%s = %s\"\n        lastval_str = format_value(value.last_resolvable_value,\n                                   truncation=truncation, indent=3, depth=depth+1)\n        val_str = val_tpl % (value.last_resolvable_name, lastval_str)\n        indent = 10\n\n    elif isinstance(value, (list, tuple, set)):\n        val_str = format_iterable(value, truncation, max_depth, depth)\n\n    elif isinstance(value, dict):\n        val_str = format_dict(value, truncation, max_depth, depth)\n\n    elif np and isinstance(value, np.ndarray):\n        val_str = format_array(value, minimize=depth > 0)\n\n    elif callable(value):\n        name, filepath, method_owner, ln = inspect_callable(value)\n        filename = os.path.basename(filepath) if filepath is not None else None\n        if filename is None:\n            val_str = safe_repr(value)\n        elif method_owner is None:\n            name_s = safe_str(name)\n            filename_s = safe_str(filename)\n            ln_s = safe_str(ln)\n            val_str = \"<function '%s' %s:%s>\" % (name_s, filename_s, ln_s)\n        else:\n            name_s = safe_str(name)\n            filename_s = safe_str(filename)\n            method_owner_s = safe_str(method_owner)\n            ln_s = safe_str(ln)\n            val_str = \"<method '%s' of %s %s:%s>\" % (name_s, method_owner_s,\n                                                     filename_s, ln_s)\n    else:\n        val_str= safe_repr_or_str(value)\n\n    val_str = truncate(val_str, truncation)\n\n    if depth == 0:\n        val_str = wrap_lines(val_str, wrap)\n\n    if indent > 0:\n        nl_indented = '\\n' + (' ' * indent)\n        val_str = val_str.replace('\\n', nl_indented)\n\n    return val_str\n\n\ndef format_dict(value, truncation, max_depth, depth):\n    typename = value.__class__.__name__\n    prefix = '{' if type(value) == dict else \"%s\\n{\" % typename\n    postfix = '}'\n\n    if depth == max_depth:\n        val_str = '...'\n    else:\n        vstrs = []\n        char_count = 0\n        for k, v in value.items():\n            if char_count >= truncation:\n                break\n            kstr = truncate(repr(k), MAXLEN_DICT_KEY_REPR)\n            vstr = format_value(v, indent=len(kstr) + 3,\n                                truncation=truncation, depth=depth+1)\n            istr = \"%s: %s\" % (kstr, vstr)\n            vstrs.append(istr)\n            char_count += len(istr)\n\n\n        val_str = ',\\n '.join(vstrs)\n\n    return prefix + val_str + postfix\n\n\ndef format_iterable(value, truncation, max_depth, depth):\n    typename = value.__class__.__name__\n    if isinstance(value, list):\n        prefix = '[' if type(value) == list else \"%s [\" % typename\n        postfix = ']'\n\n    elif isinstance(value, tuple):\n        prefix = '(' if type(value) == tuple else \"%s (\" % typename\n        postfix = ')'\n\n    elif isinstance(value, set):\n        prefix = '{' if type(value) == set else \"%s {\" % typename\n        postfix = '}'\n\n\n    length = len(value)\n    val_str = ''\n    if depth == max_depth:\n        val_str += '...'\n    else:\n        linebreak = False\n        char_count = 0\n        for i,v in enumerate(value):\n            if char_count >= truncation:\n                val_str += \"...\"\n                break\n            item_str = ''\n            entry = format_value(v, indent=1, truncation=truncation,\n                                 depth=depth+1)\n            sep = ', ' if i < length else ''\n            if '\\n' in entry:\n                item_str += \"\\n %s%s\" % (entry, sep)\n                linebreak = True\n            else:\n                if linebreak:\n                    item_str += '\\n'\n                    linebreak = False\n                item_str += \"%s%s\" % (entry, sep)\n            val_str += item_str\n            char_count += len(item_str)\n\n    return prefix + val_str + postfix\n\n\ndef format_array(arr, minimize=False):\n    \"\"\"\n    format a numpy array (with shape information)\n\n    Params\n    ---\n    minimize: bool\n        use an extra compact oneline format\n    \"\"\"\n    if arr.ndim >= 1:\n        shape = list(arr.shape)\n        if len(shape) < 2:\n            shape.append('')\n        shape_str = \"x\".join(str(d) for d in shape)\n        if len(shape_str) < 10:\n            prefix = \"%s array(\" % shape_str\n            msg = prefix\n        else:\n            prefix = \"\"\n            msg = \"%s array(\\n\" % shape_str\n    else:\n        msg = prefix = \"array(\"\n\n    suffix = ')'\n\n    try:\n        array_rep = np.array2string(arr, max_line_width=9000, threshold=50,\n                                    edgeitems=2, prefix=prefix, suffix=suffix)\n    except TypeError:\n        # some super old numpy versions (< 1.14) don't accept all these arguments\n        array_rep = np.array2string(arr, max_line_width=9000, prefix=prefix)\n\n    if minimize and (len(array_rep) > 50 or arr.ndim > 1):\n        array_rep = \"%s%s...%s\" % ('[' * arr.ndim, arr.flatten()[0], ']' * arr.ndim)\n\n\n    msg += array_rep + suffix\n    return msg\n\ndef safe_repr(value):\n    try:\n        return repr(value)\n    except:\n        return '# error calling repr'\n\n\ndef safe_str(value):\n    try:\n        return str(value)\n    except:\n        return '# error calling str'\n\n\ndef safe_repr_or_str(value):\n    try:\n        return repr(value)\n    except:\n        try:\n            return str(value)\n        except:\n            return '# error calling repr and str'\n\n\ndef truncate(string, n):\n    if not n:\n        return string\n    n = max(n, 0)\n    if len(string) > (n+3):\n        string = \"%s...\" % string[:n].rstrip()\n    return string\n\n\ndef wrap_lines(string, max_width=80):\n    if not max_width or max_width <= 0:\n        return string\n\n    def wrap(lines):\n        for l in lines:\n            length = len(l)\n            if length <= max_width:\n                yield l\n            else:\n                k = 0\n                while k < length:\n                    snippet = l[k:k+max_width]\n                    if k > 0:\n                        snippet = \" \" + snippet\n\n                    yield snippet\n                    k += max_width\n\n    wrapped_lines = wrap(string.splitlines())\n    return '\\n'.join(wrapped_lines)\n", "metadata": {"source": "github.com/cknd/stackprinter", "file_path": "stackprinter/prettyprinting.py", "language": "python", "complexity": "complex", "star_count": 1290, "quality_score": 0.78, "features": {"has_classes": false, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": true, "has_documentation": true, "has_type_hints": true, "has_testing": false, "has_logging": false, "function_count": 10, "class_count": 0, "line_count": 260}, "authority_level": "medium", "authentic": true}}
{"domain": "code_generation", "category": "error_handling", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create robust error handling with logging and validation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"Mock utilities that are async aware.\"\"\"\n\nfrom unittest.mock import *  # noqa: F401, F403\n\n\nclass _IntSentinelObject(int):\n    \"\"\"Sentinel-like object that is also an integer subclass.\n\n    Allows sentinels to be used\n    in loggers that perform int-specific string formatting.\n    \"\"\"\n\n    def __new__(cls, name):\n        instance = super().__new__(cls, 0)\n        instance.name = name\n        return instance\n\n    def __repr__(self):\n        return f\"int_sentinel.{self.name}\"\n\n    def __hash__(self):\n        return hash((int(self), self.name))\n\n    def __eq__(self, other):\n        return self is other\n\n    __str__ = __reduce__ = __repr__\n\n\nclass _IntSentinel:\n    def __init__(self):\n        self._sentinels = {}\n\n    def __getattr__(self, name):\n        if name == \"__bases__\":\n            raise AttributeError\n        return self._sentinels.setdefault(name, _IntSentinelObject(name))\n\n    def __reduce__(self):\n        return \"int_sentinel\"\n\n\nint_sentinel = _IntSentinel()\n", "metadata": {"source": "github.com/zigpy/zha-device-handlers", "file_path": "tests/async_mock.py", "language": "python", "complexity": "medium", "star_count": 930, "quality_score": 0.6799999999999999, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": true, "has_type_hints": true, "has_testing": true, "has_logging": true, "function_count": 7, "class_count": 2, "line_count": 44}, "authority_level": "medium", "authentic": true}}
{"domain": "code_generation", "category": "error_handling", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create robust error handling with logging and validation with asyncio coroutines\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"Tests for Philips quirks.\"\"\"\n\nfrom unittest import mock\n\nimport pytest\nfrom zigpy.quirks import CustomEndpoint\nfrom zigpy.zcl import Cluster\nfrom zigpy.zcl.clusters.general import OnOff\nfrom zigpy.zcl.foundation import ZCLHeader\n\nfrom tests.common import ClusterListener\nimport zhaquirks\nfrom zhaquirks.const import (\n    BUTTON_1,\n    BUTTON_2,\n    BUTTON_3,\n    BUTTON_4,\n    CLUSTER_ID,\n    COMMAND,\n    COMMAND_HOLD,\n    DIM_DOWN,\n    DIM_UP,\n    DOUBLE_PRESS,\n    ENDPOINT_ID,\n    LONG_PRESS,\n    LONG_RELEASE,\n    PARAMS,\n    QUADRUPLE_PRESS,\n    QUINTUPLE_PRESS,\n    RIGHT,\n    SHORT_PRESS,\n    SHORT_RELEASE,\n    TRIPLE_PRESS,\n    TURN_OFF,\n    TURN_ON,\n)\nimport zhaquirks.philips\nfrom zhaquirks.philips import Button, ButtonPressQueue, PhilipsRemoteCluster, PressType\nfrom zhaquirks.philips.rdm002 import PhilipsRDM002\nfrom zhaquirks.philips.rom001 import PhilipsROM001\nfrom zhaquirks.philips.rwl022 import PhilipsRWL022\nfrom zhaquirks.philips.rwlfirstgen import PhilipsRWLFirstGen, PhilipsRWLFirstGen2\nfrom zhaquirks.philips.wall_switch import PhilipsWallSwitch\n\nzhaquirks.setup()\n\n\n@pytest.mark.parametrize(\n    \"classes, triggers\",\n    (\n        (\n            [PhilipsRWLFirstGen, PhilipsRWLFirstGen2, PhilipsRWL022],\n            {\n                (SHORT_PRESS, TURN_ON): {COMMAND: \"on_press\"},\n                (SHORT_PRESS, TURN_OFF): {COMMAND: \"off_press\"},\n                (SHORT_PRESS, DIM_UP): {COMMAND: \"up_press\"},\n                (SHORT_PRESS, DIM_DOWN): {COMMAND: \"down_press\"},\n                (LONG_PRESS, TURN_ON): {COMMAND: \"on_hold\"},\n                (LONG_PRESS, TURN_OFF): {COMMAND: \"off_hold\"},\n                (LONG_PRESS, DIM_UP): {COMMAND: \"up_hold\"},\n                (LONG_PRESS, DIM_DOWN): {COMMAND: \"down_hold\"},\n                (DOUBLE_PRESS, TURN_ON): {COMMAND: \"on_double_press\"},\n                (DOUBLE_PRESS, TURN_OFF): {COMMAND: \"off_double_press\"},\n                (DOUBLE_PRESS, DIM_UP): {COMMAND: \"up_double_press\"},\n                (DOUBLE_PRESS, DIM_DOWN): {COMMAND: \"down_double_press\"},\n                (TRIPLE_PRESS, TURN_ON): {COMMAND: \"on_triple_press\"},\n                (TRIPLE_PRESS, TURN_OFF): {COMMAND: \"off_triple_press\"},\n                (TRIPLE_PRESS, DIM_UP): {COMMAND: \"up_triple_press\"},\n                (TRIPLE_PRESS, DIM_DOWN): {COMMAND: \"down_triple_press\"},\n                (QUADRUPLE_PRESS, TURN_ON): {COMMAND: \"on_quadruple_press\"},\n                (QUADRUPLE_PRESS, TURN_OFF): {COMMAND: \"off_quadruple_press\"},\n                (QUADRUPLE_PRESS, DIM_UP): {COMMAND: \"up_quadruple_press\"},\n                (QUADRUPLE_PRESS, DIM_DOWN): {COMMAND: \"down_quadruple_press\"},\n                (QUINTUPLE_PRESS, TURN_ON): {COMMAND: \"on_quintuple_press\"},\n                (QUINTUPLE_PRESS, TURN_OFF): {COMMAND: \"off_quintuple_press\"},\n                (QUINTUPLE_PRESS, DIM_UP): {COMMAND: \"up_quintuple_press\"},\n                (QUINTUPLE_PRESS, DIM_DOWN): {COMMAND: \"down_quintuple_press\"},\n                (SHORT_RELEASE, TURN_ON): {COMMAND: \"on_short_release\"},\n                (SHORT_RELEASE, TURN_OFF): {COMMAND: \"off_short_release\"},\n                (SHORT_RELEASE, DIM_UP): {COMMAND: \"up_short_release\"},\n                (SHORT_RELEASE, DIM_DOWN): {COMMAND: \"down_short_release\"},\n                (LONG_RELEASE, TURN_ON): {COMMAND: \"on_long_release\"},\n                (LONG_RELEASE, TURN_OFF): {COMMAND: \"off_long_release\"},\n                (LONG_RELEASE, DIM_UP): {COMMAND: \"up_long_release\"},\n                (LONG_RELEASE, DIM_DOWN): {COMMAND: \"down_long_release\"},\n            },\n        ),\n        (\n            [PhilipsROM001],\n            {\n                (SHORT_PRESS, TURN_ON): {COMMAND: \"on_press\"},\n                (LONG_PRESS, TURN_ON): {COMMAND: \"on_hold\"},\n                (DOUBLE_PRESS, TURN_ON): {COMMAND: \"on_double_press\"},\n                (TRIPLE_PRESS, TURN_ON): {COMMAND: \"on_triple_press\"},\n                (QUADRUPLE_PRESS, TURN_ON): {COMMAND: \"on_quadruple_press\"},\n                (QUINTUPLE_PRESS, TURN_ON): {COMMAND: \"on_quintuple_press\"},\n                (SHORT_RELEASE, TURN_ON): {COMMAND: \"on_short_release\"},\n                (LONG_RELEASE, TURN_ON): {COMMAND: \"on_long_release\"},\n            },\n        ),\n        (\n            [PhilipsWallSwitch],\n            {\n                (SHORT_PRESS, TURN_ON): {COMMAND: \"left_press\"},\n                (LONG_PRESS, TURN_ON): {COMMAND: \"left_hold\"},\n                (DOUBLE_PRESS, TURN_ON): {COMMAND: \"left_double_press\"},\n                (TRIPLE_PRESS, TURN_ON): {COMMAND: \"left_triple_press\"},\n                (QUADRUPLE_PRESS, TURN_ON): {COMMAND: \"left_quadruple_press\"},\n                (QUINTUPLE_PRESS, TURN_ON): {COMMAND: \"left_quintuple_press\"},\n                (SHORT_RELEASE, TURN_ON): {COMMAND: \"left_short_release\"},\n                (LONG_RELEASE, TURN_ON): {COMMAND: \"left_long_release\"},\n                (SHORT_PRESS, RIGHT): {COMMAND: \"right_press\"},\n                (LONG_PRESS, RIGHT): {COMMAND: \"right_hold\"},\n                (DOUBLE_PRESS, RIGHT): {COMMAND: \"right_double_press\"},\n                (TRIPLE_PRESS, RIGHT): {COMMAND: \"right_triple_press\"},\n                (QUADRUPLE_PRESS, RIGHT): {COMMAND: \"right_quadruple_press\"},\n                (QUINTUPLE_PRESS, RIGHT): {COMMAND: \"right_quintuple_press\"},\n                (SHORT_RELEASE, RIGHT): {COMMAND: \"right_short_release\"},\n                (LONG_RELEASE, RIGHT): {COMMAND: \"right_long_release\"},\n            },\n        ),\n    ),\n)\ndef test_legacy_remote_automation_triggers(classes, triggers):\n    \"\"\"Ensure we don't break any automation triggers by changing their values.\"\"\"\n\n    for cls in classes:\n        assert cls.device_automation_triggers == triggers\n\n\nclass _SimpleRemote(PhilipsRemoteCluster):\n    BUTTONS = {\n        1: Button(\"turn_on\"),\n        2: Button(\"right\"),\n    }\n    PRESS_TYPES = {\n        0: PressType(\"remote_button_long_release\", \"remote_button_long_release\"),\n    }\n    SIMULATE_SHORT_EVENTS = None\n\n\nclass _RemoteWithTriggerOverrides(_SimpleRemote):\n    BUTTONS = {\n        1: Button(\"turn_on\", \"turn_on\", \"left\"),\n        2: Button(\"right\"),\n    }\n\n\nclass _RemoteWithSimulatedRelease(_SimpleRemote):\n    SIMULATE_SHORT_EVENTS = [\n        PressType(\"remote_button_short_press\", \"remote_button_short_press\"),\n        PressType(\"remote_button_short_release\", \"remote_button_short_release\"),\n    ]\n\n\nclass _RemoteWithCommandOverrides(_SimpleRemote):\n    PRESS_TYPES: dict[int, PressType] = {\n        1: PressType(LONG_PRESS, COMMAND_HOLD),\n        3: PressType(LONG_RELEASE, \"release\"),\n    }\n\n\nclass _RemoteWithSimulatedReleaseAndCommandOverride(_SimpleRemote):\n    SIMULATE_SHORT_EVENTS = [\n        PressType(\"remote_button_short_press\", \"short_press\"),\n        PressType(\"remote_button_short_release\", \"remote_button_short_release\"),\n    ]\n\n\n@pytest.mark.parametrize(\n    \"cls, expected_value\",\n    (\n        (\n            _SimpleRemote,\n            {\n                (\"remote_button_long_release\", \"turn_on\"): {\n                    COMMAND: \"turn_on_remote_button_long_release\"\n                },\n                (\"remote_button_long_release\", \"right\"): {\n                    COMMAND: \"right_remote_button_long_release\"\n                },\n            },\n        ),\n        (\n            _RemoteWithTriggerOverrides,\n            {\n                (\"remote_button_long_release\", \"turn_on\"): {\n                    COMMAND: \"left_remote_button_long_release\"\n                },\n                (\"remote_button_long_release\", \"right\"): {\n                    COMMAND: \"right_remote_button_long_release\"\n                },\n            },\n        ),\n        (\n            _RemoteWithSimulatedRelease,\n            {\n                (\"remote_button_long_release\", \"turn_on\"): {\n                    COMMAND: \"turn_on_remote_button_long_release\"\n                },\n                (\"remote_button_long_release\", \"right\"): {\n                    COMMAND: \"right_remote_button_long_release\"\n                },\n                (\"remote_button_short_press\", \"turn_on\"): {\n                    COMMAND: \"turn_on_remote_button_short_press\"\n                },\n                (\"remote_button_short_press\", \"right\"): {\n                    COMMAND: \"right_remote_button_short_press\"\n                },\n                (\"remote_button_short_release\", \"turn_on\"): {\n                    COMMAND: \"turn_on_remote_button_short_release\"\n                },\n                (\"remote_button_short_release\", \"right\"): {\n                    COMMAND: \"right_remote_button_short_release\"\n                },\n                (\"remote_button_double_press\", \"turn_on\"): {\n                    COMMAND: \"turn_on_double_press\"\n                },\n                (\"remote_button_double_press\", \"right\"): {\n                    COMMAND: \"right_double_press\"\n                },\n                (\"remote_button_triple_press\", \"turn_on\"): {\n                    COMMAND: \"turn_on_triple_press\"\n                },\n                (\"remote_button_triple_press\", \"right\"): {\n                    COMMAND: \"right_triple_press\"\n                },\n                (\"remote_button_quadruple_press\", \"turn_on\"): {\n                    COMMAND: \"turn_on_quadruple_press\"\n                },\n                (\"remote_button_quadruple_press\", \"right\"): {\n                    COMMAND: \"right_quadruple_press\"\n                },\n                (\"remote_button_quintuple_press\", \"turn_on\"): {\n                    COMMAND: \"turn_on_quintuple_press\"\n                },\n                (\"remote_button_quintuple_press\", \"right\"): {\n                    COMMAND: \"right_quintuple_press\"\n                },\n            },\n        ),\n        (\n            _RemoteWithCommandOverrides,\n            {\n                (\"remote_button_long_press\", \"turn_on\"): {COMMAND: \"turn_on_hold\"},\n                (\"remote_button_long_press\", \"right\"): {COMMAND: \"right_hold\"},\n                (\"remote_button_long_release\", \"turn_on\"): {COMMAND: \"turn_on_release\"},\n                (\"remote_button_long_release\", \"right\"): {COMMAND: \"right_release\"},\n            },\n        ),\n        (\n            _RemoteWithSimulatedReleaseAndCommandOverride,\n            {\n                (\"remote_button_long_release\", \"turn_on\"): {\n                    COMMAND: \"turn_on_remote_button_long_release\"\n                },\n                (\"remote_button_long_release\", \"right\"): {\n                    COMMAND: \"right_remote_button_long_release\"\n                },\n                (\"remote_button_short_press\", \"turn_on\"): {\n                    COMMAND: \"turn_on_short_press\"\n                },\n                (\"remote_button_short_press\", \"right\"): {COMMAND: \"right_short_press\"},\n                (\"remote_button_short_release\", \"turn_on\"): {\n                    COMMAND: \"turn_on_remote_button_short_release\"\n                },\n                (\"remote_button_short_release\", \"right\"): {\n                    COMMAND: \"right_remote_button_short_release\"\n                },\n                (\"remote_button_double_press\", \"turn_on\"): {\n                    COMMAND: \"turn_on_double_press\"\n                },\n                (\"remote_button_double_press\", \"right\"): {\n                    COMMAND: \"right_double_press\"\n                },\n                (\"remote_button_triple_press\", \"turn_on\"): {\n                    COMMAND: \"turn_on_triple_press\"\n                },\n                (\"remote_button_triple_press\", \"right\"): {\n                    COMMAND: \"right_triple_press\"\n                },\n                (\"remote_button_quadruple_press\", \"turn_on\"): {\n                    COMMAND: \"turn_on_quadruple_press\"\n                },\n                (\"remote_button_quadruple_press\", \"right\"): {\n                    COMMAND: \"right_quadruple_press\"\n                },\n                (\"remote_button_quintuple_press\", \"turn_on\"): {\n                    COMMAND: \"turn_on_quintuple_press\"\n                },\n                (\"remote_button_quintuple_press\", \"right\"): {\n                    COMMAND: \"right_quintuple_press\"\n                },\n            },\n        ),\n    ),\n)\ndef test_generate_device_automation_triggers(cls, expected_value):\n    \"\"\"Test trigger generation and button overrides.\"\"\"\n\n    assert cls.generate_device_automation_triggers() == expected_value\n\n\nclass ManuallyFiredButtonPressQueue:\n    \"\"\"Philips button queue to derive multiple press events.\"\"\"\n\n    def __init__(self):\n        \"\"\"Init.\"\"\"\n        self.reset()\n\n    def fire(self):\n        \"\"\"Fire the callback. Trigger after all inputs, before running assertions.\"\"\"\n\n        if self._callback is not None:\n            self._callback(self._click_counter)\n\n    def reset(self):\n        \"\"\"Reset the button press queue.\"\"\"\n\n        self._click_counter = 0\n        self._callback = None\n\n    def press(self, callback):\n        \"\"\"Process a button press.\"\"\"\n        self._click_counter += 1\n        self._callback = callback\n\n\n@pytest.mark.parametrize(\n    \"dev, ep, button, events\",\n    (\n        (\n            PhilipsWallSwitch,\n            1,\n            \"left\",\n            [\"press\", \"short_release\"],\n        ),\n        (\n            PhilipsROM001,\n            1,\n            \"on\",\n            [\"press\", \"short_release\"],\n        ),\n        (\n            PhilipsRWLFirstGen,\n            2,\n            \"on\",\n            [\"press\", \"short_release\"],\n        ),\n        (\n            PhilipsRWLFirstGen2,\n            2,\n            \"on\",\n            [\"press\", \"short_release\"],\n        ),\n        (\n            PhilipsRWL022,\n            1,\n            \"on\",\n            [\"press\", \"short_release\"],\n        ),\n    ),\n)\ndef test_PhilipsRemoteCluster_short_press(\n    zigpy_device_from_quirk, dev, ep, button, events\n):\n    \"\"\"Test PhilipsRemoteCluster short button press logic.\"\"\"\n\n    device = zigpy_device_from_quirk(dev)\n\n    cluster = device.endpoints[ep].philips_remote_cluster\n    listener = mock.MagicMock()\n    cluster.add_listener(listener)\n    cluster.button_press_queue = {\n        k: ManuallyFiredButtonPressQueue() for k in cluster.BUTTONS\n    }\n\n    cluster.handle_cluster_request(ZCLHeader(), [1, 0, 0, 0, 0])\n    cluster.handle_cluster_request(ZCLHeader(), [1, 0, 2, 0, 0])\n    for q in cluster.button_press_queue.values():\n        q.fire()\n\n    assert listener.zha_send_event.call_count == 2\n\n    calls = [\n        mock.call(\n            f\"{button}_{events[0]}\",\n            {\n                \"button\": button,\n                \"press_type\": events[0],\n                \"command_id\": None,\n                \"duration\": 0,\n                \"args\": [1, 0, 0, 0, 0],\n            },\n        ),\n        mock.call(\n            f\"{button}_{events[1]}\",\n            {\n                \"button\": button,\n                \"press_type\": events[1],\n                \"command_id\": None,\n                \"duration\": 0,\n                \"args\": [1, 0, 2, 0, 0],\n            },\n        ),\n    ]\n\n    listener.zha_send_event.assert_has_calls(calls)\n\n\n@pytest.mark.parametrize(\n    \"dev, ep, button\",\n    (\n        (\n            PhilipsROM001,\n            1,\n            \"on\",\n        ),\n        (\n            PhilipsWallSwitch,\n            1,\n            \"left\",\n        ),\n        (\n            PhilipsRWLFirstGen,\n            2,\n            \"on\",\n        ),\n        (\n            PhilipsRWLFirstGen2,\n            2,\n            \"on\",\n        ),\n        (\n            PhilipsRWL022,\n            1,\n            \"on\",\n        ),\n    ),\n)\n@pytest.mark.parametrize(\n    \"count, action_press_type\",\n    (\n        (2, \"double_press\"),\n        (3, \"triple_press\"),\n        (4, \"quadruple_press\"),\n        (5, \"quintuple_press\"),\n    ),\n)\ndef test_PhilipsRemoteCluster_multi_press(\n    zigpy_device_from_quirk,\n    dev,\n    ep,\n    button,\n    count,\n    action_press_type,\n):\n    \"\"\"Test PhilipsRemoteCluster button multi-press logic.\"\"\"\n\n    device = zigpy_device_from_quirk(dev)\n\n    cluster = device.endpoints[ep].philips_remote_cluster\n    listener = mock.MagicMock()\n    cluster.add_listener(listener)\n    cluster.button_press_queue = {\n        k: ManuallyFiredButtonPressQueue() for k in cluster.BUTTONS\n    }\n\n    for _ in range(0, count):\n        # btn1 short press\n        cluster.handle_cluster_request(ZCLHeader(), [1, 0, 0, 0, 0])\n        # btn1 short release\n        cluster.handle_cluster_request(ZCLHeader(), [1, 0, 2, 0, 0])\n    for q in cluster.button_press_queue.values():\n        q.fire()\n\n    assert listener.zha_send_event.call_count == 1\n    args_button_id = count + 2\n    listener.zha_send_event.assert_has_calls(\n        [\n            mock.call(\n                f\"{button}_{action_press_type}\",\n                {\n                    \"button\": button,\n                    \"press_type\": action_press_type,\n                    \"command_id\": None,\n                    \"duration\": 0,\n                    \"args\": [1, 0, args_button_id, 0, 0],\n                },\n            ),\n        ]\n    )\n\n\n@pytest.mark.parametrize(\n    \"dev, ep\",\n    (\n        (PhilipsWallSwitch, 1),\n        (PhilipsROM001, 1),\n        (PhilipsRWLFirstGen, 2),\n        (PhilipsRWLFirstGen2, 2),\n        (PhilipsRWL022, 1),\n    ),\n)\ndef test_PhilipsRemoteCluster_ignore_unknown_buttons(zigpy_device_from_quirk, dev, ep):\n    \"\"\"Ensure PhilipsRemoteCluster ignores unknown buttons.\"\"\"\n\n    device = zigpy_device_from_quirk(dev)\n\n    cluster = device.endpoints[ep].philips_remote_cluster\n    listener = mock.MagicMock()\n    cluster.add_listener(listener)\n\n    cluster.handle_cluster_request(ZCLHeader(), [99, 0, 0, 0, 0])\n\n    assert listener.zha_send_event.call_count == 0\n\n\n@pytest.mark.parametrize(\n    \"dev, ep, button, hold_press_type, release_press_type, release_press_type_arg\",\n    (\n        (PhilipsROM001, 1, \"on\", \"hold\", \"long_release\", \"hold_release\"),\n        (\n            PhilipsWallSwitch,\n            1,\n            \"left\",\n            \"hold\",\n            \"long_release\",\n            \"hold_release\",\n        ),\n        (\n            PhilipsRWLFirstGen,\n            2,\n            \"on\",\n            \"hold\",\n            \"long_release\",\n            \"long_release\",\n        ),\n        (\n            PhilipsRWLFirstGen2,\n            2,\n            \"on\",\n            \"hold\",\n            \"long_release\",\n            \"long_release\",\n        ),\n        (\n            PhilipsRWL022,\n            1,\n            \"on\",\n            \"hold\",\n            \"long_release\",\n            \"long_release\",\n        ),\n    ),\n)\n@pytest.mark.parametrize(\n    \"count\",\n    (\n        (1),\n        (2),\n        (3),\n    ),\n)\ndef test_PhilipsRemoteCluster_long_press(\n    zigpy_device_from_quirk,\n    dev,\n    ep,\n    button,\n    hold_press_type,\n    release_press_type,\n    release_press_type_arg,\n    count,\n):\n    \"\"\"Test PhilipsRemoteCluster button long press logic.\"\"\"\n\n    device = zigpy_device_from_quirk(dev)\n\n    cluster = device.endpoints[ep].philips_remote_cluster\n    listener = mock.MagicMock()\n    cluster.add_listener(listener)\n    cluster.button_press_queue = ManuallyFiredButtonPressQueue()\n\n    cluster.handle_cluster_request(ZCLHeader(), [1, 0, 0, 0, 0])\n    for i in range(0, count):\n        # btn1 long press\n        cluster.handle_cluster_request(ZCLHeader(), [1, 0, 1, 0, (i + 1) * 40])\n\n    # btn1 long release\n    cluster.handle_cluster_request(ZCLHeader(), [1, 0, 3, 0, count * 40 + 10])\n    cluster.button_press_queue.fire()\n\n    assert listener.zha_send_event.call_count == count + 1\n\n    calls = []\n    for i in range(0, count):\n        calls.append(\n            mock.call(\n                f\"{button}_{hold_press_type}\",\n                {\n                    \"button\": button,\n                    \"press_type\": hold_press_type,\n                    \"command_id\": None,\n                    \"duration\": (i + 1) * 40,\n                    \"args\": [1, 0, 1, 0, (i + 1) * 40],\n                },\n            )\n        )\n    calls.append(\n        mock.call(\n            f\"{button}_{release_press_type}\",\n            {\n                \"button\": button,\n                \"press_type\": release_press_type_arg,\n                \"command_id\": None,\n                \"duration\": count * 40 + 10,\n                \"args\": [1, 0, 3, 0, count * 40 + 10],\n            },\n        )\n    )\n\n    listener.zha_send_event.assert_has_calls(calls)\n\n\n@pytest.mark.parametrize(\n    \"button_presses\",\n    (\n        (1),\n        (2),\n        (4),\n    ),\n)\nasync def test_ButtonPressQueue_presses_without_pause(button_presses):\n    \"\"\"Test ButtonPressQueue presses without pause in between presses.\"\"\"\n\n    q = ButtonPressQueue()\n    q._ms_threshold = 50\n    cb = mock.MagicMock()\n    for _ in range(button_presses):\n        q.press(cb)\n\n    # await cluster.button_press_queue._task\n    # Instead of awaiting the job, significantly extending the time\n    # these tests need, we just abort it and call the callback\n    # ourselves.\n    assert q._task is not None\n    q._task.cancel()\n    q._ms_last_click = 0\n    q._callback(q._click_counter)\n    cb.assert_called_once_with(button_presses)\n\n\n@pytest.mark.parametrize(\n    \"press_sequence\",\n    (\n        ((2, 3)),\n        ((3, 1)),\n    ),\n)\nasync def test_ButtonPressQueue_presses_with_pause(press_sequence):\n    \"\"\"Test ButtonPressQueue with pauses in between button press sequences.\"\"\"\n\n    q = ButtonPressQueue()\n    q._ms_threshold = 50\n    cb = mock.MagicMock()\n\n    for seq in press_sequence:\n        for _ in range(seq):\n            q.press(cb)\n        await q._task\n\n    assert cb.call_count == len(press_sequence)\n\n    calls = []\n    for res in press_sequence:\n        calls.append(mock.call(res))\n\n    cb.assert_has_calls(calls)\n\n\ndef test_rdm002_triggers():\n    \"\"\"Ensure RDM002 triggers won't break.\"\"\"\n\n    buttons = [BUTTON_1, BUTTON_2, BUTTON_3, BUTTON_4]\n    actions = {\n        \"remote_button_short_press\": \"press\",\n        \"remote_button_long_press\": \"hold\",\n        \"remote_button_short_release\": \"short_release\",\n        \"remote_button_long_release\": \"long_release\",\n        \"remote_button_double_press\": \"double_press\",\n        \"remote_button_triple_press\": \"triple_press\",\n        \"remote_button_quadruple_press\": \"quadruple_press\",\n        \"remote_button_quintuple_press\": \"quintuple_press\",\n    }\n\n    expected_triggers = {}\n    for button in buttons:\n        for action, command in actions.items():\n            expected_triggers[(action, button)] = {COMMAND: f\"{button}_{command}\"}\n\n    expected_triggers.update(\n        {\n            (\"remote_button_short_press\", \"dim_up\"): {\n                COMMAND: \"step_with_on_off\",\n                CLUSTER_ID: 8,\n                ENDPOINT_ID: 1,\n                PARAMS: {\"step_mode\": 0},\n            },\n            (\"remote_button_short_press\", \"dim_down\"): {\n                COMMAND: \"step_with_on_off\",\n                CLUSTER_ID: 8,\n                ENDPOINT_ID: 1,\n                PARAMS: {\"step_mode\": 1},\n            },\n        }\n    )\n\n    actual_triggers = PhilipsRDM002.device_automation_triggers\n\n    assert actual_triggers == expected_triggers\n\n\ndef test_contact_sensor(zigpy_device_from_v2_quirk):\n    \"\"\"Test that the Hue contact attribute is forwarded to the OnOff cluster.\"\"\"\n    quirk = zigpy_device_from_v2_quirk(\n        \"Signify Netherlands B.V.\", \"SOC001\", endpoint_ids=[1, 2]\n    )\n    # Add output OnOff cluster to the endpoint. The device has this,\n    # but the quirk doesn't modify that cluster, so we need to add it manually.\n    quirk.endpoints[2].add_output_cluster(OnOff.cluster_id)\n\n    hue_cluster = quirk.endpoints[2].philips_contact_cluster\n    on_off_cluster = quirk.endpoints[2].out_clusters[OnOff.cluster_id]\n    on_off_listener = ClusterListener(on_off_cluster)\n\n    # update the contact attribute and check that it is forwarded to the OnOff cluster\n    hue_cluster.update_attribute(hue_cluster.AttributeDefs.contact.id, 0)\n    assert on_off_listener.attribute_updates[0] == (0, 0)\n\n    hue_cluster.update_attribute(hue_cluster.AttributeDefs.contact.id, 1)\n    assert on_off_listener.attribute_updates[1] == (0, 1)\n\n    # check we didn't exceed the number of expected updates\n    assert len(on_off_listener.attribute_updates) == 2\n\n    # update again with the same value and except no new update\n    hue_cluster.update_attribute(hue_cluster.AttributeDefs.contact.id, 1)\n    assert len(on_off_listener.attribute_updates) == 2\n\n\n@pytest.mark.parametrize(\n    \"dev, ep, button_events, expected_actions\",\n    (\n        (\n            PhilipsWallSwitch,\n            1,\n            (\n                [\n                    b\"\\x1d\\x0b\\x106\\x00\\x01\\x00\\x000\\x00!\\x00\\x00\",\n                    b\"\\x1d\\x0b\\x107\\x00\\x01\\x00\\x000\\x02!\\x01\\x00\",\n                ],\n                [\n                    b\"\\x1d\\x0b\\x108\\x00\\x02\\x00\\x000\\x00!\\x00\\x00\",\n                    b\"\\x1d\\x0b\\x109\\x00\\x02\\x00\\x000\\x02!\\x01\\x00\",\n                ],\n            ),\n            [\"left_press\", \"left_short_release\", \"right_press\", \"right_short_release\"],\n        ),\n    ),\n)\ndef test_PhilipsRemoteCluster_multi_button_press(\n    zigpy_device_from_quirk, dev, ep, button_events, expected_actions\n):\n    \"\"\"Test PhilipsRemoteCluster short button press logic.\"\"\"\n\n    device = zigpy_device_from_quirk(dev)\n\n    remote_cluster = device.endpoints[ep].philips_remote_cluster\n    remote_cluster.button_press_queue = {\n        k: ManuallyFiredButtonPressQueue() for k in remote_cluster.BUTTONS\n    }\n    remote_listener = mock.MagicMock()\n    remote_cluster.add_listener(remote_listener)\n\n    expected_event_count = 0\n    for button in button_events:\n        for eventData in button:\n            hdr, args = remote_cluster.deserialize(eventData)\n            remote_cluster.handle_message(hdr, args)\n            expected_event_count += 1\n\n    for q in remote_cluster.button_press_queue.values():\n        q.fire()\n\n    assert remote_listener.zha_send_event.call_count == expected_event_count\n\n    for i, expected_action in enumerate(expected_actions):\n        assert remote_listener.zha_send_event.call_args_list[i][0][0] == expected_action\n\n\ndef listen_to_all(device) -> mock.MagicMock:\n    \"\"\"Add a mock listener to all clusters of all endpoints of a device.\"\"\"\n\n    listener = mock.MagicMock()\n\n    for endpoint in device.endpoints.values():\n        if not isinstance(endpoint, CustomEndpoint):\n            continue\n\n        for cluster in endpoint.in_clusters.values():\n            if isinstance(cluster, Cluster):\n                cluster.add_listener(listener)\n\n        for cluster in endpoint.out_clusters.values():\n            if isinstance(cluster, Cluster):\n                cluster.add_listener(listener)\n\n    return listener\n\n\ndef test_RDM002_no_levelcontrol_on_long_press(zigpy_device_from_quirk):\n    \"\"\"Button long-presses shouldn't trigger LevelControl events.\"\"\"\n\n    device = zigpy_device_from_quirk(PhilipsRDM002)\n    listener = listen_to_all(device)\n\n    # All below messages are triggered when long pressing button 1 for ~1.5 seconds\n\n    # Received command 0x00 (TSN 223): notification(button=1, param2=3145728, press_type=0, param4=33, param5=0)\n    hdr, args = (\n        device.endpoints[1]\n        .in_clusters[0xFC00]\n        .deserialize(b\"\\x1d\\x0b\\x10\\xdf\\x00\\x01\\x00\\x000\\x00!\\x00\\x00\")\n    )\n    device.endpoints[1].in_clusters[0xFC00].handle_message(hdr, args)\n\n    # Received command 0x06 (TSN 224): step_with_on_off(step_mode=<StepMode.Down: 1>, step_size=255, transition_time=8)\n    hdr, args = (\n        device.endpoints[1]\n        .out_clusters[0x0008]\n        .deserialize(b\"\\x01\\xe0\\x06\\x01\\xff\\x08\\x00\")\n    )\n    device.endpoints[1].out_clusters[0x0008].handle_message(hdr, args)\n\n    # Received command 0x00 (TSN 225): notification(button=1, param2=3145728, press_type=1, param4=33, param5=8)\n    hdr, args = (\n        device.endpoints[1]\n        .in_clusters[0xFC00]\n        .deserialize(b\"\\x1d\\x0b\\x10\\xe1\\x00\\x01\\x00\\x000\\x01!\\x08\\x00\")\n    )\n    device.endpoints[1].in_clusters[0xFC00].handle_message(hdr, args)\n\n    # Received command 0x00 (TSN 226): notification(button=1, param2=3145728, press_type=3, param4=33, param5=10)\n    hdr, args = (\n        device.endpoints[1]\n        .in_clusters[0xFC00]\n        .deserialize(b\"\\x1d\\x0b\\x10\\xe2\\x00\\x01\\x00\\x000\\x03!\\n\\x00\")\n    )\n    device.endpoints[1].in_clusters[0xFC00].handle_message(hdr, args)\n\n    # we emit those from PhilipsRdm002RemoteCluster. One hold, one long_press_release event.\n    assert listener.zha_send_event.call_count == 2\n\n    # one for each frame received, except for the one we balckhole, so 4 - 1\n    assert listener.cluster_command.call_count == 3\n\n\ndef test_RDM002_levelcontrol_on_dial_rotary_event(zigpy_device_from_quirk):\n    \"\"\"Dial rotary events shouldn't be muted by PhilipsRdm002LevelControl.\"\"\"\n\n    device = zigpy_device_from_quirk(PhilipsRDM002)\n    listener = listen_to_all(device)\n\n    # All below messages are triggered when quickly rotating the dial counterclockwise\n\n    # Received command 0x06 (TSN 231): step_with_on_off(step_mode=<StepMode.Down: 1>, step_size=68, transition_time=4)\n    hdr, args = (\n        device.endpoints[1]\n        .out_clusters[0x0008]\n        .deserialize(b\"\\x01\\xe7\\x06\\x01D\\x04\\x00\")\n    )\n    device.endpoints[1].out_clusters[0x0008].handle_message(hdr, args)\n\n    # Received command 0x00 (TSN 232): notification(button=20, param2=3145984, press_type=1, param4=41, param5=65370)\n    hdr, args = (\n        device.endpoints[1]\n        .in_clusters[0xFC00]\n        .deserialize(\n            b\"\\x1d\\x0b\\x10\\xe8\\x00\\x14\\x00\\x010\\x01)Z\\xff!d\\x00)Z\\xff!d\\x00)Z\\xff!\\x90\\x01\"\n        )\n    )\n    device.endpoints[1].in_clusters[0xFC00].handle_message(hdr, args)\n\n    # These call counts is the same, regardless of whether PhilipsRdm002LevelControl is used or not.\n    assert listener.zha_send_event.call_count == 0\n    assert listener.cluster_command.call_count == 2\n", "metadata": {"source": "github.com/zigpy/zha-device-handlers", "file_path": "tests/test_philips.py", "language": "python", "complexity": "complex", "star_count": 930, "quality_score": 0.71, "features": {"has_classes": true, "has_functions": true, "has_async": true, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": true, "has_type_hints": true, "has_testing": true, "has_logging": true, "function_count": 18, "class_count": 6, "line_count": 897}, "authority_level": "medium", "authentic": true}}
{"domain": "code_generation", "category": "error_handling", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create robust error handling with logging and validation with SQLAlchemy/Django ORM patterns\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"General quirk tests.\"\"\"\n\nfrom __future__ import annotations\n\nimport collections\nimport importlib\nimport json\nfrom pathlib import Path\nfrom unittest import mock\n\nimport pytest\nfrom zigpy import zcl\nimport zigpy.device\nimport zigpy.endpoint\nimport zigpy.profiles\nimport zigpy.quirks as zq\nfrom zigpy.quirks import CustomDevice, DeviceRegistry\nfrom zigpy.quirks.v2 import QuirkBuilder\nimport zigpy.types\nfrom zigpy.zcl import foundation\nimport zigpy.zdo.types\n\nimport zhaquirks\nfrom zhaquirks import const\nimport zhaquirks.bosch.motion\nimport zhaquirks.centralite.cl_3310S\nfrom zhaquirks.const import (\n    ARGS,\n    COMMAND,\n    COMMAND_MOVE,\n    COMMAND_MOVE_COLOR_TEMP,\n    COMMAND_MOVE_ON_OFF,\n    COMMAND_MOVE_SATURATION,\n    COMMAND_MOVE_TO_LEVEL_ON_OFF,\n    COMMAND_MOVE_TO_SATURATION,\n    COMMAND_STEP,\n    COMMAND_STEP_COLOR_TEMP,\n    COMMAND_STEP_HUE,\n    COMMAND_STEP_ON_OFF,\n    COMMAND_STEP_SATURATION,\n    COMMAND_STOP,\n    COMMAND_STOP_ON_OFF,\n    DEVICE_TYPE,\n    ENDPOINTS,\n    INPUT_CLUSTERS,\n    MANUFACTURER,\n    MODEL,\n    MODELS_INFO,\n    NODE_DESCRIPTOR,\n    OUTPUT_CLUSTERS,\n    PARAMS,\n    PROFILE_ID,\n    SKIP_CONFIGURATION,\n)\nimport zhaquirks.konke\nimport zhaquirks.philips\nfrom zhaquirks.xiaomi import XIAOMI_NODE_DESC\nimport zhaquirks.xiaomi.aqara.vibration_aq1\n\nzhaquirks.setup()\n\nALL_QUIRK_CLASSES = []\nfor manufacturer in zq.DEVICE_REGISTRY.registry_v1.values():\n    for model_quirk_list in manufacturer.values():\n        for quirk in model_quirk_list:\n            if quirk in ALL_QUIRK_CLASSES:\n                continue\n            ALL_QUIRK_CLASSES.append(quirk)\n\ndel quirk, model_quirk_list, manufacturer\n\n\nALL_ZIGPY_CLUSTERS = frozenset(zcl.clusters.CLUSTERS_BY_NAME.values())\n\n\nSIGNATURE_ALLOWED = {\n    ENDPOINTS,\n    MANUFACTURER,\n    MODEL,\n    MODELS_INFO,\n    NODE_DESCRIPTOR,\n}\nSIGNATURE_EP_ALLOWED = {\n    DEVICE_TYPE,\n    INPUT_CLUSTERS,\n    PROFILE_ID,\n    OUTPUT_CLUSTERS,\n}\nSIGNATURE_REPLACEMENT_ALLOWED = {\n    ENDPOINTS,\n    MANUFACTURER,\n    MODEL,\n    NODE_DESCRIPTOR,\n    SKIP_CONFIGURATION,\n}\n\n\n@pytest.mark.parametrize(\"quirk\", ALL_QUIRK_CLASSES)\ndef test_quirk_replacements(quirk: CustomDevice) -> None:\n    \"\"\"Test all quirks have a replacement.\"\"\"\n\n    assert quirk.signature\n    assert quirk.replacement\n\n    assert quirk.replacement[ENDPOINTS]\n\n\n@pytest.fixture\ndef raw_device() -> zigpy.device.Device:\n    \"\"\"Raw device.\"\"\"\n    app = mock.MagicMock()\n    ieee = zigpy.types.EUI64.convert(\"11:22:33:44:55:66:77:88\")\n    nwk = 0x1234\n    return zigpy.device.Device(app, ieee, nwk)\n\n\ndef test_dev_from_signature_incomplete_sig(raw_device: zigpy.device.Device) -> None:\n    \"\"\"Test device initialization from quirk's based on incomplete signature.\"\"\"\n\n    class BadSigNoSignature(zhaquirks.QuickInitDevice):\n        pass\n\n    with pytest.raises(AssertionError):\n        BadSigNoSignature.from_signature(raw_device, model=\"test_model\")\n\n    class BadSigNoModel(zhaquirks.QuickInitDevice):\n        signature = {\n            MODELS_INFO: [(\"manufacturer_1\", \"model_1\")],\n            NODE_DESCRIPTOR: XIAOMI_NODE_DESC,\n            ENDPOINTS: {\n                3: {\n                    PROFILE_ID: zigpy.profiles.zha.PROFILE_ID,\n                    DEVICE_TYPE: zigpy.profiles.zha.DeviceType.LEVEL_CONTROL_SWITCH,\n                    INPUT_CLUSTERS: [1, 6],\n                    OUTPUT_CLUSTERS: [0x19],\n                }\n            },\n        }\n\n    with pytest.raises(KeyError):\n        BadSigNoModel.from_signature(raw_device)\n\n    # require model in method, if none is present in signature\n    BadSigNoModel.from_signature(raw_device, model=\"model_model\")\n\n    # require manufacturer, if no signature[MODELS_INFO]\n    BadSigNoModel.signature.pop(MODELS_INFO)\n    with pytest.raises(KeyError):\n        BadSigNoModel.from_signature(raw_device, model=\"model_model\")\n    BadSigNoModel.signature[MANUFACTURER] = \"some manufacturer\"\n    BadSigNoModel.from_signature(raw_device, model=\"model_model\")\n\n    ep_sig_complete = {\n        PROFILE_ID: zigpy.profiles.zha.PROFILE_ID,\n        DEVICE_TYPE: zigpy.profiles.zha.DeviceType.LEVEL_CONTROL_SWITCH,\n        INPUT_CLUSTERS: [1, 6],\n        OUTPUT_CLUSTERS: [0x19],\n    }\n\n    class BadSigIncompleteEp(zhaquirks.QuickInitDevice):\n        signature = {\n            MANUFACTURER: \"manufacturer\",\n            MODEL: \"model\",\n            NODE_DESCRIPTOR: XIAOMI_NODE_DESC,\n            ENDPOINTS: {3: {**ep_sig_complete}},\n        }\n\n    BadSigIncompleteEp.from_signature(raw_device)\n\n    for missing_item in ep_sig_complete:\n        incomplete_ep = {**ep_sig_complete}\n        incomplete_ep.pop(missing_item)\n        BadSigIncompleteEp.signature[ENDPOINTS][3] = incomplete_ep\n        with pytest.raises(KeyError):\n            BadSigIncompleteEp.from_signature(raw_device)\n\n\n@pytest.mark.parametrize(\n    \"quirk_signature\",\n    (\n        {\n            ENDPOINTS: {\n                1: {\n                    PROFILE_ID: 11,\n                    DEVICE_TYPE: 22,\n                    INPUT_CLUSTERS: [0, 1, 4],\n                    OUTPUT_CLUSTERS: [0, 6, 8, 768],\n                }\n            },\n            MANUFACTURER: \"manufacturer_3\",\n            MODEL: \"model_3\",\n            NODE_DESCRIPTOR: XIAOMI_NODE_DESC,\n        },\n        {\n            ENDPOINTS: {\n                2: {\n                    PROFILE_ID: 33,\n                    DEVICE_TYPE: 44,\n                    INPUT_CLUSTERS: [0, 0x0201, 0x0402],\n                    OUTPUT_CLUSTERS: [0x0019, 0x0401],\n                },\n                5: {\n                    PROFILE_ID: 55,\n                    DEVICE_TYPE: 66,\n                    INPUT_CLUSTERS: [0, 6, 8],\n                    OUTPUT_CLUSTERS: [0x0019, 0x0500],\n                },\n            },\n            MANUFACTURER: \"manufacturer_4\",\n            MODEL: \"model_4\",\n            NODE_DESCRIPTOR: XIAOMI_NODE_DESC,\n        },\n        {\n            ENDPOINTS: {\n                2: {\n                    PROFILE_ID: 33,\n                    DEVICE_TYPE: 44,\n                    INPUT_CLUSTERS: [0, 0x0201, 0x0402],\n                    OUTPUT_CLUSTERS: [0x0019, 0x0401],\n                },\n                5: {\n                    PROFILE_ID: 55,\n                    DEVICE_TYPE: 66,\n                    INPUT_CLUSTERS: [0, 6, 8],\n                    OUTPUT_CLUSTERS: [0x0019, 0x0500],\n                },\n            },\n            MODELS_INFO: ((\"LUMI\", \"model_4\"),),\n            MODEL: \"model_4\",\n            NODE_DESCRIPTOR: XIAOMI_NODE_DESC,\n        },\n        {\n            ENDPOINTS: {\n                1: {\n                    \"profile_id\": 260,\n                    \"device_type\": 0x0100,\n                    INPUT_CLUSTERS: [\n                        0x0000,\n                        0x0003,\n                        0x0004,\n                        0x0005,\n                        0x0006,\n                        0x0702,\n                        0x0B05,\n                    ],\n                    OUTPUT_CLUSTERS: [0x000A, 0x0019],\n                },\n                2: {\n                    \"profile_id\": 260,\n                    \"device_type\": 0x0103,\n                    INPUT_CLUSTERS: [0x0000, 0x0003, 0x0B05],\n                    OUTPUT_CLUSTERS: [0x0003, 0x0006],\n                },\n            },\n            MANUFACTURER: \"manufacturer 5\",\n            MODEL: \"model 5\",\n            NODE_DESCRIPTOR: XIAOMI_NODE_DESC,\n        },\n    ),\n)\ndef test_dev_from_signature(\n    raw_device: zigpy.device.Device, quirk_signature: dict\n) -> None:\n    \"\"\"Test device initialization from quirk's based on signature.\"\"\"\n\n    class QuirkDevice(zhaquirks.QuickInitDevice):\n        signature = quirk_signature\n\n    device = QuirkDevice.from_signature(raw_device)\n    assert device.status == zigpy.device.Status.ENDPOINTS_INIT\n    if MANUFACTURER in quirk_signature:\n        assert device.manufacturer == quirk_signature[MANUFACTURER]\n    else:\n        assert device.manufacturer == \"LUMI\"\n    assert device.model == quirk_signature[MODEL]\n\n    ep_signature = quirk_signature[ENDPOINTS]\n    assert len(device.endpoints) == len(ep_signature) + 1  # ZDO endpoint\n    for ep_id, ep_data in ep_signature.items():\n        assert ep_id in device.endpoints\n        ep = device.endpoints[ep_id]\n        assert ep.status == zigpy.endpoint.Status.ZDO_INIT\n        assert ep.profile_id == ep_data[PROFILE_ID]\n        assert ep.device_type == ep_data[DEVICE_TYPE]\n        assert list(ep.in_clusters) == ep_data[INPUT_CLUSTERS]\n        assert list(ep.out_clusters) == ep_data[OUTPUT_CLUSTERS]\n\n\n@pytest.mark.parametrize(\n    \"quirk\",\n    (q for q in ALL_QUIRK_CLASSES if issubclass(q, zhaquirks.QuickInitDevice)),\n)\ndef test_quirk_quickinit(quirk: zigpy.quirks.CustomDevice) -> None:\n    \"\"\"Make sure signature in QuickInit Devices have all required attributes.\"\"\"\n\n    if not issubclass(quirk, zhaquirks.QuickInitDevice):\n        return\n\n    assert quirk.signature.get(MODELS_INFO) or quirk.signature[MANUFACTURER]\n    assert quirk.signature[NODE_DESCRIPTOR]\n    assert quirk.signature[ENDPOINTS]\n    for ep_id, ep_data in quirk.signature[ENDPOINTS].items():\n        assert ep_id\n        assert PROFILE_ID in ep_data\n        assert DEVICE_TYPE in ep_data\n        assert isinstance(ep_data[INPUT_CLUSTERS], list)\n        assert isinstance(ep_data[OUTPUT_CLUSTERS], list)\n\n\n@pytest.mark.parametrize(\n    \"quirk\",\n    ALL_QUIRK_CLASSES,\n)\ndef test_signature(quirk: CustomDevice) -> None:\n    \"\"\"Make sure signature look sane for all custom devices.\"\"\"\n\n    def _check_range(cluster: zcl.Cluster) -> bool:\n        for range in zcl.Cluster._registry_range:\n            if range[0] <= cluster <= range[1]:\n                return True\n        return False\n\n    # enforce new style of signature\n    assert ENDPOINTS in quirk.signature\n    numeric = [eid for eid in quirk.signature if isinstance(eid, int)]\n    assert not numeric\n    assert set(quirk.signature).issubset(SIGNATURE_ALLOWED)\n    models_info = quirk.signature.get(MODELS_INFO)\n    if models_info is not None:\n        for manufacturer, model in models_info:\n            if manufacturer is not None:\n                assert isinstance(manufacturer, str)\n                assert manufacturer\n            if model is not None:\n                assert isinstance(model, str)\n                assert model\n\n    for m_m in (MANUFACTURER, MODEL):\n        value = quirk.signature.get(m_m)\n        if value is not None:\n            assert isinstance(value, str)\n            assert value\n\n    # Check that the signature data is OK\n    ep_signature = quirk.signature[ENDPOINTS]\n    for ep_id, ep_data in ep_signature.items():\n        assert isinstance(ep_id, int)\n        assert 0x01 <= ep_id <= 0xFE\n        assert set(ep_data).issubset(SIGNATURE_EP_ALLOWED)\n        for sig_attr in (DEVICE_TYPE, PROFILE_ID):\n            value = ep_data.get(sig_attr)\n            if value is not None:\n                assert isinstance(value, int)\n                assert 0x0000 <= value <= 0xFFFF\n        for clusters_type in (INPUT_CLUSTERS, OUTPUT_CLUSTERS):\n            clusters = ep_data.get(clusters_type)\n            if clusters is not None:\n                assert all(isinstance(cluster_id, int) for cluster_id in clusters)\n                assert all(0 <= cluster_id <= 0xFFFF for cluster_id in clusters)\n\n        for m_m in (MANUFACTURER, MODEL):\n            value = ep_data.get(m_m)\n            if value is not None:\n                assert isinstance(value, str)\n                assert value\n\n    # Check that the replacement data is OK\n    assert set(quirk.replacement).issubset(SIGNATURE_REPLACEMENT_ALLOWED)\n    for ep_id, ep_data in quirk.replacement[ENDPOINTS].items():\n        assert isinstance(ep_id, int)\n        assert 0x01 <= ep_id <= 0xFE\n        assert set(ep_data).issubset(SIGNATURE_EP_ALLOWED)\n\n        for sig_attr in (DEVICE_TYPE, PROFILE_ID):\n            value = ep_data.get(sig_attr)\n            if value is not None:\n                assert isinstance(value, int)\n                assert 0x0000 <= value <= 0xFFFF\n\n        for clusters_type in (INPUT_CLUSTERS, OUTPUT_CLUSTERS):\n            clusters = ep_data.get(clusters_type, [])\n            for cluster in clusters:\n                if clusters is not None:\n                    if isinstance(cluster, int):\n                        assert cluster in zcl.Cluster._registry or _check_range(cluster)\n                    else:\n                        assert issubclass(cluster, zcl.Cluster)\n\n        for m_m in (MANUFACTURER, MODEL):\n            value = ep_data.get(m_m)\n            if value is not None:\n                assert isinstance(value, str)\n                assert value\n\n        node_desc = ep_data.get(NODE_DESCRIPTOR)\n        if node_desc is not None:\n            assert isinstance(node_desc, zigpy.zdo.types.NodeDescriptor)\n\n\n@pytest.mark.parametrize(\n    \"quirk\",\n    [\n        quirk_cls\n        for quirk_cls in ALL_QUIRK_CLASSES\n        if quirk_cls\n        not in (\n            # Some quirks do not yet have model info:\n            zhaquirks.xbee.xbee_io.XBeeSensor,\n            zhaquirks.xbee.xbee3_io.XBee3Sensor,\n            zhaquirks.tuya.ts0201.MoesTemperatureHumidtySensorWithScreen,\n            zhaquirks.smartthings.tag_v4.SmartThingsTagV4,\n            zhaquirks.smartthings.multi.SmartthingsMultiPurposeSensor,\n            zhaquirks.netvox.z308e3ed.Z308E3ED,\n            zhaquirks.gledopto.soposhgu10.SoposhGU10,\n        )\n    ],\n)\ndef test_signature_model_info_given(quirk: CustomDevice) -> None:\n    \"\"\"Verify that quirks have MODELS_INFO, MODEL or MANUFACTURER in their signature.\"\"\"\n\n    if (\n        not quirk.signature.get(MODELS_INFO)\n        and not quirk.signature.get(MODEL)\n        and not quirk.signature.get(MANUFACTURER)\n    ):\n        pytest.fail(\n            f\"Quirk {quirk} does not have MODELS_INFO, MODEL or MANUFACTURER in its signature. \"\n            f\"At least one of these is required.\"\n        )\n\n\n@pytest.mark.parametrize(\"quirk\", ALL_QUIRK_CLASSES)\ndef test_quirk_importable(quirk: CustomDevice) -> None:\n    \"\"\"Ensure all quirks can be imported with a normal Python `import` statement.\"\"\"\n\n    path = f\"{quirk.__module__}.{quirk.__name__}\"\n    assert all(m and m.isidentifier() for m in path.split(\".\")), (\n        f\"{path} is not importable\"\n    )\n\n\ndef test_quirk_loading_error(tmp_path: Path, caplog) -> None:\n    \"\"\"Ensure quirks silently fail to load.\"\"\"\n\n    custom_quirks = tmp_path / \"custom_zha_quirks\"\n    custom_quirks.mkdir()\n\n    (custom_quirks / \"__init__.py\").touch()\n\n    (custom_quirks / \"bosch\").mkdir()\n    (custom_quirks / \"bosch/__init__.py\").touch()\n\n    # Syntax errors are swallowed\n    (custom_quirks / \"bosch/custom_quirk.py\").write_text(\"1/\")\n\n    caplog.clear()\n    zhaquirks.setup(custom_quirks_path=str(custom_quirks))\n    assert (\n        \"Unexpected exception importing custom quirk 'bosch.custom_quirk'\"\n        in caplog.text\n    )\n    assert \"SyntaxError\" in caplog.text\n\n    # And so are import errors\n    (custom_quirks / \"bosch/custom_quirk.py\").write_text(\"from os import foobarbaz7\")\n\n    caplog.clear()\n    zhaquirks.setup(custom_quirks_path=str(custom_quirks))\n    assert (\n        \"Unexpected exception importing custom quirk 'bosch.custom_quirk'\"\n        in caplog.text\n    )\n    assert \"cannot import name 'foobarbaz7' from 'os'\" in caplog.text\n\n\ndef test_custom_quirk_loading(\n    zigpy_device_from_quirk: CustomDevice, tmp_path: Path\n) -> None:\n    \"\"\"Make sure custom quirks take priority over regular quirks.\"\"\"\n\n    device = zigpy_device_from_quirk(\n        zhaquirks.bosch.motion.ISWZPR1WP13, apply_quirk=False\n    )\n    assert type(device) is zigpy.device.Device\n\n    # Make sure our target quirk will load after we re-setup zhaquirks\n    zhaquirks.setup()\n    assert type(zq.get_device(device)) is zhaquirks.bosch.motion.ISWZPR1WP13\n\n    custom_quirks = tmp_path / \"custom_zha_quirks\"\n    custom_quirks.mkdir()\n\n    # Make our own custom quirk\n    (custom_quirks / \"__init__.py\").touch()\n\n    (custom_quirks / \"bosch\").mkdir()\n    (custom_quirks / \"bosch/__init__.py\").touch()\n    (custom_quirks / \"bosch/custom_quirk.py\").write_text(\n        '''\n\"\"\"Device handler for Bosch motion sensors.\"\"\"\nfrom zigpy.profiles import zha\nfrom zigpy.quirks import CustomDevice\nfrom zigpy.zcl.clusters.general import Basic, Identify, Ota, PollControl\nfrom zigpy.zcl.clusters.homeautomation import Diagnostic\nfrom zigpy.zcl.clusters.measurement import TemperatureMeasurement\nfrom zigpy.zcl.clusters.security import IasZone\n\nfrom zhaquirks import PowerConfigurationCluster\n\nfrom zhaquirks.bosch import BOSCH\nfrom zhaquirks.const import (\n    DEVICE_TYPE,\n    ENDPOINTS,\n    INPUT_CLUSTERS,\n    MODELS_INFO,\n    OUTPUT_CLUSTERS,\n    PROFILE_ID,\n)\n\nclass TestReplacementISWZPR1WP13(CustomDevice):\n    \"\"\"Custom device representing Bosch motion sensors.\"\"\"\n\n    signature = {\n        #  <SimpleDescriptor endpoint=1 profile=260 device_type=1026\n        #  device_version=0\n        #  input_clusters=[0, 1, 3, 1026, 1280, 32, 2821]\n        #  output_clusters=[25]>\n        MODELS_INFO: [(BOSCH, \"ISW-ZPR1-WP13\")],\n        ENDPOINTS: {\n            5: {\n                PROFILE_ID: zha.PROFILE_ID,\n                DEVICE_TYPE: zha.DeviceType.IAS_ZONE,\n                INPUT_CLUSTERS: [\n                    Basic.cluster_id,\n                    PowerConfigurationCluster.cluster_id,\n                    Identify.cluster_id,\n                    PollControl.cluster_id,\n                    TemperatureMeasurement.cluster_id,\n                    IasZone.cluster_id,\n                    Diagnostic.cluster_id,\n                ],\n                OUTPUT_CLUSTERS: [Ota.cluster_id],\n            }\n        },\n    }\n\n    replacement = {\n        ENDPOINTS: {\n            5: {\n                INPUT_CLUSTERS: [\n                    Basic.cluster_id,\n                    PowerConfigurationCluster.cluster_id,\n                    Identify.cluster_id,\n                    PollControl.cluster_id,\n                    TemperatureMeasurement.cluster_id,\n                    IasZone.cluster_id,\n                    Diagnostic.cluster_id,\n                ],\n                OUTPUT_CLUSTERS: [Ota.cluster_id],\n            }\n        }\n    }\n'''\n    )\n\n    zhaquirks.setup(custom_quirks_path=str(custom_quirks))\n\n    assert not isinstance(zq.get_device(device), zhaquirks.bosch.motion.ISWZPR1WP13)\n    assert type(zq.get_device(device)).__name__ == \"TestReplacementISWZPR1WP13\"\n\n\ndef test_zigpy_custom_cluster_pollution() -> None:\n    \"\"\"Ensure all quirks subclass `CustomCluster`.\"\"\"\n    non_zigpy_clusters = {\n        cluster\n        for cluster in zcl.Cluster._registry.values()\n        if not cluster.__module__.startswith(\"zigpy.\")\n    }\n\n    if non_zigpy_clusters:\n        raise RuntimeError(\n            f\"Custom clusters must subclass `CustomCluster`: {non_zigpy_clusters}\"\n        )\n\n\n@pytest.mark.parametrize(\n    \"module_name\",\n    sorted({q.__module__ for q in ALL_QUIRK_CLASSES}),\n)\ndef test_no_module_level_device_automation_triggers(module_name: str) -> None:\n    \"\"\"Ensure no quirk module has a module-level `device_automation_triggers` dict.\"\"\"\n\n    mod = importlib.import_module(module_name)\n    assert not hasattr(mod, \"device_automation_triggers\")\n\n\n@pytest.mark.parametrize(\"quirk\", ALL_QUIRK_CLASSES)\ndef test_migrated_lighting_automation_triggers(quirk: CustomDevice) -> None:\n    \"\"\"Ensure quirks with lighting or level control clusters are using PARAMS.\"\"\"\n\n    if not hasattr(quirk, \"device_automation_triggers\"):\n        return\n\n    for event in quirk.device_automation_triggers.values():\n        if COMMAND not in event:\n            continue\n\n        command = event[COMMAND]\n\n        # We only consider lighting commands for now\n        if command in (\n            COMMAND_MOVE_SATURATION,\n            COMMAND_MOVE_TO_SATURATION,\n            COMMAND_MOVE_COLOR_TEMP,\n            COMMAND_STEP_HUE,\n            COMMAND_STEP_SATURATION,\n            COMMAND_STEP_COLOR_TEMP,\n        ):\n            cluster = zcl.clusters.lighting.Color\n        elif command in (\n            COMMAND_MOVE,\n            COMMAND_MOVE_ON_OFF,\n            COMMAND_STEP,\n            COMMAND_STEP_ON_OFF,\n            COMMAND_STOP,\n            COMMAND_STOP_ON_OFF,\n            COMMAND_MOVE_TO_LEVEL_ON_OFF,\n        ):\n            cluster = zcl.clusters.general.LevelControl\n        else:\n            continue\n\n        if ARGS in event:\n            raise ValueError(f\"ARGS should be migrated to PARAMS: {command!r}\")\n        elif PARAMS not in event:\n            continue\n\n        schema = cluster.commands_by_name[command].schema\n        schema(**event[PARAMS])\n\n\nKNOWN_DUPLICATE_TRIGGERS = {\n    zhaquirks.aurora.aurora_dimmer.AuroraDimmerBatteryPowered: [\n        [\n            # XXX: why is this constant defined in the module?\n            (zhaquirks.aurora.aurora_dimmer.COLOR_UP, const.RIGHT),\n            (zhaquirks.aurora.aurora_dimmer.COLOR_UP, const.LEFT),\n        ],\n        [\n            (zhaquirks.aurora.aurora_dimmer.COLOR_DOWN, const.RIGHT),\n            (zhaquirks.aurora.aurora_dimmer.COLOR_DOWN, const.LEFT),\n        ],\n    ],\n    zhaquirks.paulmann.fourbtnremote.PaulmannRemote4Btn: [\n        [\n            (const.LONG_RELEASE, const.BUTTON_1),\n            (const.LONG_RELEASE, const.BUTTON_2),\n        ],\n        [\n            (const.LONG_RELEASE, const.BUTTON_3),\n            (const.LONG_RELEASE, const.BUTTON_4),\n        ],\n    ],\n    zhaquirks.thirdreality.button.Button: [\n        [\n            (const.LONG_PRESS, const.LONG_PRESS),\n            (const.LONG_RELEASE, const.LONG_RELEASE),\n        ]\n    ],\n}\n\n\n# XXX: Test does not handle v2 quirks\n@pytest.mark.parametrize(\n    \"quirk\",\n    [q for q in ALL_QUIRK_CLASSES if getattr(q, \"device_automation_triggers\", None)],\n)\ndef test_quirk_device_automation_triggers_unique(quirk):\n    \"\"\"Ensure all quirks have unique device automation triggers.\"\"\"\n\n    events = collections.defaultdict(list)\n\n    for trigger, event in quirk.device_automation_triggers.items():\n        # XXX: Dictionary objects are not hashable\n        frozen_event = json.dumps(event, sort_keys=True)\n        events[frozen_event].append((trigger, event))\n\n    for triggers_and_events in events.values():\n        triggers = [trigger for trigger, _ in triggers_and_events]\n\n        if len(triggers_and_events) > 1:\n            if (\n                quirk in KNOWN_DUPLICATE_TRIGGERS\n                and triggers in KNOWN_DUPLICATE_TRIGGERS[quirk]\n            ):\n                fail_func = pytest.xfail\n            else:\n                fail_func = pytest.fail\n\n            triggers_text = \"\\n\".join(\n                [f\" * {event} <- {trigger}\" for trigger, event in triggers_and_events]\n            )\n            fail_func(f\"Triggers are not unique for {quirk}:\\n{triggers_text}\")\n\n\n@pytest.mark.parametrize(\n    \"quirk\",\n    [\n        quirk_cls\n        for quirk_cls in ALL_QUIRK_CLASSES\n        if quirk_cls\n        not in (\n            zhaquirks.xbee.xbee_io.XBeeSensor,\n            zhaquirks.xbee.xbee3_io.XBee3Sensor,\n        )\n    ],\n)\ndef test_attributes_updated_not_replaced(quirk: CustomDevice) -> None:\n    \"\"\"Verify no quirks subclass a ZCL cluster but delete its attributes list.\"\"\"\n\n    base_cluster_attrs_name = {}\n    base_cluster_attrs_id = {}\n\n    for cluster in zcl.clusters.CLUSTERS_BY_NAME.values():\n        assert cluster.ep_attribute not in base_cluster_attrs_name\n        base_cluster_attrs_name[cluster.ep_attribute] = set(\n            cluster.attributes_by_name.keys()\n        )\n        base_cluster_attrs_id[cluster.cluster_id] = set(\n            cluster.attributes_by_name.keys()\n        )\n\n    for ep_data in quirk.replacement[ENDPOINTS].values():\n        for cluster in ep_data.get(INPUT_CLUSTERS, []) + ep_data.get(\n            OUTPUT_CLUSTERS, []\n        ):\n            if (\n                isinstance(cluster, int)\n                or not issubclass(cluster, zcl.Cluster)\n                or cluster in ALL_ZIGPY_CLUSTERS\n            ):\n                continue\n\n            assert issubclass(cluster, zigpy.quirks.CustomCluster)\n\n            # Check if attributes match based on cluster endpoint attribute\n            if not (\n                base_cluster_attrs_name.get(cluster.ep_attribute, set())\n                <= set(cluster.attributes_by_name.keys())\n            ):\n                missing_attrs = base_cluster_attrs_name[cluster.ep_attribute] - set(\n                    cluster.attributes_by_name.keys()\n                )\n\n                # A few are expected to fail and are handled by ZHA\n                if cluster not in (\n                    zhaquirks.centralite.cl_3310S.SmartthingsRelativeHumidityCluster,\n                ):\n                    pytest.fail(\n                        f\"Cluster {cluster} with endpoint name {cluster.ep_attribute!r}\"\n                        f\" does not contain all named attributes: {missing_attrs}\"\n                    )\n\n            # Check if attributes match based on cluster ID\n            if not (\n                base_cluster_attrs_id.get(cluster.cluster_id, set())\n                <= set(cluster.attributes_by_name.keys())\n            ):\n                missing_attrs = base_cluster_attrs_id[cluster.cluster_id] - set(\n                    cluster.attributes_by_name.keys()\n                )\n\n                # A few are expected to fail and are handled by ZHA\n                if cluster not in (\n                    zhaquirks.konke.KonkeOnOffCluster,\n                    zhaquirks.philips.PhilipsOccupancySensing,\n                    zhaquirks.xiaomi.aqara.vibration_aq1.VibrationAQ1.MultistateInputCluster,\n                ):\n                    pytest.fail(\n                        f\"Cluster {cluster} with endpoint ID 0x{cluster.cluster_id:04X}\"\n                        f\" does not contain all named attributes: {missing_attrs}\"\n                    )\n\n            base_clusters = set(cluster.__mro__) & ALL_ZIGPY_CLUSTERS\n\n            # Completely custom cluster\n            if len(base_clusters) == 0:\n                continue\n            elif len(base_clusters) > 1:\n                pytest.fail(f\"Cluster has more than one zigpy base class: {cluster}\")\n\n            base_cluster = list(base_clusters)[0]\n\n            # Ensure the attribute IDs are extended\n            base_attr_ids = set(base_cluster.attributes)\n            quirk_attr_ids = set(cluster.attributes)\n\n            if not base_attr_ids <= quirk_attr_ids:\n                pytest.fail(\n                    f\"Cluster {cluster} deletes parent class's attributes instead of\"\n                    f\" extending them: {base_attr_ids - quirk_attr_ids}\"\n                )\n\n            # Ensure the attribute names are extended\n            base_attr_names = {a.name for a in base_cluster.attributes.values()}\n            quirk_attr_names = {a.name for a in cluster.attributes.values()}\n\n            if not base_attr_names <= quirk_attr_names:\n                pytest.fail(\n                    f\"Cluster {cluster} deletes parent class's attributes instead of\"\n                    f\" extending them: {base_attr_names - quirk_attr_names}\"\n                )\n\n\n@pytest.mark.parametrize(\"quirk\", ALL_QUIRK_CLASSES)\ndef test_no_duplicate_clusters(quirk: CustomDevice) -> None:\n    \"\"\"Verify no quirks contain clusters with duplicate cluster ids in the replacement.\"\"\"\n\n    def check_for_duplicate_cluster_ids(clusters) -> None:\n        used_cluster_ids = set()\n\n        for cluster in clusters:\n            if isinstance(cluster, int):\n                cluster_id = cluster\n            else:\n                cluster_id = cluster.cluster_id\n\n            if cluster_id in used_cluster_ids:\n                pytest.fail(\n                    f\"Cluster ID 0x{cluster_id:04X} is used more than once in the\"\n                    f\" replacement for endpoint {ep_id} in {quirk}\"\n                )\n            used_cluster_ids.add(cluster_id)\n\n    for ep_id, ep_data in quirk.replacement[ENDPOINTS].items():  # noqa: B007\n        check_for_duplicate_cluster_ids(ep_data.get(INPUT_CLUSTERS, []))\n        check_for_duplicate_cluster_ids(ep_data.get(OUTPUT_CLUSTERS, []))\n\n\nasync def test_local_data_cluster(device_mock) -> None:\n    \"\"\"Ensure reading attributes from a LocalDataCluster works as expected.\"\"\"\n    registry = DeviceRegistry()\n\n    class TestLocalCluster(zhaquirks.LocalDataCluster):\n        \"\"\"Test cluster.\"\"\"\n\n        cluster_id = 0x1234\n        _CONSTANT_ATTRIBUTES = {1: 10}\n        _VALID_ATTRIBUTES = [2]\n\n    (\n        QuirkBuilder(device_mock.manufacturer, device_mock.model, registry=registry)\n        .adds(TestLocalCluster)\n        .add_to_registry()\n    )\n    device = registry.get_device(device_mock)\n    assert isinstance(device.endpoints[1].in_clusters[0x1234], TestLocalCluster)\n\n    # reading invalid attribute return unsupported attribute\n    assert await device.endpoints[1].in_clusters[0x1234].read_attributes([0]) == (\n        {},\n        {0: foundation.Status.UNSUPPORTED_ATTRIBUTE},\n    )\n\n    # reading constant attribute works\n    assert await device.endpoints[1].in_clusters[0x1234].read_attributes([1]) == (\n        {1: 10},\n        {},\n    )\n\n    # reading valid attribute returns None with success status\n    assert await device.endpoints[1].in_clusters[0x1234].read_attributes([2]) == (\n        {2: None},\n        {},\n    )\n", "metadata": {"source": "github.com/zigpy/zha-device-handlers", "file_path": "tests/test_quirks.py", "language": "python", "complexity": "complex", "star_count": 930, "quality_score": 0.74, "features": {"has_classes": true, "has_functions": true, "has_async": true, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": true, "has_type_hints": true, "has_testing": true, "has_logging": true, "function_count": 19, "class_count": 9, "line_count": 876}, "authority_level": "medium", "authentic": true}}
{"domain": "code_generation", "category": "error_handling", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create robust error handling with logging and validation with asyncio coroutines\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"Tests for TuyaQuirkBuilder.\"\"\"\n\nimport datetime\nfrom unittest import mock\n\nimport pytest\nfrom zigpy.quirks.registry import DeviceRegistry\nfrom zigpy.quirks.v2 import CustomDeviceV2\nimport zigpy.types as t\nfrom zigpy.zcl import foundation\nfrom zigpy.zcl.clusters.general import Basic\nfrom zigpy.zcl.clusters.homeautomation import ElectricalMeasurement\n\nfrom tests.common import ClusterListener, MockDatetime, wait_for_zigpy_tasks\nimport zhaquirks\nfrom zhaquirks.const import BatterySize\nfrom zhaquirks.tuya import (\n    TUYA_QUERY_DATA,\n    TUYA_SEND_DATA,\n    TUYA_SET_TIME,\n    TuyaCommand,\n    TuyaData,\n    TuyaDatapointData,\n    TuyaLocalCluster,\n    TuyaPowerConfigurationCluster,\n    TuyaPowerConfigurationCluster2AAA,\n)\nfrom zhaquirks.tuya.builder import (\n    TuyaAirQualityVOC,\n    TuyaCO2Concentration,\n    TuyaFormaldehydeConcentration,\n    TuyaIasContact,\n    TuyaIasFire,\n    TuyaIasGas,\n    TuyaIasVibration,\n    TuyaIlluminance,\n    TuyaPM25Concentration,\n    TuyaQuirkBuilder,\n    TuyaRelativeHumidity,\n    TuyaSoilMoisture,\n    TuyaTemperatureMeasurement,\n    TuyaValveWaterConsumedNoInstDemand,\n)\nfrom zhaquirks.tuya.mcu import DPToAttributeMapping, TuyaMCUCluster, TuyaOnOffNM\nfrom zhaquirks.tuya.tuya_sensor import NoManufTimeTuyaMCUCluster\n\nZCL_TUYA_SET_TIME = b\"\\x09\\x12\\x24\\x0d\\x00\"\n\nzhaquirks.setup()\n\n\n@pytest.mark.parametrize(\n    \"method_name,attr_name,exp_class\",\n    [\n        (\"tuya_battery\", \"power\", TuyaPowerConfigurationCluster),\n        (\"tuya_metering\", \"smartenergy_metering\", TuyaValveWaterConsumedNoInstDemand),\n        (\"tuya_onoff\", \"on_off\", TuyaOnOffNM),\n        (\"tuya_soil_moisture\", \"soil_moisture\", TuyaSoilMoisture),\n        (\"tuya_temperature\", \"temperature\", TuyaTemperatureMeasurement),\n        (\"tuya_humidity\", \"humidity\", TuyaRelativeHumidity),\n        (\"tuya_smoke\", \"ias_zone\", TuyaIasFire),\n        (\"tuya_contact\", \"ias_zone\", TuyaIasContact),\n        (\"tuya_co2\", \"carbon_dioxide_concentration\", TuyaCO2Concentration),\n        (\"tuya_pm25\", \"pm25\", TuyaPM25Concentration),\n        (\"tuya_voc\", \"voc_level\", TuyaAirQualityVOC),\n        (\"tuya_vibration\", \"ias_zone\", TuyaIasVibration),\n        (\n            \"tuya_formaldehyde\",\n            \"formaldehyde_concentration\",\n            TuyaFormaldehydeConcentration,\n        ),\n        (\"tuya_gas\", \"ias_zone\", TuyaIasGas),\n        (\"tuya_illuminance\", \"illuminance\", TuyaIlluminance),\n    ],\n)\nasync def test_convenience_methods(device_mock, method_name, attr_name, exp_class):\n    \"\"\"Test TuyaQuirkBuilder convenience methods.\"\"\"\n\n    registry = DeviceRegistry()\n\n    entry = TuyaQuirkBuilder(\n        device_mock.manufacturer, device_mock.model, registry=registry\n    )\n    entry = getattr(entry, method_name)(dp_id=1)\n    entry.skip_configuration().add_to_registry()\n\n    quirked = registry.get_device(device_mock)\n    assert isinstance(quirked, CustomDeviceV2)\n    assert quirked in registry\n\n    ep = quirked.endpoints[1]\n\n    ep_attr = getattr(ep, attr_name)\n    assert ep_attr is not None\n    assert isinstance(ep_attr, exp_class)\n\n\n@pytest.mark.parametrize(\n    \"power_cfg,battery_type,battery_qty,battery_voltage,\"\n    \"expected_size,expected_qty,expected_voltage\",\n    [\n        (TuyaPowerConfigurationCluster2AAA, None, None, None, BatterySize.AAA, 2, 15),\n        (None, BatterySize.CR123A, 1, 60, BatterySize.CR123A, 1, 60),\n        (None, BatterySize.CR123A, 1, None, BatterySize.CR123A, 1, 30),\n        (None, BatterySize.CR2450, 1, None, BatterySize.CR2450, 1, 30),\n        (None, BatterySize.CR2032, 1, None, BatterySize.CR2032, 1, 30),\n        (None, BatterySize.CR1632, 1, None, BatterySize.CR1632, 1, 30),\n        (None, BatterySize.AA, None, None, BatterySize.AA, None, None),\n        (None, None, None, None, None, None, None),\n    ],\n)\nasync def test_battery_methods(\n    device_mock,\n    power_cfg,\n    battery_type,\n    battery_qty,\n    battery_voltage,\n    expected_size,\n    expected_qty,\n    expected_voltage,\n):\n    \"\"\"Test the battery convenience method.\"\"\"\n\n    registry = DeviceRegistry()\n\n    (\n        TuyaQuirkBuilder(device_mock.manufacturer, device_mock.model, registry=registry)\n        .tuya_battery(\n            dp_id=1,\n            power_cfg=power_cfg,\n            battery_type=battery_type,\n            battery_qty=battery_qty,\n            battery_voltage=battery_voltage,\n        )\n        .tuya_onoff(dp_id=3)\n        .skip_configuration()\n        .add_to_registry()\n    )\n\n    quirked = registry.get_device(device_mock)\n    ep = quirked.endpoints[1]\n\n    assert ep.power is not None\n    assert isinstance(ep.power, power_cfg or TuyaPowerConfigurationCluster)\n\n    assert ep.power.get(\"battery_size\") == expected_size\n    assert ep.power.get(\"battery_quantity\") == expected_qty\n    assert ep.power.get(\"battery_rated_voltage\") == expected_voltage\n\n\nasync def test_tuya_quirkbuilder(device_mock):\n    \"\"\"Test adding a v2 Tuya Quirk to the registry and getting back a quirked device.\"\"\"\n\n    registry = DeviceRegistry()\n\n    class TestEnum(t.enum8):\n        \"\"\"Test Enum.\"\"\"\n\n        A = 0x00\n        B = 0x01\n\n    class ModTuyaMCUCluster(TuyaMCUCluster):\n        \"\"\"Modified Cluster.\"\"\"\n\n    class Tuya3PhaseElectricalMeasurement(ElectricalMeasurement, TuyaLocalCluster):\n        \"\"\"Tuya Electrical Measurement cluster.\"\"\"\n\n    def dpToPower(data: bytes) -> int:\n        return data[0]\n\n    def dpToCurrent(data: bytes) -> int:\n        return data[1]\n\n    def dpToVoltage(data: bytes) -> int:\n        return data[2]\n\n    entry = (\n        TuyaQuirkBuilder(device_mock.manufacturer, device_mock.model, registry=registry)\n        .tuya_battery(dp_id=1)\n        .tuya_onoff(dp_id=3)\n        .tuya_switch(\n            dp_id=6,\n            attribute_name=\"test_onoff\",\n            translation_key=\"test_onoff\",\n            fallback_name=\"Test on/off\",\n        )\n        .tuya_number(\n            dp_id=7,\n            attribute_name=\"test_number\",\n            type=t.uint16_t,\n            translation_key=\"test_number\",\n            fallback_name=\"Test number\",\n        )\n        .tuya_binary_sensor(\n            dp_id=8,\n            attribute_name=\"test_binary\",\n            translation_key=\"test_binary\",\n            fallback_name=\"Test binary\",\n        )\n        .tuya_sensor(\n            dp_id=9,\n            attribute_name=\"test_sensor\",\n            type=t.uint8_t,\n            translation_key=\"test_sensor\",\n            fallback_name=\"Test sensor\",\n        )\n        .tuya_enum(\n            dp_id=10,\n            attribute_name=\"test_enum\",\n            enum_class=TestEnum,\n            translation_key=\"test_enum\",\n            fallback_name=\"Test enum\",\n        )\n        .tuya_dp_multi(\n            dp_id=11,\n            attribute_mapping=[\n                DPToAttributeMapping(\n                    ep_attribute=Tuya3PhaseElectricalMeasurement.ep_attribute,\n                    attribute_name=\"active_power\",\n                    converter=dpToPower,\n                ),\n                DPToAttributeMapping(\n                    ep_attribute=Tuya3PhaseElectricalMeasurement.ep_attribute,\n                    attribute_name=\"rms_current\",\n                    converter=dpToCurrent,\n                ),\n                DPToAttributeMapping(\n                    ep_attribute=Tuya3PhaseElectricalMeasurement.ep_attribute,\n                    attribute_name=\"rms_voltage\",\n                    converter=dpToVoltage,\n                ),\n            ],\n        )\n        .adds(Tuya3PhaseElectricalMeasurement)\n        .skip_configuration()\n        .add_to_registry(replacement_cluster=ModTuyaMCUCluster)\n    )\n\n    # coverage for overridden __eq__ method\n    assert entry.adds_metadata[0] != entry.adds_metadata[1]\n    assert entry.adds_metadata[0] != entry\n\n    quirked = registry.get_device(device_mock)\n    assert isinstance(quirked, CustomDeviceV2)\n    assert quirked in registry\n\n    ep = quirked.endpoints[1]\n\n    assert ep.basic is not None\n    assert isinstance(ep.basic, Basic)\n\n    assert ep.tuya_manufacturer is not None\n    assert isinstance(ep.tuya_manufacturer, ModTuyaMCUCluster)\n    assert isinstance(ep.tuya_manufacturer, TuyaMCUCluster)\n\n    tuya_cluster = ep.tuya_manufacturer\n    tuya_listener = ClusterListener(tuya_cluster)\n    assert tuya_cluster.attributes_by_name[\"mcu_version\"].id == 0xEF00\n    assert tuya_cluster.attributes_by_name[\"test_onoff\"].id == 0xEF06\n    assert tuya_cluster.attributes_by_name[\"test_number\"].id == 0xEF07\n    assert tuya_cluster.attributes_by_name[\"test_binary\"].id == 0xEF08\n    assert tuya_cluster.attributes_by_name[\"test_sensor\"].id == 0xEF09\n    assert tuya_cluster.attributes_by_name[\"test_enum\"].id == 0xEF0A\n\n    with mock.patch.object(\n        tuya_cluster.endpoint, \"request\", return_value=foundation.Status.SUCCESS\n    ) as m1:\n        (status,) = await tuya_cluster.write_attributes(\n            {\n                \"test_enum\": 0x01,\n            }\n        )\n\n        await wait_for_zigpy_tasks()\n        m1.assert_called_with(\n            cluster=61184,\n            sequence=1,\n            data=b\"\\x01\\x01\\x00\\x00\\x01\\n\\x04\\x00\\x01\\x01\",\n            command_id=0,\n            timeout=5,\n            expect_reply=False,\n            use_ieee=False,\n            ask_for_ack=None,\n            priority=t.PacketPriority.NORMAL,\n        )\n        assert status == [\n            foundation.WriteAttributesStatusRecord(foundation.Status.SUCCESS)\n        ]\n\n    assert tuya_listener.attribute_updates[0][0] == 0xEF0A\n    assert tuya_listener.attribute_updates[0][1] == TestEnum.B\n\n    electric_data = TuyaCommand(\n        status=0,\n        tsn=2,\n        datapoints=[TuyaDatapointData(11, TuyaData(\"345\"))],\n    )\n    tuya_cluster.handle_get_data(electric_data)\n    electrical_meas_cluster = ep.electrical_measurement\n    assert electrical_meas_cluster.get(\"active_power\") == \"3\"\n    assert electrical_meas_cluster.get(\"rms_current\") == \"4\"\n    assert electrical_meas_cluster.get(\"rms_voltage\") == \"5\"\n\n\nasync def test_tuya_quirkbuilder_duplicated_mappings(device_mock):\n    \"\"\"Test that mapping the same DP multiple times will raise.\"\"\"\n\n    registry = DeviceRegistry()\n\n    with pytest.raises(ValueError):\n        (\n            TuyaQuirkBuilder(\n                device_mock.manufacturer, device_mock.model, registry=registry\n            )\n            .tuya_battery(dp_id=1)\n            .tuya_onoff(dp_id=1)\n            .skip_configuration()\n            .add_to_registry()\n        )\n\n    with pytest.raises(ValueError):\n        (\n            TuyaQuirkBuilder(\n                device_mock.manufacturer, device_mock.model, registry=registry\n            )\n            .tuya_battery(dp_id=1)\n            .tuya_dp_multi(\n                dp_id=1,\n                attribute_mapping=[\n                    DPToAttributeMapping(\n                        ep_attribute=ElectricalMeasurement.ep_attribute,\n                        attribute_name=\"active_power\",\n                    ),\n                ],\n            )\n            .add_to_registry()\n        )\n\n\n@pytest.mark.parametrize(\n    \"read_attr_spell,data_query_spell\",\n    [\n        (True, False),\n        (False, True),\n        (True, True),\n        (False, False),\n    ],\n)\nasync def test_tuya_spell(device_mock, read_attr_spell, data_query_spell):\n    \"\"\"Test that enchanted Tuya devices have their spells applied during configuration.\"\"\"\n    registry = DeviceRegistry()\n\n    entry = (\n        TuyaQuirkBuilder(device_mock.manufacturer, device_mock.model, registry=registry)\n        .tuya_battery(dp_id=1)\n        .tuya_onoff(dp_id=3)\n        .tuya_enchantment(\n            read_attr_spell=read_attr_spell, data_query_spell=data_query_spell\n        )\n        .skip_configuration()\n        .add_to_registry()\n    )\n\n    # coverage for overridden __eq__ method\n    assert entry.adds_metadata[0] != entry.adds_metadata[1]\n    assert entry.adds_metadata[0] != entry\n\n    quirked = registry.get_device(device_mock)\n\n    assert isinstance(quirked, CustomDeviceV2)\n    assert quirked in registry\n\n    request_patch = mock.patch(\"zigpy.zcl.Cluster.request\", mock.AsyncMock())\n    with request_patch as request_mock:\n        request_mock.return_value = (foundation.Status.SUCCESS, \"done\")\n\n        # call apply_custom_configuration() on each EnchantedDevice\n        # ZHA does this during device configuration normally\n        await quirked.apply_custom_configuration()\n\n        # the number of Tuya spells that are allowed to be cast, so the sum of enabled Tuya spells\n        enabled_tuya_spells_num = (\n            quirked.tuya_spell_read_attributes + quirked.tuya_spell_data_query\n        )\n\n        # verify request was called the correct number of times\n        assert request_mock.call_count == enabled_tuya_spells_num\n\n        # used to check list of mock calls below\n        messages = 0\n\n        # check 'attribute read spell' was cast correctly (if enabled)\n        if quirked.tuya_spell_read_attributes:\n            assert (\n                request_mock.mock_calls[messages][1][1]\n                == foundation.GeneralCommand.Read_Attributes\n            )\n            assert request_mock.mock_calls[messages][1][3] == [4, 0, 1, 5, 7, 65534]\n            messages += 1\n\n        # check 'query data spell' was cast correctly (if enabled)\n        if quirked.tuya_spell_data_query:\n            assert not request_mock.mock_calls[messages][1][0]\n            assert request_mock.mock_calls[messages][1][1] == TUYA_QUERY_DATA\n            messages += 1\n\n        request_mock.reset_mock()\n\n\nasync def test_tuya_mcu_set_time(device_mock):\n    \"\"\"Test TuyaQuirkBuilder replacement cluster, set_time requests (0x24) messages for MCU devices.\"\"\"\n\n    registry = DeviceRegistry()\n\n    (\n        TuyaQuirkBuilder(device_mock.manufacturer, device_mock.model, registry=registry)\n        .tuya_battery(dp_id=1)\n        .skip_configuration()\n        .add_to_registry(replacement_cluster=NoManufTimeTuyaMCUCluster)\n    )\n\n    quirked = registry.get_device(device_mock)\n    assert isinstance(quirked, CustomDeviceV2)\n    assert quirked in registry\n\n    ep = quirked.endpoints[1]\n\n    assert not ep.tuya_manufacturer._is_manuf_specific\n    assert not ep.tuya_manufacturer.server_commands[\n        TUYA_SET_TIME\n    ].is_manufacturer_specific\n\n    # Mock datetime\n    origdatetime = datetime.datetime\n    datetime.datetime = MockDatetime\n\n    # simulate a SET_TIME message\n    hdr, args = ep.tuya_manufacturer.deserialize(ZCL_TUYA_SET_TIME)\n    assert hdr.command_id == TUYA_SET_TIME\n\n    with mock.patch.object(\n        ep.tuya_manufacturer._endpoint,\n        \"request\",\n        return_value=foundation.Status.SUCCESS,\n    ) as m1:\n        ep.tuya_manufacturer.handle_message(hdr, args)\n        await wait_for_zigpy_tasks()\n\n        res_hdr = foundation.ZCLHeader.deserialize(m1.await_args[1][\"data\"])\n        assert not res_hdr[0].manufacturer\n        assert not res_hdr[0].frame_control.is_manufacturer_specific\n\n    datetime.datetime = origdatetime  # restore datetime\n\n\n@pytest.mark.parametrize(\n    \"force\",\n    [\n        (False),\n        (True),\n    ],\n)\nasync def test_tuya_quirkbuilder_force(device_mock, force):\n    \"\"\"Test adding an empty TuyaQuirkBuilder doesn't add an MCU cluster unless forced to.\"\"\"\n\n    registry = DeviceRegistry()\n\n    (\n        TuyaQuirkBuilder(device_mock.manufacturer, device_mock.model, registry=registry)\n        .skip_configuration()\n        .add_to_registry(force_add_cluster=force)\n    )\n\n    quirked = registry.get_device(device_mock)\n    assert isinstance(quirked, CustomDeviceV2)\n    assert quirked in registry\n\n    ep = quirked.endpoints[1]\n\n    if force:\n        assert ep.tuya_manufacturer is not None\n        assert isinstance(ep.tuya_manufacturer, TuyaMCUCluster)\n    else:\n        assert not hasattr(ep, \"tuya_manufacturer\")\n\n\n@pytest.mark.parametrize(\n    \"kwargs,expected_cmd,expected_data\",\n    [\n        (\n            {},  # no kwarg should use default of TUYA_SET_DATA\n            0,\n            b\"\\x01\\x01\\x00\\x00\\x01\\n\\x04\\x00\\x01\\x01\",\n        ),\n        (\n            {\"mcu_write_command\": TUYA_SEND_DATA},\n            4,\n            b\"\\x01\\x01\\x04\\x00\\x01\\n\\x04\\x00\\x01\\x01\",\n        ),\n    ],\n)\nasync def test_tuya_override_mcu_command(\n    device_mock, kwargs, expected_cmd, expected_data\n):\n    \"\"\"Test TuyaQuirkBuilder overriding MCU datapoint write command.\"\"\"\n\n    registry = DeviceRegistry()\n\n    class TestEnum(t.enum8):\n        \"\"\"Test Enum.\"\"\"\n\n        A = 0x00\n        B = 0x01\n\n    (\n        TuyaQuirkBuilder(device_mock.manufacturer, device_mock.model, registry=registry)\n        .tuya_enum(\n            dp_id=10,\n            attribute_name=\"test_enum\",\n            enum_class=TestEnum,\n            translation_key=\"test_enum\",\n            fallback_name=\"Test enum\",\n        )\n        .skip_configuration()\n        .add_to_registry(**kwargs)\n    )\n\n    quirked = registry.get_device(device_mock)\n    assert isinstance(quirked, CustomDeviceV2)\n    assert quirked in registry\n\n    ep = quirked.endpoints[1]\n\n    assert ep.tuya_manufacturer is not None\n    assert isinstance(ep.tuya_manufacturer, TuyaMCUCluster)\n\n    tuya_cluster = ep.tuya_manufacturer\n    tuya_listener = ClusterListener(tuya_cluster)\n    assert tuya_cluster.attributes_by_name[\"test_enum\"].id == 0xEF0A\n\n    with mock.patch.object(\n        tuya_cluster.endpoint, \"request\", return_value=foundation.Status.SUCCESS\n    ) as m1:\n        (status,) = await tuya_cluster.write_attributes(\n            {\n                \"test_enum\": 0x01,\n            },\n        )\n\n        await wait_for_zigpy_tasks()\n        m1.assert_called_with(\n            cluster=61184,\n            sequence=1,\n            data=expected_data,\n            command_id=expected_cmd,\n            timeout=5,\n            expect_reply=False,\n            use_ieee=False,\n            ask_for_ack=None,\n            priority=t.PacketPriority.NORMAL,\n        )\n        assert status == [\n            foundation.WriteAttributesStatusRecord(foundation.Status.SUCCESS)\n        ]\n\n    assert tuya_listener.attribute_updates[0][0] == 0xEF0A\n    assert tuya_listener.attribute_updates[0][1] == TestEnum.B\n", "metadata": {"source": "github.com/zigpy/zha-device-handlers", "file_path": "tests/test_tuya_builder.py", "language": "python", "complexity": "complex", "star_count": 930, "quality_score": 0.74, "features": {"has_classes": true, "has_functions": true, "has_async": true, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": true, "has_type_hints": true, "has_testing": true, "has_logging": false, "function_count": 11, "class_count": 4, "line_count": 568}, "authority_level": "medium", "authentic": true}}
{"domain": "code_generation", "category": "error_handling", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create robust error handling with logging and validation with pytest testing framework\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"Test units for new Tuya cluster framework.\"\"\"\n\nfrom unittest import mock\n\nimport pytest\nimport zigpy.types as t\nimport zigpy.zcl.foundation as zcl_f\n\nfrom zhaquirks.tuya import (\n    TUYA_ACTIVE_STATUS_RPT,\n    TUYA_GET_DATA,\n    TUYA_SET_DATA_RESPONSE,\n    TUYA_SET_TIME,\n    TuyaCommand,\n    TuyaData,\n    TuyaDatapointData,\n    TuyaNewManufCluster,\n)\n\n\n@pytest.fixture(name=\"TuyaCluster\")\ndef tuya_cluster(zigpy_device_mock):\n    \"\"\"Mock of the new Tuya manufacturer cluster.\"\"\"\n    device = zigpy_device_mock()\n    endpoint = device.add_endpoint(1)\n    cluster = TuyaNewManufCluster(endpoint)\n    return cluster\n\n\ndef test_tuya_data_raw():\n    \"\"\"Test tuya \"Raw\" datatype.\"\"\"\n\n    class Test(t.Struct):\n        test_bool: t.Bool\n        test_uint16_t_be: t.uint16_t_be\n\n    data = b\"\\x00\\x00\\x03\\x01\\x02\\x46\"\n    extra = b\"extra data\"\n\n    r, rest = TuyaData.deserialize(data + extra)\n    assert rest == extra\n\n    assert r.dp_type == 0\n    assert r.raw == b\"\\x01\\x02\\x46\"\n    assert Test.deserialize(r.payload)[0] == Test(True, 582)\n\n    r.payload = Test(False, 314)\n    assert r.raw == b\"\\x00\\x01\\x3a\"\n\n\ndef test_tuya_data_value():\n    \"\"\"Test tuya \"Value\" datatype.\"\"\"\n\n    data = b\"\\x02\\x00\\x04\\x00\\x00\\x02\\xdb\"\n    extra = b\"extra data\"\n\n    r, rest = TuyaData.deserialize(data + extra)\n    assert rest == extra\n\n    assert r.dp_type == 2\n    assert r.raw == b\"\\x00\\x00\\x02\\xdb\"\n    assert r.payload == 731\n\n    r.payload = 582\n    assert r.raw == b\"\\x00\\x00\\x02\\x46\"\n\n\ndef test_tuya_negative_value():\n    \"\"\"Test tuya negative \"Value\" datatype.\"\"\"\n\n    data = b\"\\x02\\x00\\x04\\xff\\xff\\xff\\xf8\"\n    extra = b\"extra data\"\n\n    r, rest = TuyaData.deserialize(data + extra)\n    assert rest == extra\n\n    assert r.dp_type == 2\n    assert r.raw == b\"\\xff\\xff\\xff\\xf8\"\n    assert r.payload == -8\n\n\ndef test_tuya_data_bool():\n    \"\"\"Test tuya Bool datatype.\"\"\"\n\n    data = b\"\\x01\\x00\\x01\\x00\"\n    extra = b\"extra data\"\n\n    r, rest = TuyaData.deserialize(data + extra)\n    assert rest == extra\n\n    assert r.dp_type == 1\n    assert r.raw == b\"\\x00\"\n    assert not r.payload\n\n    r.payload = True\n    assert r.raw == b\"\\x01\"\n\n    data = b\"\\x01\\x00\\x01\\x01\"\n    extra = b\"extra data\"\n\n    r, rest = TuyaData.deserialize(data + extra)\n    assert rest == extra\n\n    assert r.dp_type == 1\n    assert r.raw == b\"\\x01\"\n    assert r.payload\n\n    r.payload = False\n    assert r.raw == b\"\\x00\"\n\n\ndef test_tuya_data_enum():\n    \"\"\"Test tuya Enum datatype.\"\"\"\n\n    data = b\"\\x04\\x00\\x01\\x40\"\n    extra = b\"extra data\"\n\n    r, rest = TuyaData.deserialize(data + extra)\n    assert rest == extra\n\n    assert r.dp_type == 4\n    assert r.raw == b\"\\x40\"\n    assert r.payload == 0x40\n\n    r.payload = 0x42\n    assert r.raw == b\"\\x42\"\n\n\ndef test_tuya_data_string():\n    \"\"\"Test tuya String datatype.\"\"\"\n\n    data = b\"\\x03\\x00\\x04Tuya\"\n    extra = b\"extra data\"\n\n    r, rest = TuyaData.deserialize(data + extra)\n    assert rest == extra\n\n    assert r.dp_type == 3\n    assert r.raw == b\"Tuya\"\n    assert r.payload == \"Tuya\"\n\n    r.payload = \"Data\"\n    assert r.raw == b\"Data\"\n\n\ndef test_tuya_data_bitmap():\n    \"\"\"Test tuya Bitmap datatype.\"\"\"\n\n    data = b\"\\x05\\x00\\x01\\x40\"\n    extra = b\"extra data\"\n\n    r, rest = TuyaData.deserialize(data + extra)\n    assert rest == extra\n\n    assert r.dp_type == 5\n    assert r.raw == b\"\\x40\"\n    assert r.payload == 0x40\n\n    r.payload = 0x82\n    assert r.raw == b\"\\x82\"\n\n    data = b\"\\x05\\x00\\x02\\x40\\x02\"\n    r, _ = TuyaData.deserialize(data)\n    assert r.payload == 0x0240\n\n    r.payload = t.bitmap16(0x2004)\n    assert r.raw == b\"\\x20\\x04\"\n\n    data = b\"\\x05\\x00\\x04\\x40\\x02\\x80\\x01\"\n    r, _ = TuyaData.deserialize(data)\n    assert r.payload == 0x1800240\n\n    r.payload = t.bitmap32(0x10082004)\n    assert r.raw == b\"\\x10\\x08\\x20\\x04\"\n\n\ndef test_tuya_data_bitmap_invalid():\n    \"\"\"Test tuya Bitmap datatype.\"\"\"\n\n    data = b\"\\x05\\x00\\x03\\x4012\"\n    extra = b\"extra data\"\n\n    r, rest = TuyaData.deserialize(data + extra)\n    assert rest == extra\n\n    with pytest.raises(ValueError):\n        r.payload\n\n\ndef test_tuya_data_unknown():\n    \"\"\"Test tuya unknown datatype.\"\"\"\n\n    data = b\"\\x06\\x00\\x04\\x03\\x02\\x01\\x00\"\n    extra = b\"extra data\"\n\n    r, rest = TuyaData.deserialize(data + extra)\n    assert rest == extra\n\n    assert r.dp_type == 6\n    assert r.raw == b\"\\x03\\x02\\x01\\x00\"\n\n    with pytest.raises(ValueError):\n        r.payload\n\n    with pytest.raises(ValueError):\n        r.payload = 0\n\n\n@pytest.mark.parametrize(\n    \"cmd_id, handler_name, args\",\n    (\n        (\n            TUYA_GET_DATA,\n            \"handle_get_data\",\n            (\n                TuyaCommand(\n                    status=0,\n                    tsn=2,\n                    datapoints=[TuyaDatapointData(2, TuyaData(1, 0, b\"\\x01\\x01\"))],\n                ),\n            ),\n        ),\n        (\n            TUYA_SET_DATA_RESPONSE,\n            \"handle_set_data_response\",\n            (\n                TuyaCommand(\n                    status=0,\n                    tsn=2,\n                    datapoints=[TuyaDatapointData(2, TuyaData(1, 0, b\"\\x01\\x01\"))],\n                ),\n            ),\n        ),\n        (\n            TUYA_ACTIVE_STATUS_RPT,\n            \"handle_active_status_report\",\n            (\n                TuyaCommand(\n                    status=0,\n                    tsn=2,\n                    datapoints=[TuyaDatapointData(2, TuyaData(1, 0, b\"\\x01\\x01\"))],\n                ),\n            ),\n        ),\n        (TUYA_SET_TIME, \"handle_set_time_request\", (0x1234,)),\n    ),\n)\n@mock.patch(\"zhaquirks.tuya.TuyaNewManufCluster.send_default_rsp\")\ndef test_tuya_cluster_request(\n    default_rsp_mock, cmd_id, handler_name, args, TuyaCluster\n):\n    \"\"\"Test cluster specific request.\"\"\"\n\n    hdr = zcl_f.ZCLHeader.general(1, cmd_id, direction=zcl_f.Direction.Server_to_Client)\n    hdr.frame_control = hdr.frame_control.replace(disable_default_response=False)\n\n    with mock.patch.object(TuyaCluster, handler_name) as handler:\n        handler.return_value = mock.sentinel.status\n        TuyaCluster.handle_cluster_request(hdr, args)\n        assert handler.call_count == 1\n        assert default_rsp_mock.call_count == 1\n        assert default_rsp_mock.call_args[1][\"status\"] is mock.sentinel.status\n\n\n@mock.patch(\"zhaquirks.tuya.TuyaNewManufCluster.send_default_rsp\")\ndef test_tuya_cluster_request_unk_command(default_rsp_mock, TuyaCluster):\n    \"\"\"Test cluster specific request handler -- no handler.\"\"\"\n\n    hdr = zcl_f.ZCLHeader.general(1, 0xFE, direction=zcl_f.Direction.Server_to_Client)\n    hdr.frame_control = hdr.frame_control.replace(disable_default_response=False)\n\n    TuyaCluster.handle_cluster_request(hdr, (mock.sentinel.args,))\n    assert default_rsp_mock.call_count == 1\n    assert default_rsp_mock.call_args[1][\"status\"] == zcl_f.Status.UNSUP_CLUSTER_COMMAND\n\n\n@mock.patch(\"zhaquirks.tuya.TuyaNewManufCluster.send_default_rsp\")\ndef test_tuya_cluster_request_no_handler(default_rsp_mock, TuyaCluster):\n    \"\"\"Test cluster specific request handler -- no handler.\"\"\"\n\n    hdr = zcl_f.ZCLHeader.general(1, 0xFE, direction=zcl_f.Direction.Server_to_Client)\n    hdr.frame_control = hdr.frame_control.replace(disable_default_response=False)\n\n    new_client_commands = TuyaCluster.client_commands.copy()\n    new_client_commands[0xFE] = zcl_f.ZCLCommandDef(\n        \"no_such_handler\", {}, is_manufacturer_specific=True\n    )\n\n    with mock.patch.object(TuyaCluster, \"client_commands\", new_client_commands):\n        TuyaCluster.handle_cluster_request(hdr, (mock.sentinel.args,))\n\n    assert default_rsp_mock.call_count == 1\n    assert default_rsp_mock.call_args[1][\"status\"] == zcl_f.Status.UNSUP_CLUSTER_COMMAND\n", "metadata": {"source": "github.com/zigpy/zha-device-handlers", "file_path": "tests/test_tuya_clusters.py", "language": "python", "complexity": "complex", "star_count": 930, "quality_score": 0.71, "features": {"has_classes": true, "has_functions": true, "has_async": false, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": true, "has_type_hints": true, "has_testing": true, "has_logging": false, "function_count": 13, "class_count": 1, "line_count": 294}, "authority_level": "medium", "authentic": true}}
{"domain": "code_generation", "category": "error_handling", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create robust error handling with logging and validation with asyncio coroutines\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"Tests for Tuya quirks.\"\"\"\n\nimport datetime\nfrom unittest import mock\n\nimport pytest\nfrom zigpy.zcl import foundation\n\nfrom tests.common import ClusterListener, MockDatetime\nimport zhaquirks\nfrom zhaquirks.tuya import TUYA_MCU_VERSION_RSP, TUYA_SET_TIME, TuyaDPType\nfrom zhaquirks.tuya.mcu import (\n    ATTR_MCU_VERSION,\n    TUYA_MCU_CONNECTION_STATUS,\n    TuyaAttributesCluster,\n    TuyaClusterData,\n    TuyaMCUCluster,\n)\n\nzhaquirks.setup()\n\nZCL_TUYA_VERSION_RSP = b\"\\x09\\x06\\x11\\x01\\x6d\\x82\"\nZCL_TUYA_SET_TIME = b\"\\x09\\x12\\x24\\x0d\\x00\"\n\n\n@pytest.mark.parametrize(\n    \"quirk\", (zhaquirks.tuya.ts0601_dimmer.TuyaDoubleSwitchDimmer,)\n)\n@pytest.mark.parametrize(\n    \"frame, zcl_tsn, mcu_tsn\",\n    (\n        (\n            b\"\\t@%\\t\\x00\",\n            64,\n            9,\n        ),\n        (\n            b\"\\tA%\\x0b\\x00\",\n            65,\n            11,\n        ),\n        (\n            b\"\\tB%\\r\\x00\",\n            66,\n            13,\n        ),\n        (\n            b\"\\t\\x13%/\\x00\",\n            19,\n            47,\n        ),\n        (\n            b\"\\t\\x14%1\\x00\",\n            20,\n            49,\n        ),\n        (\n            b\"\\t\\x15%3\\x00\",\n            21,\n            51,\n        ),\n        (\n            b\"\\t\\x1b%\\xf1\\x00\",\n            27,\n            241,\n        ),\n        (\n            b\"\\t\\x1c%\\xf3\\x00\",\n            28,\n            243,\n        ),\n        (\n            b\"\\t@%\\xb9\\x00\",\n            64,\n            185,\n        ),\n        (\n            b\"\\t'%\\x11\\x00\",\n            39,\n            17,\n        ),\n        (\n            b\"\\tS%\\xec\\x00\",\n            83,\n            236,\n        ),\n        (\n            b\"\\tu%\\xd4\\x00\",\n            117,\n            212,\n        ),\n        (\n            b\"\\tv%\\xd6\\x00\",\n            118,\n            214,\n        ),\n        (\n            b\"\\tl%\\x1e\\x00\",\n            108,\n            30,\n        ),\n    ),\n)\nasync def test_tuya_connection_status(\n    zigpy_device_from_quirk, quirk, frame, zcl_tsn, mcu_tsn\n):\n    \"\"\"Test TUYA_MCU_CONNECTION_STATUS messages.\"\"\"\n\n    tuya_device = zigpy_device_from_quirk(quirk)\n\n    tuya_cluster = tuya_device.endpoints[1].tuya_manufacturer\n    cluster_listener = ClusterListener(tuya_cluster)\n\n    assert len(cluster_listener.attribute_updates) == 0\n\n    # simulate a TUYA_MCU_CONNECTION_STATUS message\n    hdr, args = tuya_cluster.deserialize(frame)\n    assert hdr.command_id == TUYA_MCU_CONNECTION_STATUS\n    assert hdr.tsn == zcl_tsn\n\n    with mock.patch.object(\n        TuyaAttributesCluster, \"command\"\n    ) as m1:  # tuya_cluster parent class (because of super() call)\n        tuya_cluster.handle_message(hdr, args)\n\n        assert len(cluster_listener.cluster_commands) == 1\n        assert cluster_listener.cluster_commands[0][1] == TUYA_MCU_CONNECTION_STATUS\n\n        assert cluster_listener.cluster_commands[0][2].payload.tsn == mcu_tsn\n        assert cluster_listener.cluster_commands[0][2].payload.status == b\"\"\n\n        m1.assert_called_once_with(\n            TUYA_MCU_CONNECTION_STATUS,\n            tuya_cluster.TuyaConnectionStatus(tsn=mcu_tsn, status=b\"\\x01\"),\n            expect_reply=False,\n        )\n\n\n@pytest.mark.parametrize(\n    \"quirk\", (zhaquirks.tuya.ts0601_dimmer.TuyaDoubleSwitchDimmer,)\n)\nasync def test_tuya_version(zigpy_device_from_quirk, quirk):\n    \"\"\"Test TUYA_MCU_VERSION_RSP messages.\"\"\"\n\n    tuya_device = zigpy_device_from_quirk(quirk)\n\n    tuya_cluster = tuya_device.endpoints[1].tuya_manufacturer\n    cluster_listener = ClusterListener(tuya_cluster)\n\n    assert len(cluster_listener.attribute_updates) == 0\n\n    # simulate a TUYA_MCU_VERSION_RSP message\n    hdr, args = tuya_cluster.deserialize(ZCL_TUYA_VERSION_RSP)\n    assert hdr.command_id == TUYA_MCU_VERSION_RSP\n\n    tuya_cluster.handle_message(hdr, args)\n    assert len(cluster_listener.attribute_updates) == 1\n    assert cluster_listener.attribute_updates[0][0] == ATTR_MCU_VERSION\n    assert cluster_listener.attribute_updates[0][1] == \"2.0.2\"\n\n    with mock.patch.object(tuya_cluster, \"handle_mcu_version_response\") as m1:\n        tuya_cluster.handle_message(hdr, args)\n\n        assert len(cluster_listener.cluster_commands) == 2\n        assert cluster_listener.cluster_commands[1][1] == TUYA_MCU_VERSION_RSP\n        assert cluster_listener.cluster_commands[1][2].version.version_raw == 130\n        assert cluster_listener.cluster_commands[1][2].version.version == \"2.0.2\"\n\n        m1.assert_called_once_with(\n            tuya_cluster.MCUVersion(status=1, tsn=109, version_raw=130)\n        )\n\n    # read 'mcu_version' from cluster's attributes\n    succ, fail = await tuya_cluster.read_attributes((\"mcu_version\",))\n    assert succ[\"mcu_version\"] == \"2.0.2\"\n\n\n@pytest.mark.parametrize(\n    \"quirk\", (zhaquirks.tuya.ts0601_dimmer.TuyaDoubleSwitchDimmer,)\n)\nasync def test_tuya_mcu_set_time(zigpy_device_from_quirk, quirk):\n    \"\"\"Test set_time requests (0x24) messages for MCU devices.\"\"\"\n\n    tuya_device = zigpy_device_from_quirk(quirk)\n\n    tuya_cluster = tuya_device.endpoints[1].tuya_manufacturer\n    cluster_listener = ClusterListener(tuya_cluster)\n\n    # Mock datetime\n    origdatetime = datetime.datetime\n    datetime.datetime = MockDatetime\n\n    # simulate a SET_TIME message\n    hdr, args = tuya_cluster.deserialize(ZCL_TUYA_SET_TIME)\n    assert hdr.command_id == TUYA_SET_TIME\n\n    with mock.patch.object(\n        TuyaAttributesCluster, \"command\"\n    ) as m1:  # tuya_cluster parent class (because of super() call)\n        tuya_cluster.handle_message(hdr, args)\n\n        assert len(cluster_listener.cluster_commands) == 1\n        assert cluster_listener.cluster_commands[0][1] == TUYA_SET_TIME\n\n        m1.assert_called_once_with(\n            TUYA_SET_TIME, [0, 0, 28, 32, 0, 0, 14, 16], expect_reply=False\n        )\n\n    # restore datetime\n    datetime.datetime = origdatetime  # restore datetime\n\n\n@pytest.mark.parametrize(\n    \"quirk\", (zhaquirks.tuya.ts0601_dimmer.TuyaDoubleSwitchDimmer,)\n)\nasync def test_tuya_methods(zigpy_device_from_quirk, quirk):\n    \"\"\"Test TUYA_MCU_VERSION_RSP messages.\"\"\"\n\n    tuya_device = zigpy_device_from_quirk(quirk)\n\n    tuya_cluster = tuya_device.endpoints[1].tuya_manufacturer\n    dimmer2_cluster = tuya_device.endpoints[2].level\n    switch1_cluster = tuya_device.endpoints[1].on_off\n\n    tcd_1 = TuyaClusterData(endpoint_id=2, cluster_attr=\"minimum_level\", attr_value=25)\n\n    tcd_switch1_on = TuyaClusterData(\n        endpoint_id=1,\n        cluster_name=\"on_off\",\n        cluster_attr=\"on_off\",\n        attr_value=1,\n        expect_reply=True,\n    )\n    tcd_dimmer2_on = TuyaClusterData(\n        endpoint_id=2,\n        cluster_name=\"on_off\",\n        cluster_attr=\"on_off\",\n        attr_value=1,\n        expect_reply=True,\n    )\n    tcd_dimmer2_off = TuyaClusterData(\n        endpoint_id=2,\n        cluster_name=\"on_off\",\n        cluster_attr=\"on_off\",\n        attr_value=0,\n        expect_reply=True,\n    )\n    tcd_dimmer2_level = TuyaClusterData(\n        endpoint_id=2,\n        cluster_name=\"level\",\n        cluster_attr=\"current_level\",\n        attr_value=75,\n        expect_reply=True,\n    )\n    tcd_dimmer2_level0 = TuyaClusterData(\n        endpoint_id=2,\n        cluster_name=\"level\",\n        cluster_attr=\"current_level\",\n        attr_value=0,\n        expect_reply=True,\n    )\n\n    result_1 = tuya_cluster.from_cluster_data(tcd_1)\n    assert result_1\n    assert len(result_1) == 1\n    assert result_1[0].datapoints\n    assert len(result_1[0].datapoints) == 1\n    assert result_1[0].datapoints[0].dp == 9\n    assert result_1[0].datapoints[0].data.dp_type == TuyaDPType.VALUE\n    assert result_1[0].datapoints[0].data.raw == b\"\\x00\\x00\\x00b\"\n\n    tcd_2 = TuyaClusterData(\n        endpoint_id=7, cluster_attr=\"not_exists_attribute\", attr_value=25\n    )\n    result_2 = tuya_cluster.from_cluster_data(tcd_2)\n    assert not result_2\n\n    with mock.patch.object(tuya_cluster, \"create_catching_task\") as m1:\n        tuya_cluster.tuya_mcu_command(tcd_2)\n        # no DP resolution will not call TUYA_SET_DATA command\n        m1.assert_not_called()\n\n    result_3 = await dimmer2_cluster.command(0x0006)\n    assert result_3.status == foundation.Status.UNSUP_CLUSTER_COMMAND\n\n    with mock.patch.object(tuya_cluster, \"tuya_mcu_command\") as m1:\n        rsp = await switch1_cluster.command(0x0001)\n\n        m1.assert_called_once_with(tcd_switch1_on)\n        assert rsp.status == foundation.Status.SUCCESS\n        assert m1.call_count == 1\n\n        rsp = await switch1_cluster.command(0x0004)\n        m1.assert_called_once_with(tcd_switch1_on)  # no extra calls\n        assert rsp.status == foundation.Status.UNSUP_CLUSTER_COMMAND\n        assert m1.call_count == 1\n\n        # test `move_to_level_with_on_off` quirk (call on_off + current_level)\n        rsp = await dimmer2_cluster.command(0x0004, 75)\n        assert rsp.status == foundation.Status.SUCCESS\n        m1.assert_any_call(tcd_dimmer2_on)  # on_off\n        m1.assert_called_with(tcd_dimmer2_level)  # current_level\n        assert m1.call_count == 3\n\n        # test `move_to_level_with_on_off` quirk (call on_off + current_level)\n        rsp = await dimmer2_cluster.command(\n            0x0004, 75, 0\n        )  # extra args ¿transition time?. Not on_off for sure\n        assert rsp.status == foundation.Status.SUCCESS\n        m1.assert_any_call(tcd_dimmer2_on)  # on_off\n        m1.assert_called_with(tcd_dimmer2_level)  # current_level\n        assert m1.call_count == 5\n\n        # test `move_to_level_with_on_off` quirk (call on_off + current_level)\n        rsp = await dimmer2_cluster.command(0x0004, 0, level=75)\n        assert rsp.status == foundation.Status.SUCCESS\n        m1.assert_any_call(tcd_dimmer2_on)  # on_off\n        m1.assert_called_with(tcd_dimmer2_level)  # current_level\n        assert m1.call_count == 7\n\n        # test `move_to_level_with_on_off` quirk (call on_off only)\n        rsp = await dimmer2_cluster.command(0x0004)\n        assert rsp.status == foundation.Status.SUCCESS\n        m1.assert_called_with(tcd_dimmer2_off)  # on_off\n        assert m1.call_count == 8\n\n        # test `move_to_level` quirk (only call current_level)\n        rsp = await dimmer2_cluster.command(0x0000)\n        assert rsp.status == foundation.Status.SUCCESS\n        m1.assert_called_with(tcd_dimmer2_level0)  # current_level\n        assert m1.call_count == 9\n\n        # test `move_to_level` quirk (only call current_level)\n        rsp = await dimmer2_cluster.command(0x0000, 75)\n        assert rsp.status == foundation.Status.SUCCESS\n        m1.assert_called_with(tcd_dimmer2_level)  # current_level\n        assert m1.call_count == 10\n\n        # test `move_to_level` quirk (only call current_level)\n        rsp = await dimmer2_cluster.command(0x0000, level=75)\n        assert rsp.status == foundation.Status.SUCCESS\n        m1.assert_called_with(tcd_dimmer2_level)  # current_level\n        assert m1.call_count == 11\n\n\nasync def test_tuya_mcu_classes():\n    \"\"\"Test tuya conversion from Data to ztype and reverse.\"\"\"\n\n    # Test TuyaMCUCluster.MCUVersion class\n    mcu_version = TuyaMCUCluster.MCUVersion.deserialize(b\"\\x00\\x03\\x82\")[0]\n    assert mcu_version\n    assert mcu_version.tsn == 3\n    assert mcu_version.version_raw == 130\n    assert mcu_version.version == \"2.0.2\"\n    mcu_version = TuyaMCUCluster.MCUVersion.deserialize(b\"\\x00\\x04\\x01\")[0]\n    assert mcu_version\n    assert mcu_version.version_raw == 1\n    assert mcu_version.version == \"0.0.1\"\n    mcu_version = TuyaMCUCluster.MCUVersion.deserialize(b\"\\x00\\x05\\xff\")[0]\n    assert mcu_version\n    assert mcu_version.version_raw == 255\n    assert mcu_version.version == \"3.3.15\"\n    mcu_version = TuyaMCUCluster.MCUVersion()\n    assert mcu_version\n    assert not mcu_version.version\n\n    # test TuyaClusterData.manufacturer values\n    t_c_d = TuyaClusterData(manufacturer=foundation.ZCLHeader.NO_MANUFACTURER_ID)\n    assert t_c_d.manufacturer == -1\n    t_c_d = TuyaClusterData(manufacturer=4619)\n    assert t_c_d.manufacturer == 4619\n    t_c_d = TuyaClusterData(manufacturer=\"4098\")\n    assert t_c_d.manufacturer == 4098\n    with pytest.raises(ValueError):\n        TuyaClusterData(manufacturer=\"xiaomi\")\n    with pytest.raises(ValueError):\n        TuyaClusterData(manufacturer=b\"\")\n", "metadata": {"source": "github.com/zigpy/zha-device-handlers", "file_path": "tests/test_tuya_mcu.py", "language": "python", "complexity": "complex", "star_count": 930, "quality_score": 0.71, "features": {"has_classes": true, "has_functions": true, "has_async": true, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": true, "has_type_hints": true, "has_testing": true, "has_logging": false, "function_count": 5, "class_count": 2, "line_count": 378}, "authority_level": "medium", "authentic": true}}
{"domain": "code_generation", "category": "error_handling", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate function-based implementation for Create robust error handling with logging and validation with asyncio coroutines\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"Test XBee device.\"\"\"\n\nfrom unittest import mock\n\nimport pytest\nimport zigpy.types as t\nfrom zigpy.zcl import foundation\nfrom zigpy.zcl.clusters.general import AnalogOutput, Basic, LevelControl, OnOff\n\nfrom tests.common import ClusterListener\nimport zhaquirks\nfrom zhaquirks.xbee import (\n    XBEE_AT_ENDPOINT,\n    XBEE_AT_REQUEST_CLUSTER,\n    XBEE_AT_RESPONSE_CLUSTER,\n    XBEE_DATA_CLUSTER,\n    XBEE_DATA_ENDPOINT,\n    XBEE_IO_CLUSTER,\n    XBEE_PROFILE_ID,\n)\nfrom zhaquirks.xbee.xbee3_io import XBee3Sensor\nfrom zhaquirks.xbee.xbee_io import XBeeSensor\n\nzhaquirks.setup()\n\n\nasync def test_basic_cluster(zigpy_device_from_quirk):\n    \"\"\"Test Basic cluster.\"\"\"\n\n    xbee3_device = zigpy_device_from_quirk(XBee3Sensor)\n    assert xbee3_device.model == \"XBee3\"\n\n    basic_cluster = xbee3_device.endpoints[XBEE_DATA_ENDPOINT].in_clusters[\n        Basic.cluster_id\n    ]\n\n    # Check mandatory attributes\n    succ, fail = await basic_cluster.read_attributes((\"zcl_version\", \"power_source\"))\n    assert succ[\"zcl_version\"] == 2\n    assert succ[\"power_source\"] == Basic.PowerSource.Unknown\n\n\nasync def test_digital_output(zigpy_device_from_quirk):\n    \"\"\"Test XBeeOnOff cluster.\"\"\"\n\n    xbee3_device = zigpy_device_from_quirk(XBee3Sensor)\n    onoff_cluster = xbee3_device.endpoints[0xD0].in_clusters[OnOff.cluster_id]\n\n    with mock.patch.object(\n        xbee3_device.application,\n        \"remote_at_command\",\n        create=True,\n        new_callable=mock.AsyncMock,\n    ) as m1:\n        # Turn the switch on\n        _, status = await onoff_cluster.command(1)\n\n        m1.assert_awaited_once()\n        assert m1.await_args[0][1] == \"D0\"\n        assert m1.await_args[0][2] == 5\n\n        succ, fail = await onoff_cluster.read_attributes((\"on_off\",))\n        assert succ[\"on_off\"] == 1\n        m1.reset_mock()\n\n        # Turn the switch off\n        _, status = await onoff_cluster.command(0)\n\n        m1.assert_awaited_once()\n        assert m1.await_args[0][1] == \"D0\"\n        assert m1.await_args[0][2] == 4\n        m1.reset_mock()\n\n        succ, fail = await onoff_cluster.read_attributes((\"on_off\",))\n        assert succ[\"on_off\"] == 0\n\n\nasync def test_analog_output(zigpy_device_from_quirk):\n    \"\"\"Test XBeePWM cluster.\"\"\"\n\n    xbee3_device = zigpy_device_from_quirk(XBee3Sensor)\n    pwm_cluster = xbee3_device.endpoints[0xDA].in_clusters[AnalogOutput.cluster_id]\n\n    # Check mandatory attributes\n    succ, fail = await pwm_cluster.read_attributes(\n        (\n            \"max_present_value\",\n            \"min_present_value\",\n            \"out_of_service\",\n            \"resolution\",\n            \"status_flags\",\n        )\n    )\n    assert succ[\"max_present_value\"] == 1023.0\n    assert succ[\"min_present_value\"] == 0.0\n    assert succ[\"out_of_service\"] == 0\n    assert succ[\"resolution\"] == 1.0\n    assert succ[\"status_flags\"] == 0\n\n    with mock.patch.object(\n        xbee3_device.application,\n        \"remote_at_command\",\n        create=True,\n        new_callable=mock.AsyncMock,\n    ) as m1:\n        # Write value\n        (status,) = await pwm_cluster.write_attributes({\"present_value\": 122.9})\n\n        assert m1.await_count == 2\n        assert m1.await_args_list[0][0][1] == \"M0\"\n        assert m1.await_args_list[0][0][2] == 123\n        assert m1.await_args_list[1][0][1] == \"P0\"\n        assert m1.await_args_list[1][0][2] == 2\n        assert status == [\n            foundation.WriteAttributesStatusRecord(foundation.Status.SUCCESS)\n        ]\n        m1.reset_mock()\n\n        # Write value with numeric attribute id\n        (status,) = await pwm_cluster.write_attributes({0x0055: 234})\n\n        assert m1.await_count == 2\n        assert m1.await_args_list[0][0][1] == \"M0\"\n        assert m1.await_args_list[0][0][2] == 234\n        assert m1.await_args_list[1][0][1] == \"P0\"\n        assert m1.await_args_list[1][0][2] == 2\n        assert status == [\n            foundation.WriteAttributesStatusRecord(foundation.Status.SUCCESS)\n        ]\n        m1.reset_mock()\n\n        # Read value\n        m1.configure_mock(return_value=345)\n\n        succ, fail = await pwm_cluster.read_attributes((\"present_value\",))\n\n        assert succ[\"present_value\"] == 345.0\n        m1.assert_awaited_once()\n        assert m1.await_args[0][1] == \"M0\"\n        assert len(m1.await_args[0]) == 2\n        m1.reset_mock()\n\n\nasync def test_send_serial_data(zigpy_device_from_quirk):\n    \"\"\"Test sending serial data to XBee device.\"\"\"\n\n    xbee3_device = zigpy_device_from_quirk(XBee3Sensor)\n    xbee3_device.application.request.reset_mock()\n\n    # Send serial data\n    _, status = (\n        await xbee3_device.endpoints[XBEE_DATA_ENDPOINT]\n        .out_clusters[XBEE_DATA_CLUSTER]\n        .command(0, \"Test UART data\")\n    )\n\n    xbee3_device.application.request.assert_awaited_once_with(\n        xbee3_device,\n        XBEE_PROFILE_ID,\n        XBEE_DATA_CLUSTER,\n        XBEE_DATA_ENDPOINT,\n        XBEE_DATA_ENDPOINT,\n        1,\n        b\"Test UART data\",\n        expect_reply=False,\n    )\n    assert status == foundation.Status.SUCCESS\n\n\nasync def test_receive_serial_data(zigpy_device_from_quirk):\n    \"\"\"Test receiving serial data to XBee device.\"\"\"\n\n    xbee3_device = zigpy_device_from_quirk(XBee3Sensor)\n\n    listener = mock.MagicMock()\n    xbee3_device.endpoints[XBEE_DATA_ENDPOINT].out_clusters[\n        LevelControl.cluster_id\n    ].add_listener(listener)\n\n    # Receive serial data\n    xbee3_device.packet_received(\n        t.ZigbeePacket(\n            profile_id=XBEE_PROFILE_ID,\n            cluster_id=XBEE_DATA_CLUSTER,\n            src_ep=XBEE_DATA_ENDPOINT,\n            dst_ep=XBEE_DATA_ENDPOINT,\n            data=t.SerializableBytes(b\"Test UART data\"),\n        )\n    )\n\n    listener.zha_send_event.assert_called_once_with(\n        \"receive_data\", {\"data\": \"Test UART data\"}\n    )\n\n\n@pytest.mark.parametrize(\n    \"command_id, request_value, request_data, response_data, response_command, response_value\",\n    (\n        # Read positive int16_t argument\n        (\n            67,\n            None,\n            b\"2\\x00\\x02\\x01\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xfeTP\",\n            b\"\\x01TP\\x00\\x00\\x18\",\n            \"tp_command_response\",\n            24,\n        ),\n        # Read negative int16_t argument\n        (\n            67,\n            None,\n            b\"2\\x00\\x02\\x01\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xfeTP\",\n            b\"\\x01TP\\x00\\xff\\xfc\",\n            \"tp_command_response\",\n            -4,\n        ),\n        # Read string argument\n        (\n            8,\n            None,\n            b\"2\\x00\\x02\\x01\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xfeNI\",\n            b\"\\x01NI\\x00My Test XBee Device\",\n            \"ni_command_response\",\n            b\"My Test XBee Device\",\n        ),\n        # Write string argument\n        (\n            8,\n            b\"My Test XBee Device\",  # TODO: Check how hass sends strings in zha.issue_zigbee_cluster_command\n            b\"2\\x00\\x02\\x01\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xfeNIMy Test XBee Device\",\n            b\"\\x01NI\\x00\",\n            \"ni_command_response\",\n            None,\n        ),\n        # Read uint64_t argument\n        (\n            15,\n            None,\n            b\"2\\x00\\x02\\x01\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xfeOP\",\n            b\"\\x01OP\\x00\\xff\\xee\\xdd\\xcc\\xbb\\xaa\\x99\\x88\",\n            \"op_command_response\",\n            0xFFEEDDCCBBAA9988,\n        ),\n        # Write uint64_t argument\n        (\n            15,\n            0xFEDCBA9876543210,\n            b\"2\\x00\\x02\\x01\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xfeOP\\xfe\\xdc\\xba\\x98\\x76\\x54\\x32\\x10\",\n            b\"\\x01OP\\x00\",\n            \"op_command_response\",\n            None,\n        ),\n        # Read uint32_t argument\n        (\n            1,\n            None,\n            b\"2\\x00\\x02\\x01\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xfeDH\",\n            b\"\\x01DH\\x00\\xff\\xee\\xdd\\xcc\",\n            \"dh_command_response\",\n            0xFFEEDDCC,\n        ),\n        # Write uint32_t argument\n        (\n            2,\n            0,\n            b\"2\\x00\\x02\\x01\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xfeDL\\x00\\x00\\x00\\x00\",\n            b\"\\x01DL\\x00\",\n            \"dl_command_response\",\n            None,\n        ),\n        # Read uint16_t argument\n        (\n            65,\n            None,\n            b\"2\\x00\\x02\\x01\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xfe%V\",\n            b\"\\x01%V\\x00\\x0c\\xe4\",\n            \"percentv_command_response\",\n            3300,\n        ),\n        # Write uint16_t argument\n        (\n            66,\n            2700,\n            b\"2\\x00\\x02\\x01\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xfeV+\\x0a\\x8c\",\n            b\"\\x01V+\\x00\",\n            \"vplus_command_response\",\n            None,\n        ),\n        # Read uint8_t argument\n        (\n            54,\n            None,\n            b\"2\\x00\\x02\\x01\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xfeP0\",\n            b\"\\x01P0\\x00\\x00\",\n            \"p0_command_response\",\n            0,\n        ),\n        # Write uint8_t argument\n        (\n            46,\n            5,\n            b\"2\\x00\\x02\\x01\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xfeD0\\x05\",\n            b\"\\x01D0\\x00\",\n            \"d0_command_response\",\n            None,\n        ),\n        # Read bool argument\n        (\n            31,\n            None,\n            b\"2\\x00\\x02\\x01\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xfePM\",\n            b\"\\x01PM\\x00\\x00\",\n            \"pm_command_response\",\n            False,\n        ),\n        # Write bool argument\n        (\n            31,\n            True,\n            b\"2\\x00\\x02\\x01\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xfePM\\x01\",\n            b\"\\x01PM\\x00\",\n            \"pm_command_response\",\n            None,\n        ),\n        # Command with no arguments\n        (\n            93,\n            None,\n            b\"2\\x00\\x02\\x01\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xfeAS\",\n            b\"\\x01AS\\x00\",\n            \"as_command_response\",\n            None,\n        ),\n    ),\n)\nasync def test_remote_at_non_native(\n    zigpy_device_from_quirk,\n    command_id,\n    request_value,\n    request_data,\n    response_data,\n    response_command,\n    response_value,\n):\n    \"\"\"Test remote AT commands with non-XBee coordinator.\"\"\"\n\n    xbee3_device = zigpy_device_from_quirk(XBee3Sensor)\n\n    listener = mock.MagicMock()\n    xbee3_device.endpoints[XBEE_DATA_ENDPOINT].out_clusters[\n        LevelControl.cluster_id\n    ].add_listener(listener)\n\n    def mock_at_response(*args, **kwargs):\n        \"\"\"Simulate remote AT command response from device.\"\"\"\n        xbee3_device.packet_received(\n            t.ZigbeePacket(\n                profile_id=XBEE_PROFILE_ID,\n                cluster_id=XBEE_AT_RESPONSE_CLUSTER,\n                src_ep=XBEE_AT_ENDPOINT,\n                dst_ep=XBEE_AT_ENDPOINT,\n                data=t.SerializableBytes(response_data),\n            )\n        )\n        return mock.DEFAULT\n\n    xbee3_device.application.request.reset_mock()\n    xbee3_device.application.request.configure_mock(side_effect=mock_at_response)\n\n    # Send remote AT command request\n    _, status = (\n        await xbee3_device.endpoints[XBEE_AT_ENDPOINT]\n        .out_clusters[XBEE_AT_REQUEST_CLUSTER]\n        .command(command_id, request_value)\n    )\n\n    xbee3_device.application.request.configure_mock(side_effect=None)\n\n    xbee3_device.application.request.assert_awaited_once_with(\n        xbee3_device,\n        XBEE_PROFILE_ID,\n        XBEE_AT_REQUEST_CLUSTER,\n        XBEE_AT_ENDPOINT,\n        XBEE_AT_ENDPOINT,\n        1,\n        request_data,\n        expect_reply=False,\n    )\n    assert status == foundation.Status.SUCCESS\n    listener.zha_send_event.assert_called_once_with(\n        response_command, {\"response\": response_value}\n    )\n\n\n@pytest.mark.parametrize(\n    \"command_id, command, request_value, response_value\",\n    (\n        # Read positive int16_t argument\n        (67, \"TP\", None, 24),\n        # Read negative int16_t argument\n        (67, \"TP\", None, -4),\n        # Read string argument\n        (8, \"NI\", None, \"My Test XBee Device\"),\n        # Write string argument\n        (8, \"NI\", \"My Test XBee Device\", None),\n        # Read uint64_t argument\n        (15, \"OP\", None, 0xFFEEDDCCBBAA9988),\n        # Write uint64_t argument\n        (15, \"OP\", 0xFEDCBA9876543210, None),\n        # Read uint32_t argument\n        (1, \"DH\", None, 0xFFEEDDCC),\n        # Write uint32_t argument\n        (2, \"DL\", 0, None),\n        # Read uint16_t argument\n        (65, \"%V\", None, 3300),\n        # Write uint16_t argument\n        (66, \"V+\", 2700, None),\n        # Read uint8_t argument\n        (54, \"P0\", None, 0),\n        # Write uint8_t argument\n        (46, \"D0\", 5, None),\n        # Read bool argument\n        (31, \"PM\", None, False),\n        # Write bool argument\n        (31, \"PM\", True, None),\n        # Command with no arguments\n        (93, \"AS\", None, None),\n    ),\n)\nasync def test_remote_at_native(\n    zigpy_device_from_quirk, command_id, command, request_value, response_value\n):\n    \"\"\"Test remote AT commands with XBee coordinator.\"\"\"\n\n    xbee3_device = zigpy_device_from_quirk(XBee3Sensor)\n\n    listener = mock.MagicMock()\n    xbee3_device.endpoints[XBEE_DATA_ENDPOINT].out_clusters[\n        LevelControl.cluster_id\n    ].add_listener(listener)\n\n    with mock.patch.object(\n        xbee3_device.application,\n        \"remote_at_command\",\n        create=True,\n        new_callable=mock.AsyncMock,\n    ) as m1:\n        m1.configure_mock(return_value=response_value)\n\n        # Send remote AT command request\n        _, status = (\n            await xbee3_device.endpoints[XBEE_AT_ENDPOINT]\n            .out_clusters[XBEE_AT_REQUEST_CLUSTER]\n            .command(command_id, request_value)\n        )\n\n        if request_value is None:\n            m1.assert_awaited_once_with(\n                0x1234, command, apply_changes=True, encryption=False\n            )\n        else:\n            m1.assert_awaited_once_with(\n                0x1234, command, request_value, apply_changes=True, encryption=False\n            )\n        assert status == foundation.Status.SUCCESS\n        listener.zha_send_event.assert_called_once_with(\n            command.replace(\"%V\", \"PercentV\").replace(\"V+\", \"VPlus\").lower()\n            + \"_command_response\",\n            {\"response\": response_value},\n        )\n\n\nasync def test_remote_at_tx_failure(zigpy_device_from_quirk):\n    \"\"\"Test remote AT commands with non-XBee coordinator with tx failure.\"\"\"\n\n    xbee3_device = zigpy_device_from_quirk(XBee3Sensor)\n\n    listener = mock.MagicMock()\n    xbee3_device.endpoints[XBEE_DATA_ENDPOINT].out_clusters[\n        LevelControl.cluster_id\n    ].add_listener(listener)\n\n    def mock_at_response(*args, **kwargs):\n        \"\"\"Simulate remote AT command response from device.\"\"\"\n        xbee3_device.packet_received(\n            t.ZigbeePacket(\n                profile_id=XBEE_PROFILE_ID,\n                cluster_id=XBEE_AT_RESPONSE_CLUSTER,\n                src_ep=XBEE_AT_ENDPOINT,\n                dst_ep=XBEE_AT_ENDPOINT,\n                data=t.SerializableBytes(b\"\\x01TP\\x04\"),\n            )\n        )\n        return mock.DEFAULT\n\n    xbee3_device.application.request.reset_mock()\n    xbee3_device.application.request.configure_mock(side_effect=mock_at_response)\n\n    # Send remote AT command request\n    with pytest.raises(RuntimeError) as excinfo:\n        _, status = (\n            await xbee3_device.endpoints[XBEE_AT_ENDPOINT]\n            .out_clusters[XBEE_AT_REQUEST_CLUSTER]\n            .command(67)\n        )\n\n    assert str(excinfo.value) == \"AT Command response: TX_FAILURE\"\n    xbee3_device.application.request.configure_mock(side_effect=None)\n\n\nasync def test_io_sample_report(zigpy_device_from_quirk):\n    \"\"\"Test DigitalIOCluster cluster.\"\"\"\n\n    xbee3_device = zigpy_device_from_quirk(XBee3Sensor)\n\n    digital_listeners = [\n        ClusterListener(xbee3_device.endpoints[e].on_off) for e in range(0xD0, 0xDF)\n    ]\n    analog_listeners = [\n        ClusterListener(xbee3_device.endpoints[e if e != 0xD4 else 0xD7].analog_input)\n        for e in range(0xD0, 0xD5)\n    ]\n\n    #   {'digital_samples': [1, None, 0, None, 1, None, 0, None, 1, None, 0, None, 1, None, 0], 'analog_samples': [341, None, 682, None, None, None, None, 3305]}\n    xbee3_device.packet_received(\n        t.ZigbeePacket(\n            profile_id=XBEE_PROFILE_ID,\n            cluster_id=XBEE_IO_CLUSTER,\n            src_ep=XBEE_DATA_ENDPOINT,\n            dst_ep=XBEE_DATA_ENDPOINT,\n            data=t.SerializableBytes(\n                b\"\\x01\\x55\\x55\\x85\\x11\\x11\\x01\\x55\\x02\\xaa\\x0c\\xe9\"\n            ),\n        )\n    )\n\n    for i in range(len(digital_listeners)):\n        assert len(digital_listeners[i].cluster_commands) == 0\n        assert len(digital_listeners[i].attribute_updates) == (i + 1) % 2\n        if (i + 1) % 2:\n            assert digital_listeners[i].attribute_updates[0] == (\n                0x0000,\n                (i / 2 + 1) % 2,\n            )\n\n    for i in range(len(analog_listeners)):\n        assert len(analog_listeners[i].cluster_commands) == 0\n        assert len(analog_listeners[i].attribute_updates) == (i + 1) % 2\n        if (i + 1) % 2:\n            assert analog_listeners[i].attribute_updates[0][0] == 0x0055\n\n    assert 33.33333 < analog_listeners[0].attribute_updates[0][1] < 33.33334\n    assert 66.66666 < analog_listeners[2].attribute_updates[0][1] < 66.66667\n    assert analog_listeners[4].attribute_updates[0] == (0x0055, 3.305)\n\n\nasync def test_io_sample_report_on_at_response(zigpy_device_from_quirk):\n    \"\"\"Test update samples on non-native IS command response.\"\"\"\n\n    xbee_device = zigpy_device_from_quirk(XBeeSensor)\n    assert xbee_device.model == \"XBee2\"\n\n    digital_listeners = [\n        ClusterListener(xbee_device.endpoints[e].on_off) for e in range(0xD0, 0xDF)\n    ]\n    analog_listeners = [\n        ClusterListener(xbee_device.endpoints[e if e != 0xD4 else 0xD7].analog_input)\n        for e in range(0xD0, 0xD5)\n    ]\n\n    listener = mock.MagicMock()\n    xbee_device.endpoints[XBEE_DATA_ENDPOINT].out_clusters[\n        LevelControl.cluster_id\n    ].add_listener(listener)\n\n    def mock_at_response(*args, **kwargs):\n        \"\"\"Simulate remote AT command response from device.\"\"\"\n        xbee_device.packet_received(\n            t.ZigbeePacket(\n                profile_id=XBEE_PROFILE_ID,\n                cluster_id=XBEE_AT_RESPONSE_CLUSTER,\n                src_ep=XBEE_AT_ENDPOINT,\n                dst_ep=XBEE_AT_ENDPOINT,\n                data=t.SerializableBytes(\n                    b\"\\x01IS\\x00\\x01\\x55\\x55\\x85\\x11\\x11\\x01\\x55\\x02\\xaa\\x0c\\xe9\"\n                ),\n            )\n        )\n        return mock.DEFAULT\n\n    xbee_device.application.request.reset_mock()\n    xbee_device.application.request.configure_mock(side_effect=mock_at_response)\n\n    # Send remote AT command request\n    _, status = (\n        await xbee_device.endpoints[XBEE_AT_ENDPOINT]\n        .out_clusters[XBEE_AT_REQUEST_CLUSTER]\n        .command(92)\n    )\n\n    xbee_device.application.request.configure_mock(side_effect=None)\n\n    xbee_device.application.request.assert_awaited_once_with(\n        xbee_device,\n        XBEE_PROFILE_ID,\n        XBEE_AT_REQUEST_CLUSTER,\n        XBEE_AT_ENDPOINT,\n        XBEE_AT_ENDPOINT,\n        1,\n        b\"2\\x00\\x02\\x01\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xfeIS\",\n        expect_reply=False,\n    )\n    assert status == foundation.Status.SUCCESS\n    listener.zha_send_event.assert_called_once_with(\n        \"is_command_response\",\n        {\n            \"response\": {\n                \"digital_samples\": [\n                    1,\n                    None,\n                    0,\n                    None,\n                    1,\n                    None,\n                    0,\n                    None,\n                    1,\n                    None,\n                    0,\n                    None,\n                    1,\n                    None,\n                    0,\n                ],\n                \"analog_samples\": [341, None, 682, None, None, None, None, 3305],\n            }\n        },\n    )\n\n    for i in range(len(digital_listeners)):\n        assert len(digital_listeners[i].cluster_commands) == 0\n        assert len(digital_listeners[i].attribute_updates) == (i + 1) % 2\n        if (i + 1) % 2:\n            assert digital_listeners[i].attribute_updates[0] == (\n                0x0000,\n                (i / 2 + 1) % 2,\n            )\n\n    for i in range(len(analog_listeners)):\n        assert len(analog_listeners[i].cluster_commands) == 0\n        assert len(analog_listeners[i].attribute_updates) == (i + 1) % 2\n        if (i + 1) % 2:\n            assert analog_listeners[i].attribute_updates[0][0] == 0x0055\n\n    assert 33.33333 < analog_listeners[0].attribute_updates[0][1] < 33.33334\n    assert 66.66666 < analog_listeners[2].attribute_updates[0][1] < 66.66667\n    assert analog_listeners[4].attribute_updates[0] == (0x0055, 3.305)\n\n\n@mock.patch(\"zigpy.zdo.ZDO.handle_ieee_addr_req\")\nasync def test_zdo(handle_mgmt_lqi_resp, zigpy_device_from_quirk):\n    \"\"\"Test receiving ZDO data from XBee device.\"\"\"\n\n    xbee3_device = zigpy_device_from_quirk(XBee3Sensor)\n\n    # Receive ZDOCmd.IEEE_addr_req\n    xbee3_device.packet_received(\n        t.ZigbeePacket(\n            profile_id=0,\n            cluster_id=0x01,\n            src_ep=0,\n            dst_ep=0,\n            data=t.SerializableBytes(b\"\\x07\\x34\\x12\\x00\\x00\"),\n        )\n    )\n\n    assert handle_mgmt_lqi_resp.call_count == 1\n    assert len(handle_mgmt_lqi_resp.call_args_list[0][0]) == 4\n    assert handle_mgmt_lqi_resp.call_args_list[0][0][0].tsn == 0x07\n    assert handle_mgmt_lqi_resp.call_args_list[0][0][0].command_id == 0x0001\n    assert handle_mgmt_lqi_resp.call_args_list[0][0][1] == 0x1234\n    assert handle_mgmt_lqi_resp.call_args_list[0][0][2] == 0\n    assert handle_mgmt_lqi_resp.call_args_list[0][0][3] == 0\n", "metadata": {"source": "github.com/zigpy/zha-device-handlers", "file_path": "tests/test_xbee.py", "language": "python", "complexity": "complex", "star_count": 930, "quality_score": 0.71, "features": {"has_classes": false, "has_functions": true, "has_async": true, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": true, "has_type_hints": true, "has_testing": true, "has_logging": true, "function_count": 14, "class_count": 0, "line_count": 684}, "authority_level": "medium", "authentic": true}}
{"domain": "code_generation", "category": "error_handling", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create robust error handling with logging and validation with asyncio coroutines\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"Quirks implementations for the ZHA component of Homeassistant.\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\nimport importlib\nimport importlib.util\nimport logging\nimport pathlib\nimport pkgutil\nimport sys\nimport typing\nfrom typing import Any\n\nimport zigpy.device\nimport zigpy.endpoint\nfrom zigpy.quirks import DEVICE_REGISTRY, CustomCluster, CustomDevice\nimport zigpy.types as t\nfrom zigpy.util import ListenableMixin\nfrom zigpy.zcl import foundation\nfrom zigpy.zcl.clusters.general import PowerConfiguration\nfrom zigpy.zcl.clusters.measurement import OccupancySensing\nfrom zigpy.zcl.clusters.security import IasZone\nfrom zigpy.zdo import types as zdotypes\n\nfrom .const import (\n    ATTRIBUTE_ID,\n    ATTRIBUTE_NAME,\n    CLUSTER_COMMAND,\n    COMMAND_ATTRIBUTE_UPDATED,\n    DEVICE_TYPE,\n    ENDPOINTS,\n    INPUT_CLUSTERS,\n    MANUFACTURER,\n    MODEL,\n    MODELS_INFO,\n    MOTION_EVENT,\n    NODE_DESCRIPTOR,\n    OCCUPANCY_EVENT,\n    OCCUPANCY_STATE,\n    OFF,\n    ON,\n    OUTPUT_CLUSTERS,\n    PROFILE_ID,\n    UNKNOWN,\n    VALUE,\n    ZHA_SEND_EVENT,\n    ZONE_STATUS_CHANGE_COMMAND,\n)\n\n_LOGGER = logging.getLogger(__name__)\n\n\nclass Bus(ListenableMixin):\n    \"\"\"Event bus implementation.\"\"\"\n\n    def __init__(self, *args, **kwargs):\n        \"\"\"Init event bus.\"\"\"\n        super().__init__(*args, **kwargs)\n        self._listeners = {}\n\n\nclass LocalDataCluster(CustomCluster):\n    \"\"\"Cluster meant to prevent remote calls.\n\n    Set _CONSTANT_ATTRIBUTES to provide constant values for attribute ids.\n    Set _VALID_ATTRIBUTES to provide a list of valid attribute ids that will never be shown as unsupported.\n    These are attributes that should be populated later.\n    \"\"\"\n\n    _CONSTANT_ATTRIBUTES: dict[int, typing.Any] = {}\n    _VALID_ATTRIBUTES: set[int] = set()\n\n    async def bind(self):\n        \"\"\"Prevent bind.\"\"\"\n        self.debug(\"binding LocalDataCluster\")\n        return (foundation.Status.SUCCESS,)\n\n    async def unbind(self):\n        \"\"\"Prevent unbind.\"\"\"\n        self.debug(\"unbinding LocalDataCluster\")\n        return (foundation.Status.SUCCESS,)\n\n    async def _configure_reporting(self, *args, **kwargs):  # pylint: disable=W0221\n        \"\"\"Prevent remote configure reporting.\"\"\"\n        self.debug(\"configuring reporting for LocalDataCluster\")\n        return (foundation.ConfigureReportingResponse.deserialize(b\"\\x00\")[0],)\n\n    async def read_attributes_raw(self, attributes, manufacturer=None, **kwargs):\n        \"\"\"Prevent remote reads.\"\"\"\n        msg = \"reading attributes for LocalDataCluster\"\n        self.debug(f\"{msg}: attributes={attributes} manufacturer={manufacturer}\")\n        records = [\n            foundation.ReadAttributeRecord(\n                attr, foundation.Status.UNSUPPORTED_ATTRIBUTE, foundation.TypeValue()\n            )\n            for attr in attributes\n        ]\n        for record in records:\n            if record.attrid in self._CONSTANT_ATTRIBUTES:\n                record.value.value = self._CONSTANT_ATTRIBUTES[record.attrid]\n            else:\n                record.value.value = self._attr_cache.get(record.attrid)\n            if (\n                record.value.value is not None\n                or record.attrid in self._VALID_ATTRIBUTES\n            ):\n                record.status = foundation.Status.SUCCESS\n        return (records,)\n\n    async def write_attributes(self, attributes, manufacturer=None, **kwargs):\n        \"\"\"Prevent remote writes.\"\"\"\n        msg = \"writing attributes for LocalDataCluster\"\n        self.debug(f\"{msg}: attributes={attributes} manufacturer={manufacturer}\")\n        for attrid, value in attributes.items():\n            if isinstance(attrid, str):\n                attrid = self.attributes_by_name[attrid].id\n            elif attrid not in self.attributes:\n                self.error(\"%d is not a valid attribute id\", attrid)\n                continue\n            self._update_attribute(attrid, value)\n        return ([foundation.WriteAttributesStatusRecord(foundation.Status.SUCCESS)],)\n\n\nclass EventableCluster(CustomCluster):\n    \"\"\"Cluster that generates events.\"\"\"\n\n    def handle_cluster_request(\n        self,\n        hdr: foundation.ZCLHeader,\n        args: list[Any],\n        *,\n        dst_addressing: None\n        | (t.Addressing.Group | t.Addressing.IEEE | t.Addressing.NWK) = None,\n    ):\n        \"\"\"Send cluster requests as events.\"\"\"\n        if (\n            self.server_commands is not None\n            and self.server_commands.get(hdr.command_id) is not None\n        ):\n            self.listener_event(\n                ZHA_SEND_EVENT,\n                self.server_commands.get(hdr.command_id, (hdr.command_id)).name,\n                args,\n            )\n\n    def _update_attribute(self, attrid, value):\n        super()._update_attribute(attrid, value)\n\n        if attrid in self.attributes:\n            attribute_name = self.attributes[attrid].name\n        else:\n            attribute_name = UNKNOWN\n\n        self.listener_event(\n            ZHA_SEND_EVENT,\n            COMMAND_ATTRIBUTE_UPDATED,\n            {\n                ATTRIBUTE_ID: attrid,\n                ATTRIBUTE_NAME: attribute_name,\n                VALUE: value,\n            },\n        )\n\n\nclass GroupBoundCluster(CustomCluster):\n    \"\"\"Cluster that can only bind to a group instead of direct to hub.\n\n    Binding this cluster results in binding to a group that the coordinator\n    is a member of.\n    \"\"\"\n\n    COORDINATOR_GROUP_ID = 0x30  # Group id with only coordinator as a member\n\n    async def bind(self):\n        \"\"\"Bind cluster to a group.\"\"\"\n        # Ensure coordinator is a member of the group\n        application = self._endpoint.device.application\n        coordinator = application.get_device(application.state.node_info.ieee)\n        await coordinator.add_to_group(\n            self.COORDINATOR_GROUP_ID,\n            name=\"Coordinator Group - Created by ZHAQuirks\",\n        )\n\n        # Bind cluster to group\n        dstaddr = zdotypes.MultiAddress()\n        dstaddr.addrmode = 1\n        dstaddr.nwk = self.COORDINATOR_GROUP_ID\n        dstaddr.endpoint = self._endpoint.endpoint_id\n        return await self._endpoint.device.zdo.Bind_req(\n            self._endpoint.device.ieee,\n            self._endpoint.endpoint_id,\n            self.cluster_id,\n            dstaddr,\n        )\n\n\nclass DoublingPowerConfigurationCluster(CustomCluster, PowerConfiguration):\n    \"\"\"PowerConfiguration cluster implementation.\n\n    This implementation doubles battery pct remaining for non standard devices\n    that don't follow the reporting spec.\n    \"\"\"\n\n    BATTERY_PERCENTAGE_REMAINING = 0x0021\n\n    def _update_attribute(self, attrid, value):\n        if attrid == self.BATTERY_PERCENTAGE_REMAINING:\n            value = value * 2\n        super()._update_attribute(attrid, value)\n\n\nclass PowerConfigurationCluster(CustomCluster, PowerConfiguration):\n    \"\"\"Common use power configuration cluster.\"\"\"\n\n    BATTERY_VOLTAGE_ATTR = 0x0020\n    BATTERY_PERCENTAGE_REMAINING = 0x0021\n    MIN_VOLTS = 1.5  # old 2.1\n    MAX_VOLTS = 2.8  # old 3.2\n\n    def _update_attribute(self, attrid, value):\n        super()._update_attribute(attrid, value)\n        if attrid == self.BATTERY_VOLTAGE_ATTR and value not in (0, 255):\n            super()._update_attribute(\n                self.BATTERY_PERCENTAGE_REMAINING,\n                self._calculate_battery_percentage(value),\n            )\n\n    def _calculate_battery_percentage(self, raw_value):\n        volts = raw_value / 10\n        volts = max(volts, self.MIN_VOLTS)\n        volts = min(volts, self.MAX_VOLTS)\n\n        percent = round(\n            ((volts - self.MIN_VOLTS) / (self.MAX_VOLTS - self.MIN_VOLTS)) * 200\n        )\n\n        self.debug(\n            \"Voltage [RAW]:%s [Max]:%s [Min]:%s, Battery Percent: %s\",\n            raw_value,\n            self.MAX_VOLTS,\n            self.MIN_VOLTS,\n            percent / 2,\n        )\n\n        return percent\n\n\nclass _Motion(CustomCluster, IasZone):\n    \"\"\"Self reset Motion cluster.\"\"\"\n\n    reset_s: int = 30\n\n    def __init__(self, *args, **kwargs):\n        \"\"\"Init.\"\"\"\n        super().__init__(*args, **kwargs)\n        self._loop = asyncio.get_running_loop()\n        self._timer_handle = None\n\n    def _turn_off(self):\n        self._timer_handle = None\n        self.debug(\"%s - Resetting motion sensor\", self.endpoint.device.ieee)\n        self.listener_event(\n            CLUSTER_COMMAND, 253, ZONE_STATUS_CHANGE_COMMAND, [OFF, 0, 0, 0]\n        )\n\n\nclass MotionWithReset(_Motion):\n    \"\"\"Self reset Motion cluster.\n\n    Optionally send event over device bus.\n    \"\"\"\n\n    send_occupancy_event: bool = False\n\n    def handle_cluster_request(\n        self,\n        hdr: foundation.ZCLHeader,\n        args: list[Any],\n        *,\n        dst_addressing: None\n        | (t.Addressing.Group | t.Addressing.IEEE | t.Addressing.NWK) = None,\n    ):\n        \"\"\"Handle the cluster command.\"\"\"\n        # check if the command is for a zone status change of ZoneStatus.Alarm_1 or ZoneStatus.Alarm_2\n        if hdr.command_id == ZONE_STATUS_CHANGE_COMMAND and args[0] & 3:\n            if self._timer_handle:\n                self._timer_handle.cancel()\n            self._timer_handle = self._loop.call_later(self.reset_s, self._turn_off)\n            if self.send_occupancy_event:\n                self.endpoint.device.occupancy_bus.listener_event(OCCUPANCY_EVENT)\n\n\nclass MotionOnEvent(_Motion):\n    \"\"\"Motion based on received events from occupancy.\"\"\"\n\n    reset_s: int = 120\n\n    def __init__(self, *args, **kwargs):\n        \"\"\"Init.\"\"\"\n        super().__init__(*args, **kwargs)\n        self.endpoint.device.motion_bus.add_listener(self)\n\n    def motion_event(self):\n        \"\"\"Motion event.\"\"\"\n        super().listener_event(\n            CLUSTER_COMMAND, 254, ZONE_STATUS_CHANGE_COMMAND, [ON, 0, 0, 0]\n        )\n\n        self.debug(\"%s - Received motion event message\", self.endpoint.device.ieee)\n\n        if self._timer_handle:\n            self._timer_handle.cancel()\n\n        self._timer_handle = self._loop.call_later(self.reset_s, self._turn_off)\n\n\nclass _Occupancy(CustomCluster, OccupancySensing):\n    \"\"\"Self reset Occupancy cluster.\"\"\"\n\n    reset_s: int = 600\n\n    def __init__(self, *args, **kwargs):\n        \"\"\"Init.\"\"\"\n        super().__init__(*args, **kwargs)\n        self._timer_handle = None\n        self._loop = asyncio.get_running_loop()\n\n    def _turn_off(self):\n        self._timer_handle = None\n        self._update_attribute(OCCUPANCY_STATE, OFF)\n\n\nclass OccupancyOnEvent(_Occupancy):\n    \"\"\"Self reset occupancy from bus.\"\"\"\n\n    def __init__(self, *args, **kwargs):\n        \"\"\"Init.\"\"\"\n        super().__init__(*args, **kwargs)\n        self.endpoint.device.occupancy_bus.add_listener(self)\n\n    def occupancy_event(self):\n        \"\"\"Occupancy event.\"\"\"\n        self._update_attribute(OCCUPANCY_STATE, ON)\n\n        if self._timer_handle:\n            self._timer_handle.cancel()\n\n        self._timer_handle = self._loop.call_later(self.reset_s, self._turn_off)\n\n\nclass OccupancyWithReset(_Occupancy):\n    \"\"\"Self reset Occupancy cluster and send event on motion bus.\"\"\"\n\n    def _update_attribute(self, attrid, value):\n        super()._update_attribute(attrid, value)\n\n        if attrid == OCCUPANCY_STATE and value == ON:\n            if self._timer_handle:\n                self._timer_handle.cancel()\n            self.endpoint.device.motion_bus.listener_event(MOTION_EVENT)\n            self._timer_handle = self._loop.call_later(self.reset_s, self._turn_off)\n\n\nclass QuickInitDevice(CustomDevice):\n    \"\"\"Devices with quick initialization from quirk signature.\"\"\"\n\n    signature: dict[str, Any] | None = None\n\n    @classmethod\n    def from_signature(\n        cls, device: zigpy.device.Device, model: str | None = None\n    ) -> zigpy.device.Device:\n        \"\"\"Update device accordingly to quirk signature.\"\"\"\n\n        assert isinstance(cls.signature, dict)\n        if model is None:\n            model = cls.signature[MODEL]\n        manufacturer = cls.signature.get(MANUFACTURER)\n        if manufacturer is None:\n            manufacturer = cls.signature[MODELS_INFO][0][0]\n\n        device.node_desc = cls.signature[NODE_DESCRIPTOR]\n\n        endpoints = cls.signature[ENDPOINTS]\n        for ep_id, ep_data in endpoints.items():\n            endpoint = device.add_endpoint(ep_id)\n            endpoint.profile_id = ep_data[PROFILE_ID]\n            endpoint.device_type = ep_data[DEVICE_TYPE]\n            for cluster_id in ep_data[INPUT_CLUSTERS]:\n                cluster = endpoint.add_input_cluster(cluster_id)\n                if cluster.ep_attribute == \"basic\":\n                    manuf_attr_id = cluster.attributes_by_name[MANUFACTURER].id\n                    cluster._update_attribute(  # pylint: disable=W0212\n                        manuf_attr_id, manufacturer\n                    )\n                    cluster._update_attribute(  # pylint: disable=W0212\n                        cluster.attributes_by_name[MODEL].id, model\n                    )\n            for cluster_id in ep_data[OUTPUT_CLUSTERS]:\n                endpoint.add_output_cluster(cluster_id)\n            endpoint.status = zigpy.endpoint.Status.ZDO_INIT\n\n        device.status = zigpy.device.Status.ENDPOINTS_INIT\n        device.manufacturer = manufacturer\n        device.model = model\n\n        return device\n\n\nclass NoReplyMixin:\n    \"\"\"A simple mixin.\n\n    Allows a cluster to have configurable list of command\n    ids that do not generate an explicit reply.\n    \"\"\"\n\n    void_input_commands: set[int] = {}\n\n    async def command(self, command, *args, expect_reply=None, **kwargs):\n        \"\"\"Override the default Cluster command.\n\n        expect_reply behavior is based on void_input_commands.\n        Note that this method changes the default value of\n        expect_reply to None. This allows the caller to explicitly force\n        expect_reply to true.\n        \"\"\"\n\n        if expect_reply is None and command in self.void_input_commands:\n            cmd_expect_reply = False\n        elif expect_reply is None:\n            cmd_expect_reply = True  # the default\n        else:\n            cmd_expect_reply = expect_reply\n\n        rsp = await super().command(\n            command, *args, expect_reply=cmd_expect_reply, **kwargs\n        )\n\n        if expect_reply is None and command in self.void_input_commands:\n            # Pretend we received a default reply\n            return foundation.GENERAL_COMMANDS[\n                foundation.GeneralCommand.Default_Response\n            ].schema(command_id=command, status=foundation.Status.SUCCESS)\n\n        return rsp\n\n\ndef setup(custom_quirks_path: str | None = None) -> None:\n    \"\"\"Register all quirks with zigpy, including optional custom quirks.\"\"\"\n\n    if custom_quirks_path is not None:\n        DEVICE_REGISTRY.purge_custom_quirks(custom_quirks_path)\n\n    # Import all quirks in the `zhaquirks` package first\n    for _importer, modname, _ispkg in pkgutil.walk_packages(\n        path=__path__,\n        prefix=__name__ + \".\",\n    ):\n        _LOGGER.debug(\"Loading quirks module %r\", modname)\n        importlib.import_module(modname)\n\n    if custom_quirks_path is None:\n        return\n\n    path = pathlib.Path(custom_quirks_path)\n    _LOGGER.debug(\"Loading custom quirks from %r\", path)\n\n    loaded = False\n\n    # Treat the custom quirk path (e.g. `/config/custom_quirks/`) itself as a module\n    for importer, modname, _ispkg in pkgutil.walk_packages(path=[str(path)]):\n        _LOGGER.debug(\"Loading custom quirk module %r\", modname)\n\n        try:\n            spec = importer.find_spec(modname)\n            module = importlib.util.module_from_spec(spec)\n            sys.modules[modname] = module\n            spec.loader.exec_module(module)\n        except Exception:\n            _LOGGER.exception(\"Unexpected exception importing custom quirk %r\", modname)\n        else:\n            loaded = True\n\n    if loaded:\n        _LOGGER.warning(\n            \"Loaded custom quirks. Please contribute them to\"\n            \" https://github.com/zigpy/zha-device-handlers\"\n        )\n", "metadata": {"source": "github.com/zigpy/zha-device-handlers", "file_path": "zhaquirks/__init__.py", "language": "python", "complexity": "complex", "star_count": 930, "quality_score": 0.77, "features": {"has_classes": true, "has_functions": true, "has_async": true, "has_decorators": true, "has_imports": true, "has_error_handling": true, "has_documentation": true, "has_type_hints": true, "has_testing": true, "has_logging": true, "function_count": 25, "class_count": 14, "line_count": 490}, "authority_level": "medium", "authentic": true}}
{"domain": "code_generation", "category": "error_handling", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create robust error handling with logging and validation with asyncio coroutines\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"Module to handle quirks of the Danfoss thermostat.\n\nManufacturer specific attributes to control displaying and specific configuration.\n\nZCL attributes supported:\n    0x0201 - ThermostatProgrammingOperationMode (0x0025):\n        Danfoss writes in a presentation document that it implemented a preheat function with the second bit,\n        but this is contradicted by a detailed and up-to-date document.\n    all    - ClusterRevision (0xFFFD)\n\n    0x0201 - PIHeatingDemand (0x0008),\n    0x0201 - MinHeatSetpointLimit (0x0015)\n    0x0201 - MaxHeatSetpointLimit (0x0016)\n    0x0201 - SetpointChangeSource (0x0030)\n    0x0201 - AbsMinHeatSetpointLimit (0x0003)=5\n    0x0201 - AbsMaxHeatSetpointLimit (0x0004)=35\n    0x0201 - StartOfWeek (0x0020)=Monday\n    0x0201 - NumberOfWeeklyTransitions (0x0021)=42\n    0x0201 - NumberOfDailyTransitions (0x0022)=6\n    0x0204 - KeypadLockout (0x0001)\n\nZCL commands supported:\n    0x0201 - SetWeeklySchedule (0x01)\n    0x0201 - GetWeeklySchedule (0x02)\n    0x0201 - ClearWeeklySchedule (0x03)\n\nBroken ZCL attributes:\n    0x0204 - TemperatureDisplayMode (0x0000): Writing doesn't seem to do anything\n\"\"\"\n\nfrom collections.abc import Callable\nfrom datetime import UTC, datetime\nimport time\nfrom typing import Any\n\nfrom zigpy import types\nfrom zigpy.profiles import zha\nfrom zigpy.quirks import CustomCluster, CustomDevice\nfrom zigpy.zcl.clusters.general import (\n    Basic,\n    Identify,\n    Ota,\n    PollControl,\n    PowerConfiguration,\n    Time,\n)\nfrom zigpy.zcl.clusters.homeautomation import Diagnostic\nfrom zigpy.zcl.clusters.hvac import Thermostat, UserInterface\nfrom zigpy.zcl.foundation import ZCLAttributeDef, ZCLCommandDef\n\nfrom zhaquirks.const import (\n    DEVICE_TYPE,\n    ENDPOINTS,\n    INPUT_CLUSTERS,\n    MODELS_INFO,\n    OUTPUT_CLUSTERS,\n    PROFILE_ID,\n)\nfrom zhaquirks.quirk_ids import DANFOSS_ALLY_THERMOSTAT\n\nDANFOSS = \"Danfoss\"\nHIVE = DANFOSS\nPOPP = \"D5X84YU\"\n\noccupied_heating_setpoint = Thermostat.AttributeDefs.occupied_heating_setpoint\nsystem_mode = Thermostat.AttributeDefs.system_mode\nmin_heat_setpoint_limit = Thermostat.AttributeDefs.min_heat_setpoint_limit\n\n\nclass DanfossViewingDirectionEnum(types.enum8):\n    \"\"\"Default (button above screen when looking at it) or Inverted (button below screen when looking at it).\"\"\"\n\n    Default = 0x00\n    Inverted = 0x01\n\n\nclass DanfossAdaptationRunControlEnum(types.enum8):\n    \"\"\"Initiate or Cancel adaptation run.\"\"\"\n\n    Nothing = 0x00  # not documented in all documentation, but in some places and seems to work\n    Initiate = 0x01\n    Cancel = 0x02\n\n\nclass DanfossExerciseDayOfTheWeekEnum(types.enum8):\n    \"\"\"Day of the week.\"\"\"\n\n    Sunday = 0\n    Monday = 1\n    Tuesday = 2\n    Wednesday = 3\n    Thursday = 4\n    Friday = 5\n    Saturday = 6\n    Undefined = 7\n\n\nclass DanfossOpenWindowDetectionEnum(types.enum8):\n    \"\"\"Danfoss open window detection judgments.\"\"\"\n\n    Quarantine = 0x00\n    Closed = 0x01\n    Maybe = 0x02\n    Open = 0x03\n    External = 0x04\n\n\nclass DanfossSoftwareErrorCodeBitmap(types.bitmap16):\n    \"\"\"Danfoss software error code bitmap.\"\"\"\n\n    Top_pcb_sensor_error = 0x0001\n    Side_pcb_sensor_error = 0x0002\n    Non_volatile_memory_error = 0x0004\n    Unknown_hw_error = 0x0008\n    # 0x0010 = N/A\n    Motor_error = 0x0020\n    # 0x0040 = N/A\n    Invalid_internal_communication = 0x0080\n    # 0x0100 = N/A\n    Invalid_clock_information = 0x0200\n    # 0x0400 = N/A\n    Radio_communication_error = 0x0800\n    Encoder_jammed = 0x1000\n    Low_battery = 0x2000\n    Critical_low_battery = 0x4000\n    # 0x8000 = Reserved\n\n\nclass DanfossAdaptationRunStatusBitmap(types.bitmap8):\n    \"\"\"Danfoss Adaptation run status bitmap.\"\"\"\n\n    In_progress = 0x0001\n    Valve_characteristic_found = 0x0002\n    Valve_characteristic_lost = 0x0004\n\n\nclass DanfossAdaptationRunSettingsBitmap(types.bitmap8):\n    \"\"\"Danfoss Adaptation run settings bitmap.\"\"\"\n\n    Disabled = 0x00  # Undocumented, but seems to work\n    Enabled = 0x01\n\n\nclass DanfossSetpointCommandEnum(types.enum8):\n    \"\"\"Set behaviour to change the setpoint.\"\"\"\n\n    Schedule = 0  # relatively slow\n    User_interaction = 1  # aggressive change\n    Preheat = 2  # invisible to user\n\n\nclass DanfossPreheatCommandEnum(types.enum8):\n    \"\"\"Set behaviour of preheat command.\n\n    Only one option available, but other values are possible in the future.\n    \"\"\"\n\n    Force = 0\n\n\nclass CustomizedStandardCluster(CustomCluster):\n    \"\"\"Danfoss customized standard clusters by adding custom attributes.\n\n    Danfoss doesn't allow all standard attributes when manufacturer specific is requested.\n    Therefore, this subclass separates manufacturer specific and standard attributes for Zigbee commands allowing\n    manufacturer specific to be passed for specific attributes, but not for standard attributes.\n    \"\"\"\n\n    @staticmethod\n    def combine_results(*result_lists):\n        \"\"\"Combine results from 1 or more result lists from zigbee commands.\"\"\"\n        success_global = []\n        failure_global = []\n        for result in result_lists:\n            if len(result) == 1:\n                success_global.extend(result[0])\n            elif len(result) == 2:\n                success_global.extend(result[0])\n                failure_global.extend(result[1])\n\n        if failure_global:\n            return [success_global, failure_global]\n        else:\n            return [success_global]\n\n    async def split_command(\n        self,\n        records: list[Any],\n        func: Callable,\n        extract_attrid: Callable[[Any], int],\n        *args,\n        **kwargs,\n    ):\n        \"\"\"Split execution of command in one for manufacturer specific and one for standard attributes.\"\"\"\n        records_specific = [\n            e\n            for e in records\n            if self.attributes[extract_attrid(e)].is_manufacturer_specific\n        ]\n        records_standard = [\n            e\n            for e in records\n            if not self.attributes[extract_attrid(e)].is_manufacturer_specific\n        ]\n\n        result_specific = (\n            await func(records_specific, *args, **kwargs) if records_specific else []\n        )\n\n        result_standard = (\n            await func(records_standard, *args, **kwargs) if records_standard else []\n        )\n\n        return self.combine_results(result_specific, result_standard)\n\n    async def _configure_reporting(self, records, *args, **kwargs):\n        \"\"\"Configure reporting ZCL foundation command.\"\"\"\n        return await self.split_command(\n            records, super()._configure_reporting, lambda x: x.attrid, *args, **kwargs\n        )\n\n    async def _read_attributes(self, attr_ids, *args, **kwargs):\n        \"\"\"Read attributes ZCL foundation command.\"\"\"\n        return await self.split_command(\n            attr_ids, super()._read_attributes, lambda x: x, *args, **kwargs\n        )\n\n\nclass DanfossThermostatCluster(CustomizedStandardCluster, Thermostat):\n    \"\"\"Danfoss cluster for standard and proprietary danfoss attributes.\"\"\"\n\n    class ServerCommandDefs(Thermostat.ServerCommandDefs):\n        \"\"\"Server Command Definitions.\"\"\"\n\n        setpoint_command = ZCLCommandDef(\n            id=0x40,\n            schema={\n                \"type\": DanfossSetpointCommandEnum,\n                \"heating_setpoint\": types.int16s,\n            },\n            is_manufacturer_specific=True,\n        )\n\n        preheat_command = ZCLCommandDef(\n            id=0x42,\n            schema={\"force\": DanfossPreheatCommandEnum, \"timestamp\": types.uint32_t},\n            is_manufacturer_specific=True,\n        )\n\n    class AttributeDefs(Thermostat.AttributeDefs):\n        \"\"\"Attribute Definitions.\"\"\"\n\n        open_window_detection = ZCLAttributeDef(  # etrv_open_window_detection\n            id=0x4000,\n            type=DanfossOpenWindowDetectionEnum,\n            access=\"rp\",\n            is_manufacturer_specific=True,\n        )\n        external_open_window_detected = ZCLAttributeDef(\n            id=0x4003, type=types.Bool, access=\"rw\", is_manufacturer_specific=True\n        )  # non-configurable reporting\n        exercise_day_of_week = ZCLAttributeDef(\n            id=0x4010,\n            type=DanfossExerciseDayOfTheWeekEnum,\n            access=\"rw\",\n            is_manufacturer_specific=True,\n        )\n        exercise_trigger_time = ZCLAttributeDef(\n            id=0x4011, type=types.uint16_t, access=\"rw\", is_manufacturer_specific=True\n        )\n        mounting_mode_active = ZCLAttributeDef(\n            id=0x4012, type=types.Bool, access=\"rp\", is_manufacturer_specific=True\n        )\n        mounting_mode_control = ZCLAttributeDef(\n            id=0x4013, type=types.Bool, access=\"rw\", is_manufacturer_specific=True\n        )  # non-configurable reporting\n        orientation = ZCLAttributeDef(  # etrv_orientation (Horizontal = False and Vertical = True)\n            id=0x4014, type=types.Bool, access=\"rw\", is_manufacturer_specific=True\n        )  # non-configurable reporting\n        external_measured_room_sensor = ZCLAttributeDef(\n            id=0x4015, type=types.int16s, access=\"rw\", is_manufacturer_specific=True\n        )\n        radiator_covered = ZCLAttributeDef(\n            id=0x4016, type=types.Bool, access=\"rw\", is_manufacturer_specific=True\n        )  # non-configurable reporting\n        control_algorithm_scale_factor = (\n            ZCLAttributeDef(  # values in [0x01, 0x0A] and disabled by 0x1X\n                id=0x4020,\n                type=types.uint8_t,\n                access=\"rw\",\n                is_manufacturer_specific=True,\n            )\n        )  # non-configurable reporting\n        heat_available = ZCLAttributeDef(\n            id=0x4030, type=types.Bool, access=\"rw\", is_manufacturer_specific=True\n        )  # non-configurable reporting\n        heat_required = ZCLAttributeDef(  # heat_supply_request\n            id=0x4031, type=types.Bool, access=\"rp\", is_manufacturer_specific=True\n        )\n        load_balancing_enable = ZCLAttributeDef(\n            id=0x4032, type=types.Bool, access=\"rw\", is_manufacturer_specific=True\n        )  # non-configurable reporting\n        load_room_mean = ZCLAttributeDef(  # load_radiator_room_mean\n            id=0x4040, type=types.int16s, access=\"rw\", is_manufacturer_specific=True\n        )  # non-configurable reporting (according to the documentation, you cannot read it, but it works anyway)\n        load_estimate = ZCLAttributeDef(  # load_estimate_on_this_radiator\n            id=0x404A, type=types.int16s, access=\"rp\", is_manufacturer_specific=True\n        )\n        regulation_setpoint_offset = ZCLAttributeDef(\n            id=0x404B, type=types.int8s, access=\"rw\", is_manufacturer_specific=True\n        )\n        adaptation_run_control = ZCLAttributeDef(\n            id=0x404C,\n            type=DanfossAdaptationRunControlEnum,\n            access=\"rw\",\n            is_manufacturer_specific=True,\n        )  # non-configurable reporting\n        adaptation_run_status = ZCLAttributeDef(\n            id=0x404D,\n            type=DanfossAdaptationRunStatusBitmap,\n            access=\"rp\",\n            is_manufacturer_specific=True,\n        )\n        adaptation_run_settings = ZCLAttributeDef(\n            id=0x404E,\n            type=DanfossAdaptationRunSettingsBitmap,\n            access=\"rw\",\n            is_manufacturer_specific=True,\n        )\n        preheat_status = ZCLAttributeDef(\n            id=0x404F, type=types.Bool, access=\"rp\", is_manufacturer_specific=True\n        )\n        preheat_time = ZCLAttributeDef(\n            id=0x4050, type=types.uint32_t, access=\"rp\", is_manufacturer_specific=True\n        )\n        window_open_feature = ZCLAttributeDef(  # window_open_feature_on_off\n            id=0x4051, type=types.Bool, access=\"rw\", is_manufacturer_specific=True\n        )  # non-configurable reporting\n\n    async def write_attributes(self, attributes, manufacturer=None):\n        \"\"\"There are 2 types of setpoint changes: Fast and Slow.\n\n        Fast is used for immediate changes; this is done using a command (setpoint_command).\n        Slow is used for scheduled changes; this is done using an attribute (occupied_heating_setpoint).\n        In case of a change on occupied_heating_setpoint, a setpoint_command is used.\n\n        Thermostatic radiator valves from Danfoss cannot be turned off to prevent damage during frost.\n        This is emulated by setting setpoint to the minimum setpoint.\n        \"\"\"\n\n        fast_setpoint_change = None\n\n        if occupied_heating_setpoint.name in attributes:\n            # Store setpoint for use in command\n            fast_setpoint_change = attributes[occupied_heating_setpoint.name]\n\n        if attributes.get(system_mode.name) == system_mode.type.Off:\n            # Just turn setpoint down to minimum temperature using fast_setpoint_change\n            fast_setpoint_change = self._attr_cache[min_heat_setpoint_limit.id]\n            attributes[occupied_heating_setpoint.name] = fast_setpoint_change\n            attributes[system_mode.name] = system_mode.type.Heat\n\n        # Attributes cannot be empty, because write_res cannot be empty, but it can contain unrequested items\n        write_res = await super().write_attributes(\n            attributes, manufacturer=manufacturer\n        )\n\n        if fast_setpoint_change is not None:\n            # On Danfoss a fast setpoint change is done through a command\n            await self.setpoint_command(\n                DanfossSetpointCommandEnum.User_interaction,\n                fast_setpoint_change,\n                manufacturer=manufacturer,\n            )\n\n        return write_res\n\n    async def bind(self):\n        \"\"\"According to the documentation of Zigbee2MQTT there is a bug in the Danfoss firmware with the time.\n\n        It doesn't request it, so it has to be fed the correct time.\n        \"\"\"\n        await self.endpoint.time.write_time()\n\n        return await super().bind()\n\n\nclass DanfossUserInterfaceCluster(CustomizedStandardCluster, UserInterface):\n    \"\"\"Danfoss cluster for standard and proprietary danfoss attributes.\"\"\"\n\n    class AttributeDefs(UserInterface.AttributeDefs):\n        \"\"\"Attribute Definitions.\"\"\"\n\n        viewing_direction = ZCLAttributeDef(\n            id=0x4000,\n            type=DanfossViewingDirectionEnum,\n            access=\"rw\",\n            is_manufacturer_specific=True,\n        )  # non-configurable reporting\n\n\nclass DanfossDiagnosticCluster(CustomizedStandardCluster, Diagnostic):\n    \"\"\"Danfoss cluster for standard and proprietary danfoss attributes.\"\"\"\n\n    class AttributeDefs(Diagnostic.AttributeDefs):\n        \"\"\"Attribute Definitions.\"\"\"\n\n        sw_error_code = ZCLAttributeDef(\n            id=0x4000,\n            type=DanfossSoftwareErrorCodeBitmap,\n            access=\"rpw\",\n            is_manufacturer_specific=True,\n        )\n        wake_time_avg = ZCLAttributeDef(\n            id=0x4001, type=types.uint32_t, access=\"r\", is_manufacturer_specific=True\n        )\n        wake_time_max_duration = ZCLAttributeDef(\n            id=0x4002, type=types.uint32_t, access=\"r\", is_manufacturer_specific=True\n        )\n        wake_time_min_duration = ZCLAttributeDef(\n            id=0x4003, type=types.uint32_t, access=\"r\", is_manufacturer_specific=True\n        )\n        sleep_postponed_count_avg = ZCLAttributeDef(\n            id=0x4004, type=types.uint32_t, access=\"r\", is_manufacturer_specific=True\n        )\n        sleep_postponed_count_max = ZCLAttributeDef(\n            id=0x4005, type=types.uint32_t, access=\"r\", is_manufacturer_specific=True\n        )\n        sleep_postponed_count_min = ZCLAttributeDef(\n            id=0x4006, type=types.uint32_t, access=\"r\", is_manufacturer_specific=True\n        )\n        motor_step_counter = ZCLAttributeDef(\n            id=0x4010, type=types.uint32_t, access=\"rp\", is_manufacturer_specific=True\n        )\n\n        data_logger = ZCLAttributeDef(\n            id=0x4020,\n            type=types.LimitedLVBytes(50),\n            access=\"rpw\",\n            is_manufacturer_specific=True,\n        )\n        control_diagnostics = ZCLAttributeDef(\n            id=0x4021,\n            type=types.LimitedLVBytes(30),\n            access=\"rp\",\n            is_manufacturer_specific=True,\n        )\n        control_diagnostics_frequency = ZCLAttributeDef(\n            id=0x4022, type=types.uint16_t, access=\"rw\", is_manufacturer_specific=True\n        )  # non-configurable reporting\n\n\nclass DanfossTimeCluster(CustomizedStandardCluster, Time):\n    \"\"\"Danfoss cluster for fixing the time.\"\"\"\n\n    async def write_time(self):\n        \"\"\"Write time info to Time Cluster.\n\n        It supports adjusting for daylight saving time, but this is not trivial to retrieve with the modules:\n            zoneinfo, datetime or time\n        \"\"\"\n        epoch = datetime(2000, 1, 1, 0, 0, 0, 0, tzinfo=UTC)\n        current_time = (datetime.now(UTC) - epoch).total_seconds()\n\n        await self.write_attributes(\n            {\n                \"time\": current_time,\n                \"time_status\": 0b00000010,  # only bit 1 can be set\n                \"time_zone\": time.timezone,\n            }\n        )\n\n    async def bind(self):\n        \"\"\"According to the documentation of Zigbee2MQTT there is a bug in the Danfoss firmware with the time.\n\n        It doesn't request it, so it has to be fed the correct time.\n        \"\"\"\n        result = await super().bind()\n        await self.write_time()\n        return result\n\n\nclass DanfossThermostat(CustomDevice):\n    \"\"\"DanfossThermostat custom device.\"\"\"\n\n    quirk_id = DANFOSS_ALLY_THERMOSTAT\n\n    manufacturer_code = 0x1246\n\n    signature = {\n        # <SimpleDescriptor endpoint=1 profile=260 device_type=769\n        # device_version=0 input_clusters=[0, 1, 3, 10, 32, 513, 516, 1026, 2821]\n        # output_clusters=[0, 25]>\n        MODELS_INFO: [\n            (DANFOSS, \"eTRV0100\"),\n            (DANFOSS, \"eTRV0101\"),\n            (DANFOSS, \"eTRV0103\"),\n            (POPP, \"eT093WRO\"),\n            (POPP, \"eT093WRG\"),\n            (HIVE, \"TRV001\"),\n            (HIVE, \"TRV003\"),\n        ],\n        ENDPOINTS: {\n            1: {\n                PROFILE_ID: zha.PROFILE_ID,\n                DEVICE_TYPE: zha.DeviceType.THERMOSTAT,\n                INPUT_CLUSTERS: [\n                    Basic.cluster_id,\n                    PowerConfiguration.cluster_id,\n                    Identify.cluster_id,\n                    Time.cluster_id,\n                    PollControl.cluster_id,\n                    Thermostat.cluster_id,\n                    UserInterface.cluster_id,\n                    Diagnostic.cluster_id,\n                ],\n                OUTPUT_CLUSTERS: [Basic.cluster_id, Ota.cluster_id],\n            }\n        },\n    }\n\n    replacement = {\n        ENDPOINTS: {\n            1: {\n                INPUT_CLUSTERS: [\n                    Basic.cluster_id,\n                    PowerConfiguration.cluster_id,\n                    Identify.cluster_id,\n                    PollControl.cluster_id,\n                    DanfossTimeCluster,\n                    DanfossThermostatCluster,\n                    DanfossUserInterfaceCluster,\n                    DanfossDiagnosticCluster,\n                ],\n                OUTPUT_CLUSTERS: [Basic.cluster_id, Ota.cluster_id],\n            }\n        }\n    }\n", "metadata": {"source": "github.com/zigpy/zha-device-handlers", "file_path": "zhaquirks/danfoss/thermostat.py", "language": "python", "complexity": "complex", "star_count": 930, "quality_score": 0.74, "features": {"has_classes": true, "has_functions": true, "has_async": true, "has_decorators": true, "has_imports": true, "has_error_handling": false, "has_documentation": true, "has_type_hints": true, "has_testing": false, "has_logging": true, "function_count": 8, "class_count": 20, "line_count": 539}, "authority_level": "medium", "authentic": true}}
{"domain": "code_generation", "category": "error_handling", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create robust error handling with logging and validation with asyncio coroutines\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"Develco Air Quality Sensor.\"\"\"\n\nimport logging\n\nfrom zigpy.profiles import zha\nfrom zigpy.quirks import CustomCluster, CustomDevice\nimport zigpy.types as t\nfrom zigpy.zcl.clusters.general import (\n    Basic,\n    Identify,\n    OnOff,\n    Ota,\n    PollControl,\n    PowerConfiguration,\n    Scenes,\n    Time,\n)\nfrom zigpy.zcl.clusters.measurement import RelativeHumidity, TemperatureMeasurement\n\nfrom zhaquirks import Bus, LocalDataCluster\nfrom zhaquirks.const import (\n    DEVICE_TYPE,\n    ENDPOINTS,\n    INPUT_CLUSTERS,\n    MODELS_INFO,\n    OUTPUT_CLUSTERS,\n    PROFILE_ID,\n)\nfrom zhaquirks.develco import DEVELCO, DevelcoPowerConfiguration\n\nMANUFACTURER = 0x1015\nVOC_MEASURED_VALUE = 0x0000\nVOC_MIN_MEASURED_VALUE = 0x0001\nVOC_MAX_MEASURED_VALUE = 0x0002\nVOC_RESOLUTION = 0x0003\n\nVOC_REPORTED = \"voc_reported\"\nMIN_VOC_REPORTED = \"min_voc_reported\"\nMAX_VOC_REPORTED = \"max_voc_reported\"\nVOC_RESOLUTION_REPORTED = \"voc_resolution_reported\"\n\n_LOGGER = logging.getLogger(__name__)\n\n\nclass DevelcoVOCMeasurement(CustomCluster):\n    \"\"\"Input Cluster to route manufacturer specific VOC cluster to actual VOC cluster.\"\"\"\n\n    cluster_id = 0xFC03\n    name = \"VOC Level\"\n    ep_attribute = \"voc_level\"\n    attributes = {\n        VOC_MEASURED_VALUE: (\"measured_value\", t.uint16_t, True),\n        VOC_MIN_MEASURED_VALUE: (\"min_measured_value\", t.uint16_t, True),\n        VOC_MAX_MEASURED_VALUE: (\"max_measured_value\", t.uint16_t, True),\n        VOC_RESOLUTION: (\"resolution\", t.uint16_t, True),\n    }\n    server_commands = {}\n    client_commands = {}\n\n    def __init__(self, *args, **kwargs):\n        \"\"\"Init.\"\"\"\n        self._current_state = {}\n        super().__init__(*args, **kwargs)\n        self.endpoint.device.app_cluster = self\n\n    def _update_attribute(self, attrid, value):\n        super()._update_attribute(attrid, value)\n        if attrid == VOC_MEASURED_VALUE and value is not None:\n            self.endpoint.device.voc_bus.listener_event(VOC_REPORTED, value)\n        if attrid == VOC_MIN_MEASURED_VALUE and value is not None:\n            self.endpoint.device.voc_bus.listener_event(MIN_VOC_REPORTED, value)\n        if attrid == VOC_MAX_MEASURED_VALUE and value is not None:\n            self.endpoint.device.voc_bus.listener_event(MAX_VOC_REPORTED, value)\n        if attrid == VOC_RESOLUTION and value is not None:\n            self.endpoint.device.voc_bus.listener_event(VOC_RESOLUTION_REPORTED, value)\n        _LOGGER.debug(\n            \"%s Develco VOC : [%s]\",\n            self.endpoint.device.ieee,\n            self._attr_cache,\n        )\n\n\nclass DevelcoRelativeHumidity(CustomCluster, RelativeHumidity):\n    \"\"\"Handles invalid values for Humidity.\"\"\"\n\n    def _update_attribute(self, attrid, value):\n        # Drop values out of specified range (0-100% RH)\n        if 0 <= value <= 10000:\n            super()._update_attribute(attrid, value)\n        _LOGGER.debug(\n            \"%s Develco Humidity : [%s]\",\n            self.endpoint.device.ieee,\n            self._attr_cache,\n        )\n\n\nclass DevelcoTemperatureMeasurement(CustomCluster, TemperatureMeasurement):\n    \"\"\"Handles invalid values for Temperature.\"\"\"\n\n    def _update_attribute(self, attrid, value):\n        # Drop values out of specified range (0-50°C)\n        if 0 <= value <= 5000:\n            super()._update_attribute(attrid, value)\n        _LOGGER.debug(\n            \"%s Develco Temperature : [%s]\",\n            self.endpoint.device.ieee,\n            self._attr_cache,\n        )\n\n\nclass EmulatedVOCMeasurement(LocalDataCluster):\n    \"\"\"VOC measurement cluster to receive reports from the Develco VOC cluster.\"\"\"\n\n    cluster_id = 0x042E\n    name = \"VOC Level\"\n    ep_attribute = \"voc_level\"\n    attributes = {\n        VOC_MEASURED_VALUE: (\"measured_value\", t.uint16_t, True),\n        VOC_MIN_MEASURED_VALUE: (\"min_measured_value\", t.uint16_t, True),\n        VOC_MAX_MEASURED_VALUE: (\"max_measured_value\", t.uint16_t, True),\n        VOC_RESOLUTION: (\"resolution\", t.uint16_t, True),\n    }\n    MEASURED_VALUE_ID = 0x0000\n    MIN_MEASURED_VALUE_ID = 0x0001\n    MAX_MEASURED_VALUE_ID = 0x0002\n    RESOLUTION_ID = 0x0003\n\n    def __init__(self, *args, **kwargs):\n        \"\"\"Init.\"\"\"\n        super().__init__(*args, **kwargs)\n        self.endpoint.device.voc_bus.add_listener(self)\n\n    async def bind(self):\n        \"\"\"Bind cluster.\"\"\"\n        result = await self.endpoint.device.app_cluster.bind()\n        return result\n\n    async def write_attributes(self, attributes, manufacturer=None):\n        \"\"\"Ignore write_attributes.\"\"\"\n        return (0,)\n\n    def _update_attribute(self, attrid, value):\n        # Drop values out of specified range (0-60000 ppb)\n        if 0 <= value <= 60000:\n            # Convert ppb into mg/m³ approximation according to develco spec\n            value = value * 0.0000045\n            super()._update_attribute(attrid, value)\n\n    def voc_reported(self, value):\n        \"\"\"VOC reported.\"\"\"\n        self._update_attribute(self.MEASURED_VALUE_ID, value)\n\n    def min_voc_reported(self, value):\n        \"\"\"Minimum Measured VOC reported.\"\"\"\n        self._update_attribute(self.MIN_MEASURED_VALUE_ID, value)\n\n    def max_voc_reported(self, value):\n        \"\"\"Maximum Measured VOC reported.\"\"\"\n        self._update_attribute(self.MAX_MEASURED_VALUE_ID, value)\n\n    def voc_resolution_reported(self, value):\n        \"\"\"VOC Resolution reported.\"\"\"\n        self._update_attribute(self.RESOLUTION_ID, value)\n\n\nclass AQSZB110(CustomDevice):\n    \"\"\"Custom device Develco air quality sensor.\"\"\"\n\n    manufacturer_id_override = MANUFACTURER\n\n    def __init__(self, *args, **kwargs):\n        \"\"\"Init.\"\"\"\n        self.voc_bus = Bus()\n        super().__init__(*args, **kwargs)\n\n    signature = {\n        # <SimpleDescriptor endpoint=1 profile=49353 device_type=1 device_version=1\n        # input_clusters=[3, 5, 6] output_clusters=[]>\n        # <SimpleDescriptor endpoint=38 profile=260 device_type=770 device_version=0\n        # input_clusters=[0, 1, 3, 32, 1026, 1029, 64515] output_clusters=[3, 10, 25]>\n        MODELS_INFO: [\n            (DEVELCO, \"AQSZB-110\"),\n            (\"frient A/S\", \"AQSZB-110\"),\n        ],\n        ENDPOINTS: {\n            1: {\n                PROFILE_ID: 0xC0C9,\n                DEVICE_TYPE: 1,\n                INPUT_CLUSTERS: [\n                    Identify.cluster_id,\n                    Scenes.cluster_id,\n                    OnOff.cluster_id,\n                ],\n                OUTPUT_CLUSTERS: [],\n            },\n            38: {\n                PROFILE_ID: zha.PROFILE_ID,\n                DEVICE_TYPE: zha.DeviceType.TEMPERATURE_SENSOR,\n                INPUT_CLUSTERS: [\n                    Basic.cluster_id,\n                    PowerConfiguration.cluster_id,\n                    Identify.cluster_id,\n                    PollControl.cluster_id,\n                    TemperatureMeasurement.cluster_id,\n                    RelativeHumidity.cluster_id,\n                    0xFC03,\n                ],\n                OUTPUT_CLUSTERS: [Identify.cluster_id, Time.cluster_id, Ota.cluster_id],\n            },\n        },\n    }\n\n    replacement = {\n        ENDPOINTS: {\n            1: {\n                PROFILE_ID: 0xC0C9,\n                DEVICE_TYPE: 1,\n                INPUT_CLUSTERS: [\n                    Identify.cluster_id,\n                    Scenes.cluster_id,\n                    OnOff.cluster_id,\n                ],\n                OUTPUT_CLUSTERS: [],\n            },\n            38: {\n                PROFILE_ID: zha.PROFILE_ID,\n                DEVICE_TYPE: zha.DeviceType.TEMPERATURE_SENSOR,\n                INPUT_CLUSTERS: [\n                    Basic.cluster_id,\n                    DevelcoPowerConfiguration,\n                    Identify.cluster_id,\n                    PollControl.cluster_id,\n                    DevelcoTemperatureMeasurement,\n                    DevelcoRelativeHumidity,\n                    DevelcoVOCMeasurement,\n                    EmulatedVOCMeasurement,\n                ],\n                OUTPUT_CLUSTERS: [Identify.cluster_id, Time.cluster_id, Ota.cluster_id],\n            },\n        },\n    }\n", "metadata": {"source": "github.com/zigpy/zha-device-handlers", "file_path": "zhaquirks/develco/air_quality.py", "language": "python", "complexity": "complex", "star_count": 930, "quality_score": 0.6799999999999999, "features": {"has_classes": true, "has_functions": true, "has_async": true, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": true, "has_type_hints": true, "has_testing": false, "has_logging": true, "function_count": 13, "class_count": 5, "line_count": 242}, "authority_level": "medium", "authentic": true}}
{"domain": "code_generation", "category": "error_handling", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create robust error handling with logging and validation with asyncio coroutines\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"Eurotronic devices.\"\"\"\n\nimport logging\n\nfrom zigpy.quirks import CustomCluster\nimport zigpy.types as t\nfrom zigpy.zcl import foundation\nfrom zigpy.zcl.clusters.hvac import Thermostat\n\nEUROTRONIC = \"Eurotronic\"\n\nTHERMOSTAT_CHANNEL = \"thermostat\"\n\nMANUFACTURER = 0x1037  # 4151\n\nOCCUPIED_HEATING_SETPOINT_ATTR = 0x0012\nCTRL_SEQ_OF_OPER_ATTR = 0x001B\nSYSTEM_MODE_ATTR = 0x001C\n\nTRV_MODE_ATTR = 0x4000\nSET_VALVE_POS_ATTR = 0x4001\nERRORS_ATTR = 0x4002\nCURRENT_TEMP_SETPOINT_ATTR = 0x4003\nHOST_FLAGS_ATTR = 0x4008\n\n\n# Host Flags\n# unknown (defaults to 1)       = 0b00000001 # 1\nMIRROR_SCREEN_FLAG = 0b00000010  # 2\nBOOST_FLAG = 0b00000100  # 4\n# unknown                       = 0b00001000 # 8\nCLR_OFF_MODE_FLAG = 0b00010000  # 16\nSET_OFF_MODE_FLAG = 0b00100000  # 32, reported back as 16\n# unknown                       = 0b01000000 # 64\nCHILD_LOCK_FLAG = 0b10000000  # 128\n\n\n_LOGGER = logging.getLogger(__name__)\n\n\nclass ThermostatCluster(CustomCluster, Thermostat):\n    \"\"\"Thermostat cluster.\"\"\"\n\n    attributes = Thermostat.attributes.copy()\n    attributes.update(\n        {\n            TRV_MODE_ATTR: (\"trv_mode\", t.enum8, True),\n            SET_VALVE_POS_ATTR: (\"set_valve_position\", t.uint8_t, True),\n            ERRORS_ATTR: (\"errors\", t.uint8_t, True),\n            CURRENT_TEMP_SETPOINT_ATTR: (\n                \"current_temperature_setpoint\",\n                t.int16s,\n                True,\n            ),\n            HOST_FLAGS_ATTR: (\"host_flags\", t.uint24_t, True),\n        }\n    )\n\n    def _update_attribute(self, attrid, value):\n        _LOGGER.debug(\"update attribute %04x to %s... \", attrid, value)\n\n        if attrid == CURRENT_TEMP_SETPOINT_ATTR:\n            super()._update_attribute(OCCUPIED_HEATING_SETPOINT_ATTR, value)\n        elif attrid == HOST_FLAGS_ATTR:\n            if value & CLR_OFF_MODE_FLAG == CLR_OFF_MODE_FLAG:\n                super()._update_attribute(SYSTEM_MODE_ATTR, 0x0)\n                _LOGGER.debug(\"set system_mode to [off ]\")\n            else:\n                super()._update_attribute(SYSTEM_MODE_ATTR, 0x4)\n                _LOGGER.debug(\"set system_mode to [heat]\")\n\n        _LOGGER.debug(\"update attribute %04x to %s... [ ok ]\", attrid, value)\n        super()._update_attribute(attrid, value)\n\n    async def read_attributes_raw(self, attributes, manufacturer=None, **kwargs):\n        \"\"\"Override wrong attribute reports from the thermostat.\"\"\"\n        success = []\n        error = []\n\n        if CTRL_SEQ_OF_OPER_ATTR in attributes:\n            rar = foundation.ReadAttributeRecord(\n                CTRL_SEQ_OF_OPER_ATTR, foundation.Status.SUCCESS, foundation.TypeValue()\n            )\n            rar.value.value = 0x2\n            success.append(rar)\n\n        if SYSTEM_MODE_ATTR in attributes:\n            rar = foundation.ReadAttributeRecord(\n                SYSTEM_MODE_ATTR, foundation.Status.SUCCESS, foundation.TypeValue()\n            )\n            rar.value.value = 0x4\n            success.append(rar)\n\n        if OCCUPIED_HEATING_SETPOINT_ATTR in attributes:\n            _LOGGER.debug(\"intercepting OCC_HS\")\n\n            values = await super().read_attributes_raw(\n                [CURRENT_TEMP_SETPOINT_ATTR], manufacturer=MANUFACTURER, **kwargs\n            )\n\n            if len(values) == 2:\n                current_temp_setpoint = values[1][0]\n                current_temp_setpoint.attrid = OCCUPIED_HEATING_SETPOINT_ATTR\n\n                error.extend(values[1])\n            else:\n                current_temp_setpoint = values[0][0]\n                current_temp_setpoint.attrid = OCCUPIED_HEATING_SETPOINT_ATTR\n\n                success.extend(values[0])\n\n        attributes = list(\n            filter(\n                lambda x: x\n                not in (\n                    CTRL_SEQ_OF_OPER_ATTR,\n                    SYSTEM_MODE_ATTR,\n                    OCCUPIED_HEATING_SETPOINT_ATTR,\n                ),\n                attributes,\n            )\n        )\n\n        if attributes:\n            values = await super().read_attributes_raw(\n                attributes, manufacturer, **kwargs\n            )\n\n            success.extend(values[0])\n\n            if len(values) == 2:\n                error.extend(values[1])\n\n        return success, error\n\n    async def write_attributes(self, attributes, manufacturer=None, **kwargs):\n        \"\"\"Override wrong writes to thermostat attributes.\"\"\"\n        if \"system_mode\" in attributes:\n            host_flags = self._attr_cache.get(HOST_FLAGS_ATTR, 1)\n            _LOGGER.debug(\"current host_flags: %s\", host_flags)\n\n            if attributes.get(\"system_mode\") == 0x0:\n                return await super().write_attributes(\n                    {\"host_flags\": host_flags | SET_OFF_MODE_FLAG}, MANUFACTURER\n                )\n            if attributes.get(\"system_mode\") == 0x4:\n                return await super().write_attributes(\n                    {\"host_flags\": host_flags | CLR_OFF_MODE_FLAG}, MANUFACTURER\n                )\n\n        return await super().write_attributes(attributes, manufacturer, **kwargs)\n", "metadata": {"source": "github.com/zigpy/zha-device-handlers", "file_path": "zhaquirks/eurotronic/__init__.py", "language": "python", "complexity": "complex", "star_count": 930, "quality_score": 0.6799999999999999, "features": {"has_classes": true, "has_functions": true, "has_async": true, "has_decorators": false, "has_imports": true, "has_error_handling": false, "has_documentation": true, "has_type_hints": true, "has_testing": false, "has_logging": true, "function_count": 3, "class_count": 1, "line_count": 152}, "authority_level": "medium", "authentic": true}}
{"domain": "code_generation", "category": "error_handling", "prompt": "### Code_Generation Request (CLAUDE.md):\nCreate class-based implementation for Create robust error handling with logging and validation with asyncio coroutines\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"Ikea module.\"\"\"\n\nimport logging\n\nfrom zigpy.quirks import CustomCluster\nimport zigpy.types as t\nfrom zigpy.zcl import foundation\nfrom zigpy.zcl.clusters.general import Basic, PowerConfiguration, Scenes\n\nfrom zhaquirks import EventableCluster\nfrom zhaquirks.const import BatterySize\n\n_LOGGER = logging.getLogger(__name__)\n\nIKEA = \"IKEA of Sweden\"\nIKEA_CLUSTER_ID = 0xFC7C  # decimal = 64636\nWWAH_CLUSTER_ID = 0xFC57  # decimal = 64599 ('Works with all Hubs' cluster)\n\nIKEA_SHORTCUT_CLUSTER_V1_ID = 0xFC7F  # decimal = 64639 Shortcut V1 commands\nIKEA_MATTER_SWITCH_CLUSTER_ID = 0xFC80  # decimal = 64640 Shortcut V2 commands\nCOMMAND_SHORTCUT_V1 = \"shortcut_v1_events\"\n\n# PowerConfiguration cluster attributes\nBATTERY_VOLTAGE = PowerConfiguration.attributes_by_name[\"battery_voltage\"].id\nBATTERY_SIZE = PowerConfiguration.attributes_by_name[\"battery_size\"].id\nBATTERY_QUANTITY = PowerConfiguration.attributes_by_name[\"battery_quantity\"].id\nBATTERY_RATED_VOLTAGE = PowerConfiguration.attributes_by_name[\n    \"battery_rated_voltage\"\n].id\n\n\nclass ScenesCluster(CustomCluster, Scenes):\n    \"\"\"Ikea Scenes cluster.\"\"\"\n\n    server_commands = Scenes.server_commands.copy()\n    server_commands.update(\n        {\n            0x0007: foundation.ZCLCommandDef(\n                \"press\",\n                {\"param1\": t.int16s, \"param2\": t.int8s, \"param3\": t.int8s},\n                False,\n                is_manufacturer_specific=True,\n            ),\n            0x0008: foundation.ZCLCommandDef(\n                \"hold\",\n                {\"param1\": t.int16s, \"param2\": t.int8s},\n                False,\n                is_manufacturer_specific=True,\n            ),\n            0x0009: foundation.ZCLCommandDef(\n                \"release\",\n                {\n                    \"param1\": t.int16s,\n                },\n                False,\n                is_manufacturer_specific=True,\n            ),\n        }\n    )\n\n\nclass ShortcutV1Cluster(EventableCluster):\n    \"\"\"Ikea Shortcut Button Cluster Variant 1.\"\"\"\n\n    cluster_id = IKEA_SHORTCUT_CLUSTER_V1_ID\n\n    server_commands = {\n        0x01: foundation.ZCLCommandDef(\n            COMMAND_SHORTCUT_V1,\n            {\n                \"shortcut_button\": t.int8s,\n                \"shortcut_event\": t.int8s,\n            },\n            False,\n            is_manufacturer_specific=True,\n        ),\n    }\n\n\nclass ShortcutV2Cluster(EventableCluster):\n    \"\"\"Ikea Shortcut Button Cluster Variant 2.\"\"\"\n\n    cluster_id = IKEA_MATTER_SWITCH_CLUSTER_ID\n\n    server_commands = {\n        0x00: foundation.ZCLCommandDef(\n            \"switch_latched\",\n            {\n                \"new_position\": t.int8s,\n            },\n            False,\n            is_manufacturer_specific=True,\n        ),\n        0x01: foundation.ZCLCommandDef(\n            \"initial_press\",\n            {\n                \"new_position\": t.int8s,\n            },\n            False,\n            is_manufacturer_specific=True,\n        ),\n        0x02: foundation.ZCLCommandDef(\n            \"long_press\",\n            {\n                \"previous_position\": t.int8s,\n            },\n            False,\n            is_manufacturer_specific=True,\n        ),\n        0x03: foundation.ZCLCommandDef(\n            \"short_release\",\n            {\n                \"previous_position\": t.int8s,\n            },\n            False,\n            is_manufacturer_specific=True,\n        ),\n        0x04: foundation.ZCLCommandDef(\n            \"long_release\",\n            {\n                \"previous_position\": t.int8s,\n            },\n            False,\n            is_manufacturer_specific=True,\n        ),\n        0x05: foundation.ZCLCommandDef(\n            \"multi_press_ongoing\",\n            {\n                \"new_position\": t.int8s,\n                # \"current_number_of_presses_counted\": t.int8s, # not implemented\n            },\n            False,\n            is_manufacturer_specific=True,\n        ),\n        0x06: foundation.ZCLCommandDef(\n            \"multi_press_complete\",\n            {\n                \"previous_position\": t.int8s,\n                \"total_number_of_presses_counted\": t.int8s,\n            },\n            False,\n            is_manufacturer_specific=True,\n        ),\n    }\n\n\n# ZCL compliant IKEA power configuration clusters:\nclass PowerConfig1AAACluster(CustomCluster, PowerConfiguration):\n    \"\"\"Updating power attributes: 2 AAA.\"\"\"\n\n    _CONSTANT_ATTRIBUTES = {\n        BATTERY_SIZE: BatterySize.AAA,\n        BATTERY_QUANTITY: 1,\n        BATTERY_RATED_VOLTAGE: 15,\n    }\n\n\nclass PowerConfig2AAACluster(CustomCluster, PowerConfiguration):\n    \"\"\"Updating power attributes: 2 AAA.\"\"\"\n\n    _CONSTANT_ATTRIBUTES = {\n        BATTERY_SIZE: BatterySize.AAA,\n        BATTERY_QUANTITY: 2,\n        BATTERY_RATED_VOLTAGE: 15,\n    }\n\n\nclass PowerConfig2CRCluster(CustomCluster, PowerConfiguration):\n    \"\"\"Updating power attributes: 2 CR2032.\"\"\"\n\n    _CONSTANT_ATTRIBUTES = {\n        BATTERY_SIZE: BatterySize.CR2032,\n        BATTERY_QUANTITY: 2,\n        BATTERY_RATED_VOLTAGE: 30,\n    }\n\n\nclass PowerConfig1CRCluster(CustomCluster, PowerConfiguration):\n    \"\"\"Updating power attributes: 1 CR2032.\"\"\"\n\n    _CONSTANT_ATTRIBUTES = {\n        BATTERY_SIZE: BatterySize.CR2032,\n        BATTERY_QUANTITY: 1,\n        BATTERY_RATED_VOLTAGE: 30,\n    }\n\n\nclass PowerConfig1CRXCluster(CustomCluster, PowerConfiguration):\n    \"\"\"Updating power attributes: 1 CR2032 and zero voltage.\"\"\"\n\n    _CONSTANT_ATTRIBUTES = {\n        BATTERY_VOLTAGE: 0,\n        BATTERY_SIZE: BatterySize.CR2032,\n        BATTERY_QUANTITY: 1,\n        BATTERY_RATED_VOLTAGE: 30,\n    }\n\n\n# doubling IKEA power configuration clusters:\n\n\nclass DoublingPowerConfigClusterIKEA(CustomCluster, PowerConfiguration):\n    \"\"\"PowerConfiguration cluster implementation for IKEA devices.\n\n    This implementation doubles battery pct remaining for IKEA devices with old firmware.\n    \"\"\"\n\n    async def bind(self):\n        \"\"\"Bind cluster and read the sw_build_id for later use.\"\"\"\n        result = await super().bind()\n        await self.endpoint.basic.read_attributes([Basic.AttributeDefs.sw_build_id.id])\n        return result\n\n    def _is_firmware_new(self):\n        \"\"\"Check if new firmware is installed that does not require battery doubling.\"\"\"\n        # get sw_build_id from attribute cache if available\n        sw_build_id = self.endpoint.basic.get(Basic.AttributeDefs.sw_build_id.id)\n\n        # sw_build_id is not cached or empty, so we consider it new firmware for now\n        if not sw_build_id:\n            return True\n\n        # split sw_build_id into parts to check for new firmware\n        split_fw_version = sw_build_id.split(\".\")\n        if len(split_fw_version) >= 2:\n            # guard against possible future version formatting which includes more than just numbers\n            try:\n                first_part = int(split_fw_version[0])\n                second_part = int(split_fw_version[1])\n\n                # new firmware is either 24.4.5 or above, or 2.4.5 or above\n                # old firmware is 2.3.x or below\n                return first_part >= 3 or (first_part >= 2 and second_part >= 4)\n            except ValueError:\n                _LOGGER.warning(\n                    \"sw_build_id is not a number: %s for device %s\",\n                    sw_build_id,\n                    self.endpoint.device.ieee,\n                )\n                # sw_build_id is not a number, so it must be new firmware\n                return True\n\n        # unknown formatting of sw_build_id, so it must be new firmware\n        return True\n\n    async def _read_fw_and_update_battery_pct(self, reported_battery_pct):\n        \"\"\"Read firmware version and update battery percentage remaining if necessary.\"\"\"\n        # read sw_build_id from device\n        await self.endpoint.basic.read_attributes([Basic.AttributeDefs.sw_build_id.id])\n\n        # check if sw_build_id was read successfully and old firmware is installed\n        # if so, update cache with reported battery percentage (doubled)\n        if not self._is_firmware_new():\n            self._update_attribute(\n                PowerConfiguration.AttributeDefs.battery_percentage_remaining.id,\n                reported_battery_pct * 2,\n            )\n\n    def _update_attribute(self, attrid, value):\n        \"\"\"Update attribute to double battery percentage if firmware is old.\n\n        If the firmware version is unknown, a background task to read the firmware version is also started,\n        but the percentage is not doubled for now then, as that task happens asynchronously.\n        \"\"\"\n        if attrid == PowerConfiguration.AttributeDefs.battery_percentage_remaining.id:\n            # if sw_build_id is not cached, create task to read from device, since it should be awake now\n            if (\n                self.endpoint.basic.get(Basic.AttributeDefs.sw_build_id.id, None)\n                is None\n            ):\n                self.create_catching_task(self._read_fw_and_update_battery_pct(value))\n\n            # double percentage if the firmware is confirmed old\n            # The coroutine above will not have executed yet if the firmware is unknown,\n            # so we don't double for now. The coro doubles the value later if needed.\n            if not self._is_firmware_new():\n                value = value * 2\n        super()._update_attribute(attrid, value)\n\n\nclass DoublingPowerConfig2AAACluster(\n    DoublingPowerConfigClusterIKEA, PowerConfig2AAACluster\n):\n    \"\"\"Doubling power configuration cluster. Updating power attributes: 2 AAA.\"\"\"\n\n\nclass DoublingPowerConfig2CRCluster(\n    DoublingPowerConfigClusterIKEA, PowerConfig2CRCluster\n):\n    \"\"\"Doubling power configuration cluster. Updating power attributes: 2 CR2032.\"\"\"\n\n\nclass DoublingPowerConfig1CRCluster(\n    DoublingPowerConfigClusterIKEA, PowerConfig1CRCluster\n):\n    \"\"\"Doubling power configuration cluster. Updating power attributes: 1 CR2032.\"\"\"\n\n\nclass DoublingPowerConfig1CRXCluster(\n    DoublingPowerConfigClusterIKEA, PowerConfig1CRXCluster\n):\n    \"\"\"Doubling power configuration cluster. Updating power attributes: 1 CR2032 and zero voltage.\"\"\"\n", "metadata": {"source": "github.com/zigpy/zha-device-handlers", "file_path": "zhaquirks/ikea/__init__.py", "language": "python", "complexity": "complex", "star_count": 930, "quality_score": 0.74, "features": {"has_classes": true, "has_functions": true, "has_async": true, "has_decorators": false, "has_imports": true, "has_error_handling": true, "has_documentation": true, "has_type_hints": true, "has_testing": false, "has_logging": true, "function_count": 4, "class_count": 13, "line_count": 303}, "authority_level": "medium", "authentic": true}}
