{"prompt": "Write a shell script for deploying infrastructure and services using AWS CLI and CloudFormation.", "completion": "```bash\n#!/bin/bash\n# deploy-core-api.sh\n# Main script for deploying core API endpoints to specified AWS region\n\nset -e  # Exit on error\n\n# Check if region parameter is provided\nif [ $# -lt 1 ]; then\n    echo \"Usage: $0 <region> [environment]\"\n    echo \"Example: $0 eu-south-1 production\"\n    exit 1\nfi\n\n# Set variables\nTARGET_REGION=\"$1\"\nENVIRONMENT=\"${2:-production}\"  # Default to production if not specified\nACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)\nSCRIPT_DIR=\"$( cd \"$( dirname \"${BASH_SOURCE[0]}\" )\" &> /dev/null && pwd )\"\n\n# Define common tags for all resources\nTAGS=\"Environment=$ENVIRONMENT \\\n      Service=OnaAPI \\\n      Component=Core \\\n      CostCenter=EnergyForecasting \\\n      Owner=PlatformTeam \\\n      UptimeSLA=99.9 \\\n      DataClassification=Internal \\\n      Compliance=ISO27001 \\\n      ManagedBy=CloudFormation \\\n      Version=1.0\"\n\n# Function to check if a CloudFormation stack exists\nstack_exists() {\n    local stack_name=$1\n    aws cloudformation describe-stacks --region \"$TARGET_REGION\" \\\n        --stack-name \"$stack_name\" \\\n        --query 'Stacks[0].StackStatus' \\\n        --output text 2>/dev/null || echo \"DOES_NOT_EXIST\"\n}\n\n# Function to check if a stack is in a stable state\nis_stack_stable() {\n    local stack_name=$1\n    local status\n    status=$(stack_exists \"$stack_name\")\n    [[ \"$status\" =~ ^(CREATE_COMPLETE|UPDATE_COMPLETE|UPDATE_ROLLBACK_COMPLETE)$ ]]\n}\n\n# Function to wait for a stack to be stable\nwait_for_stack() {\n    local stack_name=$1\n    local max_attempts=30\n    local attempt=1\n    \n    echo \"Waiting for stack $stack_name to be stable...\"\n    while [ $attempt -le $max_attempts ]; do\n        if is_stack_stable \"$stack_name\"; then\n            echo \"Stack $stack_name is stable\"\n            return 0\n        fi\n        echo \"Attempt $attempt/$max_attempts: Stack $stack_name is not stable yet...\"\n        sleep 30\n        ((attempt++))\n    done\n    echo \"Stack $stack_name did not become stable in time\"\n    return 1\n}\n\n# Function to check if ECR repository exists\necr_repo_exists() {\n    local repo_name=$1\n    aws ecr describe-repositories --region \"$TARGET_REGION\" \\\n        --repository-names \"$repo_name\" \\\n        --query 'repositories[0].repositoryName' \\\n        --output text 2>/dev/null || echo \"DOES_NOT_EXIST\"\n}\n\n# Function to check if API Gateway exists\napi_exists() {\n    local api_name=$1\n    aws apigateway get-rest-apis --region \"$TARGET_REGION\" \\\n        --query \"items[?name=='$api_name'].id\" \\\n        --output text 2>/dev/null || echo \"DOES_NOT_EXIST\"\n}\n\necho \"=============================================\"\necho \"Deploying Core API to region: $TARGET_REGION\"\necho \"Environment: $ENVIRONMENT\"\necho \"Account ID: $ACCOUNT_ID\"\necho \"=============================================\"\n\n# Load region-specific configuration\nCONFIG_FILE=\"$SCRIPT_DIR/config/$TARGET_REGION.env\"\nif [ ! -f \"$CONFIG_FILE\" ]; then\n    echo \"Error: Configuration file not found: $CONFIG_FILE\"\n    echo \"Please create a configuration file for this region first.\"\n    exit 1\nfi\n\nsource \"$CONFIG_FILE\"\n\n# Load environment-specific configuration (if exists)\nENV_CONFIG_FILE=\"$SCRIPT_DIR/config/$TARGET_REGION-$ENVIRONMENT.env\"\nif [ -f \"$ENV_CONFIG_FILE\" ]; then\n    source \"$ENV_CONFIG_FILE\"\n    echo \"Loaded environment-specific configuration from $ENV_CONFIG_FILE\"\nfi\n\n# Step 1: Create ECR repositories (if they don't exist)\necho \"Creating ECR repositories if they don't exist...\"\nfor repo in ingesthistoricaldata ingestnowcastdata trainforecaster generatepresignedurl \\\n            interpolatedata generateforecast returnforecastingresults marketpriceforecast \\\n            electricitydispatch weather; do\n    if [ \"$(ecr_repo_exists \"$repo/lambda\")\" = \"DOES_NOT_EXIST\" ]; then\n        echo \"Creating ECR repository: $repo/lambda\"\n        aws ecr create-repository --region \"$TARGET_REGION\" \\\n            --repository-name \"$repo/lambda\" \\\n            --tags $TAGS\n    else\n        echo \"ECR repository $repo/lambda already exists\"\n    fi\ndone\n\n# Step 2: Build and push Docker images\necho \"Building and pushing Docker images...\"\n\"$SCRIPT_DIR/scripts/build-and-push-images.sh\" \"$TARGET_REGION\"\n\n# Step 3: Deploy CloudFormation stack for S3 buckets\necho \"Deploying S3 buckets...\"\nS3_STACK_NAME=\"${REGION_PREFIX}-s3-buckets\"\naws cloudformation deploy \\\n    --region \"$TARGET_REGION\" \\\n    --template-file \"$SCRIPT_DIR/templates/s3-buckets.yaml\" \\\n    --stack-name \"$S3_STACK_NAME\" \\\n    --parameter-overrides \\\n        RegionPrefix=\"$REGION_PREFIX\" \\\n        Environment=\"$ENVIRONMENT\" \\\n    --tags $TAGS\n\n# Wait for S3 stack to be stable before proceeding\nwait_for_stack \"$S3_STACK_NAME\"\n\n# Step 4: Deploy CloudFormation stack for IAM roles\necho \"Deploying IAM roles...\"\nIAM_STACK_NAME=\"${REGION_PREFIX}-iam-roles\"\naws cloudformation deploy \\\n    --region \"$TARGET_REGION\" \\\n    --template-file \"$SCRIPT_DIR/templates/iam-roles.yaml\" \\\n    --stack-name \"$IAM_STACK_NAME\" \\\n    --capabilities CAPABILITY_NAMED_IAM \\\n    --parameter-overrides \\\n        RegionPrefix=\"$REGION_PREFIX\" \\\n        Environment=\"$ENVIRONMENT\" \\\n    --tags $TAGS\n\n# Wait for IAM stack to be stable before proceeding\nwait_for_stack \"$IAM_STACK_NAME\"\n\n# Step 5: Deploy CloudFormation stack for Lambda functions\necho \"Deploying Lambda functions...\"\nLAMBDA_STACK_NAME=\"${REGION_PREFIX}-lambda-functions\"\naws cloudformation deploy \\\n    --region \"$TARGET_REGION\" \\\n    --template-file \"$SCRIPT_DIR/templates/lambda-functions.yaml\" \\\n    --stack-name \"$LAMBDA_STACK_NAME\" \\\n    --capabilities CAPABILITY_IAM \\\n    --parameter-overrides \\\n        RegionPrefix=\"$REGION_PREFIX\" \\\n        Environment=\"$ENVIRONMENT\" \\\n        AccountId=\"$ACCOUNT_ID\" \\\n        LambdaMemory=\"$LAMBDA_MEMORY\" \\\n        IngestHistoricalDataImage=\"${ACCOUNT_ID}.dkr.ecr.${TARGET_REGION}.amazonaws.com/ingesthistoricaldata/lambda:latest\" \\\n        IngestNowcastDataImage=\"${ACCOUNT_ID}.dkr.ecr.${TARGET_REGION}.amazonaws.com/ingestnowcastdata/lambda:latest\" \\\n        TrainForecasterImage=\"${ACCOUNT_ID}.dkr.ecr.${TARGET_REGION}.amazonaws.com/trainforecaster/lambda:latest\" \\\n        GeneratePresignedUrlImage=\"${ACCOUNT_ID}.dkr.ecr.${TARGET_REGION}.amazonaws.com/generatepresignedurl/lambda:latest\" \\\n        InterpolateDataImage=\"${ACCOUNT_ID}.dkr.ecr.${TARGET_REGION}.amazonaws.com/interpolatedata/lambda:latest\" \\\n        GenerateForecastImage=\"${ACCOUNT_ID}.dkr.ecr.${TARGET_REGION}.amazonaws.com/generateforecast/lambda:latest\" \\\n        ReturnForecastingResultsImage=\"${ACCOUNT_ID}.dkr.ecr.${TARGET_REGION}.amazonaws.com/returnforecastingresults/lambda:latest\" \\\n        MarketPriceForecastImage=\"${ACCOUNT_ID}.dkr.ecr.${TARGET_REGION}.amazonaws.com/marketpriceforecast/lambda:latest\" \\\n        ElectricityDispatchImage=\"${ACCOUNT_ID}.dkr.ecr.${TARGET_REGION}.amazonaws.com/electricitydispatch/lambda:latest\" \\\n        WeatherImage=\"${ACCOUNT_ID}.dkr.ecr.${TARGET_REGION}.amazonaws.com/weather/lambda:latest\" \\\n    --tags $TAGS\n\n# Wait for Lambda stack to be stable before proceeding\nwait_for_stack \"$LAMBDA_STACK_NAME\"\n\n# Step 6: Deploy CloudFormation stack for API Gateway\necho \"Deploying API Gateway...\"\nAPI_STACK_NAME=\"${REGION_PREFIX}-api-gateway\"\naws cloudformation deploy \\\n    --region \"$TARGET_REGION\" \\\n    --template-file \"$SCRIPT_DIR/templates/api-gateway.yaml\" \\\n    --stack-name \"$API_STACK_NAME\" \\\n    --parameter-overrides \\\n        RegionPrefix=\"$REGION_PREFIX\" \\\n        Environment=\"$ENVIRONMENT\" \\\n        EndpointType=\"$API_ENDPOINT_TYPE\" \\\n        IngestHistoricalDataFunctionArn=$(aws cloudformation describe-stacks --region \"$TARGET_REGION\" --stack-name \"$LAMBDA_STACK_NAME\" --query \"Stacks[0].Outputs[?OutputKey=='IngestHistoricalDataFunctionArn'].OutputValue\" --output text) \\\n        IngestNowcastDataFunctionArn=$(aws cloudformation describe-stacks --region \"$TARGET_REGION\" --stack-name \"$LAMBDA_STACK_NAME\" --query \"Stacks[0].Outputs[?OutputKey=='IngestNowcastDataFunctionArn'].OutputValue\" --output text) \\\n        TrainForecasterFunctionArn=$(aws cloudformation describe-stacks --region \"$TARGET_REGION\" --stack-name \"$LAMBDA_STACK_NAME\" --query \"Stacks[0].Outputs[?OutputKey=='TrainForecasterFunctionArn'].OutputValue\" --output text) \\\n        GeneratePresignedUrlFunctionArn=$(aws cloudformation describe-stacks --region \"$TARGET_REGION\" --stack-name \"$LAMBDA_STACK_NAME\" --query \"Stacks[0].Outputs[?OutputKey=='GeneratePresignedUrlFunctionArn'].OutputValue\" --output text) \\\n        InterpolateDataFunctionArn=$(aws cloudformation describe-stacks --region \"$TARGET_REGION\" --stack-name \"$LAMBDA_STACK_NAME\" --query \"Stacks[0].Outputs[?OutputKey=='InterpolateDataFunctionArn'].OutputValue\" --output text) \\\n        GenerateForecastFunctionArn=$(aws cloudformation describe-stacks --region \"$TARGET_REGION\" --stack-name \"$LAMBDA_STACK_NAME\" --query \"Stacks[0].Outputs[?OutputKey=='GenerateForecastFunctionArn'].OutputValue\" --output text) \\\n        ReturnForecastingResultsFunctionArn=$(aws cloudformation describe-stacks --region \"$TARGET_REGION\" --stack-name \"$LAMBDA_STACK_NAME\" --query \"Stacks[0].Outputs[?OutputKey=='ReturnForecastingResultsFunctionArn'].OutputValue\" --output text) \\\n        MarketPriceForecastFunctionArn=$(aws cloudformation describe-stacks --region \"$TARGET_REGION\" --stack-name \"$LAMBDA_STACK_NAME\" --query \"Stacks[0].Outputs[?OutputKey=='MarketPriceForecastFunctionArn'].OutputValue\" --output text) \\\n        ElectricityDispatchFunctionArn=$(aws cloudformation describe-stacks --region \"$TARGET_REGION\" --stack-name \"$LAMBDA_STACK_NAME\" --query \"Stacks[0].Outputs[?OutputKey=='ElectricityDispatchFunctionArn'].OutputValue\" --output text) \\\n        WeatherFunctionArn=$(aws cloudformation describe-stacks --region \"$TARGET_REGION\" --stack-name \"$LAMBDA_STACK_NAME\" --query \"Stacks[0].Outputs[?OutputKey=='WeatherFunctionArn'].OutputValue\" --output text) \\\n    --tags $TAGS\n\n# Wait for API Gateway stack to be stable before proceeding\nwait_for_stack \"$API_STACK_NAME\"\n\n# Step 7: Create and configure API keys\necho \"Creating API keys...\"\n\"$SCRIPT_DIR/scripts/create-api-keys.sh\" \"$TARGET_REGION\" \"$ENVIRONMENT\"\n\n# Step 8: Deploy the API\necho \"Deploying API Gateway endpoints...\"\nAPIS=$(aws apigateway get-rest-apis --region \"$TARGET_REGION\" --query \"items[?contains(name, '${REGION_PREFIX}')].id\" --output text)\nfor API_ID in $APIS; do\n    echo \"Deploying API: $API_ID\"\n    aws apigateway create-deployment \\\n        --region \"$TARGET_REGION\" \\\n        --rest-api-id \"$API_ID\" \\\n        --stage-name \"$ENVIRONMENT\" \\\n        --description \"Deployment from deploy-core-api.sh script\" \\\n        --tags $TAGS\ndone\n\n# Step 9: Set up CloudWatch Alarms for monitoring\necho \"Setting up CloudWatch Alarms...\"\n\"$SCRIPT_DIR/scripts/setup-monitoring.sh\" \"$TARGET_REGION\" \"$ENVIRONMENT\"\n\necho \"=============================================\"\necho \"Core API deployment to $TARGET_REGION completed!\"\necho \"=============================================\"\n\n# Output API endpoints\necho \"API Endpoints:\"\nfor API_ID in $APIS; do\n    API_NAME=$(aws apigateway get-rest-api --region \"$TARGET_REGION\" --rest-api-id \"$API_ID\" --query \"name\" --output text)\n    echo \"$API_NAME: https://$API_ID.execute-api.$TARGET_REGION.amazonaws.com/$ENVIRONMENT\"\ndone\n\necho \"See CloudFormation stacks for more details:\"\necho \"<URL>\"\n```", "metadata": {"source": "/home/shingai/api/deploy/deploy-core-api.sh", "category": "deployment", "type": "complete_script"}}
{"prompt": "Get CloudFormation stack information using AWS CLI", "completion": "```bash\naws cloudformation describe-stacks --region \"$TARGET_REGION\" \\\n```", "metadata": {"category": "deployment", "type": "aws_command"}}
{"prompt": "Use AWS CLI for ecr describe-repositories", "completion": "```bash\naws ecr describe-repositories --region \"$TARGET_REGION\" \\\n```", "metadata": {"category": "deployment", "type": "aws_command"}}
{"prompt": "Use AWS CLI for apigateway get-rest-apis", "completion": "```bash\naws apigateway get-rest-apis --region \"$TARGET_REGION\" \\\n```", "metadata": {"category": "deployment", "type": "aws_command"}}
{"prompt": "Use AWS CLI for ecr create-repository", "completion": "```bash\naws ecr create-repository --region \"$TARGET_REGION\" \\\n```", "metadata": {"category": "deployment", "type": "aws_command"}}
{"prompt": "Deploy CloudFormation stack using AWS CLI", "completion": "```bash\naws cloudformation deploy \\\n```", "metadata": {"category": "deployment", "type": "aws_command"}}
{"prompt": "Deploy CloudFormation stack using AWS CLI", "completion": "```bash\naws cloudformation deploy \\\n```", "metadata": {"category": "deployment", "type": "aws_command"}}
{"prompt": "Deploy CloudFormation stack using AWS CLI", "completion": "```bash\naws cloudformation deploy \\\n```", "metadata": {"category": "deployment", "type": "aws_command"}}
{"prompt": "Deploy CloudFormation stack using AWS CLI", "completion": "```bash\naws cloudformation deploy \\\n```", "metadata": {"category": "deployment", "type": "aws_command"}}
{"prompt": "Deploy API Gateway using AWS CLI", "completion": "```bash\naws apigateway create-deployment \\\n```", "metadata": {"category": "deployment", "type": "aws_command"}}
{"prompt": "Write a shell script for deploying infrastructure and services using AWS CLI and CloudFormation.", "completion": "```bash\n#!/bin/bash\n# setup-monitoring.sh\n# Sets up CloudWatch alarms for monitoring core API components\n\nset -e  # Exit on error\n\n# Check if region parameter is provided\nif [ $# -lt 1 ]; then\n    echo \"Usage: $0 <region> [environment]\"\n    echo \"Example: $0 eu-south-1 production\"\n    exit 1\nfi\n\n# Set variables\nTARGET_REGION=\"$1\"\nENVIRONMENT=\"${2:-production}\"  # Default to production if not specified\nSCRIPT_DIR=\"$( cd \"$( dirname \"${BASH_SOURCE[0]}\" )\" &> /dev/null && pwd )\"\nALARM_EMAIL=\"<EMAIL>\"  # Replace with actual alert email\n\n# Load region-specific configuration\nCONFIG_FILE=\"$SCRIPT_DIR/../config/$TARGET_REGION.env\"\nif [ ! -f \"$CONFIG_FILE\" ]; then\n    echo \"Error: Configuration file not found: $CONFIG_FILE\"\n    echo \"Please create a configuration file for this region first.\"\n    exit 1\nfi\n\nsource \"$CONFIG_FILE\"\n\n# Function to check if SNS topic exists\nget_sns_topic_arn() {\n    local topic_name=\"${REGION_PREFIX}-api-alarms\"\n    local topic_arn\n    \n    topic_arn=$(aws sns list-topics --region \"$TARGET_REGION\" \\\n        --query \"Topics[?contains(TopicArn, '$topic_name')].TopicArn\" \\\n        --output text)\n    \n    if [ -n \"$topic_arn\" ]; then\n        echo \"$topic_arn\"\n        return 0\n    else\n        return 1\n    fi\n}\n\n# Function to check if email is subscribed to SNS topic\nis_email_subscribed() {\n    local topic_arn=$1\n    local email=$2\n    \n    aws sns list-subscriptions-by-topic --region \"$TARGET_REGION\" \\\n        --topic-arn \"$topic_arn\" \\\n        --query \"Subscriptions[?Protocol=='email' && Endpoint=='$email'].SubscriptionArn\" \\\n        --output text | grep -q .\n}\n\n# Create or get SNS topic for alarms\necho \"Setting up SNS topic for alarms...\"\nSNS_TOPIC_ARN=$(get_sns_topic_arn) || {\n    echo \"Creating new SNS topic...\"\n    SNS_TOPIC_ARN=$(aws sns create-topic \\\n        --region \"$TARGET_REGION\" \\\n        --name \"${REGION_PREFIX}-api-alarms\" \\\n        --query 'TopicArn' \\\n        --output text)\n}\n\n# Subscribe email to SNS topic if not already subscribed\nif ! is_email_subscribed \"$SNS_TOPIC_ARN\" \"$ALARM_EMAIL\"; then\n    echo \"Subscribing email to SNS topic...\"\n    aws sns subscribe \\\n        --region \"$TARGET_REGION\" \\\n        --topic-arn \"$SNS_TOPIC_ARN\" \\\n        --protocol email \\\n        --notification-endpoint \"$ALARM_EMAIL\"\nelse\n    echo \"Email already subscribed to SNS topic\"\nfi\n\n# Function to check if alarm exists\nalarm_exists() {\n    local alarm_name=$1\n    aws cloudwatch describe-alarms --region \"$TARGET_REGION\" \\\n        --alarm-names \"$alarm_name\" \\\n        --query 'MetricAlarms[0].AlarmName' \\\n        --output text | grep -q .\n}\n\n# Function to create or update Lambda alarms\ncreate_lambda_alarms() {\n    local function_name=$1\n    local threshold=$2\n    \n    echo \"Setting up alarms for Lambda function: $function_name\"\n    \n    # Error rate alarm\n    local error_alarm_name=\"${REGION_PREFIX}-${function_name}-errors\"\n    if ! alarm_exists \"$error_alarm_name\"; then\n        echo \"Creating error rate alarm...\"\n        aws cloudwatch put-metric-alarm \\\n            --region \"$TARGET_REGION\" \\\n            --alarm-name \"$error_alarm_name\" \\\n            --alarm-description \"Alarm when Lambda function $function_name has errors\" \\\n            --metric-name Errors \\\n            --namespace AWS/Lambda \\\n            --statistic Sum \\\n            --period 300 \\\n            --evaluation-periods 2 \\\n            --threshold 1 \\\n            --comparison-operator GreaterThanThreshold \\\n            --dimensions Name=FunctionName,Value=\"$function_name\" \\\n            --alarm-actions \"$SNS_TOPIC_ARN\" \\\n            --ok-actions \"$SNS_TOPIC_ARN\"\n    else\n        echo \"Error rate alarm already exists\"\n    fi\n\n    # Duration alarm\n    local duration_alarm_name=\"${REGION_PREFIX}-${function_name}-duration\"\n    if ! alarm_exists \"$duration_alarm_name\"; then\n        echo \"Creating duration alarm...\"\n        aws cloudwatch put-metric-alarm \\\n            --region \"$TARGET_REGION\" \\\n            --alarm-name \"$duration_alarm_name\" \\\n            --alarm-description \"Alarm when Lambda function $function_name duration exceeds threshold\" \\\n            --metric-name Duration \\\n            --namespace AWS/Lambda \\\n            --statistic Average \\\n            --period 300 \\\n            --evaluation-periods 2 \\\n            --threshold \"$threshold\" \\\n            --comparison-operator GreaterThanThreshold \\\n            --dimensions Name=FunctionName,Value=\"$function_name\" \\\n            --alarm-actions \"$SNS_TOPIC_ARN\" \\\n            --ok-actions \"$SNS_TOPIC_ARN\"\n    else\n        echo \"Duration alarm already exists\"\n    fi\n\n    # Throttles alarm\n    local throttle_alarm_name=\"${REGION_PREFIX}-${function_name}-throttles\"\n    if ! alarm_exists \"$throttle_alarm_name\"; then\n        echo \"Creating throttles alarm...\"\n        aws cloudwatch put-metric-alarm \\\n            --region \"$TARGET_REGION\" \\\n            --alarm-name \"$throttle_alarm_name\" \\\n            --alarm-description \"Alarm when Lambda function $function_name is throttled\" \\\n            --metric-name Throttles \\\n            --namespace AWS/Lambda \\\n            --statistic Sum \\\n            --period 300 \\\n            --evaluation-periods 2 \\\n            --threshold 1 \\\n            --comparison-operator GreaterThanThreshold \\\n            --dimensions Name=FunctionName,Value=\"$function_name\" \\\n            --alarm-actions \"$SNS_TOPIC_ARN\" \\\n            --ok-actions \"$SNS_TOPIC_ARN\"\n    else\n        echo \"Throttles alarm already exists\"\n    fi\n}\n\n# Function to create or update API Gateway alarms\ncreate_api_alarms() {\n    local api_id=$1\n    local api_name=$2\n    \n    echo \"Setting up alarms for API Gateway: $api_name\"\n    \n    # 4XX errors alarm\n    local error_4xx_alarm_name=\"${REGION_PREFIX}-${api_name}-4xx-errors\"\n    if ! alarm_exists \"$error_4xx_alarm_name\"; then\n        echo \"Creating 4XX errors alarm...\"\n        aws cloudwatch put-metric-alarm \\\n            --region \"$TARGET_REGION\" \\\n            --alarm-name \"$error_4xx_alarm_name\" \\\n            --alarm-description \"Alarm when API Gateway $api_name has 4XX errors\" \\\n            --metric-name 4XXError \\\n            --namespace AWS/ApiGateway \\\n            --statistic Sum \\\n            --period 300 \\\n            --evaluation-periods 2 \\\n            --threshold 10 \\\n            --comparison-operator GreaterThanThreshold \\\n            --dimensions Name=ApiId,Value=\"$api_id\" \\\n            --alarm-actions \"$SNS_TOPIC_ARN\" \\\n            --ok-actions \"$SNS_TOPIC_ARN\"\n    else\n        echo \"4XX errors alarm already exists\"\n    fi\n\n    # 5XX errors alarm\n    local error_5xx_alarm_name=\"${REGION_PREFIX}-${api_name}-5xx-errors\"\n    if ! alarm_exists \"$error_5xx_alarm_name\"; then\n        echo \"Creating 5XX errors alarm...\"\n        aws cloudwatch put-metric-alarm \\\n            --region \"$TARGET_REGION\" \\\n            --alarm-name \"$error_5xx_alarm_name\" \\\n            --alarm-description \"Alarm when API Gateway $api_name has 5XX errors\" \\\n            --metric-name 5XXError \\\n            --namespace AWS/ApiGateway \\\n            --statistic Sum \\\n            --period 300 \\\n            --evaluation-periods 2 \\\n            --threshold 1 \\\n            --comparison-operator GreaterThanThreshold \\\n            --dimensions Name=ApiId,Value=\"$api_id\" \\\n            --alarm-actions \"$SNS_TOPIC_ARN\" \\\n            --ok-actions \"$SNS_TOPIC_ARN\"\n    else\n        echo \"5XX errors alarm already exists\"\n    fi\n\n    # Latency alarm\n    local latency_alarm_name=\"${REGION_PREFIX}-${api_name}-latency\"\n    if ! alarm_exists \"$latency_alarm_name\"; then\n        echo \"Creating latency alarm...\"\n        aws cloudwatch put-metric-alarm \\\n            --region \"$TARGET_REGION\" \\\n            --alarm-name \"$latency_alarm_name\" \\\n            --alarm-description \"Alarm when API Gateway $api_name latency is high\" \\\n            --metric-name Latency \\\n            --namespace AWS/ApiGateway \\\n            --statistic Average \\\n            --period 300 \\\n            --evaluation-periods 2 \\\n            --threshold 1000 \\\n            --comparison-operator GreaterThanThreshold \\\n            --dimensions Name=ApiId,Value=\"$api_id\" \\\n            --alarm-actions \"$SNS_TOPIC_ARN\" \\\n            --ok-actions \"$SNS_TOPIC_ARN\"\n    else\n        echo \"Latency alarm already exists\"\n    fi\n}\n\n# Function to create or update S3 bucket alarms\ncreate_s3_alarms() {\n    local bucket_name=$1\n    \n    echo \"Setting up alarms for S3 bucket: $bucket_name\"\n    \n    # Bucket size alarm\n    local size_alarm_name=\"${REGION_PREFIX}-${bucket_name}-size\"\n    if ! alarm_exists \"$size_alarm_name\"; then\n        echo \"Creating bucket size alarm...\"\n        aws cloudwatch put-metric-alarm \\\n            --region \"$TARGET_REGION\" \\\n            --alarm-name \"$size_alarm_name\" \\\n            --alarm-description \"Alarm when S3 bucket $bucket_name size is high\" \\\n            --metric-name BucketSizeBytes \\\n            --namespace AWS/S3 \\\n            --statistic Average \\\n            --period 86400 \\\n            --evaluation-periods 1 \\\n            --threshold 10737418240 \\\n            --comparison-operator GreaterThanThreshold \\\n            --dimensions Name=BucketName,Value=\"$bucket_name\" Name=StorageType,Value=StandardStorage \\\n            --alarm-actions \"$SNS_TOPIC_ARN\" \\\n            --ok-actions \"$SNS_TOPIC_ARN\"\n    else\n        echo \"Bucket size alarm already exists\"\n    fi\n\n    # Number of objects alarm\n    local objects_alarm_name=\"${REGION_PREFIX}-${bucket_name}-objects\"\n    if ! alarm_exists \"$objects_alarm_name\"; then\n        echo \"Creating objects count alarm...\"\n        aws cloudwatch put-metric-alarm \\\n            --region \"$TARGET_REGION\" \\\n            --alarm-name \"$objects_alarm_name\" \\\n            --alarm-description \"Alarm when S3 bucket $bucket_name has too many objects\" \\\n            --metric-name NumberOfObjects \\\n            --namespace AWS/S3 \\\n            --statistic Average \\\n            --period 86400 \\\n            --evaluation-periods 1 \\\n            --threshold 1000000 \\\n            --comparison-operator GreaterThanThreshold \\\n            --dimensions Name=BucketName,Value=\"$bucket_name\" Name=StorageType,Value=AllStorageTypes \\\n            --alarm-actions \"$SNS_TOPIC_ARN\" \\\n            --ok-actions \"$SNS_TOPIC_ARN\"\n    else\n        echo \"Objects count alarm already exists\"\n    fi\n}\n\n# Create alarms for all Lambda functions\necho \"Setting up Lambda function alarms...\"\ncreate_lambda_alarms \"ingestHistoricalData\" 300000  # 5 minutes\ncreate_lambda_alarms \"ingestNowcastData\" 300000\ncreate_lambda_alarms \"trainForecaster\" 900000  # 15 minutes\ncreate_lambda_alarms \"generatePresignedUrl\" 30000  # 30 seconds\ncreate_lambda_alarms \"interpolateData\" 300000\ncreate_lambda_alarms \"generateForecast\" 300000\ncreate_lambda_alarms \"returnForecastingResults\" 300000\ncreate_lambda_alarms \"marketPriceForecast\" 300000\ncreate_lambda_alarms \"electricityDispatch\" 300000\ncreate_lambda_alarms \"weather\" 300000\n\n# Create alarms for all API Gateways\necho \"Setting up API Gateway alarms...\"\nAPIS=$(aws apigateway get-rest-apis --region \"$TARGET_REGION\" --query \"items[?contains(name, '${REGION_PREFIX}')].[id,name]\" --output text)\nwhile read -r API_ID API_NAME; do\n    create_api_alarms \"$API_ID\" \"$API_NAME\"\ndone <<< \"$APIS\"\n\n# Create alarms for all S3 buckets\necho \"Setting up S3 bucket alarms...\"\ncreate_s3_alarms \"${REGION_PREFIX}-api-client-input\"\ncreate_s3_alarms \"${REGION_PREFIX}-api-client-output\"\ncreate_s3_alarms \"${REGION_PREFIX}-api-client-facing\"\n\necho \"=============================================\"\necho \"CloudWatch alarms setup completed!\"\necho \"Alarm notifications will be sent to: $ALARM_EMAIL\"\necho \"=============================================\"\n```", "metadata": {"source": "/home/shingai/api/deploy/scripts/setup-monitoring.sh", "category": "deployment", "type": "complete_script"}}
{"prompt": "Use AWS CLI for sns list-subscriptions-by-topic", "completion": "```bash\naws sns list-subscriptions-by-topic --region \"$TARGET_REGION\" \\\n```", "metadata": {"category": "deployment", "type": "aws_command"}}
{"prompt": "Use AWS CLI for sns subscribe", "completion": "```bash\naws sns subscribe \\\n```", "metadata": {"category": "deployment", "type": "aws_command"}}
{"prompt": "Use AWS CLI for cloudwatch describe-alarms", "completion": "```bash\naws cloudwatch describe-alarms --region \"$TARGET_REGION\" \\\n```", "metadata": {"category": "deployment", "type": "aws_command"}}
{"prompt": "Create CloudWatch alarm using AWS CLI", "completion": "```bash\naws cloudwatch put-metric-alarm \\\n```", "metadata": {"category": "deployment", "type": "aws_command"}}
{"prompt": "Create CloudWatch alarm using AWS CLI", "completion": "```bash\naws cloudwatch put-metric-alarm \\\n```", "metadata": {"category": "deployment", "type": "aws_command"}}
{"prompt": "Create CloudWatch alarm using AWS CLI", "completion": "```bash\naws cloudwatch put-metric-alarm \\\n```", "metadata": {"category": "deployment", "type": "aws_command"}}
{"prompt": "Create CloudWatch alarm using AWS CLI", "completion": "```bash\naws cloudwatch put-metric-alarm \\\n```", "metadata": {"category": "deployment", "type": "aws_command"}}
{"prompt": "Create CloudWatch alarm using AWS CLI", "completion": "```bash\naws cloudwatch put-metric-alarm \\\n```", "metadata": {"category": "deployment", "type": "aws_command"}}
{"prompt": "Create CloudWatch alarm using AWS CLI", "completion": "```bash\naws cloudwatch put-metric-alarm \\\n```", "metadata": {"category": "deployment", "type": "aws_command"}}
{"prompt": "Create CloudWatch alarm using AWS CLI", "completion": "```bash\naws cloudwatch put-metric-alarm \\\n```", "metadata": {"category": "deployment", "type": "aws_command"}}
{"prompt": "Create CloudWatch alarm using AWS CLI", "completion": "```bash\naws cloudwatch put-metric-alarm \\\n```", "metadata": {"category": "deployment", "type": "aws_command"}}
{"prompt": "Write a shell script that orchestrates multiple deployment steps using AWS CLI and CloudFormation.", "completion": "```bash\n#!/bin/bash\nset -e\necho \"Starting orchestration...\"\necho \"Running redeploy2.sh...\"\nREDEPLOY2_OUTPUT=$(./redeploy2.sh)\necho \"redeploy2.sh output:\"\necho \"$REDEPLOY2_OUTPUT\"\nJOB_RUN_ID=$(echo \"$REDEPLOY2_OUTPUT\" | jq -r '.JobRunId')\nif [ -z \"$JOB_RUN_ID\" ] || [ \"$JOB_RUN_ID\" == \"null\" ]; then\n  echo \"Error: Could not extract JobRunId from redeploy2.sh output.\"\n  exit 1\nfi\necho \"KnowledgeBaseETLJob JobRunId: $JOB_RUN_ID\"\necho \"Waiting for KnowledgeBaseETLJob to complete...\"\nwhile true; do\n  STATUS=$(aws glue get-job-run --job-name KnowledgeBaseETLJob --run-id \"$JOB_RUN_ID\" --query 'JobRun.JobRunState' --output text)\n  echo \"Current status: $STATUS\"\n  if [ \"$STATUS\" == \"SUCCEEDED\" ]; then\n    echo \"KnowledgeBaseETLJob succeeded.\"\n    break\n  elif [[ \"$STATUS\" == \"FAILED\" || \"$STATUS\" == \"TIMEOUT\" || \"$STATUS\" == \"STOPPED\" ]]; then\n    echo \"KnowledgeBaseETLJob ended with terminal state: $STATUS\"\n    exit 1\n  fi\n  sleep 60\ndone\necho \"Running redeploy3.sh...\"\nREDEPLOY3_OUTPUT=$(./redeploy3.sh)\necho \"redeploy3.sh output:\"\necho \"$REDEPLOY3_OUTPUT\"\nJOB_RUN_ID2=$(echo \"$REDEPLOY3_OUTPUT\" | jq -r '.JobRunId')\nif [ -z \"$JOB_RUN_ID2\" ] || [ \"$JOB_RUN_ID2\" == \"null\" ]; then\n  echo \"Error: Could not extract JobRunId from redeploy3.sh output.\"\n  exit 1\nfi\necho \"KnowledgeBaseETL2ndChanceJob JobRunId: $JOB_RUN_ID2\"\necho \"Waiting for KnowledgeBaseETL2ndChanceJob to complete...\"\nwhile true; do\n  STATUS2=$(aws glue get-job-run --job-name KnowledgeBaseETL2ndChanceJob --run-id \"$JOB_RUN_ID2\" --query 'JobRun.JobRunState' --output text)\n  echo \"Current status: $STATUS2\"\n  if [ \"$STATUS2\" == \"SUCCEEDED\" ]; then\n    echo \"KnowledgeBaseETL2ndChanceJob succeeded.\"\n    break\n  elif [[ \"$STATUS2\" == \"FAILED\" || \"$STATUS2\" == \"TIMEOUT\" || \"$STATUS2\" == \"STOPPED\" ]]; then\n    echo \"KnowledgeBaseETL2ndChanceJob ended with terminal state: $STATUS2\"\n    exit 1\n  fi\n  sleep 60\ndone\necho \"Running redeploy4.sh...\"\n./redeploy4.sh\necho \"Orchestration complete.\"\necho \"Shutting down the instance now...\"\nsudo shutdown -h now\n```", "metadata": {"source": "/home/shingai/api/RAG/ETL/scripts/orchestrator.sh", "category": "orchestration", "type": "complete_script"}}
{"prompt": "Write a shell script for deploying infrastructure and services using AWS CLI and CloudFormation.", "completion": "```bash\n#!/bin/bash\n# deploy.sh\n# Comprehensive deployment script for Knowledge Base ETL system\n#\n# This script handles deployment of all components:\n#   1. Lambda Function (etl-google-alerts)\n#   2. Primary Glue Job (KnowledgeBaseETLJob) - etl-s3.py\n#   3. Second Chance Glue Job (KnowledgeBaseETL2ndChanceJob) - etl-s3-failed.py\n#   4. Rehoming Glue Job (KnowledgeBaseETLRehomeJob) - rehoming_etl.py\n#\n# Usage:\n#   ./deploy.sh [--all] [--lambda] [--primary] [--second-chance] [--rehome]\n#\n# Options:\n#   --all            Deploy all components (default if no flag is provided)\n#   --lambda         Deploy only the Lambda function (etl-google-alerts)\n#   --primary        Deploy only the primary ETL job\n#   --second-chance  Deploy only the second chance ETL job\n#   --rehome         Deploy only the rehoming ETL job\n#\n# Prerequisites:\n# - AWS CLI must be installed and configured.\n# - Script files must be present in the respective directories.\n# - IAM role with necessary permissions must exist.\n\nset -e  # Exit immediately if any command fails\n\n# Common variables\nREGION=\"us-east-1\"\nS3_BUCKET=\"policy-database\"\nTEMP_DIR=\"s3://$S3_BUCKET/temp/\"\nROLE=\"arn:aws:iam::<ACCOUNT_ID>:role/AWSGluePolicyDatabaseRole\"\nROLE_NAME=\"AWSGluePolicyDatabaseRole\"  # Extracted from the ARN\nGLUE_VERSION=\"2.0\"\nMAX_CAPACITY=\"2.0\"\nTIMEOUT=\"2880\"\n\n# Lambda variables\nLAMBDA_FUNCTION_NAME=\"etl-google-alerts\"\nLAMBDA_ZIP_FILE=\"../lambda/etl-lambda.zip\"\nLAMBDA_HANDLER_FILE=\"../lambda/etl-google-alerts-lambda.js\"\nNODE_MODULES_DIR=\"../node_modules\"\n\n# Glue job variables\nPRIMARY_JOB_NAME=\"KnowledgeBaseETLJob\"\nPRIMARY_SCRIPT_LOCAL=\"../glue/etl-s3.py\"\nPRIMARY_SCRIPT_S3=\"s3://$S3_BUCKET/etl/etl-s3.py\"\nPRIMARY_MODULES=\"PyPDF2==3.0.1,pycryptodome\"\n\nSECOND_CHANCE_JOB_NAME=\"KnowledgeBaseETL2ndChanceJob\"\nSECOND_CHANCE_SCRIPT_LOCAL=\"../glue/etl-s3-failed.py\"\nSECOND_CHANCE_SCRIPT_S3=\"s3://$S3_BUCKET/etl/etl-s3-failed.py\"\nSECOND_CHANCE_MODULES=\"PyPDF2==3.0.1,pycryptodome\"\n\nREHOME_JOB_NAME=\"KnowledgeBaseETLRehomeJob\"\nREHOME_SCRIPT_LOCAL=\"../glue/rehoming_etl.py\"\nREHOME_SCRIPT_S3=\"s3://$S3_BUCKET/etl/rehoming_etl.py\"\nREHOME_MODULES=\"PyPDF2==3.0.1\"\n\n# CloudWatch logs inline policy document for rehoming job\nINLINE_POLICY_NAME=\"GlueCloudWatchLogAccessPolicy\"\nINLINE_POLICY_DOCUMENT=$(cat <<EOF\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n          \"logs:FilterLogEvents\",\n          \"logs:DescribeLogStreams\",\n          \"logs:GetLogEvents\"\n      ],\n      \"Resource\": \"arn:aws:logs:$REGION:<ACCOUNT_ID>:log-group:/aws-glue/jobs/output:*\"\n    }\n  ]\n}\nEOF\n)\n\n# Function to deploy a Glue job\ndeploy_glue_job() {\n    local job_name=$1\n    local script_local=$2\n    local script_s3=$3\n    local additional_modules=$4\n    local extra_setup=$5\n\n    echo \"Uploading script $script_local to $script_s3...\"\n    aws s3 cp \"$script_local\" \"$script_s3\" --region \"$REGION\"\n\n    # Run any extra setup if provided (e.g., for rehome job IAM policy)\n    if [ -n \"$extra_setup\" ]; then\n        eval \"$extra_setup\"\n    fi\n\n    echo \"Checking if Glue job $job_name exists...\"\n    if aws glue get-job --job-name \"$job_name\" --region \"$REGION\" >/dev/null 2>&1; then\n        echo \"Job exists. Updating job configuration...\"\n        aws glue update-job \\\n          --job-name \"$job_name\" \\\n          --job-update '{\n            \"Role\": \"'\"$ROLE\"'\",\n            \"Command\": {\n              \"Name\": \"glueetl\",\n              \"ScriptLocation\": \"'\"$script_s3\"'\",\n              \"PythonVersion\": \"3\"\n            },\n            \"DefaultArguments\": {\n              \"--TempDir\": \"'\"$TEMP_DIR\"'\",\n              \"--job-language\": \"python\",\n              \"--additional-python-modules\": \"'\"$additional_modules\"'\"\n            },\n            \"MaxRetries\": 0,\n            \"Timeout\": '\"$TIMEOUT\"',\n            \"MaxCapacity\": '\"$MAX_CAPACITY\"',\n            \"GlueVersion\": \"'\"$GLUE_VERSION\"'\"\n          }' \\\n          --region \"$REGION\"\n    else\n        echo \"Job does not exist. Creating Glue job $job_name...\"\n        aws glue create-job \\\n          --name \"$job_name\" \\\n          --role \"$ROLE\" \\\n          --command \"Name=glueetl,ScriptLocation=$script_s3,PythonVersion=3\" \\\n          --default-arguments \"{\\\"--TempDir\\\":\\\"$TEMP_DIR\\\",\\\"--job-language\\\":\\\"python\\\",\\\"--additional-python-modules\\\":\\\"$additional_modules\\\"}\" \\\n          --max-retries 0 \\\n          --timeout \"$TIMEOUT\" \\\n          --max-capacity \"$MAX_CAPACITY\" \\\n          --glue-version \"$GLUE_VERSION\" \\\n          --region \"$REGION\"\n    fi\n\n    echo \"Starting a new run of the Glue job $job_name...\"\n    JOB_RUN_OUTPUT=$(aws glue start-job-run --job-name \"$job_name\" --region \"$REGION\")\n    echo \"Job run started:\"\n    echo \"$JOB_RUN_OUTPUT\"\n    echo \"---------------------------------------------\"\n}\n\n# Function to deploy Lambda\ndeploy_lambda() {\n    echo \"Starting deployment of Lambda function: $LAMBDA_FUNCTION_NAME\"\n    \n    # 1. Install production dependencies\n    echo \"Installing production dependencies...\"\n    (cd .. && npm install --production)\n    \n    # 2. Create the deployment package zip file\n    echo \"Creating deployment package: $LAMBDA_ZIP_FILE\"\n    (cd .. && zip -r \"lambda/etl-lambda.zip\" \"lambda/etl-google-alerts-lambda.js\" \"node_modules\")\n    \n    # 3. Upload the ZIP file to the same S3 location where Glue scripts are stored\n    echo \"Uploading Lambda package to S3...\"\n    aws s3 cp \"$LAMBDA_ZIP_FILE\" \"s3://$S3_BUCKET/etl/etl-lambda.zip\" --region \"$REGION\"\n    \n    # 4. Update the Lambda function code using the S3 reference\n    echo \"Updating Lambda function code from S3 in region $REGION...\"\n    aws lambda update-function-code \\\n      --function-name \"$LAMBDA_FUNCTION_NAME\" \\\n      --s3-bucket \"$S3_BUCKET\" \\\n      --s3-key \"etl/etl-lambda.zip\" \\\n      --region \"$REGION\"\n    \n    echo \"Lambda deployment complete.\"\n    echo \"---------------------------------------------\"\n}\n\n# Process command-line arguments\nDEPLOY_LAMBDA=false\nDEPLOY_PRIMARY=false\nDEPLOY_SECOND_CHANCE=false\nDEPLOY_REHOME=false\n\n# Default to deploying all if no arguments provided\nif [ $# -eq 0 ]; then\n    DEPLOY_LAMBDA=true\n    DEPLOY_PRIMARY=true\n    DEPLOY_SECOND_CHANCE=true\n    DEPLOY_REHOME=true\nelse\n    while [ $# -gt 0 ]; do\n        case \"$1\" in\n            --all)\n                DEPLOY_LAMBDA=true\n                DEPLOY_PRIMARY=true\n                DEPLOY_SECOND_CHANCE=true\n                DEPLOY_REHOME=true\n                ;;\n            --lambda)\n                DEPLOY_LAMBDA=true\n                ;;\n            --primary)\n                DEPLOY_PRIMARY=true\n                ;;\n            --second-chance)\n                DEPLOY_SECOND_CHANCE=true\n                ;;\n            --rehome)\n                DEPLOY_REHOME=true\n                ;;\n            *)\n                echo \"Unknown option: $1\"\n                echo \"Usage: ./deploy.sh [--all] [--lambda] [--primary] [--second-chance] [--rehome]\"\n                exit 1\n                ;;\n        esac\n        shift\n    done\nfi\n\n# Extra setup command for rehome job (IAM policy update)\nREHOME_EXTRA_SETUP='echo \"Updating IAM role ($ROLE_NAME) to allow CloudWatch Logs access...\"; aws iam put-role-policy --role-name \"$ROLE_NAME\" --policy-name \"$INLINE_POLICY_NAME\" --policy-document \"$INLINE_POLICY_DOCUMENT\" --region \"$REGION\"'\n\n# Deploy selected components\nif [ \"$DEPLOY_LAMBDA\" = true ]; then\n    echo \"Deploying Lambda Function ($LAMBDA_FUNCTION_NAME)...\"\n    deploy_lambda\nfi\n\nif [ \"$DEPLOY_PRIMARY\" = true ]; then\n    echo \"Deploying Primary ETL Job ($PRIMARY_JOB_NAME)...\"\n    deploy_glue_job \"$PRIMARY_JOB_NAME\" \"$PRIMARY_SCRIPT_LOCAL\" \"$PRIMARY_SCRIPT_S3\" \"$PRIMARY_MODULES\"\nfi\n\nif [ \"$DEPLOY_SECOND_CHANCE\" = true ]; then\n    echo \"Deploying Second Chance ETL Job ($SECOND_CHANCE_JOB_NAME)...\"\n    deploy_glue_job \"$SECOND_CHANCE_JOB_NAME\" \"$SECOND_CHANCE_SCRIPT_LOCAL\" \"$SECOND_CHANCE_SCRIPT_S3\" \"$SECOND_CHANCE_MODULES\"\nfi\n\nif [ \"$DEPLOY_REHOME\" = true ]; then\n    echo \"Deploying Rehoming ETL Job ($REHOME_JOB_NAME)...\"\n    deploy_glue_job \"$REHOME_JOB_NAME\" \"$REHOME_SCRIPT_LOCAL\" \"$REHOME_SCRIPT_S3\" \"$REHOME_MODULES\" \"$REHOME_EXTRA_SETUP\"\nfi\n\necho \"Deployment completed successfully!\"\n```", "metadata": {"source": "/home/shingai/api/RAG/ETL/scripts/deploy.sh", "category": "deployment", "type": "complete_script"}}
{"prompt": "Use AWS CLI for s3 cp", "completion": "```bash\naws s3 cp \"$script_local\" \"$script_s3\" --region \"$REGION\"\n```", "metadata": {"category": "deployment", "type": "aws_command"}}
{"prompt": "Use AWS CLI for glue update-job", "completion": "```bash\naws glue update-job \\\n```", "metadata": {"category": "deployment", "type": "aws_command"}}
{"prompt": "Use AWS CLI for glue create-job", "completion": "```bash\naws glue create-job \\\n```", "metadata": {"category": "deployment", "type": "aws_command"}}
{"prompt": "Use AWS CLI for s3 cp", "completion": "```bash\naws s3 cp \"$LAMBDA_ZIP_FILE\" \"s3://$S3_BUCKET/etl/etl-lambda.zip\" --region \"$REGION\"\n```", "metadata": {"category": "deployment", "type": "aws_command"}}
{"prompt": "Update Lambda function code using AWS CLI", "completion": "```bash\naws lambda update-function-code \\\n```", "metadata": {"category": "deployment", "type": "aws_command"}}
{"prompt": "Write a shell script for deploying infrastructure and services using AWS CLI and CloudFormation.", "completion": "```bash\n#!/bin/bash\n# Deploy script for the Visual Crossing weather data fetching Lambda functions\n# Sets up S3 bucket and Lambda functions to fetch and store weather data for various cities\n# Deployment region: af-south-1 (Cape Town)\n\n# Exit on error\nset -e\n\n# Check if .env file exists and load it\nif [ -f .env ]; then\n    export $(grep -v '^#' .env | xargs)\n    echo \"Loaded environment variables from .env file\"\nelse\n    echo \"No .env file found. Make sure you have set the VISUALCROSSING_API_KEY environment variable.\"\nfi\n\n# Check if API key is set\nif [ -z \"$VISUALCROSSING_API_KEY\" ]; then\n    echo \"VISUALCROSSING_API_KEY environment variable is not set.\"\n    echo \"Please set it using 'export VISUALCROSSING_API_KEY=your_api_key_here'\"\n    exit 1\nfi\n\n# Configuration\nREGION=\"us-east-1\"\nACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)\nLAMBDA_ROLE_NAME=\"lambda-weather-execution-role\"\nS3_BUCKET=\"visualcrossing-city-database\"\nLAMBDA_RUNTIME=\"python3.9\"\nLAMBDA_MEMORY=256\nLAMBDA_TIMEOUT=120\nLAMBDA_DAILY_WEATHER_NAME=\"daily_weather_fetch\"\n\n# Check if virtual environment exists, if not create one\nif [ ! -d \"venv\" ]; then\n    echo \"Creating virtual environment...\"\n    python3 -m venv venv\nfi\n\n# Activate virtual environment\nsource venv/bin/activate\n\n# Install dependencies\necho \"Installing dependencies...\"\npip install boto3 requests\n\n# Create output directory for packages\nmkdir -p build\n\necho \"Creating S3 bucket if it doesn't exist...\"\nif ! aws s3api head-bucket --bucket $S3_BUCKET --region $REGION 2>/dev/null; then\n    echo \"Creating S3 bucket: $S3_BUCKET\"\n    aws s3api create-bucket \\\n        --bucket $S3_BUCKET \\\n        --region $REGION \\\n        --create-bucket-configuration LocationConstraint=$REGION\n    \n    # Wait for bucket to be available\n    echo \"Waiting for bucket to become available...\"\n    sleep 5\nelse\n    echo \"S3 bucket already exists: $S3_BUCKET\"\nfi\n\n# Create IAM role for Lambda if it doesn't exist\necho \"Setting up IAM role for Lambda...\"\nif ! aws iam get-role --role-name $LAMBDA_ROLE_NAME 2>/dev/null; then\n    echo \"Creating IAM role: $LAMBDA_ROLE_NAME\"\n    \n    # Create a trust policy document for Lambda\n    cat > /tmp/trust-policy.json << EOF\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Principal\": {\n        \"Service\": \"lambda.amazonaws.com\"\n      },\n      \"Action\": \"sts:AssumeRole\"\n    }\n  ]\n}\nEOF\n    \n    # Create the role\n    aws iam create-role \\\n        --role-name $LAMBDA_ROLE_NAME \\\n        --assume-role-policy-document file:///tmp/trust-policy.json\n    \n    # Attach policies\n    echo \"Attaching policies to IAM role...\"\n    \n    # Basic execution role for CloudWatch Logs\n    aws iam attach-role-policy \\\n        --role-name $LAMBDA_ROLE_NAME \\\n        --policy-arn arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole\n    \n    # S3 access for storing weather data\n    aws iam attach-role-policy \\\n        --role-name $LAMBDA_ROLE_NAME \\\n        --policy-arn arn:aws:iam::aws:policy/AmazonS3FullAccess\n    \n    # Wait for role to be ready\n    echo \"Waiting for IAM role to be ready...\"\n    sleep 10\nelse\n    echo \"IAM role already exists: $LAMBDA_ROLE_NAME\"\nfi\n\n# Get the IAM role ARN\nLAMBDA_ROLE_ARN=\"arn:aws:iam::$ACCOUNT_ID:role/$LAMBDA_ROLE_NAME\"\necho \"Using IAM role ARN: $LAMBDA_ROLE_ARN\"\n\n# Deploy daily weather fetch Lambda\necho \"Deploying daily weather fetch Lambda...\"\n\n# Package the code\necho \"Packaging daily_weather_fetch.py...\"\nmkdir -p build/daily_weather\ncp daily_weather_fetch.py build/daily_weather/\ncd build/daily_weather\nzip -r ../daily_weather_function.zip .\ncd ../..\n\n# Check if the Lambda function exists\nif aws lambda get-function --function-name $LAMBDA_DAILY_WEATHER_NAME --region $REGION 2>/dev/null; then\n    echo \"Updating existing Lambda function: $LAMBDA_DAILY_WEATHER_NAME\"\n    \n    # FIRST: Update the Lambda function configuration\n    echo \"Updating Lambda configuration...\"\n    aws lambda update-function-configuration \\\n        --function-name $LAMBDA_DAILY_WEATHER_NAME \\\n        --timeout $LAMBDA_TIMEOUT \\\n        --memory-size $LAMBDA_MEMORY \\\n        --environment \"Variables={VISUALCROSSING_API_KEY=$VISUALCROSSING_API_KEY,S3_BUCKET=$S3_BUCKET}\" \\\n        --region $REGION\n    \n    # Wait for configuration update to complete\n    echo \"Waiting for configuration update to complete...\"\n    sleep 10\n    \n    # Verify environment variables\n    CURRENT_ENV=$(aws lambda get-function --function-name $LAMBDA_DAILY_WEATHER_NAME --region $REGION --query 'Configuration.Environment.Variables' --output json)\n    echo \"Updated environment variables: $CURRENT_ENV\"\n    \n    # SECOND: Update the Lambda function code\n    echo \"Updating Lambda code...\"\n    aws lambda update-function-code \\\n        --function-name $LAMBDA_DAILY_WEATHER_NAME \\\n        --zip-file fileb://build/daily_weather_function.zip \\\n        --region $REGION\nelse\n    echo \"Creating new Lambda function: $LAMBDA_DAILY_WEATHER_NAME\"\n    \n    # Create the Lambda function\n    aws lambda create-function \\\n        --function-name $LAMBDA_DAILY_WEATHER_NAME \\\n        --runtime $LAMBDA_RUNTIME \\\n        --role $LAMBDA_ROLE_ARN \\\n        --handler daily_weather_fetch.lambda_handler \\\n        --zip-file fileb://build/daily_weather_function.zip \\\n        --timeout $LAMBDA_TIMEOUT \\\n        --memory-size $LAMBDA_MEMORY \\\n        --environment \"Variables={VISUALCROSSING_API_KEY=$VISUALCROSSING_API_KEY,S3_BUCKET=$S3_BUCKET}\" \\\n        --region $REGION\nfi\n\n# Create or update EventBridge rule to trigger the Lambda daily at midnight UTC\nRULE_NAME=\"daily-weather-fetch-trigger\"\necho \"Setting up EventBridge rule: $RULE_NAME\"\n\naws events put-rule \\\n    --name $RULE_NAME \\\n    --schedule-expression \"cron(0 0 * * ? *)\" \\\n    --state ENABLED \\\n    --region $REGION\n\n# Add the daily weather Lambda as target\naws events put-targets \\\n    --rule $RULE_NAME \\\n    --targets \"Id\"=\"1\",\"Arn\"=\"arn:aws:lambda:$REGION:$ACCOUNT_ID:function:$LAMBDA_DAILY_WEATHER_NAME\" \\\n    --region $REGION\n\n# Add permission for EventBridge to invoke the Lambda\necho \"Adding permission for EventBridge to invoke Lambda...\"\naws lambda add-permission \\\n    --function-name $LAMBDA_DAILY_WEATHER_NAME \\\n    --statement-id \"AllowExecutionFromEventBridge\" \\\n    --action \"lambda:InvokeFunction\" \\\n    --principal \"events.amazonaws.com\" \\\n    --source-arn \"arn:aws:events:$REGION:$ACCOUNT_ID:rule/$RULE_NAME\" \\\n    --region $REGION || true  # Ignore error if permission already exists\n\necho \"Deployment completed successfully!\"\necho \"The daily weather fetch Lambda will run every day at midnight UTC.\"\necho \"Weather data will be stored in s3://$S3_BUCKET/<city_name>/daily/ directories.\"\necho \"Historical data: s3://$S3_BUCKET/<city_name>/daily/YYYY-MM-DD.json\"\necho \"Forecast data: s3://$S3_BUCKET/<city_name>/daily/forecast_YYYY-MM-DD.json\"\n\n# Deactivate virtual environment\ndeactivate\n\necho \"You can test the Lambda function by running:\"\necho \"aws lambda invoke --function-name $LAMBDA_DAILY_WEATHER_NAME --payload '{}' --region $REGION output.json\"\n```", "metadata": {"source": "/home/shingai/api/core/weather/deploy.sh", "category": "deployment", "type": "complete_script"}}
{"prompt": "Use AWS CLI for s3api create-bucket", "completion": "```bash\naws s3api create-bucket \\\n```", "metadata": {"category": "deployment", "type": "aws_command"}}
{"prompt": "Create IAM role using AWS CLI", "completion": "```bash\naws iam create-role \\\n```", "metadata": {"category": "deployment", "type": "aws_command"}}
{"prompt": "Use AWS CLI for iam attach-role-policy", "completion": "```bash\naws iam attach-role-policy \\\n```", "metadata": {"category": "deployment", "type": "aws_command"}}
{"prompt": "Use AWS CLI for iam attach-role-policy", "completion": "```bash\naws iam attach-role-policy \\\n```", "metadata": {"category": "deployment", "type": "aws_command"}}
{"prompt": "Use AWS CLI for lambda update-function-configuration", "completion": "```bash\naws lambda update-function-configuration \\\n```", "metadata": {"category": "deployment", "type": "aws_command"}}
{"prompt": "Update Lambda function code using AWS CLI", "completion": "```bash\naws lambda update-function-code \\\n```", "metadata": {"category": "deployment", "type": "aws_command"}}
{"prompt": "Create Lambda function using AWS CLI", "completion": "```bash\naws lambda create-function \\\n```", "metadata": {"category": "deployment", "type": "aws_command"}}
{"prompt": "Use AWS CLI for events put-rule", "completion": "```bash\naws events put-rule \\\n```", "metadata": {"category": "deployment", "type": "aws_command"}}
{"prompt": "Use AWS CLI for events put-targets", "completion": "```bash\naws events put-targets \\\n```", "metadata": {"category": "deployment", "type": "aws_command"}}
{"prompt": "Use AWS CLI for lambda add-permission", "completion": "```bash\naws lambda add-permission \\\n```", "metadata": {"category": "deployment", "type": "aws_command"}}
{"prompt": "Write a shell script for deploying infrastructure and services using AWS CLI and CloudFormation.", "completion": "```bash\n#!/bin/bash\n# create-ecr-repos.sh\n# Script to create ECR repositories for core API components\n\nset -e  # Exit on error\n\n# Check if region parameter is provided\nif [ $# -lt 1 ]; then\n    echo \"Usage: $0 <region>\"\n    echo \"Example: $0 eu-south-1\"\n    exit 1\nfi\n\nTARGET_REGION=\"$1\"\n\necho \"Creating ECR repositories in region: $TARGET_REGION\"\n\n# List of repository names to create\nREPO_NAMES=(\n    \"ingesthistoricaldata/lambda\"\n    \"ingestnowcastdata/lambda\"\n    \"trainforecaster/lambda\"\n    \"generatepresignedurl/lambda\"\n)\n\n# Create each repository if it doesn't exist\nfor REPO_NAME in \"${REPO_NAMES[@]}\"; do\n    echo \"Checking repository: $REPO_NAME\"\n    \n    # Check if repository exists\n    if aws ecr describe-repositories --region \"$TARGET_REGION\" --repository-names \"$REPO_NAME\" >/dev/null 2>&1; then\n        echo \"Repository $REPO_NAME already exists\"\n    else\n        echo \"Creating repository: $REPO_NAME\"\n        aws ecr create-repository \\\n            --region \"$TARGET_REGION\" \\\n            --repository-name \"$REPO_NAME\" \\\n            --image-scanning-configuration scanOnPush=true\n        \n        # Set lifecycle policy to keep only the last 5 images\n        echo \"Setting lifecycle policy for $REPO_NAME\"\n        aws ecr put-lifecycle-policy \\\n            --region \"$TARGET_REGION\" \\\n            --repository-name \"$REPO_NAME\" \\\n            --lifecycle-policy-text '{\n                \"rules\": [\n                    {\n                        \"rulePriority\": 1,\n                        \"description\": \"Keep only the last 5 images\",\n                        \"selection\": {\n                            \"tagStatus\": \"any\",\n                            \"countType\": \"imageCountMoreThan\",\n                            \"countNumber\": 5\n                        },\n                        \"action\": {\n                            \"type\": \"expire\"\n                        }\n                    }\n                ]\n            }'\n        \n        echo \"Repository $REPO_NAME created successfully\"\n    fi\ndone\n\necho \"ECR repository setup complete\"\n```", "metadata": {"source": "/home/shingai/api/deploy/scripts/create-ecr-repos.sh", "category": "deployment", "type": "complete_script"}}
{"prompt": "Use AWS CLI for ecr create-repository", "completion": "```bash\naws ecr create-repository \\\n```", "metadata": {"category": "deployment", "type": "aws_command"}}
{"prompt": "Use AWS CLI for ecr put-lifecycle-policy", "completion": "```bash\naws ecr put-lifecycle-policy \\\n```", "metadata": {"category": "deployment", "type": "aws_command"}}
{"prompt": "Write a shell script for deploying infrastructure and services using AWS CLI and CloudFormation.", "completion": "```bash\n#!/bin/bash\nset -euo pipefail\n\n# Comprehensive monitoring setup for Flux model loading\necho \"Setting up monitoring infrastructure...\"\n\n# Install monitoring tools\nsudo apt update\nsudo apt install -y htop iotop sysstat nethogs\n\n# Create monitoring directories\nsudo mkdir -p /var/log/flux-monitoring\nsudo chown ubuntu:ubuntu /var/log/flux-monitoring\n\n# System resource monitoring script\ncat > /tmp/system_monitor.py << 'EOF'\n#!/usr/bin/env python3\nimport psutil\nimport subprocess\nimport time\nimport json\nimport sys\nfrom datetime import datetime\n\ndef get_gpu_info():\n    try:\n        result = subprocess.run(['nvidia-smi', '--query-gpu=memory.used,memory.total,utilization.gpu,temperature.gpu'], \n                              capture_output=True, text=True, check=True)\n        lines = result.stdout.strip().split('\\n')[1:]  # Skip header\n        if lines:\n            memory_used, memory_total, gpu_util, temp = lines[0].split(', ')\n            return {\n                'memory_used_mb': int(memory_used.split()[0]),\n                'memory_total_mb': int(memory_total.split()[0]),\n                'utilization_percent': int(gpu_util.split()[0]),\n                'temperature_c': int(temp.split()[0])\n            }\n    except:\n        pass\n    return {'error': 'GPU info unavailable'}\n\ndef get_process_info(process_name):\n    processes = []\n    for proc in psutil.process_iter(['pid', 'name', 'memory_info', 'cpu_percent', 'status']):\n        try:\n            if process_name.lower() in proc.info['name'].lower():\n                processes.append({\n                    'pid': proc.info['pid'],\n                    'memory_mb': proc.info['memory_info'].rss / 1024 / 1024,\n                    'cpu_percent': proc.info['cpu_percent'],\n                    'status': proc.info['status']\n                })\n        except:\n            continue\n    return processes\n\ndef monitor_system():\n    timestamp = datetime.now().isoformat()\n    \n    # System metrics\n    memory = psutil.virtual_memory()\n    cpu = psutil.cpu_percent(interval=1)\n    disk = psutil.disk_usage('/')\n    \n    # Network stats\n    net = psutil.net_io_counters()\n    \n    data = {\n        'timestamp': timestamp,\n        'memory': {\n            'total_gb': memory.total / 1024**3,\n            'used_gb': memory.used / 1024**3,\n            'available_gb': memory.available / 1024**3,\n            'percent': memory.percent\n        },\n        'cpu_percent': cpu,\n        'disk': {\n            'total_gb': disk.total / 1024**3,\n            'used_gb': disk.used / 1024**3,\n            'free_gb': disk.free / 1024**3\n        },\n        'network': {\n            'bytes_sent': net.bytes_sent,\n            'bytes_recv': net.bytes_recv\n        },\n        'gpu': get_gpu_info(),\n        'python_processes': get_process_info('python')\n    }\n    \n    return data\n\nif __name__ == '__main__':\n    if len(sys.argv) > 1 and sys.argv[1] == '--continuous':\n        # Continuous monitoring mode\n        log_file = '/var/log/flux-monitoring/system_metrics.log'\n        while True:\n            try:\n                data = monitor_system()\n                with open(log_file, 'a') as f:\n                    f.write(json.dumps(data) + '\\n')\n                time.sleep(5)\n            except KeyboardInterrupt:\n                break\n            except Exception as e:\n                print(f\"Monitoring error: {e}\")\n    else:\n        # Single measurement\n        data = monitor_system()\n        print(json.dumps(data, indent=2))\nEOF\n\nchmod +x /tmp/system_monitor.py\n\n# Process monitoring script\ncat > /tmp/process_monitor.sh << 'EOF'\n#!/bin/bash\nset -euo pipefail\n\nLOG_FILE=\"/var/log/flux-monitoring/process.log\"\n\nmonitor_process() {\n    local pid=$1\n    local name=$2\n    \n    echo \"$(date): Monitoring process $name (PID: $pid)\" >> \"$LOG_FILE\"\n    \n    while kill -0 \"$pid\" 2>/dev/null; do\n        # Get process stats\n        ps -o pid,ppid,rss,vsz,pcpu,time,cmd -p \"$pid\" >> \"$LOG_FILE\" 2>/dev/null || break\n        \n        # Check if process is responding\n        if [ -d \"/proc/$pid\" ]; then\n            echo \"$(date): Process $pid still alive\" >> \"$LOG_FILE\"\n        else\n            echo \"$(date): Process $pid disappeared\" >> \"$LOG_FILE\"\n            break\n        fi\n        \n        sleep 10\n    done\n    \n    echo \"$(date): Process $name (PID: $pid) monitoring ended\" >> \"$LOG_FILE\"\n}\n\n# Export function for external use\nexport -f monitor_process\nEOF\n\nchmod +x /tmp/process_monitor.sh\n\n# Kernel message monitoring\ncat > /tmp/kernel_monitor.sh << 'EOF'\n#!/bin/bash\nset -euo pipefail\n\nLOG_FILE=\"/var/log/flux-monitoring/kernel.log\"\n\necho \"$(date): Starting kernel message monitoring\" >> \"$LOG_FILE\"\n\n# Monitor dmesg for OOM, GPU, and other critical messages\nsudo dmesg -w | while read line; do\n    echo \"$(date): $line\" >> \"$LOG_FILE\"\n    \n    # Alert on critical messages\n    if echo \"$line\" | grep -i -E \"(oom|killed|cuda|nvidia|memory|hung|panic)\"; then\n        echo \"$(date): CRITICAL: $line\" >> \"$LOG_FILE\"\n    fi\ndone &\n\necho $! > /tmp/kernel_monitor_pid\nEOF\n\nchmod +x /tmp/kernel_monitor.sh\n\necho \"Monitoring infrastructure installed\"\necho \"Available commands:\"\necho \"  - /tmp/system_monitor.py                 # Single system snapshot\"\necho \"  - /tmp/system_monitor.py --continuous    # Continuous monitoring\"\necho \"  - /tmp/process_monitor.sh                # Process monitoring functions\"\necho \"  - /tmp/kernel_monitor.sh                 # Kernel message monitoring\"\necho \"\"\necho \"Logs will be written to /var/log/flux-monitoring/\"\nEOF\n```", "metadata": {"source": "/home/shingai/sort/deployments/monitoring/setup_monitoring.sh", "category": "deployment", "type": "complete_script"}}
{"prompt": "List all EC2 instances using AWS CLI", "completion": "```bash\naws ec2 describe-instances --query \"Reservations[].Instances[].{ID:InstanceId,State:State.Name}\" --output table\n```", "metadata": {"source": "generated_pattern", "service": "ec2", "operation": "describe-instances", "type": "common_pattern"}}
{"prompt": "Launch an EC2 instance using AWS CLI", "completion": "```bash\naws ec2 run-instances --image-id ami-12345678 --count 1 --instance-type t3.micro --key-name <KEY_NAME> --security-group-ids sg-12345678\n```", "metadata": {"source": "generated_pattern", "service": "ec2", "operation": "run-instances", "type": "common_pattern"}}
{"prompt": "Terminate EC2 instances using AWS CLI", "completion": "```bash\naws ec2 terminate-instances --instance-ids <INSTANCE_ID>\n```", "metadata": {"source": "generated_pattern", "service": "ec2", "operation": "terminate-instances", "type": "common_pattern"}}
{"prompt": "List EC2 security groups using AWS CLI", "completion": "```bash\naws ec2 describe-security-groups --help\n```", "metadata": {"source": "generated_pattern", "service": "ec2", "operation": "describe-security-groups", "type": "common_pattern"}}
{"prompt": "Create EC2 security group using AWS CLI", "completion": "```bash\naws ec2 create-security-group --group-name <SECURITY_GROUP_NAME> --description \"Security group for web servers\"\n```", "metadata": {"source": "generated_pattern", "service": "ec2", "operation": "create-security-group", "type": "common_pattern"}}
{"prompt": "Deploy CloudFormation stack using AWS CLI", "completion": "```bash\naws cloudformation deploy --template-file template.yaml --stack-name <STACK_NAME> --capabilities CAPABILITY_IAM\n```", "metadata": {"source": "generated_pattern", "service": "cloudformation", "operation": "deploy", "type": "common_pattern"}}
{"prompt": "Get CloudFormation stack information using AWS CLI", "completion": "```bash\naws cloudformation describe-stacks --stack-name <STACK_NAME> --query \"Stacks[0].StackStatus\" --output text\n```", "metadata": {"source": "generated_pattern", "service": "cloudformation", "operation": "describe-stacks", "type": "common_pattern"}}
{"prompt": "List CloudFormation stacks using AWS CLI", "completion": "```bash\naws cloudformation list-stacks --stack-status-filter CREATE_COMPLETE UPDATE_COMPLETE --query \"StackSummaries[].{Name:StackName,Status:StackStatus}\"\n```", "metadata": {"source": "generated_pattern", "service": "cloudformation", "operation": "list-stacks", "type": "common_pattern"}}
{"prompt": "Delete CloudFormation stack using AWS CLI", "completion": "```bash\naws cloudformation delete-stack --stack-name <STACK_NAME>\n```", "metadata": {"source": "generated_pattern", "service": "cloudformation", "operation": "delete-stack", "type": "common_pattern"}}
{"prompt": "Validate CloudFormation template using AWS CLI", "completion": "```bash\naws cloudformation validate-template --template-body file://template.yaml\n```", "metadata": {"source": "generated_pattern", "service": "cloudformation", "operation": "validate-template", "type": "common_pattern"}}
{"prompt": "Copy files to S3 bucket using AWS CLI", "completion": "```bash\naws s3 cp <FILE_PATH> s3://<BUCKET_NAME>/<KEY>\n```", "metadata": {"source": "generated_pattern", "service": "s3", "operation": "cp", "type": "common_pattern"}}
{"prompt": "Sync directory with S3 bucket using AWS CLI", "completion": "```bash\naws s3 sync <LOCAL_DIR> s3://<BUCKET_NAME>/ --delete\n```", "metadata": {"source": "generated_pattern", "service": "s3", "operation": "sync", "type": "common_pattern"}}
{"prompt": "List S3 bucket contents using AWS CLI", "completion": "```bash\naws s3 ls s3://<BUCKET_NAME>/ --recursive\n```", "metadata": {"source": "generated_pattern", "service": "s3", "operation": "ls", "type": "common_pattern"}}
{"prompt": "Create S3 bucket using AWS CLI", "completion": "```bash\naws s3 mb s3://<BUCKET_NAME>\n```", "metadata": {"source": "generated_pattern", "service": "s3", "operation": "mb", "type": "common_pattern"}}
{"prompt": "Delete S3 bucket using AWS CLI", "completion": "```bash\naws s3 rb s3://<BUCKET_NAME> --force\n```", "metadata": {"source": "generated_pattern", "service": "s3", "operation": "rb", "type": "common_pattern"}}
{"prompt": "Create IAM role using AWS CLI", "completion": "```bash\naws iam create-role --role-name <ROLE_NAME> --assume-role-policy-document file://trust-policy.json\n```", "metadata": {"source": "generated_pattern", "service": "iam", "operation": "create-role", "type": "common_pattern"}}
{"prompt": "Attach policy to IAM role using AWS CLI", "completion": "```bash\naws iam attach-role-policy --role-name <ROLE_NAME> --policy-arn arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess\n```", "metadata": {"source": "generated_pattern", "service": "iam", "operation": "attach-role-policy", "type": "common_pattern"}}
{"prompt": "Create IAM user using AWS CLI", "completion": "```bash\naws iam create-user --user-name <USER_NAME>\n```", "metadata": {"source": "generated_pattern", "service": "iam", "operation": "create-user", "type": "common_pattern"}}
{"prompt": "List IAM roles using AWS CLI", "completion": "```bash\naws iam list-roles --query \"Roles[].{Name:RoleName,Created:CreateDate}\" --output table\n```", "metadata": {"source": "generated_pattern", "service": "iam", "operation": "list-roles", "type": "common_pattern"}}
{"prompt": "Get IAM role details using AWS CLI", "completion": "```bash\naws iam get-role --role-name <ROLE_NAME>\n```", "metadata": {"source": "generated_pattern", "service": "iam", "operation": "get-role", "type": "common_pattern"}}
{"prompt": "Create Lambda function using AWS CLI", "completion": "```bash\naws lambda create-function --function-name <FUNCTION_NAME> --runtime python3.9 --role arn:aws:iam::<ACCOUNT_ID>:role/<ROLE_NAME> --handler lambda_function.lambda_handler --zip-file fileb://function.zip\n```", "metadata": {"source": "generated_pattern", "service": "lambda", "operation": "create-function", "type": "common_pattern"}}
{"prompt": "Update Lambda function code using AWS CLI", "completion": "```bash\naws lambda update-function-code --function-name <FUNCTION_NAME> --zip-file fileb://function.zip\n```", "metadata": {"source": "generated_pattern", "service": "lambda", "operation": "update-function-code", "type": "common_pattern"}}
{"prompt": "Invoke Lambda function using AWS CLI", "completion": "```bash\naws lambda invoke --function-name <FUNCTION_NAME> --payload '{\"key\": \"value\"}' response.json\n```", "metadata": {"source": "generated_pattern", "service": "lambda", "operation": "invoke", "type": "common_pattern"}}
{"prompt": "List Lambda functions using AWS CLI", "completion": "```bash\naws lambda list-functions --query \"Functions[].{Name:FunctionName,Runtime:Runtime}\" --output table\n```", "metadata": {"source": "generated_pattern", "service": "lambda", "operation": "list-functions", "type": "common_pattern"}}
{"prompt": "Delete Lambda function using AWS CLI", "completion": "```bash\naws lambda delete-function --function-name <FUNCTION_NAME>\n```", "metadata": {"source": "generated_pattern", "service": "lambda", "operation": "delete-function", "type": "common_pattern"}}
{"prompt": "Create SNS topic using AWS CLI", "completion": "```bash\naws sns create-topic --name <TOPIC_NAME>\n```", "metadata": {"source": "generated_pattern", "service": "sns", "operation": "create-topic", "type": "common_pattern"}}
{"prompt": "Publish message to SNS topic using AWS CLI", "completion": "```bash\naws sns publish --topic-arn <TOPIC_ARN> --message \"Hello World\"\n```", "metadata": {"source": "generated_pattern", "service": "sns", "operation": "publish", "type": "common_pattern"}}
{"prompt": "Subscribe to SNS topic using AWS CLI", "completion": "```bash\naws sns subscribe --topic-arn <TOPIC_ARN> --protocol email --notification-endpoint <EMAIL>\n```", "metadata": {"source": "generated_pattern", "service": "sns", "operation": "subscribe", "type": "common_pattern"}}
{"prompt": "List SNS topics using AWS CLI", "completion": "```bash\naws sns list-topics --query \"Topics[].TopicArn\" --output table\n```", "metadata": {"source": "generated_pattern", "service": "sns", "operation": "list-topics", "type": "common_pattern"}}
{"prompt": "Delete SNS topic using AWS CLI", "completion": "```bash\naws sns delete-topic --topic-arn <TOPIC_ARN>\n```", "metadata": {"source": "generated_pattern", "service": "sns", "operation": "delete-topic", "type": "common_pattern"}}
{"prompt": "Create CloudWatch alarm using AWS CLI", "completion": "```bash\naws cloudwatch put-metric-alarm --alarm-name <ALARM_NAME> --alarm-description \"CPU utilization\" --metric-name CPUUtilization --namespace AWS/EC2 --statistic Average --period 300 --threshold 80 --comparison-operator GreaterThanThreshold --evaluation-periods 2\n```", "metadata": {"source": "generated_pattern", "service": "cloudwatch", "operation": "put-metric-alarm", "type": "common_pattern"}}
{"prompt": "List CloudWatch alarms using AWS CLI", "completion": "```bash\naws cloudwatch describe-alarms --query \"MetricAlarms[].{Name:AlarmName,State:StateValue}\" --output table\n```", "metadata": {"source": "generated_pattern", "service": "cloudwatch", "operation": "describe-alarms", "type": "common_pattern"}}
{"prompt": "Send custom metrics to CloudWatch using AWS CLI", "completion": "```bash\naws cloudwatch put-metric-data --namespace \"Custom/Application\" --metric-data MetricName=PageViews,Value=100,Unit=Count\n```", "metadata": {"source": "generated_pattern", "service": "cloudwatch", "operation": "put-metric-data", "type": "common_pattern"}}
{"prompt": "Get CloudWatch metrics using AWS CLI", "completion": "```bash\naws cloudwatch get-metric-statistics --namespace AWS/EC2 --metric-name CPUUtilization --start-time 2023-01-01T00:00:00Z --end-time 2023-01-01T23:59:59Z --period 3600 --statistics Average\n```", "metadata": {"source": "generated_pattern", "service": "cloudwatch", "operation": "get-metric-statistics", "type": "common_pattern"}}
{"prompt": "Delete CloudWatch alarms using AWS CLI", "completion": "```bash\naws cloudwatch delete-alarms --alarm-names <ALARM_NAME>\n```", "metadata": {"source": "generated_pattern", "service": "cloudwatch", "operation": "delete-alarms", "type": "common_pattern"}}
{"prompt": "Create AWS CLI script to launch EC2 instance with security group", "completion": "```bash\n#!/bin/bash\n# Launch EC2 instance with custom security group\n\n# Create security group\naws ec2 create-security-group \\\n    --group-name web-servers \\\n    --description \"Security group for web servers\"\n\n# Add SSH access rule\naws ec2 authorize-security-group-ingress \\\n    --group-name web-servers \\\n    --protocol tcp \\\n    --port 22 \\\n    --cidr 0.0.0.0/0\n\n# Add HTTP access rule  \naws ec2 authorize-security-group-ingress \\\n    --group-name web-servers \\\n    --protocol tcp \\\n    --port 80 \\\n    --cidr 0.0.0.0/0\n\n# Launch instance\naws ec2 run-instances \\\n    --image-id ami-12345678 \\\n    --count 1 \\\n    --instance-type t3.micro \\\n    --key-name <KEY_NAME> \\\n    --security-groups web-servers \\\n    --tag-specifications 'ResourceType=instance,Tags=[{Key=Name,Value=WebServer}]'\n```", "metadata": {"source": "common_pattern", "type": "multi_step_deployment"}}
{"prompt": "Create AWS CLI script to deploy and monitor CloudFormation stack", "completion": "```bash\n#!/bin/bash\n# Deploy CloudFormation stack with monitoring\n\nSTACK_NAME=\"<STACK_NAME>\"\nTEMPLATE_FILE=\"infrastructure.yaml\"\n\n# Validate template\necho \"Validating CloudFormation template...\"\naws cloudformation validate-template --template-body file://$TEMPLATE_FILE\n\n# Deploy stack\necho \"Deploying CloudFormation stack...\"\naws cloudformation deploy \\\n    --template-file $TEMPLATE_FILE \\\n    --stack-name $STACK_NAME \\\n    --capabilities CAPABILITY_IAM \\\n    --parameter-overrides Environment=production\n\n# Wait for completion\necho \"Waiting for stack deployment to complete...\"\naws cloudformation wait stack-deploy-complete --stack-name $STACK_NAME\n\n# Get stack outputs\necho \"Stack deployment completed. Outputs:\"\naws cloudformation describe-stacks \\\n    --stack-name $STACK_NAME \\\n    --query \"Stacks[0].Outputs[].{Key:OutputKey,Value:OutputValue}\" \\\n    --output table\n```", "metadata": {"source": "common_pattern", "type": "cloudformation_deployment"}}
{"prompt": "Create AWS CLI script for S3 backup with lifecycle management", "completion": "```bash\n#!/bin/bash\n# S3 backup script with lifecycle management\n\nBUCKET_NAME=\"<BUCKET_NAME>\"\nLOCAL_DIR=\"<LOCAL_DIR>\"\n\n# Create S3 bucket\naws s3 mb s3://$BUCKET_NAME\n\n# Enable versioning\naws s3api put-bucket-versioning \\\n    --bucket $BUCKET_NAME \\\n    --versioning-configuration Status=Enabled\n\n# Sync local directory to S3\naws s3 sync $LOCAL_DIR s3://$BUCKET_NAME/ \\\n    --delete \\\n    --exclude \"*.tmp\" \\\n    --exclude \".DS_Store\"\n\n# Set lifecycle policy\naws s3api put-bucket-lifecycle-configuration \\\n    --bucket $BUCKET_NAME \\\n    --lifecycle-configuration file://lifecycle-policy.json\n\necho \"Backup completed to s3://$BUCKET_NAME\"\n```", "metadata": {"source": "common_pattern", "type": "s3_backup"}}
