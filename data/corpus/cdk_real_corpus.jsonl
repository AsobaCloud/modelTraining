{"prompt": "Create an AWS CDK typescript application with Lambda functions", "completion": "```typescript\nimport events = require('aws-cdk-lib/aws-events');\nimport targets = require('aws-cdk-lib/aws-events-targets');\nimport lambda = require('aws-cdk-lib/aws-lambda');\nimport cdk = require('aws-cdk-lib');\n\nimport fs = require('fs');\n\nexport class LambdaCronStack extends cdk.Stack {\n  constructor(app: cdk.App, id: string) {\n    super(app, id);\n\n    const lambdaFn = new lambda.Function(this, 'Singleton', {\n      code: new lambda.InlineCode(fs.readFileSync('lambda-handler.py', { encoding: 'utf-8' })),\n      handler: 'index.main',\n      timeout: cdk.Duration.seconds(300),\n      runtime: lambda.Runtime.PYTHON_3_9,\n    });\n\n    // Run 6:00 PM UTC every Monday through Friday\n    // See https://docs.aws.amazon.com/lambda/latest/dg/tutorial-scheduled-events-schedule-expressions.html\n    const rule = new events.Rule(this, 'Rule', {\n      schedule: events.Schedule.expression('cron(0 18 ? * MON-FRI *)')\n    });\n\n    rule.addTarget(new targets.LambdaFunction(lambdaFn));\n  }\n}\n\nconst app = new cdk.App();\nnew LambdaCronStack(app, 'LambdaCronExample');\napp.synth();\n```", "metadata": {"source": "github_repository", "repository": "aws-samples/aws-cdk-examples", "file_path": "typescript/lambda-cron/index.ts", "language": "typescript", "category": "serverless", "authentic": true}}
{"prompt": "Create an AWS CDK typescript application with Lambda functions", "completion": "```typescript\nimport { IResource, LambdaIntegration, MockIntegration, PassthroughBehavior, RestApi } from 'aws-cdk-lib/aws-apigateway';\nimport { AttributeType, Table } from 'aws-cdk-lib/aws-dynamodb';\nimport { Runtime } from 'aws-cdk-lib/aws-lambda';\nimport { App, Stack, RemovalPolicy } from 'aws-cdk-lib';\nimport { NodejsFunction, NodejsFunctionProps } from 'aws-cdk-lib/aws-lambda-nodejs';\nimport { join } from 'path'\n\nexport class ApiLambdaCrudDynamoDBStack extends Stack {\n  constructor(app: App, id: string) {\n    super(app, id);\n\n    const dynamoTable = new Table(this, 'items', {\n      partitionKey: {\n        name: 'itemId',\n        type: AttributeType.STRING\n      },\n      tableName: 'items',\n\n      /**\n       *  The default removal policy is RETAIN, which means that cdk destroy will not attempt to delete\n       * the new table, and it will remain in your account until manually deleted. By setting the policy to\n       * DESTROY, cdk destroy will delete the table (even if it has data in it)\n       */\n      removalPolicy: RemovalPolicy.DESTROY, // NOT recommended for production code\n    });\n\n    const nodeJsFunctionProps: NodejsFunctionProps = {\n      bundling: {\n        externalModules: [\n          'aws-sdk', // Use the 'aws-sdk' available in the Lambda runtime\n        ],\n      },\n      depsLockFilePath: join(__dirname, 'lambdas', 'package-lock.json'),\n      environment: {\n        PRIMARY_KEY: 'itemId',\n        TABLE_NAME: dynamoTable.tableName,\n      },\n      runtime: Runtime.NODEJS_20_X,\n    }\n\n    // Create a Lambda function for each of the CRUD operations\n    const getOneLambda = new NodejsFunction(this, 'getOneItemFunction', {\n      entry: join(__dirname, 'lambdas', 'get-one.ts'),\n      ...nodeJsFunctionProps,\n    });\n    const getAllLambda = new NodejsFunction(this, 'getAllItemsFunction', {\n      entry: join(__dirname, 'lambdas', 'get-all.ts'),\n      ...nodeJsFunctionProps,\n    });\n    const createOneLambda = new NodejsFunction(this, 'createItemFunction', {\n      entry: join(__dirname, 'lambdas', 'create.ts'),\n      ...nodeJsFunctionProps,\n    });\n    const updateOneLambda = new NodejsFunction(this, 'updateItemFunction', {\n      entry: join(__dirname, 'lambdas', 'update-one.ts'),\n      ...nodeJsFunctionProps,\n    });\n    const deleteOneLambda = new NodejsFunction(this, 'deleteItemFunction', {\n      entry: join(__dirname, 'lambdas', 'delete-one.ts'),\n      ...nodeJsFunctionProps,\n    });\n\n    // Grant the Lambda function read access to the DynamoDB table\n    dynamoTable.grantReadWriteData(getAllLambda);\n    dynamoTable.grantReadWriteData(getOneLambda);\n    dynamoTable.grantReadWriteData(createOneLambda);\n    dynamoTable.grantReadWriteData(updateOneLambda);\n    dynamoTable.grantReadWriteData(deleteOneLambda);\n\n    // Integrate the Lambda functions with the API Gateway resource\n    const getAllIntegration = new LambdaIntegration(getAllLambda);\n    const createOneIntegration = new LambdaIntegration(createOneLambda);\n    const getOneIntegration = new LambdaIntegration(getOneLambda);\n    const updateOneIntegration = new LambdaIntegration(updateOneLambda);\n    const deleteOneIntegration = new LambdaIntegration(deleteOneLambda);\n\n\n    // Create an API Gateway resource for each of the CRUD operations\n    const api = new RestApi(this, 'itemsApi', {\n      restApiName: 'Items Service'\n      // In case you want to manage binary types, uncomment the following\n      // binaryMediaTypes: [\"*/*\"],\n    });\n\n    const items = api.root.addResource('items');\n    items.addMethod('GET', getAllIntegration);\n    items.addMethod('POST', createOneIntegration);\n    addCorsOptions(items);\n\n    const singleItem = items.addResource('{id}');\n    singleItem.addMethod('GET', getOneIntegration);\n    singleItem.addMethod('PATCH', updateOneIntegration);\n    singleItem.addMethod('DELETE', deleteOneIntegration);\n    addCorsOptions(singleItem);\n  }\n}\n\nexport function addCorsOptions(apiResource: IResource) {\n  apiResource.addMethod('OPTIONS', new MockIntegration({\n    // In case you want to use binary media types, uncomment the following line\n    // contentHandling: ContentHandling.CONVERT_TO_TEXT,\n    integrationResponses: [{\n      statusCode: '200',\n      responseParameters: {\n        'method.response.header.Access-Control-Allow-Headers': \"'Content-Type,X-Amz-Date,Authorization,X-Api-Key,X-Amz-Security-Token,X-Amz-User-Agent'\",\n        'method.response.header.Access-Control-Allow-Origin': \"'*'\",\n        'method.response.header.Access-Control-Allow-Credentials': \"'false'\",\n        'method.response.header.Access-Control-Allow-Methods': \"'OPTIONS,GET,PUT,POST,DELETE'\",\n      },\n    }],\n    // In case you want to use binary media types, comment out the following line\n    passthroughBehavior: PassthroughBehavior.NEVER,\n    requestTemplates: {\n      \"application/json\": \"{\\\"statusCode\\\": 200}\"\n    },\n  }), {\n    methodResponses: [{\n      statusCode: '200',\n      responseParameters: {\n        'method.response.header.Access-Control-Allow-Headers': true,\n        'method.response.header.Access-Control-Allow-Methods': true,\n        'method.response.header.Access-Control-Allow-Credentials': true,\n        'method.response.header.Access-Control-Allow-Origin': true,\n      },\n    }]\n  })\n}\n\nconst app = new App();\nnew ApiLambdaCrudDynamoDBStack(app, 'ApiLambdaCrudDynamoDBExample');\napp.synth();\n```", "metadata": {"source": "github_repository", "repository": "aws-samples/aws-cdk-examples", "file_path": "typescript/api-cors-lambda-crud-dynamodb/index.ts", "language": "typescript", "category": "serverless", "authentic": true}}
{"prompt": "Create an AWS CDK python application with Lambda functions", "completion": "```python\nfrom aws_cdk import (\n    aws_events as events,\n    aws_lambda as lambda_,\n    aws_events_targets as targets,\n    App, Duration, Stack\n)\n\n\nclass LambdaCronStack(Stack):\n    def __init__(self, app: App, id: str) -> None:\n        super().__init__(app, id)\n\n        with open(\"lambda-handler.py\", encoding=\"utf8\") as fp:\n            handler_code = fp.read()\n\n        lambdaFn = lambda_.Function(\n            self, \"Singleton\",\n            code=lambda_.InlineCode(handler_code),\n            handler=\"index.main\",\n            timeout=Duration.seconds(300),\n            runtime=lambda_.Runtime.PYTHON_3_12,\n        )\n\n        # Run every day at 6PM UTC\n        # See https://docs.aws.amazon.com/lambda/latest/dg/tutorial-scheduled-events-schedule-expressions.html\n        rule = events.Rule(\n            self, \"Rule\",\n            schedule=events.Schedule.cron(\n                minute='0',\n                hour='18',\n                month='*',\n                week_day='MON-FRI',\n                year='*'),\n        )\n        rule.add_target(targets.LambdaFunction(lambdaFn))\n\n\napp = App()\nLambdaCronStack(app, \"LambdaCronExample\")\napp.synth()\n```", "metadata": {"source": "github_repository", "repository": "aws-samples/aws-cdk-examples", "file_path": "python/lambda-cron/app.py", "language": "python", "category": "serverless", "authentic": true}}
{"prompt": "Create an AWS CDK typescript application with DynamoDB", "completion": "```typescript\nimport {\n  CfnOutput,\n  Stack,\n  StackProps,\n  Tags,\n  App,\n  Fn,\n  Duration,\n  RemovalPolicy,\n} from 'aws-cdk-lib';\nimport * as ec2 from 'aws-cdk-lib/aws-ec2';\nimport * as logs from 'aws-cdk-lib/aws-logs';\nimport * as rds from 'aws-cdk-lib/aws-rds';\nimport * as secretsmanager from 'aws-cdk-lib/aws-secretsmanager';\nimport { Construct } from 'constructs';\n\nexport interface OracleProps extends StackProps {\n\n  /**\n   * VPC Id\n   * @type {string}\n   * @memberof OracleProps\n   */\n  readonly vpcId?: string;\n\n  /**\n   * List of Subnet\n   * @type {string[]}\n   * @memberof OracleProps\n   */\n  readonly subnetIds?: string[];\n\n\n  /**\n   * provide the name of the database\n   * @type {string}\n   * @memberof OracleProps\n   */\n  readonly dbName?: string;\n\n  /**\n   *\n   * ec2.InstanceType.of(ec2.InstanceClass.BURSTABLE3, ec2.InstanceSize.LARGE)\n   * @type {*}\n   * @memberof OracleProps\n   * @default ec2.InstanceType.of(ec2.InstanceClass.BURSTABLE3, ec2.InstanceSize.LARGE)\n   */\n  readonly instanceType?: any;\n\n  /**\n   * provide the version of the database (rds.OracleEngineVersion.VER_19_0_0_0_2021_04_R1)\n   * @type {*}\n   * @memberof OracleProps\n   * @default rds.OracleEngineVersion.VER_19_0_0_0_2021_04_R1\n   */\n  readonly engineVersion?: any;\n\n  /**\n   * user name of the database\n   * @type {str}\n   * @memberof OracleProps\n   * @default dbadmin\n   */\n  readonly oracleUsername?: string;\n\n  /**\n   * backup retention days for example 14\n   * @type {number}\n   * @memberof OracleProps\n   * @default 14\n   */\n  readonly backupRetentionDays?: number;\n\n  /**\n   * backup window time 00:15-01:15\n   * @type {string}\n   * @memberof OracleProps\n   * @default 00:15-01:15\n   */\n\n  readonly backupWindow?: string;\n\n  /**\n   *\n   * maintenance time Sun:23:45-Mon:00:15\n   * @type {string}\n   * @memberof OracleProps\n   * @default Sun:23:45-Mon:00:15\n   */\n  readonly preferredMaintenanceWindow?: string;\n\n  /**\n   *\n   * list of ingress sources\n   * @type {any []}\n   * @memberof OracleProps\n   */\n  readonly ingressSources?: any[];\n}\n\nexport class Oracle extends Stack {\n  constructor(scope: Construct, id: string, props: OracleProps) {\n    super(scope, id);\n\n    // default database username\n    let oracleUsername = \"dbadmin\";\n    if (typeof props.oracleUsername !== 'undefined') {\n      oracleUsername = \"dbadmin\";\n    }\n    let ingressSources = [];\n    if (typeof props.ingressSources !== 'undefined') {\n      ingressSources = props.ingressSources;\n    }\n    let engineVersion = rds.OracleEngineVersion.VER_19_0_0_0_2021_04_R1;\n    if (typeof props.engineVersion !== 'undefined') {\n      engineVersion = props.engineVersion;\n    }\n\n    const azs = Fn.getAzs();\n\n    // vpc\n    const vpc = ec2.Vpc.fromVpcAttributes(this, 'ExistingVPC', {\n      vpcId: props.vpcId!,\n      availabilityZones: azs,\n    });\n\n    // Subnets\n    const subnets: any[] = [];\n\n    for (let subnetId of props.subnetIds!) {\n      const subid = subnetId\n        .replace('_', '')\n        .replace(' ', '');\n      subnets.push(\n        ec2.Subnet.fromSubnetAttributes(this, subid, {\n          subnetId: subid,\n        }),\n      );\n    }\n\n    const vpcSubnets: ec2.SubnetSelection = {\n      subnets: subnets,\n    };\n\n    const allAll = ec2.Port.allTraffic();\n    const tcp1521 = ec2.Port.tcpRange(1521, 1521);\n    const tcp1526 = ec2.Port.tcpRange(1526, 1526);\n    const tcp1575 = ec2.Port.tcpRange(1575, 1575);\n\n    const dbsg = new ec2.SecurityGroup(this, 'DatabaseSecurityGroup', {\n      vpc: vpc,\n      allowAllOutbound: true,\n      description: id + 'Database',\n      securityGroupName: id + 'Database',\n    });\n\n    dbsg.addIngressRule(dbsg, allAll, 'all from self');\n    dbsg.addEgressRule(ec2.Peer.ipv4('0.0.0.0/0'), allAll, 'all out');\n\n    const oracleConnectionPorts = [\n      { port: tcp1521, description: 'tcp1521 Oracle' },\n      { port: tcp1526, description: 'tcp1526 Oracle' },\n      { port: tcp1575, description: 'tcp1575 Oracle' },\n    ];\n\n    for (let ingressSource of ingressSources!) {\n      for (let c of oracleConnectionPorts) {\n        dbsg.addIngressRule(ingressSource, c.port, c.description);\n      }\n    }\n\n    const dbSubnetGroup = new rds.SubnetGroup(this, 'DatabaseSubnetGroup', {\n      vpc: vpc,\n      description: id + 'subnet group',\n      vpcSubnets: vpcSubnets,\n      subnetGroupName: id + 'subnet group',\n    });\n\n    const oracleSecret = new secretsmanager.Secret(this, 'OracleCredentials', {\n      secretName: props.dbName + 'OracleCredentials',\n      description: props.dbName + 'Oracle Database Crendetials',\n      generateSecretString: {\n        excludeCharacters: \"\\\"@/\\\\ '\",\n        generateStringKey: 'password',\n        passwordLength: 30,\n        secretStringTemplate: JSON.stringify({username: oracleUsername}),\n      },\n    });\n\n    const oracleCredentials = rds.Credentials.fromSecret(\n      oracleSecret,\n      oracleUsername,\n    );\n\n    const dbParameterGroup = new rds.ParameterGroup(this, 'ParameterGroup', {\n      engine: rds.DatabaseInstanceEngine.oracleEe({\n        version: engineVersion,\n      }),\n      parameters: { open_cursors: '2500' },\n    });\n\n    const oracleInstance = new rds.DatabaseInstance(this, 'OracleDatabase', {\n      databaseName: props.dbName,\n      instanceIdentifier: props.dbName,\n      credentials: oracleCredentials,\n      engine: rds.DatabaseInstanceEngine.oracleEe({\n        version: engineVersion,\n      }),\n      backupRetention: Duration.days(7),\n      allocatedStorage: 20,\n      securityGroups: [dbsg],\n      licenseModel: rds.LicenseModel.BRING_YOUR_OWN_LICENSE,\n      allowMajorVersionUpgrade: true,\n      autoMinorVersionUpgrade: true,\n      instanceType: props.instanceType,\n      vpcSubnets: vpcSubnets,\n      vpc: vpc,\n      removalPolicy: RemovalPolicy.RETAIN,\n      multiAz: true,\n      storageEncrypted: true,\n      monitoringInterval: Duration.seconds(60),\n      enablePerformanceInsights: true,\n      cloudwatchLogsExports: ['trace', 'audit', 'alert', 'listener'],\n      cloudwatchLogsRetention: logs.RetentionDays.ONE_MONTH,\n      parameterGroup: dbParameterGroup,\n      subnetGroup: dbSubnetGroup,\n      preferredBackupWindow: props.backupWindow,\n      preferredMaintenanceWindow: props.preferredMaintenanceWindow,\n      publiclyAccessible: false,\n    });\n\n    oracleInstance.addRotationSingleUser();\n\n    // Tags\n    Tags.of(oracleInstance).add('Name', 'OracleDatabase', {\n      priority: 300,\n    });\n\n\n    new CfnOutput(this, 'OracleEndpoint', {\n      exportName: 'OracleEndPoint',\n      value: oracleInstance.dbInstanceEndpointAddress,\n    });\n\n    new CfnOutput(this, 'OracleUserName', {\n      exportName: 'OracleUserName',\n      value: oracleUsername,\n    });\n\n    new CfnOutput(this, 'OracleDbName', {\n      exportName: 'OracleDbName',\n      value: props.dbName!,\n    });\n  }\n}\n\nconst app = new App();\n\nnew Oracle(app, 'OracleStack', {\n  env:{region:\"us-east-2\"}, description:\"Oracle Stack\",\n  vpcId:\"vpc-aaaaaaaa\",\n  subnetIds:[\"subnet-xxxxxxxx\", \"subnet-yyyyyyyy\", \"subnet-zzzzzzzz\"],\n  dbName:\"sampledb\"\n});\n```", "metadata": {"source": "github_repository", "repository": "aws-samples/aws-cdk-examples", "file_path": "typescript/rds/oracle/oracle.ts", "language": "typescript", "category": "database", "authentic": true}}
{"prompt": "Write an AWS CDK python application for S3 storage", "completion": "```python\n#!/usr/bin/env python3\n# -*- encoding: utf-8 -*-\n# vim: tabstop=2 shiftwidth=2 softtabstop=2 expandtab\n\nimport os\nimport aws_cdk as cdk\n\nfrom aws_cdk import (\n  Stack,\n  aws_ec2\n)\nfrom constructs import Construct\n\n\nclass VpcStack(Stack):\n\n  def __init__(self, scope: Construct, construct_id: str, **kwargs) -> None:\n    super().__init__(scope, construct_id, **kwargs)\n\n    #XXX: For creating this CDK Stack in the existing VPC,\n    # remove comments from the below codes and\n    # comments out vpc = aws_ec2.Vpc(..) codes,\n    # then pass -c vpc_name=your-existing-vpc to cdk command\n    # for example,\n    # cdk -c vpc_name=your-existing-vpc syth\n    #\n    if str(os.environ.get('USE_DEFAULT_VPC', 'false')).lower() == 'true':\n      vpc_name = self.node.try_get_context('vpc_name') or \"default\"\n      self.vpc = aws_ec2.Vpc.from_lookup(self, 'ExistingVPC',\n        is_default=True,\n        vpc_name=vpc_name\n      )\n    else:\n      #XXX: To use more than 2 AZs, be sure to specify the account and region on your stack.\n      #XXX: https://docs.aws.amazon.com/cdk/api/latest/python/aws_cdk.aws_ec2/Vpc.html\n      self.vpc = aws_ec2.Vpc(self, 'AuroraMySQLVPC',\n        ip_addresses=aws_ec2.IpAddresses.cidr(\"10.0.0.0/16\"),\n        max_azs=3,\n\n        # 'subnetConfiguration' specifies the \"subnet groups\" to create.\n        # Every subnet group will have a subnet for each AZ, so this\n        # configuration will create `2 groups Ã— 3 AZs = 6` subnets.\n        subnet_configuration=[\n          {\n            \"cidrMask\": 20,\n            \"name\": \"Public\",\n            \"subnetType\": aws_ec2.SubnetType.PUBLIC,\n          },\n          {\n            \"cidrMask\": 20,\n            \"name\": \"Private\",\n            \"subnetType\": aws_ec2.SubnetType.PRIVATE_WITH_EGRESS\n          }\n        ],\n        gateway_endpoints={\n          \"S3\": aws_ec2.GatewayVpcEndpointOptions(\n            service=aws_ec2.GatewayVpcEndpointAwsService.S3\n          )\n        }\n      )\n\n\n    cdk.CfnOutput(self, 'VPCID', value=self.vpc.vpc_id,\n      export_name=f'{self.stack_name}-VPCID')\n```", "metadata": {"source": "github_repository", "repository": "ksmin23/my-aws-cdk-examples", "file_path": "/tmp/tmp888ifwn5/ksmin23_my-aws-cdk-examples/dms/aurora_mysql-to-s3/cdk_stacks/vpc.py", "language": "python", "category": "storage", "authentic": true}}
{"prompt": "Write an AWS CDK python application for S3 storage", "completion": "```python\n#!/usr/bin/env python3\n# -*- encoding: utf-8 -*-\n# vim: tabstop=2 shiftwidth=2 softtabstop=2 expandtab\n\nimport json\n\nimport aws_cdk as cdk\n\nfrom aws_cdk import (\n  Stack,\n  aws_ec2,\n  aws_dms,\n  aws_iam,\n  aws_secretsmanager\n)\nfrom constructs import Construct\n\n\nclass DMSAuroraMysqlToS3Stack(Stack):\n\n  def __init__(self, scope: Construct, construct_id: str, vpc, **kwargs) -> None:\n    super().__init__(scope, construct_id, **kwargs)\n\n    database_name = self.node.try_get_context('source_database_name')\n    table_name = self.node.try_get_context('source_table_name')\n\n    s3_bucket_name = self.node.try_get_context('target_s3_bucket_name')\n    s3_bucket_folder_name = self.node.try_get_context('target_s3_bucket_folder_name')\n\n    db_client_sg_name = self.node.try_get_context('mysql_client_security_group_name')\n    db_client_sg = aws_ec2.SecurityGroup.from_lookup_by_name(self, 'MySQLClientSG', db_client_sg_name, vpc)\n\n    dms_replication_subnet_group = aws_dms.CfnReplicationSubnetGroup(self, 'DMSReplicationSubnetGroup',\n      replication_subnet_group_description='DMS Replication Subnet Group',\n      subnet_ids=vpc.select_subnets(subnet_type=aws_ec2.SubnetType.PRIVATE_WITH_EGRESS).subnet_ids\n    )\n\n    dms_replication_instance = aws_dms.CfnReplicationInstance(self, 'DMSReplicationInstance',\n      replication_instance_class='dms.t3.medium',\n      allocated_storage=50,\n      allow_major_version_upgrade=False,\n      auto_minor_version_upgrade=False,\n      engine_version='3.4.6',\n      multi_az=False,\n      preferred_maintenance_window='sat:03:17-sat:03:47',\n      publicly_accessible=False,\n      replication_subnet_group_identifier=dms_replication_subnet_group.ref,\n      vpc_security_group_ids=[db_client_sg.security_group_id]\n    )\n\n    #XXX: If you use `aws_cdk.SecretValue.unsafe_unwrap()` to get any secret value,\n    # you may probably encounter ValueError; for example, invalid literal for int() with base 10: '${Token[TOKEN.228]}'\n    # So you should need to make the API call in order to access a secret inside it.\n    secret_name = self.node.try_get_context('source_database_secret_name')\n    secret = aws_secretsmanager.Secret.from_secret_name_v2(self, 'MySQLAdminUserSecret',\n      secret_name)\n\n    source_endpoint_id = secret.secret_value_from_json(\"dbClusterIdentifier\").unsafe_unwrap()\n    dms_source_endpoint = aws_dms.CfnEndpoint(self, 'DMSSourceEndpoint',\n      endpoint_identifier=source_endpoint_id,\n      endpoint_type='source',\n      engine_name=secret.secret_value_from_json(\"engine\").unsafe_unwrap(),\n      server_name=secret.secret_value_from_json(\"host\").unsafe_unwrap(),\n      port=3306,\n      database_name=database_name,\n      username=secret.secret_value_from_json(\"username\").unsafe_unwrap(),\n      password=secret.secret_value_from_json(\"password\").unsafe_unwrap()\n    )\n\n    dms_s3_access_role_policy_doc = aws_iam.PolicyDocument()\n    dms_s3_access_role_policy_doc.add_statements(aws_iam.PolicyStatement(**{\n      \"effect\": aws_iam.Effect.ALLOW,\n      \"resources\": [\"*\"],\n      \"actions\": [\n        \"s3:PutObject\",\n        \"s3:DeleteObject\",\n        \"s3:PutObjectTagging\",\n        \"s3:ListBucket\"]\n    }))\n\n    dms_target_s3_access_role = aws_iam.Role(self, 'DMSTargetS3AccessRole',\n      role_name='DMSTargetS3AccessRole',\n      assumed_by=aws_iam.ServicePrincipal('dms.amazonaws.com'),\n      inline_policies={\n        'S3AccessRole': dms_s3_access_role_policy_doc\n      }\n    )\n\n    target_endpoint_id = f\"{source_endpoint_id}-to-s3\"\n    dms_target_endpoint = aws_dms.CfnEndpoint(self, 'DMSTargetEndpoint',\n      endpoint_identifier=target_endpoint_id,\n      endpoint_type='target',\n      engine_name='s3',\n      s3_settings=aws_dms.CfnEndpoint.S3SettingsProperty(\n        bucket_name=s3_bucket_name,\n        bucket_folder=s3_bucket_folder_name,\n        service_access_role_arn=dms_target_s3_access_role.role_arn,\n        data_format='parquet',\n        parquet_timestamp_in_millisecond=True\n      )\n    )\n\n    table_mappings_json = {\n      \"rules\": [\n        {\n          \"rule-type\": \"selection\",\n          \"rule-id\": \"1\",\n          \"rule-name\": \"1\",\n          \"object-locator\": {\n            \"schema-name\": database_name,\n            \"table-name\": table_name\n          },\n          \"rule-action\": \"include\",\n          \"filters\": []\n        }\n      ]\n    }\n\n    #XXX: AWS DMS - Using Amazon Kinesis Data Streams as a target for AWS Database Migration Service\n    # https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.Kinesis.html\n    task_settings_json = {\n      # Multithreaded full load task settings\n      \"FullLoadSettings\": {\n        \"MaxFullLoadSubTasks\": 8,\n      }\n    }\n\n    dms_replication_task = aws_dms.CfnReplicationTask(self, 'DMSReplicationTask',\n      replication_task_identifier='DMSMySQLToS3Task',\n      replication_instance_arn=dms_replication_instance.ref,\n      migration_type='full-load', # [ full-load | cdc | full-load-and-cdc ]\n      source_endpoint_arn=dms_source_endpoint.ref,\n      target_endpoint_arn=dms_target_endpoint.ref,\n      table_mappings=json.dumps(table_mappings_json),\n      replication_task_settings=json.dumps(task_settings_json)\n    )\n\n\n    cdk.CfnOutput(self, 'DMSReplicationTaskArn',\n      value=dms_replication_task.ref,\n      export_name=f'{self.stack_name}-DMSReplicationTaskArn')\n    cdk.CfnOutput(self, 'DMSReplicationTaskId',\n      value=dms_replication_task.replication_task_identifier,\n      export_name=f'{self.stack_name}-DMSReplicationTaskId')\n    cdk.CfnOutput(self, 'DMSSourceEndpointId',\n      value=dms_source_endpoint.endpoint_identifier,\n      export_name=f'{self.stack_name}-DMSSourceEndpointId')\n    cdk.CfnOutput(self, 'DMSTargetEndpointId',\n      value=dms_target_endpoint.endpoint_identifier,\n      export_name=f'{self.stack_name}-DMSTargetEndpointId')\n```", "metadata": {"source": "github_repository", "repository": "ksmin23/my-aws-cdk-examples", "file_path": "/tmp/tmp888ifwn5/ksmin23_my-aws-cdk-examples/dms/aurora_mysql-to-s3/cdk_stacks/dms_aurora_mysql_to_s3.py", "language": "python", "category": "storage", "authentic": true}}
{"prompt": "Write an AWS CDK python application for VPC networking", "completion": "```python\n#!/usr/bin/env python3\n# -*- encoding: utf-8 -*-\n# vim: tabstop=2 shiftwidth=2 softtabstop=2 expandtab\n\nimport aws_cdk as cdk\n\nfrom aws_cdk import (\n  Stack,\n  aws_iam\n)\nfrom constructs import Construct\n\n\nclass DmsIAMRolesStack(Stack):\n\n  def __init__(self, scope: Construct, construct_id: str, **kwargs) -> None:\n    super().__init__(scope, construct_id, **kwargs)\n\n    dms_vpc_role = aws_iam.Role(self, 'DMSVpcRole',\n      role_name='dms-vpc-role',\n      assumed_by=aws_iam.ServicePrincipal('dms.amazonaws.com'),\n      managed_policies=[\n        aws_iam.ManagedPolicy.from_aws_managed_policy_name('service-role/AmazonDMSVPCManagementRole'),\n      ]\n    )\n\n    dms_cloudwatch_logs_role = aws_iam.Role(self, 'DMSCloudWatchLogsRole',\n      role_name='dms-cloudwatch-logs-role',\n      assumed_by=aws_iam.ServicePrincipal('dms.amazonaws.com'),\n      managed_policies=[\n        aws_iam.ManagedPolicy.from_aws_managed_policy_name('service-role/AmazonDMSCloudWatchLogsRole'),\n      ]\n    )\n\n\n    cdk.CfnOutput(self, 'DMSVpcRoleArn',\n      value=dms_vpc_role.role_arn,\n      export_name=f'{self.stack_name}-DMSVpcRoleArn')\n    cdk.CfnOutput(self, 'DMSCloudWatchLogsRoleArn',\n      value=dms_cloudwatch_logs_role.role_arn,\n      export_name=f'{self.stack_name}-DMSCloudWatchLogsRoleArn')\n```", "metadata": {"source": "github_repository", "repository": "ksmin23/my-aws-cdk-examples", "file_path": "/tmp/tmp888ifwn5/ksmin23_my-aws-cdk-examples/dms/aurora_mysql-to-s3/cdk_stacks/dms_iam_roles.py", "language": "python", "category": "networking", "authentic": true}}
{"prompt": "Write an AWS CDK python application for S3 storage", "completion": "```python\n#!/usr/bin/env python3\n# -*- encoding: utf-8 -*-\n# vim: tabstop=2 shiftwidth=2 softtabstop=2 expandtab\n\nimport os\n\nfrom cdk_stacks import (\n  VpcStack,\n  DmsIAMRolesStack,\n  DMSAuroraMysqlToS3Stack\n)\n\nimport aws_cdk as cdk\n\n\nAWS_ENV = cdk.Environment(\n  account=os.getenv('CDK_DEFAULT_ACCOUNT'),\n  region=os.getenv('CDK_DEFAULT_REGION')\n)\n\napp = cdk.App()\n\nvpc_stack = VpcStack(app, \"DMSAuroraMysqlToS3VPCStack\",\n  env=AWS_ENV\n)\n\ndms_iam_permissions = DmsIAMRolesStack(app, 'DMSRequiredIAMRolesStack')\ndms_iam_permissions.add_dependency(vpc_stack)\n\ndms_task_stack = DMSAuroraMysqlToS3Stack(app, \"DMSAuroraMysqlToS3Stack\",\n  vpc_stack.vpc,\n  env=AWS_ENV\n)\ndms_task_stack.add_dependency(dms_iam_permissions)\n\napp.synth()\n```", "metadata": {"source": "github_repository", "repository": "ksmin23/my-aws-cdk-examples", "file_path": "/tmp/tmp888ifwn5/ksmin23_my-aws-cdk-examples/dms/aurora_mysql-to-s3/app.py", "language": "python", "category": "storage", "authentic": true}}
{"prompt": "Create an AWS CDK python application with DynamoDB", "completion": "```python\n#!/usr/bin/env python3\n# -*- encoding: utf-8 -*-\n# vim: tabstop=2 shiftwidth=2 softtabstop=2 expandtab\n\nimport json\n\nimport aws_cdk as cdk\n\nfrom aws_cdk import (\n  Stack,\n  aws_ec2,\n  aws_dms,\n  aws_iam,\n  aws_secretsmanager\n)\nfrom constructs import Construct\n\nclass DMSAuroraMysqlToKinesisStack(Stack):\n\n  def __init__(self, scope: Construct, construct_id: str, vpc, **kwargs) -> None:\n    super().__init__(scope, construct_id, **kwargs)\n\n    database_name = self.node.try_get_context('source_database_name')\n    table_name = self.node.try_get_context('source_table_name')\n\n    kinesis_stream_name = self.node.try_get_context('target_kinesis_stream_name')\n\n    db_client_sg_name = self.node.try_get_context('mysql_client_security_group_name')\n    db_client_sg = aws_ec2.SecurityGroup.from_lookup_by_name(self, 'MySQLClientSG', db_client_sg_name, vpc)\n\n    dms_replication_subnet_group = aws_dms.CfnReplicationSubnetGroup(self, 'DMSReplicationSubnetGroup',\n      replication_subnet_group_description='DMS Replication Subnet Group',\n      subnet_ids=vpc.select_subnets(subnet_type=aws_ec2.SubnetType.PRIVATE_WITH_EGRESS).subnet_ids\n    )\n\n    dms_replication_instance = aws_dms.CfnReplicationInstance(self, 'DMSReplicationInstance',\n      replication_instance_class='dms.t3.medium',\n      allocated_storage=50,\n      allow_major_version_upgrade=False,\n      auto_minor_version_upgrade=False,\n      engine_version='3.4.6',\n      multi_az=False,\n      preferred_maintenance_window='sat:03:17-sat:03:47',\n      publicly_accessible=False,\n      replication_subnet_group_identifier=dms_replication_subnet_group.ref,\n      vpc_security_group_ids=[db_client_sg.security_group_id]\n    )\n\n    #XXX: If you use `aws_cdk.SecretValue.unsafe_unwrap()` to get any secret value,\n    # you may probably encounter ValueError; for example, invalid literal for int() with base 10: '${Token[TOKEN.228]}'\n    # So you should need to make the API call in order to access a secret inside it.\n    secret_name = self.node.try_get_context('source_database_secret_name')\n    secret = aws_secretsmanager.Secret.from_secret_name_v2(self, 'MySQLAdminUserSecret',\n      secret_name)\n\n    source_endpoint_id = secret.secret_value_from_json(\"dbClusterIdentifier\").unsafe_unwrap()\n    dms_source_endpoint = aws_dms.CfnEndpoint(self, 'DMSSourceEndpoint',\n      endpoint_identifier=source_endpoint_id,\n      endpoint_type='source',\n      engine_name=secret.secret_value_from_json(\"engine\").unsafe_unwrap(),\n      server_name=secret.secret_value_from_json(\"host\").unsafe_unwrap(),\n      port=3306,\n      database_name=database_name,\n      username=secret.secret_value_from_json(\"username\").unsafe_unwrap(),\n      password=secret.secret_value_from_json(\"password\").unsafe_unwrap()\n    )\n\n    dms_kinesis_access_role_policy_doc = aws_iam.PolicyDocument()\n    dms_kinesis_access_role_policy_doc.add_statements(aws_iam.PolicyStatement(**{\n      \"effect\": aws_iam.Effect.ALLOW,\n      \"resources\": [\"*\"],\n      \"actions\": [\n        \"kinesis:DescribeStream\",\n        \"kinesis:PutRecord\",\n        \"kinesis:PutRecords\"]\n    }))\n\n    dms_target_kinesis_access_role = aws_iam.Role(self, 'DMSTargetKinesisAccessRole',\n      role_name='DMSTargetKinesisAccessRole',\n      assumed_by=aws_iam.ServicePrincipal('dms.amazonaws.com'),\n      inline_policies={\n        'KinesisAccessRole': dms_kinesis_access_role_policy_doc\n      }\n    )\n\n    target_endpoint_id = f\"{source_endpoint_id}-cdc-to-kinesis\"\n    dms_target_endpoint = aws_dms.CfnEndpoint(self, 'DMSTargetEndpoint',\n      endpoint_identifier=target_endpoint_id,\n      endpoint_type='target',\n      engine_name='kinesis',\n      kinesis_settings=aws_dms.CfnEndpoint.KinesisSettingsProperty(\n        message_format=\"json-unformatted\",\n        service_access_role_arn=dms_target_kinesis_access_role.role_arn,\n        # arn:{partition}:{service}:{region}:{account}:{resource}{sep}{resource-name}\n        stream_arn=self.format_arn(service='kinesis',\n          region=cdk.Aws.REGION, account=cdk.Aws.ACCOUNT_ID,\n          resource='stream', arn_format=cdk.ArnFormat.SLASH_RESOURCE_NAME,\n          resource_name=kinesis_stream_name)\n      )\n    )\n\n    table_mappings_json = {\n      \"rules\": [\n        {\n          \"rule-type\": \"selection\",\n          \"rule-id\": \"1\",\n          \"rule-name\": \"1\",\n          \"object-locator\": {\n            \"schema-name\": database_name,\n            \"table-name\": table_name\n          },\n          \"rule-action\": \"include\",\n          \"filters\": []\n        },\n        {\n          \"rule-type\": \"object-mapping\",\n          \"rule-id\": \"2\",\n          \"rule-name\": \"DefaultMapToKinesis\",\n          \"rule-action\": \"map-record-to-record\",\n          \"object-locator\": {\n            \"schema-name\": database_name,\n            \"table-name\": table_name\n          }\n        }\n      ]\n    }\n\n    #XXX: AWS DMS - Using Amazon Kinesis Data Streams as a target for AWS Database Migration Service\n    # https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.Kinesis.html\n    task_settings_json = {\n      # Multithreaded full load task settings\n      \"FullLoadSettings\": {\n        \"MaxFullLoadSubTasks\": 8,\n      },\n      \"TargetMetadata\": {\n        # Multithreaded full load task settings\n        \"ParallelLoadQueuesPerThread\": 0,\n        \"ParallelLoadThreads\": 0,\n        \"ParallelLoadBufferSize\": 0,\n\n        # Multithreaded CDC load task settings\n        \"ParallelApplyBufferSize\": 1000,\n        \"ParallelApplyQueuesPerThread\": 16,\n        \"ParallelApplyThreads\": 8,\n      }\n    }\n\n    dms_replication_task = aws_dms.CfnReplicationTask(self, 'DMSReplicationTask',\n      replication_task_identifier='CDC-MySQLToKinesisTask',\n      replication_instance_arn=dms_replication_instance.ref,\n      migration_type='cdc', # [ full-load | cdc | full-load-and-cdc ]\n      source_endpoint_arn=dms_source_endpoint.ref,\n      target_endpoint_arn=dms_target_endpoint.ref,\n      table_mappings=json.dumps(table_mappings_json),\n      replication_task_settings=json.dumps(task_settings_json)\n    )\n\n\n    cdk.CfnOutput(self, 'DMSReplicationTaskArn',\n      value=dms_replication_task.ref,\n      export_name=f'{self.stack_name}-DMSReplicationTaskArn')\n    cdk.CfnOutput(self, 'DMSReplicationTaskId',\n      value=dms_replication_task.replication_task_identifier,\n      export_name=f'{self.stack_name}-DMSReplicationTaskId')\n    cdk.CfnOutput(self, 'DMSSourceEndpointId',\n      value=dms_source_endpoint.endpoint_identifier,\n      export_name=f'{self.stack_name}-DMSSourceEndpointId')\n    cdk.CfnOutput(self, 'DMSTargetEndpointId',\n      value=dms_target_endpoint.endpoint_identifier,\n      export_name=f'{self.stack_name}-DMSTargetEndpointId')\n```", "metadata": {"source": "github_repository", "repository": "ksmin23/my-aws-cdk-examples", "file_path": "/tmp/tmp888ifwn5/ksmin23_my-aws-cdk-examples/dms/aurora_mysql-to-kinesis/cdk_stacks/dms_aurora_mysql_to_kds.py", "language": "python", "category": "database", "authentic": true}}
